{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SteadyText","text":"<p>Deterministic text generation and embeddings with zero configuration</p> <p> </p> <p>Same input \u2192 same output. Every time.</p> <p>No more flaky tests, unpredictable CLI tools, or inconsistent docs. SteadyText makes AI outputs as reliable as hash functions.</p> <p>Ever had an AI test fail randomly? Or a CLI tool give different answers each run? SteadyText makes AI outputs reproducible - perfect for testing, tooling, and anywhere you need consistent results.</p> <p>Powered by Julep</p> <p>\u2728 Powered by open-source AI workflows from Julep. \u2728</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code>pip install steadytext\n</code></pre> Python APICommand Line <pre><code>import steadytext\n\n# Deterministic text generation\ncode = steadytext.generate(\"implement binary search in Python\")\nassert \"def binary_search\" in code  # Always passes!\n\n# Streaming (also deterministic)\nfor token in steadytext.generate_iter(\"explain quantum computing\"):\n    print(token, end=\"\", flush=True)\n\n# Deterministic embeddings\nvec = steadytext.embed(\"Hello world\")  # 1024-dim numpy array\n</code></pre> <pre><code># Generate text\nst generate \"hello world\"\n\n# Stream output  \nst generate \"explain recursion\" --stream\n\n# Get embeddings\nst embed \"machine learning\"\n\n# Preload models\nst models --preload\n</code></pre>"},{"location":"#how-it-works","title":"\ud83d\udd27 How It Works","text":"<p>SteadyText achieves determinism via:</p> <ul> <li>Fixed seeds: Constant randomness seed (<code>42</code>)</li> <li>Greedy decoding: Always chooses highest-probability token</li> <li>Frecency cache: LRU cache with frequency counting\u2014popular prompts stay cached longer</li> <li>Quantized models: 8-bit quantization ensures identical results across platforms</li> </ul> <p>This means <code>generate(\"hello\")</code> returns the exact same 512 tokens on any machine, every single time.</p>"},{"location":"#installation-models","title":"\ud83d\udce6 Installation &amp; Models","text":"<p>Install stable release:</p> <pre><code>pip install steadytext\n</code></pre>"},{"location":"#models","title":"Models","text":"<p>Corresponding to pypi versions <code>0.x.y</code>:</p> <ul> <li>Generation: <code>BitCPM4-1B-Q8_0</code> (1.3GB)</li> <li>Embeddings: <code>Qwen3-0.6B-Q8_0</code> (610MB)</li> </ul> <p>Version Stability</p> <p>Each major version will use a fixed set of models only, so that only forced upgrades from pip will change the models (and the deterministic output)</p>"},{"location":"#use-cases","title":"\ud83c\udfaf Use Cases","text":"<p>Perfect for</p> <ul> <li>Testing AI features: Reliable asserts that never flake</li> <li>Deterministic CLI tooling: Consistent outputs for automation  </li> <li>Reproducible documentation: Examples that always work</li> <li>Offline/dev/staging environments: No API keys needed</li> <li>Semantic caching and embedding search: Fast similarity matching</li> </ul> <p>Not ideal for</p> <ul> <li>Creative or conversational tasks</li> <li>Latest knowledge queries  </li> <li>Large-scale chatbot deployments</li> </ul>"},{"location":"#examples","title":"\ud83d\udccb Examples","text":"<p>Use SteadyText in tests or CLI tools for consistent, reproducible results:</p> <pre><code># Testing with reliable assertions\ndef test_ai_function():\n    result = my_ai_function(\"test input\")\n    expected = steadytext.generate(\"expected output for 'test input'\")\n    assert result == expected  # No flakes!\n\n# CLI tools with consistent outputs\nimport click\n\n@click.command()\ndef ai_tool(prompt):\n    print(steadytext.generate(prompt))\n</code></pre> <p>\ud83d\udcc2 More examples \u2192</p>"},{"location":"#api-overview","title":"\ud83d\udd0d API Overview","text":"<pre><code># Text generation\nsteadytext.generate(prompt: str) -&gt; str\nsteadytext.generate(prompt, return_logprobs=True)\n\n# Streaming generation\nsteadytext.generate_iter(prompt: str)\n\n# Embeddings\nsteadytext.embed(text: str | List[str]) -&gt; np.ndarray\n\n# Model preloading\nsteadytext.preload_models(verbose=True)\n</code></pre> <p>\ud83d\udcda Full API Documentation \u2192</p>"},{"location":"#configuration","title":"\ud83d\udd27 Configuration","text":"<p>Control caching behavior via environment variables:</p> <pre><code># Generation cache (default: 256 entries, 50MB)\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50\n\n# Embedding cache (default: 512 entries, 100MB)\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100\n</code></pre>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! See Contributing Guide for guidelines.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<ul> <li>Code: MIT</li> <li>Models: MIT (BitCPM4, Qwen3)</li> </ul> <p>Built with \u2764\ufe0f for developers tired of flaky AI tests.</p>"},{"location":"api/","title":"SteadyText API Documentation","text":"<p>This document provides detailed API documentation for SteadyText.</p>"},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#text-generation","title":"Text Generation","text":""},{"location":"api/#steadytextgenerate","title":"<code>steadytext.generate()</code>","text":"<pre><code>def generate(\n    prompt: str,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\"\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre> <p>Generate deterministic text from a prompt.</p> <p>Parameters: - <code>prompt</code> (str): The input text to generate from - <code>return_logprobs</code> (bool): If True, returns log probabilities along with the text - <code>eos_string</code> (str): Custom end-of-sequence string to stop generation. Use \"[EOS]\" for model's default stop tokens</p> <p>Returns: - If <code>return_logprobs=False</code>: A string containing the generated text - If <code>return_logprobs=True</code>: A tuple of (text, logprobs_dict)</p> <p>Example: <pre><code># Simple generation\ntext = steadytext.generate(\"Write a Python function\")\n\n# With log probabilities\ntext, logprobs = steadytext.generate(\"Explain AI\", return_logprobs=True)\n\n# With custom stop string\ntext = steadytext.generate(\"List items until END\", eos_string=\"END\")\n</code></pre></p>"},{"location":"api/#steadytextgenerate_iter","title":"<code>steadytext.generate_iter()</code>","text":"<pre><code>def generate_iter(\n    prompt: str,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre> <p>Generate text iteratively, yielding tokens as they are produced.</p> <p>Parameters: - <code>prompt</code> (str): The input text to generate from - <code>eos_string</code> (str): Custom end-of-sequence string to stop generation. Use \"[EOS]\" for model's default stop tokens - <code>include_logprobs</code> (bool): If True, yields tuples of (token, logprobs) instead of just tokens</p> <p>Yields: - str: Text tokens/words as they are generated (if <code>include_logprobs=False</code>) - Tuple[str, Optional[Dict[str, Any]]]: (token, logprobs) tuples (if <code>include_logprobs=True</code>)</p> <p>Example: <pre><code># Simple streaming\nfor token in steadytext.generate_iter(\"Tell me a story\"):\n    print(token, end=\"\", flush=True)\n\n# With custom stop string\nfor token in steadytext.generate_iter(\"Generate until STOP\", eos_string=\"STOP\"):\n    print(token, end=\"\", flush=True)\n\n# With log probabilities\nfor token, logprobs in steadytext.generate_iter(\"Explain AI\", include_logprobs=True):\n    print(token, end=\"\", flush=True)\n</code></pre></p>"},{"location":"api/#embeddings","title":"Embeddings","text":""},{"location":"api/#steadytextembed","title":"<code>steadytext.embed()</code>","text":"<pre><code>def embed(text_input: Union[str, List[str]]) -&gt; np.ndarray\n</code></pre> <p>Create deterministic embeddings for text input.</p> <p>Parameters: - <code>text_input</code> (Union[str, List[str]]): A string or list of strings to embed</p> <p>Returns: - np.ndarray: A 1024-dimensional L2-normalized float32 numpy array</p> <p>Example: <pre><code># Single string\nvec = steadytext.embed(\"Hello world\")\n\n# Multiple strings (averaged)\nvec = steadytext.embed([\"Hello\", \"world\"])\n</code></pre></p>"},{"location":"api/#utility-functions","title":"Utility Functions","text":""},{"location":"api/#steadytextpreload_models","title":"<code>steadytext.preload_models()</code>","text":"<pre><code>def preload_models(verbose: bool = False) -&gt; None\n</code></pre> <p>Preload models before first use to avoid delays.</p> <p>Parameters: - <code>verbose</code> (bool): If True, prints progress information</p> <p>Example: <pre><code># Silent preloading\nsteadytext.preload_models()\n\n# Verbose preloading\nsteadytext.preload_models(verbose=True)\n</code></pre></p>"},{"location":"api/#steadytextget_model_cache_dir","title":"<code>steadytext.get_model_cache_dir()</code>","text":"<pre><code>def get_model_cache_dir() -&gt; str\n</code></pre> <p>Get the path to the model cache directory.</p> <p>Returns: - str: The absolute path to the model cache directory</p> <p>Example: <pre><code>cache_dir = steadytext.get_model_cache_dir()\nprint(f\"Models are stored in: {cache_dir}\")\n</code></pre></p>"},{"location":"api/#constants","title":"Constants","text":""},{"location":"api/#steadytextdefault_seed","title":"<code>steadytext.DEFAULT_SEED</code>","text":"<ul> <li>Type: int</li> <li>Value: 42</li> <li>Description: The fixed random seed used for deterministic generation</li> </ul>"},{"location":"api/#steadytextgeneration_max_new_tokens","title":"<code>steadytext.GENERATION_MAX_NEW_TOKENS</code>","text":"<ul> <li>Type: int</li> <li>Value: 512</li> <li>Description: Maximum number of tokens to generate</li> </ul>"},{"location":"api/#steadytextembedding_dimension","title":"<code>steadytext.EMBEDDING_DIMENSION</code>","text":"<ul> <li>Type: int</li> <li>Value: 1024</li> <li>Description: The dimensionality of embedding vectors</li> </ul>"},{"location":"api/#environment-variables","title":"Environment Variables","text":""},{"location":"api/#generation-cache","title":"Generation Cache","text":"<ul> <li><code>STEADYTEXT_GENERATION_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 256)</li> <li><code>STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 50.0)</li> </ul>"},{"location":"api/#embedding-cache","title":"Embedding Cache","text":"<ul> <li><code>STEADYTEXT_EMBEDDING_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 512)</li> <li><code>STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 100.0)</li> </ul>"},{"location":"api/#model-downloads","title":"Model Downloads","text":"<ul> <li><code>STEADYTEXT_ALLOW_MODEL_DOWNLOADS</code>: Set to \"true\" to allow automatic model downloads (mainly used for testing)</li> </ul>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>All functions are designed to never raise exceptions during normal operation. If models cannot be loaded, deterministic fallback functions are used:</p> <ul> <li>Text generation fallback: Uses hash-based word selection to generate pseudo-random but deterministic text</li> <li>Embedding fallback: Returns zero vectors of the correct dimension</li> </ul> <p>This ensures that your code never breaks, even in environments where models cannot be downloaded or loaded.</p>"},{"location":"contributing/","title":"Contributing to SteadyText","text":"<p>We welcome contributions to SteadyText! This document provides guidelines for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork: <code>git clone https://github.com/your-username/steadytext.git</code></li> <li>Create a feature branch: <code>git checkout -b feature/your-feature-name</code></li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (supports up to Python 3.13)</li> <li>Git</li> <li>Recommended: uv for faster dependency management</li> </ul>"},{"location":"contributing/#installation","title":"Installation","text":"With uv (Recommended)With pip <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext\n\n# Install in development mode\nuv sync --dev\n\n# Activate the virtual environment\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n</code></pre> <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -e .[dev]\n</code></pre>"},{"location":"contributing/#development-commands","title":"Development Commands","text":"<p>SteadyText uses poethepoet for task management:</p> <pre><code># Run tests\npoe test\n\n# Run tests with coverage\npoe test-cov\n\n# Run tests with model downloads (slower)\npoe test-models\n\n# Run linting\npoe lint\n\n# Format code\npoe format\n\n# Type checking\npoe check\n\n# Run pre-commit hooks\npoe pre-commit\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8: Use <code>poe format</code> to auto-format code</li> <li>Use type hints: Add type annotations for function parameters and returns</li> <li>Add docstrings: Document all public functions and classes</li> <li>Keep functions focused: Single responsibility principle</li> </ul> <p>Example:</p> <pre><code>def embed(text_input: Union[str, List[str]]) -&gt; np.ndarray:\n    \"\"\"Create deterministic embeddings for text input.\n\n    Args:\n        text_input: String or list of strings to embed\n\n    Returns:\n        1024-dimensional L2-normalized float32 numpy array\n    \"\"\"\n    # Implementation here\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>SteadyText has comprehensive tests covering:</p> <ul> <li>Deterministic behavior: Same input \u2192 same output</li> <li>Fallback functionality: Works without models</li> <li>Edge cases: Empty inputs, invalid types</li> <li>Performance: Caching behavior</li> </ul>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>def test_your_feature():\n    \"\"\"Test your new feature.\"\"\"\n    # Test deterministic behavior\n    result1 = your_function(\"test input\")\n    result2 = your_function(\"test input\")\n    assert result1 == result2  # Should be identical\n\n    # Test edge cases\n    result3 = your_function(\"\")\n    assert isinstance(result3, expected_type)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npoe test\n\n# Run specific test file\npytest tests/test_your_feature.py\n\n# Run with coverage\npoe test-cov\n\n# Run tests that require model downloads\npoe test-models\n\n# Run tests in parallel\npytest -n auto\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update API docs: Modify files in <code>docs/api/</code> if adding new functions</li> <li>Add examples: Include usage examples in <code>docs/examples/</code></li> <li>Update README: For major features, update the main README.md</li> </ul>"},{"location":"contributing/#architecture-guidelines","title":"Architecture Guidelines","text":"<p>SteadyText follows a layered architecture:</p> <pre><code>steadytext/\n\u251c\u2500\u2500 core/          # Core generation and embedding logic\n\u251c\u2500\u2500 models/        # Model loading and caching\n\u251c\u2500\u2500 cli/           # Command-line interface\n\u2514\u2500\u2500 utils.py       # Shared utilities\n</code></pre>"},{"location":"contributing/#core-principles","title":"Core Principles","text":"<ol> <li>Never fail: Functions should always return valid outputs</li> <li>Deterministic: Same input always produces same output</li> <li>Thread-safe: Support concurrent usage</li> <li>Cached: Use frecency caching for performance</li> </ol>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":"<ol> <li>Core functionality: Add to <code>steadytext/core/</code></li> <li>Model support: Modify <code>steadytext/models/</code></li> <li>CLI commands: Add to <code>steadytext/cli/commands/</code></li> <li>Utilities: Add to <code>steadytext/utils.py</code></li> </ol>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Run all tests: <code>poe test</code></li> <li>Check linting: <code>poe lint</code></li> <li>Format code: <code>poe format</code></li> <li>Type check: <code>poe check</code></li> <li>Update documentation: Add/update relevant docs</li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create descriptive title: \"Add feature X\" or \"Fix bug Y\"</li> <li>Write clear description: Explain what changes and why</li> <li>Reference issues: Link to related GitHub issues</li> <li>Add tests: Include tests for new functionality</li> <li>Update changelog: Add entry to CHANGELOG.md</li> </ol>"},{"location":"contributing/#pull-request-template","title":"Pull Request Template","text":"<pre><code>## Description\nBrief description of the changes\n\n## Changes Made\n- [ ] Added feature X\n- [ ] Fixed bug Y\n- [ ] Updated documentation\n\n## Testing\n- [ ] All tests pass\n- [ ] Added tests for new functionality\n- [ ] Manually tested edge cases\n\n## Checklist\n- [ ] Code follows project style guidelines\n- [ ] Self-review completed\n- [ ] Documentation updated\n- [ ] Changelog updated\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#typical-development-cycle","title":"Typical Development Cycle","text":"<ol> <li>Pick/create an issue: Find something to work on</li> <li>Create feature branch: <code>git checkout -b feature/issue-123</code></li> <li>Make changes: Implement your feature</li> <li>Test thoroughly: Run tests and manual testing</li> <li>Commit changes: Use descriptive commit messages</li> <li>Push and PR: Create pull request</li> </ol>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits:</p> <pre><code>feat: add new embedding model support\nfix: resolve caching issue with concurrent access\ndocs: update API documentation for generate()\ntest: add tests for edge cases\nchore: update dependencies\n</code></pre>"},{"location":"contributing/#branch-naming","title":"Branch Naming","text":"<ul> <li><code>feature/description</code> - New features</li> <li><code>fix/description</code> - Bug fixes  </li> <li><code>docs/description</code> - Documentation updates</li> <li><code>refactor/description</code> - Code refactoring</li> </ul>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>SteadyText follows semantic versioning:</p> <ul> <li>Major (1.0.0): Breaking changes, new model versions</li> <li>Minor (0.1.0): New features, backward compatible</li> <li>Patch (0.0.1): Bug fixes, small improvements</li> </ul>"},{"location":"contributing/#model-versioning","title":"Model Versioning","text":"<ul> <li>Models are fixed per major version</li> <li>Only major version updates change model outputs</li> <li>This ensures deterministic behavior across patch/minor updates</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>GitHub Discussions: For questions and general discussion</li> <li>Discord: Join our community chat (link in README)</li> </ul>"},{"location":"contributing/#common-issues","title":"Common Issues","text":"<p>Tests failing locally: <pre><code># Clear caches\nrm -rf ~/.cache/steadytext/\n\n# Reinstall dependencies  \npip install -e .[dev]\n\n# Run tests\npoe test\n</code></pre></p> <p>Import errors: <pre><code># Make sure you're in the right directory\ncd steadytext/\n\n# Install in development mode\npip install -e .\n</code></pre></p> <p>Model download issues: <pre><code># Set environment variable\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Run tests\npoe test-models\n</code></pre></p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive in all interactions. We want SteadyText to be a welcoming project for everyone.</p>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - README.md: Major contributors listed - CHANGELOG.md: Contributions noted in releases - GitHub: Contributor graphs and statistics</p> <p>Thank you for contributing to SteadyText! \ud83d\ude80</p>"},{"location":"eos-string-implementation/","title":"EOS String Implementation Summary","text":"<p>This document summarizes the implementation of the custom <code>eos_string</code> parameter feature.</p>"},{"location":"eos-string-implementation/#changes-made","title":"Changes Made","text":""},{"location":"eos-string-implementation/#1-core-generation-module-steadytextcoregeneratorpy","title":"1. Core Generation Module (<code>steadytext/core/generator.py</code>)","text":"<ul> <li>Added <code>eos_string</code> parameter to <code>DeterministicGenerator.generate()</code> method</li> <li>Default value: <code>\"[EOS]\"</code> (special marker for model's default EOS token)</li> <li> <p>When custom value provided, it's added to the stop sequences</p> </li> <li> <p>Added <code>eos_string</code> parameter to <code>DeterministicGenerator.generate_iter()</code> method</p> </li> <li>Supports streaming generation with custom stop strings</li> <li> <p>Added <code>include_logprobs</code> parameter for compatibility with CLI</p> </li> <li> <p>Updated caching logic to include <code>eos_string</code> in cache key when not default</p> </li> <li>Ensures different eos_strings produce separately cached results</li> </ul>"},{"location":"eos-string-implementation/#2-public-api-steadytext__init__py","title":"2. Public API (<code>steadytext/__init__.py</code>)","text":"<ul> <li> <p>Updated <code>generate()</code> function signature:   <pre><code>def generate(prompt: str, return_logprobs: bool = False, eos_string: str = \"[EOS]\")\n</code></pre></p> </li> <li> <p>Updated <code>generate_iter()</code> function signature:   <pre><code>def generate_iter(prompt: str, eos_string: str = \"[EOS]\", include_logprobs: bool = False)\n</code></pre></p> </li> </ul>"},{"location":"eos-string-implementation/#3-cli-updates","title":"3. CLI Updates","text":""},{"location":"eos-string-implementation/#generate-command-steadytextclicommandsgeneratepy","title":"Generate Command (<code>steadytext/cli/commands/generate.py</code>)","text":"<ul> <li>Added <code>--eos-string</code> parameter (default: \"[EOS]\")</li> <li>Passes eos_string to both batch and streaming generation</li> </ul>"},{"location":"eos-string-implementation/#main-cli-steadytextclimainpy","title":"Main CLI (<code>steadytext/cli/main.py</code>)","text":"<ul> <li>Added <code>--quiet</code> / <code>-q</code> flag to silence log output</li> <li>Sets logging level to ERROR for both steadytext and llama_cpp loggers when quiet mode is enabled</li> </ul>"},{"location":"eos-string-implementation/#4-tests-teststest_steadytextpy","title":"4. Tests (<code>tests/test_steadytext.py</code>)","text":"<p>Added three new test methods: - <code>test_generate_with_custom_eos_string()</code> - Tests basic eos_string functionality - <code>test_generate_iter_with_eos_string()</code> - Tests streaming with custom eos_string - <code>test_generate_eos_string_with_logprobs()</code> - Tests combination of eos_string and logprobs</p>"},{"location":"eos-string-implementation/#5-test-scripts","title":"5. Test Scripts","text":"<p>Created two test scripts for manual verification: - <code>test_eos_string.py</code> - Python script testing various eos_string scenarios - <code>test_cli_eos.sh</code> - Bash script testing CLI functionality</p>"},{"location":"eos-string-implementation/#usage-examples","title":"Usage Examples","text":""},{"location":"eos-string-implementation/#python-api","title":"Python API","text":"<pre><code>import steadytext\n\n# Use model's default EOS token\ntext = steadytext.generate(\"Hello world\", eos_string=\"[EOS]\")\n\n# Stop at custom string\ntext = steadytext.generate(\"List items until END\", eos_string=\"END\")\n\n# Streaming with custom eos\nfor token in steadytext.generate_iter(\"Generate text\", eos_string=\"STOP\"):\n    print(token, end=\"\")\n</code></pre>"},{"location":"eos-string-implementation/#cli","title":"CLI","text":"<pre><code># Default behavior\nsteadytext \"Generate some text\"\n\n# Custom eos string\nsteadytext \"Generate until DONE\" --eos-string \"DONE\"\n\n# Quiet mode (no logs)\nsteadytext --quiet \"Generate without logs\"\n\n# Streaming with custom eos\nsteadytext \"Stream until END\" --stream --eos-string \"END\"\n</code></pre>"},{"location":"eos-string-implementation/#implementation-notes","title":"Implementation Notes","text":"<ol> <li> <p>The <code>\"[EOS]\"</code> string is a special marker that tells the system to use the model's default EOS token and stop sequences.</p> </li> <li> <p>When a custom eos_string is provided, it's added to the existing stop sequences rather than replacing them.</p> </li> <li> <p>Cache keys include the eos_string when it's not the default, ensuring proper caching behavior.</p> </li> <li> <p>The quiet flag affects all loggers in the steadytext namespace and llama_cpp if present.</p> </li> </ol>"},{"location":"quick-start/","title":"Quick Start Guide","text":"<p>Get started with SteadyText in minutes.</p>"},{"location":"quick-start/#installation","title":"Installation","text":"pipuvPoetry <pre><code>pip install steadytext\n</code></pre> <pre><code>uv add steadytext\n</code></pre> <pre><code>poetry add steadytext\n</code></pre>"},{"location":"quick-start/#first-steps","title":"First Steps","text":""},{"location":"quick-start/#1-basic-text-generation","title":"1. Basic Text Generation","text":"<pre><code>import steadytext\n\n# Generate deterministic text\ntext = steadytext.generate(\"Write a Python function to calculate fibonacci\")\nprint(text)\n</code></pre>"},{"location":"quick-start/#2-streaming-generation","title":"2. Streaming Generation","text":"<p>For real-time output:</p> <pre><code>for token in steadytext.generate_iter(\"Explain machine learning\"):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"quick-start/#3-create-embeddings","title":"3. Create Embeddings","text":"<pre><code># Single text\nvector = steadytext.embed(\"Hello world\")\nprint(f\"Embedding shape: {vector.shape}\")  # (1024,)\n\n# Multiple texts (averaged)\nvector = steadytext.embed([\"Hello\", \"world\", \"AI\"])\n</code></pre>"},{"location":"quick-start/#command-line-usage","title":"Command Line Usage","text":"<p>SteadyText includes both <code>steadytext</code> and <code>st</code> commands:</p> <pre><code># Generate text\nst generate \"write a haiku about programming\"\n\n# Stream generation\nst generate \"explain quantum computing\" --stream\n\n# Create embeddings  \nst embed \"machine learning concepts\"\n\n# JSON output\nst generate \"list 3 colors\" --json\n\n# Preload models (optional)\nst models --preload\n</code></pre>"},{"location":"quick-start/#model-management","title":"Model Management","text":"<p>Models are automatically downloaded on first use to:</p> <ul> <li>Linux/Mac: <code>~/.cache/steadytext/models/</code></li> <li>Windows: <code>%LOCALAPPDATA%\\steadytext\\steadytext\\models\\</code></li> </ul> <pre><code># Check where models are stored\ncache_dir = steadytext.get_model_cache_dir()\nprint(f\"Models stored at: {cache_dir}\")\n\n# Preload models manually (optional)\nsteadytext.preload_models(verbose=True)\n</code></pre>"},{"location":"quick-start/#configuration","title":"Configuration","text":"<p>Control caching via environment variables:</p> <pre><code># Generation cache settings\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=512\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=100\n\n# Embedding cache settings  \nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=1024\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=200\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete function documentation</li> <li>Examples - Real-world usage patterns</li> <li>CLI Reference - Command-line interface details</li> </ul>"},{"location":"quick-start/#need-help","title":"Need Help?","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete documentation for SteadyText's Python API.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>SteadyText provides a simple, consistent API with two main functions:</p> <ul> <li><code>generate()</code> - Deterministic text generation</li> <li><code>embed()</code> - Deterministic embeddings</li> </ul> <p>All functions are designed to never fail - they return deterministic fallbacks when models can't be loaded.</p>"},{"location":"api/#quick-reference","title":"Quick Reference","text":"<pre><code>import steadytext\n\n# Text generation\ntext = steadytext.generate(\"your prompt\")\ntext, logprobs = steadytext.generate(\"prompt\", return_logprobs=True)\n\n# Streaming generation  \nfor token in steadytext.generate_iter(\"prompt\"):\n    print(token, end=\"\")\n\n# Embeddings\nvector = steadytext.embed(\"text to embed\")\nvectors = steadytext.embed([\"multiple\", \"texts\"])\n\n# Utilities\nsteadytext.preload_models()\ncache_dir = steadytext.get_model_cache_dir()\n</code></pre>"},{"location":"api/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>Text Generation - <code>generate()</code> and <code>generate_iter()</code></li> <li>Embeddings - <code>embed()</code> function  </li> <li>CLI Reference - Command-line interface</li> </ul>"},{"location":"api/#constants","title":"Constants","text":""},{"location":"api/#core-constants","title":"Core Constants","text":"<pre><code>steadytext.DEFAULT_SEED = 42\nsteadytext.GENERATION_MAX_NEW_TOKENS = 512  \nsteadytext.EMBEDDING_DIMENSION = 1024\n</code></pre>"},{"location":"api/#environment-variables","title":"Environment Variables","text":""},{"location":"api/#cache-configuration","title":"Cache Configuration","text":"<pre><code># Generation cache\nSTEADYTEXT_GENERATION_CACHE_CAPACITY=256      # max entries\nSTEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50.0  # max file size\n\n# Embedding cache  \nSTEADYTEXT_EMBEDDING_CACHE_CAPACITY=512       # max entries\nSTEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100.0  # max file size\n</code></pre>"},{"location":"api/#developmenttesting","title":"Development/Testing","text":"<pre><code># Allow model downloads (for testing)\nSTEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>SteadyText uses a \"never fail\" design philosophy:</p> <p>Deterministic Fallbacks</p> <ul> <li>Text generation: Uses hash-based word selection when models unavailable</li> <li>Embeddings: Returns zero vectors of correct dimensions</li> <li>No exceptions raised: Functions always return valid outputs</li> </ul> <p>This ensures your code works consistently across all environments, whether models are available or not.</p>"},{"location":"api/#thread-safety","title":"Thread Safety","text":"<p>All functions are thread-safe:</p> <ul> <li>Model loading uses singleton pattern with locks</li> <li>Caches are thread-safe with proper locking</li> <li>Multiple concurrent calls are supported</li> </ul>"},{"location":"api/#performance-notes","title":"Performance Notes","text":"<ul> <li>First call: May download models (~2GB total)</li> <li>Subsequent calls: Cached results when possible</li> <li>Memory usage: Models loaded once, cached in memory</li> <li>Disk cache: Frecency cache stores popular results</li> </ul>"},{"location":"api/cli/","title":"CLI Reference","text":"<p>Complete command-line interface documentation for SteadyText.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>The CLI is automatically installed with SteadyText:</p> <pre><code>pip install steadytext\n</code></pre> <p>Two commands are available: - <code>steadytext</code> - Full command name - <code>st</code> - Short alias</p>"},{"location":"api/cli/#global-options","title":"Global Options","text":"<pre><code>st --version     # Show version\nst --help        # Show help\n</code></pre>"},{"location":"api/cli/#generate","title":"generate","text":"<p>Generate deterministic text from a prompt.</p>"},{"location":"api/cli/#usage","title":"Usage","text":"<pre><code>st generate [OPTIONS] PROMPT\nsteadytext generate [OPTIONS] PROMPT\n</code></pre>"},{"location":"api/cli/#options","title":"Options","text":"Option Short Type Default Description <code>--stream</code> <code>-s</code> flag <code>false</code> Stream output token by token <code>--json</code> <code>-j</code> flag <code>false</code> Output as JSON with metadata <code>--logprobs</code> <code>-l</code> flag <code>false</code> Include log probabilities <code>--eos-string</code> <code>-e</code> string <code>\"[EOS]\"</code> Custom end-of-sequence string"},{"location":"api/cli/#examples","title":"Examples","text":"Basic GenerationStreaming OutputJSON OutputWith Log ProbabilitiesCustom Stop String <pre><code>st generate \"Write a Python function to calculate fibonacci\"\n</code></pre> <pre><code>st generate \"Explain machine learning\" --stream\n</code></pre> <pre><code>st generate \"Hello world\" --json\n# Output:\n# {\n#   \"text\": \"Hello! How can I help you today?...\",\n#   \"tokens\": 15,\n#   \"cached\": false\n# }\n</code></pre> <pre><code>st generate \"Explain AI\" --logprobs --json\n# Includes token probabilities in JSON output\n</code></pre> <pre><code>st generate \"List colors until STOP\" --eos-string \"STOP\"\n</code></pre>"},{"location":"api/cli/#stdin-support","title":"Stdin Support","text":"<p>Generate from stdin when no prompt provided:</p> <pre><code>echo \"Write a haiku\" | st generate\ncat prompts.txt | st generate --stream\n</code></pre>"},{"location":"api/cli/#embed","title":"embed","text":"<p>Create deterministic embeddings for text.</p>"},{"location":"api/cli/#usage_1","title":"Usage","text":"<pre><code>st embed [OPTIONS] TEXT\nsteadytext embed [OPTIONS] TEXT\n</code></pre>"},{"location":"api/cli/#options_1","title":"Options","text":"Option Short Type Default Description <code>--format</code> <code>-f</code> choice <code>json</code> Output format: <code>json</code>, <code>numpy</code>, <code>hex</code> <code>--output</code> <code>-o</code> path <code>-</code> Output file (default: stdout)"},{"location":"api/cli/#examples_1","title":"Examples","text":"Basic EmbeddingNumpy FormatHex FormatSave to File <pre><code>st embed \"machine learning\"\n# Outputs JSON array with 1024 float values\n</code></pre> <pre><code>st embed \"text to embed\" --format numpy\n# Outputs binary numpy array\n</code></pre> <pre><code>st embed \"hello world\" --format hex\n# Outputs hex-encoded float32 array\n</code></pre> <pre><code>st embed \"important text\" --output embedding.json\nst embed \"data\" --format numpy --output embedding.npy\n</code></pre>"},{"location":"api/cli/#stdin-support_1","title":"Stdin Support","text":"<p>Embed text from stdin:</p> <pre><code>echo \"text to embed\" | st embed\ncat document.txt | st embed --format numpy --output doc_embedding.npy\n</code></pre>"},{"location":"api/cli/#models","title":"models","text":"<p>Manage SteadyText models.</p>"},{"location":"api/cli/#usage_2","title":"Usage","text":"<pre><code>st models [OPTIONS]\nsteadytext models [OPTIONS]\n</code></pre>"},{"location":"api/cli/#options_2","title":"Options","text":"Option Short Description <code>--list</code> <code>-l</code> List available models <code>--preload</code> <code>-p</code> Preload all models <code>--cache-dir</code> Show model cache directory <code>--cache-info</code> Show cache usage information"},{"location":"api/cli/#examples_2","title":"Examples","text":"List ModelsPreload ModelsCache Information <pre><code>st models --list\n# Output:\n# Generation Model: BitCPM4-1B-Q8_0 (1.3GB)\n# Embedding Model: Qwen3-0.6B-Q8_0 (610MB)\n</code></pre> <pre><code>st models --preload\n# Downloads and loads all models\n</code></pre> <pre><code>st models --cache-dir\n# /home/user/.cache/steadytext/models/\n\nst models --cache-info\n# Cache directory: /home/user/.cache/steadytext/models/\n# Generation model: 1.3GB (downloaded)\n# Embedding model: 610MB (not downloaded)\n# Total size: 1.3GB / 1.9GB\n</code></pre>"},{"location":"api/cli/#cache","title":"cache","text":"<p>Manage result caches.</p>"},{"location":"api/cli/#usage_3","title":"Usage","text":"<pre><code>st cache [OPTIONS]\nsteadytext cache [OPTIONS]\n</code></pre>"},{"location":"api/cli/#options_3","title":"Options","text":"Option Short Description <code>--clear</code> <code>-c</code> Clear all caches <code>--status</code> <code>-s</code> Show cache status <code>--generation-only</code> Target only generation cache <code>--embedding-only</code> Target only embedding cache"},{"location":"api/cli/#examples_3","title":"Examples","text":"Cache StatusClear Caches <pre><code>st cache --status\n# Generation Cache: 45 entries, 12.3MB\n# Embedding Cache: 128 entries, 34.7MB\n</code></pre> <pre><code>st cache --clear\n# Cleared all caches\n\nst cache --clear --generation-only\n# Cleared generation cache only\n</code></pre>"},{"location":"api/cli/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"<p>Set these before running CLI commands:</p> <pre><code># Cache configuration\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=512\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=100\n\n# Allow model downloads (for development)\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Then run commands\nst generate \"test prompt\"\n</code></pre>"},{"location":"api/cli/#pipeline-usage","title":"Pipeline Usage","text":"<p>Chain commands with other tools:</p> <pre><code># Batch processing\ncat prompts.txt | while read prompt; do\n  echo \"Prompt: $prompt\"\n  st generate \"$prompt\" --json | jq '.text'\n  echo \"---\"\ndone\n\n# Generate and embed\ntext=$(st generate \"explain AI\")\necho \"$text\" | st embed --format hex &gt; ai_explanation.hex\n</code></pre>"},{"location":"api/cli/#scripting-examples","title":"Scripting Examples","text":"Bash ScriptPython Integration <pre><code>#!/bin/bash\n# generate_docs.sh\n\nprompts=(\n  \"Explain machine learning\"\n  \"What is deep learning?\"\n  \"Define neural networks\"\n)\n\nfor prompt in \"${prompts[@]}\"; do\n  echo \"=== $prompt ===\"\n  st generate \"$prompt\" --stream\n  echo -e \"\\n---\\n\"\ndone\n</code></pre> <pre><code>import subprocess\nimport json\n\ndef cli_generate(prompt):\n    \"\"\"Use CLI from Python.\"\"\"\n    result = subprocess.run([\n        'st', 'generate', prompt, '--json'\n    ], capture_output=True, text=True)\n\n    return json.loads(result.stdout)\n\n# Usage\nresult = cli_generate(\"Hello world\")\nprint(result['text'])\n</code></pre>"},{"location":"api/cli/#performance-tips","title":"Performance Tips","text":"<p>CLI Optimization</p> <ul> <li>Preload models: Run <code>st models --preload</code> once at startup</li> <li>Use JSON output: Easier to parse in scripts with <code>--json</code></li> <li>Batch operations: Process multiple items in single session</li> <li>Cache warmup: Generate common prompts to populate cache</li> </ul>"},{"location":"api/embedding/","title":"Embeddings API","text":"<p>Functions for creating deterministic text embeddings.</p>"},{"location":"api/embedding/#embed","title":"embed()","text":"<p>Create deterministic embeddings for text input.</p> <pre><code>def embed(text_input: Union[str, List[str]]) -&gt; np.ndarray\n</code></pre>"},{"location":"api/embedding/#parameters","title":"Parameters","text":"Parameter Type Description <code>text_input</code> <code>Union[str, List[str]]</code> Text string or list of strings to embed"},{"location":"api/embedding/#returns","title":"Returns","text":"<p>Returns: <code>np.ndarray</code> - 1024-dimensional L2-normalized float32 array</p>"},{"location":"api/embedding/#examples","title":"Examples","text":"Single TextMultiple TextsSimilarity Comparison <pre><code>import steadytext\nimport numpy as np\n\n# Embed single text\nvector = steadytext.embed(\"Hello world\")\n\nprint(f\"Shape: {vector.shape}\")        # (1024,)\nprint(f\"Type: {vector.dtype}\")         # float32\nprint(f\"Norm: {np.linalg.norm(vector):.6f}\")  # 1.000000 (L2 normalized)\n</code></pre> <pre><code># Embed multiple texts (averaged)\ntexts = [\"machine learning\", \"artificial intelligence\", \"deep learning\"]\nvector = steadytext.embed(texts)\n\nprint(f\"Combined embedding shape: {vector.shape}\")  # (1024,)\n# Result is averaged across all input texts\n</code></pre> <pre><code>import numpy as np\n\n# Create embeddings for comparison\nvec1 = steadytext.embed(\"machine learning\")\nvec2 = steadytext.embed(\"artificial intelligence\") \nvec3 = steadytext.embed(\"cooking recipes\")\n\n# Calculate cosine similarity\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\nsim_ml_ai = cosine_similarity(vec1, vec2)\nsim_ml_cooking = cosine_similarity(vec1, vec3)\n\nprint(f\"ML vs AI similarity: {sim_ml_ai:.3f}\")\nprint(f\"ML vs Cooking similarity: {sim_ml_cooking:.3f}\")\n# ML and AI should have higher similarity than ML and cooking\n</code></pre>"},{"location":"api/embedding/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/embedding/#deterministic-behavior","title":"Deterministic Behavior","text":"<p>Embeddings are completely deterministic:</p> <pre><code># These will always be identical\nvec1 = steadytext.embed(\"test text\")\nvec2 = steadytext.embed(\"test text\")\n\nassert np.array_equal(vec1, vec2)  # Always passes!\nassert np.allclose(vec1, vec2)     # Always passes!\n</code></pre>"},{"location":"api/embedding/#preprocessing","title":"Preprocessing","text":"<p>Text is automatically preprocessed before embedding:</p> <pre><code># These produce different embeddings due to different text\nvec1 = steadytext.embed(\"Hello World\")\nvec2 = steadytext.embed(\"hello world\")\nvec3 = steadytext.embed(\"HELLO WORLD\")\n\n# Case sensitivity matters\nassert not np.array_equal(vec1, vec2)\n</code></pre>"},{"location":"api/embedding/#batch-processing","title":"Batch Processing","text":"<p>For multiple texts, pass as a list:</p> <pre><code># Individual embeddings\nvec1 = steadytext.embed(\"first text\")\nvec2 = steadytext.embed(\"second text\") \nvec3 = steadytext.embed(\"third text\")\n\n# Batch embedding (averaged)\nvec_batch = steadytext.embed([\"first text\", \"second text\", \"third text\"])\n\n# The batch result is the average of individual embeddings\nexpected = (vec1 + vec2 + vec3) / 3\nassert np.allclose(vec_batch, expected, atol=1e-6)\n</code></pre>"},{"location":"api/embedding/#caching","title":"Caching","text":"<p>Embeddings are cached for performance:</p> <pre><code># First call: computes and caches embedding\nvec1 = steadytext.embed(\"common text\")  # ~0.5 seconds\n\n# Second call: returns cached result\nvec2 = steadytext.embed(\"common text\")  # ~0.01 seconds\n\nassert np.array_equal(vec1, vec2)  # Same result, much faster\n</code></pre>"},{"location":"api/embedding/#fallback-behavior","title":"Fallback Behavior","text":"<p>When models can't be loaded, zero vectors are returned:</p> <pre><code># Even without models, function never fails\nvector = steadytext.embed(\"any text\")\n\nassert vector.shape == (1024,)     # Correct shape\nassert vector.dtype == np.float32  # Correct type\nassert np.linalg.norm(vector) == 0 # Zero vector fallback\n</code></pre>"},{"location":"api/embedding/#use-cases","title":"Use Cases","text":""},{"location":"api/embedding/#document-similarity","title":"Document Similarity","text":"<pre><code>import steadytext\nimport numpy as np\n\ndef document_similarity(doc1: str, doc2: str) -&gt; float:\n    \"\"\"Calculate similarity between two documents.\"\"\"\n    vec1 = steadytext.embed(doc1)\n    vec2 = steadytext.embed(doc2)\n    return np.dot(vec1, vec2)  # Already L2 normalized\n\n# Usage\nsimilarity = document_similarity(\n    \"Machine learning algorithms\",\n    \"AI and neural networks\"\n)\nprint(f\"Similarity: {similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#semantic-search","title":"Semantic Search","text":"<pre><code>def semantic_search(query: str, documents: List[str], top_k: int = 5):\n    \"\"\"Find most similar documents to query.\"\"\"\n    query_vec = steadytext.embed(query)\n    doc_vecs = [steadytext.embed(doc) for doc in documents]\n\n    similarities = [np.dot(query_vec, doc_vec) for doc_vec in doc_vecs]\n    top_indices = np.argsort(similarities)[-top_k:][::-1]\n\n    return [(documents[i], similarities[i]) for i in top_indices]\n\n# Usage  \ndocs = [\"AI research\", \"Machine learning\", \"Cooking recipes\", \"Data science\"]\nresults = semantic_search(\"artificial intelligence\", docs, top_k=2)\n\nfor doc, score in results:\n    print(f\"{doc}: {score:.3f}\")\n</code></pre>"},{"location":"api/embedding/#clustering","title":"Clustering","text":"<pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_texts(texts: List[str], n_clusters: int = 3):\n    \"\"\"Cluster texts using their embeddings.\"\"\"\n    embeddings = np.array([steadytext.embed(text) for text in texts])\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(embeddings)\n\n    return clusters\n\n# Usage\ntexts = [\n    \"machine learning\", \"deep learning\", \"neural networks\",  # AI cluster\n    \"pizza recipe\", \"pasta cooking\", \"italian food\",        # Food cluster  \n    \"stock market\", \"trading\", \"investment\"                 # Finance cluster\n]\n\nclusters = cluster_texts(texts, n_clusters=3)\nfor text, cluster in zip(texts, clusters):\n    print(f\"Cluster {cluster}: {text}\")\n</code></pre>"},{"location":"api/embedding/#performance-notes","title":"Performance Notes","text":"<p>Optimization Tips</p> <ul> <li>Preload models: Call <code>steadytext.preload_models()</code> at startup</li> <li>Batch similar texts: Group related texts together for cache efficiency  </li> <li>Memory usage: ~610MB for embedding model (loaded once)</li> <li>Speed: ~100-500 embeddings/second depending on text length</li> </ul>"},{"location":"api/generation/","title":"Text Generation API","text":"<p>Functions for deterministic text generation.</p>"},{"location":"api/generation/#generate","title":"generate()","text":"<p>Generate deterministic text from a prompt.</p> <pre><code>def generate(\n    prompt: str,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\"\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre>"},{"location":"api/generation/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>prompt</code> <code>str</code> required Input text to generate from <code>return_logprobs</code> <code>bool</code> <code>False</code> Return log probabilities with text <code>eos_string</code> <code>str</code> <code>\"[EOS]\"</code> Custom end-of-sequence string"},{"location":"api/generation/#returns","title":"Returns","text":"Basic UsageWith Log Probabilities <p>Returns: <code>str</code> - Generated text (512 tokens max)</p> <p>Returns: <code>Tuple[str, Optional[Dict]]</code> - Generated text and log probabilities</p>"},{"location":"api/generation/#examples","title":"Examples","text":"Simple GenerationWith Log ProbabilitiesCustom Stop String <pre><code>import steadytext\n\ntext = steadytext.generate(\"Write a Python function\")\nprint(text)\n# Always returns the same 512-token completion\n</code></pre> <pre><code>text, logprobs = steadytext.generate(\n    \"Explain machine learning\", \n    return_logprobs=True\n)\n\nprint(\"Generated text:\", text)\nprint(\"Log probabilities:\", logprobs)\n</code></pre> <pre><code># Stop generation at custom string\ntext = steadytext.generate(\n    \"List programming languages until STOP\",\n    eos_string=\"STOP\"\n)\nprint(text)\n</code></pre>"},{"location":"api/generation/#generate_iter","title":"generate_iter()","text":"<p>Generate text iteratively, yielding tokens as produced.</p> <pre><code>def generate_iter(\n    prompt: str,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre>"},{"location":"api/generation/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>prompt</code> <code>str</code> required Input text to generate from <code>eos_string</code> <code>str</code> <code>\"[EOS]\"</code> Custom end-of-sequence string <code>include_logprobs</code> <code>bool</code> <code>False</code> Yield log probabilities with tokens"},{"location":"api/generation/#returns_1","title":"Returns","text":"Basic StreamingWith Log Probabilities <p>Yields: <code>str</code> - Individual tokens/words</p> <p>Yields: <code>Tuple[str, Optional[Dict]]</code> - Token and log probabilities</p>"},{"location":"api/generation/#examples_1","title":"Examples","text":"Basic StreamingWith Progress TrackingCustom Stop StringWith Log Probabilities <pre><code>import steadytext\n\nfor token in steadytext.generate_iter(\"Tell me a story\"):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code>prompt = \"Explain quantum computing\"\ntokens = []\n\nfor token in steadytext.generate_iter(prompt):\n    tokens.append(token)\n    print(f\"Generated {len(tokens)} tokens\", end=\"\\r\")\n\nprint(f\"\\nComplete! Generated {len(tokens)} tokens\")\nprint(\"Full text:\", \"\".join(tokens))\n</code></pre> <pre><code>for token in steadytext.generate_iter(\n    \"Count from 1 to 10 then say DONE\", \n    eos_string=\"DONE\"\n):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code>for token, logprobs in steadytext.generate_iter(\n    \"Explain AI\", \n    include_logprobs=True\n):\n    confidence = logprobs.get('confidence', 0) if logprobs else 0\n    print(f\"{token} (confidence: {confidence:.2f})\", end=\"\")\n</code></pre>"},{"location":"api/generation/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/generation/#deterministic-behavior","title":"Deterministic Behavior","text":"<p>Both functions return identical results for identical inputs:</p> <pre><code># These will always be identical\nresult1 = steadytext.generate(\"hello world\")\nresult2 = steadytext.generate(\"hello world\") \nassert result1 == result2  # Always passes!\n\n# Streaming produces same tokens in same order\ntokens1 = list(steadytext.generate_iter(\"hello world\"))\ntokens2 = list(steadytext.generate_iter(\"hello world\"))\nassert tokens1 == tokens2  # Always passes!\n</code></pre>"},{"location":"api/generation/#caching","title":"Caching","text":"<p>Results are automatically cached using a frecency cache (LRU + frequency):</p> <pre><code># First call: generates and caches result\ntext1 = steadytext.generate(\"common prompt\")  # ~2 seconds\n\n# Second call: returns cached result  \ntext2 = steadytext.generate(\"common prompt\")  # ~0.1 seconds\n\nassert text1 == text2  # Same result, much faster\n</code></pre>"},{"location":"api/generation/#fallback-behavior","title":"Fallback Behavior","text":"<p>When models can't be loaded, deterministic fallbacks are used:</p> <pre><code># Even without models, these always return the same results\ntext = steadytext.generate(\"test prompt\")  # Hash-based fallback\nassert len(text) &gt; 0  # Always has content\n\n# Fallback is also deterministic\ntext1 = steadytext.generate(\"fallback test\")\ntext2 = steadytext.generate(\"fallback test\") \nassert text1 == text2  # Same fallback result\n</code></pre>"},{"location":"api/generation/#performance-tips","title":"Performance Tips","text":"<p>Optimization Strategies</p> <ul> <li>Preload models: Call <code>steadytext.preload_models()</code> at startup</li> <li>Batch processing: Use <code>generate()</code> for multiple prompts rather than streaming individual tokens</li> <li>Cache warmup: Pre-generate common prompts to populate cache</li> <li>Memory management: Models stay loaded once initialized (singleton pattern)</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Real-world usage patterns and code examples for SteadyText.</p>"},{"location":"examples/#overview","title":"Overview","text":"<p>This section demonstrates practical applications of SteadyText across different use cases:</p> <ul> <li>Testing with AI - Reliable AI tests that never flake</li> <li>CLI Tools - Building deterministic command-line tools</li> </ul> <p>All examples showcase SteadyText's core principle: same input \u2192 same output, every time.</p>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#basic-usage","title":"Basic Usage","text":"<pre><code>import steadytext\n\n# Deterministic text generation\ncode = steadytext.generate(\"implement binary search in Python\")\nassert \"def binary_search\" in code  # Always passes!\n\n# Streaming generation\nfor token in steadytext.generate_iter(\"explain quantum computing\"):\n    print(token, end=\"\", flush=True)\n\n# Deterministic embeddings  \nvec = steadytext.embed(\"Hello world\")  # 1024-dim numpy array\nprint(f\"Shape: {vec.shape}, Norm: {np.linalg.norm(vec):.6f}\")\n</code></pre>"},{"location":"examples/#testing-applications","title":"Testing Applications","text":"<pre><code>def test_ai_code_generation():\n    \"\"\"Test that never flakes - same input, same output.\"\"\"\n    prompt = \"write a function to reverse a string\"\n    result = my_ai_function(prompt)\n    expected = steadytext.generate(prompt)\n    assert result == expected  # Deterministic comparison!\n\ndef test_embedding_similarity():\n    \"\"\"Reliable similarity testing.\"\"\"\n    vec1 = steadytext.embed(\"machine learning\")\n    vec2 = steadytext.embed(\"artificial intelligence\")\n    similarity = np.dot(vec1, vec2)  # Already normalized\n    assert similarity &gt; 0.7  # Always passes with same threshold\n</code></pre>"},{"location":"examples/#cli-tool-building","title":"CLI Tool Building","text":"<pre><code>import click\nimport steadytext\n\n@click.command()\n@click.argument('topic')\ndef motivate(topic):\n    \"\"\"Generate motivational quotes about any topic.\"\"\"\n    prompt = f\"Write an inspiring quote about {topic}\"\n    quote = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udcaa {quote}\")\n\n# Usage: python script.py \"programming\"\n# Always generates the same motivational quote for \"programming\"\n</code></pre>"},{"location":"examples/#use-case-categories","title":"Use Case Categories","text":""},{"location":"examples/#testing-quality-assurance","title":"\ud83e\uddea Testing &amp; Quality Assurance","text":"<p>Perfect for: - Unit tests with AI components - Integration testing with deterministic outputs - Regression testing for AI features - Mock AI services for development</p>"},{"location":"examples/#developer-tools","title":"\ud83d\udee0\ufe0f Developer Tools","text":"<p>Ideal for: - Code generation tools - Documentation generators - CLI utilities with AI features - Build system integration</p>"},{"location":"examples/#data-content-generation","title":"\ud83d\udcca Data &amp; Content Generation","text":"<p>Great for: - Synthetic data generation - Content templates - Data augmentation for testing - Reproducible research datasets</p>"},{"location":"examples/#search-similarity","title":"\ud83d\udd0d Search &amp; Similarity","text":"<p>Excellent for: - Semantic search systems - Document clustering - Content recommendation - Duplicate detection</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<ol> <li>Browse examples - Check out Testing and CLI Tools</li> <li>Run the code - All examples are fully executable</li> <li>Adapt for your use case - Copy and modify patterns that fit your needs</li> </ol>"},{"location":"examples/#example-repository","title":"Example Repository","text":"<p>All examples are available in the examples/ directory of the SteadyText repository:</p> <pre><code>git clone https://github.com/julep-ai/steadytext.git\ncd steadytext/examples\npython basic_usage.py\npython testing_with_ai.py  \npython cli_tools.py\n</code></pre> <p>Deterministic Outputs</p> <p>Remember: all examples produce identical outputs every time you run them. This predictability is SteadyText's core feature and what makes it perfect for testing and tooling applications.</p>"},{"location":"examples/testing/","title":"Testing with AI","text":"<p>Learn how to use SteadyText to build reliable AI tests that never flake.</p>"},{"location":"examples/testing/#the-problem-with-ai-testing","title":"The Problem with AI Testing","text":"<p>Traditional AI testing is challenging because:</p> <ul> <li>Non-deterministic outputs: Same input produces different results</li> <li>Flaky tests: Tests pass sometimes, fail others  </li> <li>Hard to mock: AI services are complex to replicate</li> <li>Unpredictable behavior: Edge cases are difficult to reproduce</li> </ul> <p>SteadyText solves these by providing deterministic AI outputs - same input always produces the same result.</p>"},{"location":"examples/testing/#basic-test-patterns","title":"Basic Test Patterns","text":""},{"location":"examples/testing/#deterministic-assertions","title":"Deterministic Assertions","text":"<pre><code>import steadytext\n\ndef test_ai_code_generation():\n    \"\"\"Test that never flakes - same input, same output.\"\"\"\n\n    def my_ai_function(prompt):\n        # Your actual AI function (GPT-4, Claude, etc.)\n        # For testing, we compare against SteadyText\n        return call_real_ai_service(prompt)\n\n    prompt = \"write a function to reverse a string\"\n    result = my_ai_function(prompt)\n    expected = steadytext.generate(prompt)\n\n    # This assertion is deterministic and reliable\n    assert result.strip() == expected.strip()\n</code></pre>"},{"location":"examples/testing/#embedding-similarity-tests","title":"Embedding Similarity Tests","text":"<pre><code>import numpy as np\n\ndef test_document_similarity():\n    \"\"\"Test semantic similarity calculations.\"\"\"\n\n    def calculate_similarity(doc1, doc2):\n        vec1 = steadytext.embed(doc1)\n        vec2 = steadytext.embed(doc2)\n        return np.dot(vec1, vec2)  # Already normalized\n\n    # These similarities are always the same\n    similarity = calculate_similarity(\n        \"machine learning algorithms\",\n        \"artificial intelligence methods\"\n    )\n\n    assert similarity &gt; 0.7  # Reliable threshold\n    assert similarity &lt; 1.0  # Not identical documents\n</code></pre>"},{"location":"examples/testing/#mock-ai-services","title":"Mock AI Services","text":""},{"location":"examples/testing/#simple-mock","title":"Simple Mock","text":"<pre><code>class MockAI:\n    \"\"\"Deterministic AI mock for testing.\"\"\"\n\n    def complete(self, prompt: str) -&gt; str:\n        return steadytext.generate(prompt)\n\n    def embed(self, text: str) -&gt; np.ndarray:\n        return steadytext.embed(text)\n\n    def chat(self, messages: list) -&gt; str:\n        # Convert chat format to single prompt\n        prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" \n                           for msg in messages])\n        return steadytext.generate(f\"Chat response to: {prompt}\")\n\n# Usage in tests\ndef test_chat_functionality():\n    ai = MockAI()\n    response = ai.chat([\n        {\"role\": \"user\", \"content\": \"Hello\"}\n    ])\n\n    # Response is always the same\n    assert len(response) &gt; 0\n    assert \"hello\" in response.lower()\n</code></pre>"},{"location":"examples/testing/#advanced-mock-with-state","title":"Advanced Mock with State","text":"<pre><code>class StatefulMockAI:\n    \"\"\"Mock AI that maintains conversation state.\"\"\"\n\n    def __init__(self):\n        self.conversation_history = []\n\n    def chat(self, message: str) -&gt; str:\n        # Include history in prompt for context\n        history = \"\\n\".join(self.conversation_history[-5:])  # Last 5 messages\n        full_prompt = f\"History: {history}\\nNew message: {message}\"\n\n        response = steadytext.generate(full_prompt)\n\n        # Update history\n        self.conversation_history.append(f\"User: {message}\")\n        self.conversation_history.append(f\"AI: {response}\")\n\n        return response\n\ndef test_conversation_flow():\n    \"\"\"Test multi-turn conversations.\"\"\"\n    ai = StatefulMockAI()\n\n    response1 = ai.chat(\"What's the weather like?\")\n    response2 = ai.chat(\"What about tomorrow?\")\n\n    # Both responses are deterministic\n    assert len(response1) &gt; 0\n    assert len(response2) &gt; 0\n    # Tomorrow's response considers the context\n    assert response2 != response1\n</code></pre>"},{"location":"examples/testing/#test-data-generation","title":"Test Data Generation","text":""},{"location":"examples/testing/#reproducible-fixtures","title":"Reproducible Fixtures","text":"<pre><code>def generate_test_user(user_id: int) -&gt; dict:\n    \"\"\"Generate consistent test user data.\"\"\"\n    return {\n        \"id\": user_id,\n        \"name\": steadytext.generate(f\"Generate name for user {user_id}\"),\n        \"bio\": steadytext.generate(f\"Write bio for user {user_id}\"),\n        \"interests\": steadytext.generate(f\"List interests for user {user_id}\"),\n        \"embedding\": steadytext.embed(f\"user {user_id} profile\")\n    }\n\ndef test_user_recommendation():\n    \"\"\"Test user recommendation system.\"\"\"\n    # Generate consistent test users\n    users = [generate_test_user(i) for i in range(10)]\n\n    # Test similarity calculations\n    user1 = users[0]\n    user2 = users[1]\n\n    similarity = np.dot(user1[\"embedding\"], user2[\"embedding\"])\n\n    # Similarity is always the same for these users\n    assert isinstance(similarity, float)\n    assert -1.0 &lt;= similarity &lt;= 1.0\n</code></pre>"},{"location":"examples/testing/#fuzz-testing","title":"Fuzz Testing","text":"<pre><code>def generate_fuzz_input(test_name: str, iteration: int) -&gt; str:\n    \"\"\"Generate reproducible fuzz test inputs.\"\"\"\n    seed_prompt = f\"Generate test input for {test_name} iteration {iteration}\"\n    return steadytext.generate(seed_prompt)\n\ndef test_parser_robustness():\n    \"\"\"Fuzz test with reproducible inputs.\"\"\"\n\n    def parse_user_input(text):\n        # Your parsing function\n        return {\"words\": text.split(), \"length\": len(text)}\n\n    # Generate 100 consistent fuzz inputs\n    for i in range(100):\n        fuzz_input = generate_fuzz_input(\"parser_test\", i)\n\n        try:\n            result = parse_user_input(fuzz_input)\n            assert isinstance(result, dict)\n            assert \"words\" in result\n            assert \"length\" in result\n        except Exception as e:\n            # Reproducible error case\n            print(f\"Fuzz input {i} caused error: {e}\")\n            print(f\"Input was: {fuzz_input[:100]}...\")\n</code></pre>"},{"location":"examples/testing/#integration-testing","title":"Integration Testing","text":""},{"location":"examples/testing/#api-testing","title":"API Testing","text":"<pre><code>import requests_mock\n\ndef test_ai_api_integration():\n    \"\"\"Test integration with AI API using deterministic responses.\"\"\"\n\n    with requests_mock.Mocker() as m:\n        # Mock the AI API with deterministic responses\n        def generate_response(request, context):\n            prompt = request.json().get(\"prompt\", \"\")\n            return {\"response\": steadytext.generate(prompt)}\n\n        m.post(\"https://api.ai-service.com/generate\", json=generate_response)\n\n        # Your actual API client code\n        response = requests.post(\"https://api.ai-service.com/generate\", \n                               json={\"prompt\": \"Hello world\"})\n\n        # Response is always the same\n        expected_text = steadytext.generate(\"Hello world\")\n        assert response.json()[\"response\"] == expected_text\n</code></pre>"},{"location":"examples/testing/#database-testing","title":"Database Testing","text":"<pre><code>import sqlite3\n\ndef test_ai_content_storage():\n    \"\"\"Test storing AI-generated content in database.\"\"\"\n\n    # Create in-memory database\n    conn = sqlite3.connect(\":memory:\")\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n        CREATE TABLE content (\n            id INTEGER PRIMARY KEY,\n            prompt TEXT,\n            generated_text TEXT,\n            embedding BLOB\n        )\n    \"\"\")\n\n    # Generate deterministic content\n    prompt = \"Write a short story about AI\"\n    text = steadytext.generate(prompt)\n    embedding = steadytext.embed(text)\n\n    # Store in database\n    cursor.execute(\"\"\"\n        INSERT INTO content (prompt, generated_text, embedding) \n        VALUES (?, ?, ?)\n    \"\"\", (prompt, text, embedding.tobytes()))\n\n    # Verify storage\n    cursor.execute(\"SELECT * FROM content WHERE id = 1\")\n    row = cursor.fetchone()\n\n    assert row[1] == prompt\n    assert row[2] == text\n    assert len(row[3]) == 1024 * 4  # 1024 float32 values\n\n    conn.close()\n</code></pre>"},{"location":"examples/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"examples/testing/#consistency-benchmarks","title":"Consistency Benchmarks","text":"<pre><code>import time\n\ndef test_generation_performance():\n    \"\"\"Test that generation performance is consistent.\"\"\"\n\n    prompt = \"Explain machine learning in one paragraph\"\n    times = []\n\n    # Warm up cache\n    steadytext.generate(prompt)\n\n    # Measure cached performance\n    for _ in range(10):\n        start = time.time()\n        result = steadytext.generate(prompt)\n        end = time.time()\n        times.append(end - start)\n\n    avg_time = sum(times) / len(times)\n\n    # Cached calls should be very fast\n    assert avg_time &lt; 0.1  # Less than 100ms\n\n    # All results should be identical\n    results = [steadytext.generate(prompt) for _ in range(5)]\n    assert all(r == results[0] for r in results)\n</code></pre>"},{"location":"examples/testing/#best-practices","title":"Best Practices","text":"<p>Testing Guidelines</p> <ol> <li>Use deterministic prompts: Keep test prompts simple and specific</li> <li>Cache warmup: Call functions once before timing tests</li> <li>Mock external services: Use SteadyText to replace real AI APIs</li> <li>Test edge cases: Generate consistent edge case inputs</li> <li>Version pin: Keep SteadyText version fixed for test stability</li> </ol> <p>Limitations</p> <ul> <li>Model changes: Updates to SteadyText models will change outputs</li> <li>Creative tasks: SteadyText is optimized for consistency, not creativity</li> <li>Context length: Limited to model's context window</li> </ul>"},{"location":"examples/testing/#complete-example","title":"Complete Example","text":"<pre><code>import unittest\nimport numpy as np\nimport steadytext\n\nclass TestAIFeatures(unittest.TestCase):\n\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.mock_ai = MockAI()\n        self.test_prompts = [\n            \"Write a function to sort a list\",\n            \"Explain what is machine learning\",\n            \"Generate a product description\"\n        ]\n\n    def test_deterministic_generation(self):\n        \"\"\"Test that generation is deterministic.\"\"\"\n        for prompt in self.test_prompts:\n            result1 = steadytext.generate(prompt)\n            result2 = steadytext.generate(prompt)\n            self.assertEqual(result1, result2)\n\n    def test_embedding_consistency(self):\n        \"\"\"Test that embeddings are consistent.\"\"\"\n        text = \"test embedding consistency\"\n        vec1 = steadytext.embed(text)\n        vec2 = steadytext.embed(text)\n        np.testing.assert_array_equal(vec1, vec2)\n\n    def test_mock_ai_service(self):\n        \"\"\"Test mock AI service.\"\"\"\n        response = self.mock_ai.complete(\"Hello\")\n        self.assertIsInstance(response, str)\n        self.assertGreater(len(response), 0)\n\n        # Response should be deterministic\n        response2 = self.mock_ai.complete(\"Hello\")\n        self.assertEqual(response, response2)\n\nif __name__ == \"__main__\":\n    unittest.main()\n</code></pre> <p>This comprehensive testing approach ensures your AI features are reliable, reproducible, and maintainable.</p>"},{"location":"examples/tooling/","title":"CLI Tools &amp; Tooling","text":"<p>Build deterministic command-line tools and development utilities with SteadyText.</p>"},{"location":"examples/tooling/#why-steadytext-for-cli-tools","title":"Why SteadyText for CLI Tools?","text":"<p>Traditional AI-powered CLI tools have problems:</p> <ul> <li>Inconsistent outputs: Same command gives different results</li> <li>Unreliable automation: Scripts break due to changing responses  </li> <li>Hard to test: Non-deterministic behavior makes testing difficult</li> <li>User confusion: Users expect consistent behavior from tools</li> </ul> <p>SteadyText solves these with deterministic outputs - same input always produces the same result.</p>"},{"location":"examples/tooling/#basic-cli-patterns","title":"Basic CLI Patterns","text":""},{"location":"examples/tooling/#simple-command-tools","title":"Simple Command Tools","text":"<pre><code>import click\nimport steadytext\n\n@click.command()\n@click.argument('topic')\ndef motivate(topic):\n    \"\"\"Generate motivational quotes about any topic.\"\"\"\n    prompt = f\"Write an inspiring quote about {topic}\"\n    quote = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udcaa {quote}\")\n\n# Usage: python script.py programming\n# Always generates the same quote for \"programming\"\n</code></pre>"},{"location":"examples/tooling/#error-code-explainer","title":"Error Code Explainer","text":"<pre><code>@click.command()\n@click.argument('error_code')\ndef explain(error_code):\n    \"\"\"Convert error codes to friendly explanations.\"\"\"\n    prompt = f\"Explain error {error_code} in simple, user-friendly terms\"\n    explanation = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udd0d {error_code}: {explanation}\")\n\n# Usage: python script.py ECONNREFUSED\n# Always gives the same explanation for ECONNREFUSED\n</code></pre>"},{"location":"examples/tooling/#command-generator","title":"Command Generator","text":"<pre><code>@click.command()\n@click.argument('task')\ndef git_helper(task):\n    \"\"\"Generate git commands for common tasks.\"\"\"\n    prompt = f\"Git command to {task}. Return only the command.\"\n    command = steadytext.generate(prompt).strip()\n    click.echo(f\"\ud83d\udcbb {command}\")\n\n# Usage: python script.py \"undo last commit\"\n# Always suggests the same git command\n</code></pre>"},{"location":"examples/tooling/#development-tools","title":"Development Tools","text":""},{"location":"examples/tooling/#code-generation-helper","title":"Code Generation Helper","text":"<pre><code>import os\nimport click\n\n@click.group()\ndef codegen():\n    \"\"\"Code generation CLI tool.\"\"\"\n    pass\n\n@codegen.command()\n@click.argument('function_name')\n@click.argument('description')\n@click.option('--language', '-l', default='python', help='Programming language')\ndef function(function_name, description, language):\n    \"\"\"Generate a function from description.\"\"\"\n    prompt = f\"Write a {language} function named {function_name} that {description}\"\n    code = steadytext.generate(prompt)\n\n    # Save to file\n    ext = {'python': 'py', 'javascript': 'js', 'rust': 'rs'}.get(language, 'txt')\n    filename = f\"{function_name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(code)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n    click.echo(f\"\ud83d\udcc4 Preview:\\n{code[:200]}...\")\n\n# Usage: python codegen.py function binary_search \"search for item in sorted array\"\n</code></pre>"},{"location":"examples/tooling/#documentation-generator","title":"Documentation Generator","text":"<pre><code>@codegen.command()\n@click.argument('project_name')\ndef readme(project_name):\n    \"\"\"Generate README.md for a project.\"\"\"\n    prompt = f\"Write a comprehensive README.md for a project called {project_name}\"\n    readme_content = steadytext.generate(prompt)\n\n    with open('README.md', 'w') as f:\n        f.write(readme_content)\n\n    click.echo(\"\u2705 Generated README.md\")\n\n# Usage: python codegen.py readme \"my-awesome-project\"\n</code></pre>"},{"location":"examples/tooling/#testing-qa-tools","title":"Testing &amp; QA Tools","text":""},{"location":"examples/tooling/#test-case-generator","title":"Test Case Generator","text":"<pre><code>@click.command()\n@click.argument('function_description')\ndef test_cases(function_description):\n    \"\"\"Generate test cases for a function.\"\"\"\n    prompt = f\"Generate 5 test cases for a function that {function_description}\"\n    cases = steadytext.generate(prompt)\n\n    # Save to test file\n    with open('test_cases.py', 'w') as f:\n        f.write(f\"# Test cases for: {function_description}\\n\")\n        f.write(cases)\n\n    click.echo(\"\u2705 Generated test_cases.py\")\n    click.echo(f\"\ud83d\udccb Preview:\\n{cases[:300]}...\")\n\n# Usage: python tool.py \"calculates fibonacci numbers\"\n</code></pre>"},{"location":"examples/tooling/#mock-data-generator","title":"Mock Data Generator","text":"<pre><code>@click.command()\n@click.argument('data_type')\n@click.option('--count', '-c', default=10, help='Number of items to generate')\ndef mockdata(data_type, count):\n    \"\"\"Generate mock data for testing.\"\"\"\n    items = []\n\n    for i in range(count):\n        prompt = f\"Generate realistic {data_type} data item {i+1}\"\n        item = steadytext.generate(prompt)\n        items.append(item.strip())\n\n    # Output as JSON\n    import json\n    output = {data_type: items}\n\n    with open(f'mock_{data_type}.json', 'w') as f:\n        json.dump(output, f, indent=2)\n\n    click.echo(f\"\u2705 Generated mock_{data_type}.json with {count} items\")\n\n# Usage: python tool.py user_profiles --count 20\n</code></pre>"},{"location":"examples/tooling/#content-documentation-tools","title":"Content &amp; Documentation Tools","text":""},{"location":"examples/tooling/#commit-message-generator","title":"Commit Message Generator","text":"<pre><code>@click.command()\n@click.argument('changes', nargs=-1)\ndef commit_msg(changes):\n    \"\"\"Generate commit messages from change descriptions.\"\"\"\n    change_list = \" \".join(changes)\n    prompt = f\"Write a concise git commit message for: {change_list}\"\n    message = steadytext.generate(prompt).strip()\n\n    click.echo(f\"\ud83d\udcdd Suggested commit message:\")\n    click.echo(f\"   {message}\")\n\n    # Optionally copy to clipboard or commit directly\n    if click.confirm(\"Use this commit message?\"):\n        os.system(f'git commit -m \"{message}\"')\n        click.echo(\"\u2705 Committed!\")\n\n# Usage: python tool.py \"added user authentication\" \"fixed login bug\"\n</code></pre>"},{"location":"examples/tooling/#api-documentation-generator","title":"API Documentation Generator","text":"<pre><code>@click.command()\n@click.argument('api_endpoint')\n@click.argument('description')\ndef api_docs(api_endpoint, description):\n    \"\"\"Generate API documentation for an endpoint.\"\"\"\n    prompt = f\"\"\"Generate API documentation for endpoint {api_endpoint} that {description}.\n    Include: description, parameters, example request/response, error codes.\"\"\"\n\n    docs = steadytext.generate(prompt)\n\n    # Save to markdown file\n    safe_name = api_endpoint.replace('/', '_').replace('{', '').replace('}', '')\n    filename = f\"api_{safe_name}.md\"\n\n    with open(filename, 'w') as f:\n        f.write(f\"# {api_endpoint}\\n\\n\")\n        f.write(docs)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py \"/users/{id}\" \"returns user profile information\"\n</code></pre>"},{"location":"examples/tooling/#automation-scripting","title":"Automation &amp; Scripting","text":""},{"location":"examples/tooling/#configuration-generator","title":"Configuration Generator","text":"<pre><code>@click.command()\n@click.argument('service_name')\n@click.option('--format', '-f', default='yaml', help='Config format (yaml, json, toml)')\ndef config(service_name, format):\n    \"\"\"Generate configuration files for services.\"\"\"\n    prompt = f\"Generate a {format} configuration file for {service_name} service\"\n    config_content = steadytext.generate(prompt)\n\n    ext = {'yaml': 'yml', 'json': 'json', 'toml': 'toml'}.get(format, 'txt')\n    filename = f\"{service_name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(config_content)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py database --format yaml\n</code></pre>"},{"location":"examples/tooling/#script-template-generator","title":"Script Template Generator","text":"<pre><code>@click.command()\n@click.argument('script_type')\n@click.argument('purpose')\ndef script_template(script_type, purpose):\n    \"\"\"Generate script templates for common tasks.\"\"\"\n    prompt = f\"Generate a {script_type} script template for {purpose}\"\n    script = steadytext.generate(prompt)\n\n    ext = {'bash': 'sh', 'python': 'py', 'powershell': 'ps1'}.get(script_type, 'txt')\n    filename = f\"template.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(script)\n\n    # Make executable if shell script\n    if ext == 'sh':\n        os.chmod(filename, 0o755)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py bash \"automated deployment\"\n</code></pre>"},{"location":"examples/tooling/#complete-cli-tool-example","title":"Complete CLI Tool Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nDevHelper - A deterministic development tool powered by SteadyText\n\"\"\"\n\nimport os\nimport json\nimport click\nimport steadytext\n\n@click.group()\n@click.version_option()\ndef cli():\n    \"\"\"DevHelper - Deterministic development utilities.\"\"\"\n    pass\n\n@cli.group()\ndef generate():\n    \"\"\"Code and content generation commands.\"\"\"\n    pass\n\n@generate.command()\n@click.argument('name')\n@click.argument('description')\n@click.option('--lang', '-l', default='python', help='Programming language')\ndef function(name, description, lang):\n    \"\"\"Generate a function from description.\"\"\"\n    prompt = f\"Write a {lang} function named {name} that {description}\"\n    code = steadytext.generate(prompt)\n\n    ext = {'python': 'py', 'javascript': 'js', 'rust': 'rs'}.get(lang, 'txt')\n    filename = f\"{name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(code)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n@generate.command()\n@click.argument('count', type=int)\n@click.option('--type', '-t', default='user', help='Data type to generate')\ndef testdata(count, type):\n    \"\"\"Generate test data.\"\"\"\n    data = []\n\n    for i in range(count):\n        prompt = f\"Generate realistic {type} test data item {i+1} as JSON\"\n        item = steadytext.generate(prompt)\n        data.append(item.strip())\n\n    output_file = f\"test_{type}_data.json\"\n    with open(output_file, 'w') as f:\n        json.dump({f\"{type}_data\": data}, f, indent=2)\n\n    click.echo(f\"\u2705 Generated {output_file} with {count} items\")\n\n@cli.command()\n@click.argument('error_code')\ndef explain(error_code):\n    \"\"\"Explain error codes in friendly terms.\"\"\"\n    prompt = f\"Explain error {error_code} in simple, user-friendly terms\"\n    explanation = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udd0d {error_code}:\")\n    click.echo(f\"   {explanation}\")\n\n@cli.command()\n@click.argument('task')\ndef git(task):\n    \"\"\"Generate git commands for tasks.\"\"\"\n    prompt = f\"Git command to {task}. Return only the command.\"\n    command = steadytext.generate(prompt).strip()\n    click.echo(f\"\ud83d\udcbb {command}\")\n\n    if click.confirm(\"Execute this command?\"):\n        os.system(command)\n\nif __name__ == '__main__':\n    cli()\n</code></pre> <p>Save this as <code>devhelper.py</code> and use it:</p> <pre><code># Generate a function\npython devhelper.py generate function binary_search \"search sorted array\"\n\n# Generate test data  \npython devhelper.py generate testdata 10 --type user\n\n# Explain error codes\npython devhelper.py explain ECONNREFUSED\n\n# Get git commands\npython devhelper.py git \"undo last commit but keep changes\"\n</code></pre>"},{"location":"examples/tooling/#best-practices","title":"Best Practices","text":"<p>CLI Tool Guidelines</p> <ol> <li>Keep prompts specific: Clear, detailed prompts give better results</li> <li>Add confirmation prompts: For destructive operations, ask before executing</li> <li>Save outputs to files: Generate content to files for later use</li> <li>Use consistent formatting: Same input should always produce same output</li> <li>Add help text: Use Click's built-in help system</li> </ol> <p>Benefits of Deterministic CLI Tools</p> <ul> <li>Reliable automation: Scripts work consistently</li> <li>Easier testing: Predictable outputs make testing simple</li> <li>User trust: Users know what to expect</li> <li>Debugging: Reproducible behavior makes issues easier to track</li> <li>Documentation: Examples in docs always work</li> </ul> <p>Considerations</p> <ul> <li>Creative vs. Deterministic: SteadyText prioritizes consistency over creativity</li> <li>Context limits: Model has limited context window</li> <li>Update impacts: SteadyText updates may change outputs (major versions only)</li> </ul> <p>This approach creates reliable, testable CLI tools that users can depend on for consistent behavior.</p>"}]}