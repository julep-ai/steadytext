{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SteadyText","text":"<p>Deterministic text generation and embeddings with zero configuration</p> <p> </p> <p>Same input \u2192 same output. Every time.</p> <p>No more flaky tests, unpredictable CLI tools, or inconsistent docs. SteadyText makes AI outputs as reliable as hash functions.</p> <p>Ever had an AI test fail randomly? Or a CLI tool give different answers each run? SteadyText makes AI outputs reproducible - perfect for testing, tooling, and anywhere you need consistent results.</p> <p>Powered by Julep</p> <p>\u2728 Powered by open-source AI workflows from Julep. \u2728</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Using UV (recommended - 10-100x faster)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre> Python APICommand Line <pre><code>import steadytext\n\n# Deterministic text generation\ncode = steadytext.generate(\"implement binary search in Python\")\nassert \"def binary_search\" in code  # Always passes!\n\n# Streaming (also deterministic)\nfor token in steadytext.generate_iter(\"explain quantum computing\"):\n    print(token, end=\"\", flush=True)\n\n# Deterministic embeddings\nvec = steadytext.embed(\"Hello world\")  # 1024-dim numpy array\n\n# Structured generation (v2.4.1+)\nfrom pydantic import BaseModel\nclass User(BaseModel):\n    name: str\n    age: int\n\nresult = steadytext.generate(\"Create user Alice age 30\", schema=User)\n# Returns: '...&lt;json-output&gt;{\"name\": \"Alice\", \"age\": 30}&lt;/json-output&gt;'\n</code></pre> <pre><code># Generate text (pipe syntax)\necho \"hello world\" | st\n\n# Stream output (default)  \necho \"explain recursion\" | st\n\n# Wait for complete output\necho \"explain recursion\" | st --wait\n\n# Get embeddings\necho \"machine learning\" | st embed\n\n# Start daemon for faster responses\nst daemon start\n</code></pre>"},{"location":"#how-it-works","title":"\ud83d\udd27 How It Works","text":"<p>SteadyText achieves determinism via:</p> <ul> <li>Customizable seeds: Control determinism with a <code>seed</code> parameter, while still defaulting to <code>42</code>.</li> <li>Greedy decoding: Always chooses highest-probability token</li> <li>Frecency cache: LRU cache with frequency counting\u2014popular prompts stay cached longer</li> <li>Quantized models: 8-bit quantization ensures identical results across platforms</li> </ul> <p>This means <code>generate(\"hello\")</code> returns the exact same 512 tokens on any machine, every single time.</p>"},{"location":"#ecosystem","title":"\ud83c\udf10 Ecosystem","text":"<p>SteadyText is more than just a library. It's a full ecosystem for deterministic AI:</p> <ul> <li>Python Library: The core <code>steadytext</code> library for programmatic use in your applications.</li> <li>Command-Line Interface (CLI): A powerful <code>st</code> command to use SteadyText from your shell for scripting and automation.</li> <li>Zsh Plugin: Supercharge your shell with AI-powered command suggestions and history search.</li> <li>PostgreSQL Extension: Run deterministic AI functions directly within your PostgreSQL database.</li> <li>Cloudflare Worker: Deploy SteadyText to the edge with a Cloudflare Worker for distributed, low-latency applications.</li> </ul>"},{"location":"#daemon-mode-v13","title":"Daemon Mode (v1.3+)","text":"<p>SteadyText includes a daemon mode that keeps models loaded in memory for instant responses:</p> <ul> <li>160x faster first request: No model loading overhead</li> <li>Persistent cache: Shared across all operations</li> <li>Automatic fallback: Works without daemon if unavailable</li> <li>Zero configuration: Daemon used by default when available</li> </ul> <pre><code># Start daemon\nst daemon start\n\n# Check status\nst daemon status\n\n# All operations now use daemon automatically\necho \"hello\" | st  # Instant response!\n</code></pre>"},{"location":"#faiss-indexing","title":"FAISS Indexing","text":"<p>Create and search vector indexes for retrieval-augmented generation:</p> <pre><code># Create index from documents\nst index create *.txt --output docs.faiss\n\n# Search index\nst index search docs.faiss \"query text\" --top-k 5\n\n# Use with generation (automatic with default.faiss)\necho \"explain this error\" | st --index-file docs.faiss\n</code></pre>"},{"location":"#installation-models","title":"\ud83d\udce6 Installation &amp; Models","text":"<p>Install stable release:</p> <pre><code># Using UV (recommended - 10-100x faster)\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre>"},{"location":"#models","title":"Models","text":"<p>Current models (v2.0.0+):</p> <ul> <li>Generation: <code>Gemma-3n-E2B-it-Q8_0.gguf</code> (2.0GB) - Gemma-3n-2B (default)</li> <li>Generation: <code>Gemma-3n-E4B-it-Q8_0.gguf</code> (4.2GB) - Gemma-3n-4B (optional)</li> <li>Embeddings: <code>Qwen3-Embedding-0.6B-Q8_0.gguf</code> (610MB)</li> </ul> <p>Version Stability</p> <p>Each major version will use a fixed set of models only, so that only forced upgrades from pip will change the models (and the deterministic output)</p>"},{"location":"#use-cases","title":"\ud83c\udfaf Use Cases","text":"<p>Perfect for</p> <ul> <li>Testing AI features: Reliable asserts that never flake</li> <li>Deterministic CLI tooling: Consistent outputs for automation  </li> <li>Reproducible documentation: Examples that always work</li> <li>Offline/dev/staging environments: No API keys needed</li> <li>Semantic caching and embedding search: Fast similarity matching</li> </ul> <p>Not ideal for</p> <ul> <li>Creative or conversational tasks</li> <li>Latest knowledge queries  </li> <li>Large-scale chatbot deployments</li> </ul>"},{"location":"#examples","title":"\ud83d\udccb Examples","text":"<p>Use SteadyText in tests or CLI tools for consistent, reproducible results:</p> <pre><code># Testing with reliable assertions\ndef test_ai_function():\n    result = my_ai_function(\"test input\")\n    expected = steadytext.generate(\"expected output for 'test input'\")\n    assert result == expected  # No flakes!\n\n# CLI tools with consistent outputs\nimport click\n\n@click.command()\ndef ai_tool(prompt):\n    print(steadytext.generate(prompt))\n</code></pre> <p>\ud83d\udcc2 More examples \u2192</p>"},{"location":"#api-overview","title":"\ud83d\udd0d API Overview","text":"<pre><code># Text generation\nsteadytext.generate(prompt: str) -&gt; str\nsteadytext.generate(prompt, return_logprobs=True)\n\n# Streaming generation\nsteadytext.generate_iter(prompt: str)\n\n# Embeddings\nsteadytext.embed(text: str | List[str]) -&gt; np.ndarray\n\n# Model preloading\nsteadytext.preload_models(verbose=True)\n</code></pre> <p>\ud83d\udcda Full API Documentation \u2192</p>"},{"location":"#configuration","title":"\ud83d\udd27 Configuration","text":"<p>Control caching behavior via environment variables:</p> <pre><code># Generation cache (default: 256 entries, 50MB)\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50\n\n# Embedding cache (default: 512 entries, 100MB)\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100\n</code></pre>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! See Contributing Guide for guidelines.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<ul> <li>Code: MIT</li> <li>Models: MIT (Qwen3)</li> </ul> <p>Built with \u2764\ufe0f for developers tired of flaky AI tests.</p>"},{"location":"api/","title":"SteadyText API Documentation","text":"<p>This document provides detailed API documentation for SteadyText.</p>"},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#text-generation","title":"Text Generation","text":""},{"location":"api/#steadytextgenerate","title":"<code>steadytext.generate()</code>","text":"<pre><code>def generate(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\",\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED,\n    schema: Optional[Union[Dict[str, Any], type, object]] = None,\n    regex: Optional[str] = None,\n    choices: Optional[List[str]] = None,\n    response_format: Optional[Dict[str, Any]] = None,\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre> <p>Generate deterministic text from a prompt, with optional structured output.</p> <p>Parameters: - <code>prompt</code> (str): The input text to generate from - <code>max_new_tokens</code> (int, optional): Maximum number of tokens to generate (default: 512) - <code>return_logprobs</code> (bool): If True, returns log probabilities along with the text - <code>eos_string</code> (str): Custom end-of-sequence string to stop generation. Use \"[EOS]\" for model's default stop tokens - <code>model</code> (str, optional): Model name from built-in registry (deprecated - use <code>size</code> parameter instead) - <code>model_repo</code> (str, optional): Custom Hugging Face repository ID (e.g., \"ggml-org/gemma-3n-E2B-it-GGUF\") - <code>model_filename</code> (str, optional): Custom model filename (e.g., \"gemma-3n-E2B-it-Q8_0.gguf\") - <code>size</code> (str, optional): Size shortcut for Gemma-3n models: \"small\" (2B, default), or \"large\" (4B) - recommended approach - <code>seed</code> (int): Random seed for deterministic generation (default: 42) - <code>schema</code> (Union[Dict, type, object], optional): JSON schema, Pydantic model, or Python type for structured JSON output. - <code>regex</code> (str, optional): A regular expression to constrain the output. - <code>choices</code> (List[str], optional): A list of strings to choose from. - <code>response_format</code> (Dict, optional): A dictionary specifying the output format (e.g., <code>{\"type\": \"json_object\"}</code>).</p> <p>Returns: - If <code>return_logprobs=False</code>: A string containing the generated text. For structured JSON output, the JSON is wrapped in <code>&lt;json-output&gt;</code> tags. - If <code>return_logprobs=True</code>: A tuple of (text, logprobs_dict)</p> <p>Example: <pre><code># Simple generation\ntext = steadytext.generate(\"Write a Python function\")\n\n# With custom seed for reproducible results\ntext1 = steadytext.generate(\"Write a story\", seed=123)\ntext2 = steadytext.generate(\"Write a story\", seed=123)  # Same result as text1\ntext3 = steadytext.generate(\"Write a story\", seed=456)  # Different result\n\n# With log probabilities\ntext, logprobs = steadytext.generate(\"Explain AI\", return_logprobs=True)\n\n# With custom stop string and seed\ntext = steadytext.generate(\"List items until END\", eos_string=\"END\", seed=789)\n\n# Limit output length\ntext = steadytext.generate(\"Quick summary\", max_new_tokens=100)\n\n# Using size parameter (recommended)\ntext = steadytext.generate(\"Quick task\", size=\"small\")   # Uses Gemma-3n-2B\ntext = steadytext.generate(\"Complex task\", size=\"large\")  # Uses Gemma-3n-4B\n\n# Using a custom model with seed\ntext = steadytext.generate(\n    \"Write code\",\n    model_repo=\"ggml-org/gemma-3n-E4B-it-GGUF\",\n    model_filename=\"gemma-3n-E4B-it-Q8_0.gguf\",\n    seed=999\n)\n\n# Structured generation with a regex pattern\nphone_number = steadytext.generate(\"My phone number is: \", regex=r\"\\d{3}-\\d{3}-\\d{4}\")\n\n# Structured generation with choices\nmood = steadytext.generate(\"I feel\", choices=[\"happy\", \"sad\", \"angry\"])\n\n# Structured generation with a JSON schema\nfrom pydantic import BaseModel\nclass User(BaseModel):\n    name: str\n    age: int\n\nuser_json = steadytext.generate(\"Create a user named John, age 30\", schema=User)\n# user_json will contain: '... &lt;json-output&gt;{\"name\": \"John\", \"age\": 30}&lt;/json-output&gt;'\n</code></pre></p>"},{"location":"api/#steadytextgenerate_iter","title":"<code>steadytext.generate_iter()</code>","text":"<pre><code>def generate_iter(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False,\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre> <p>Generate text iteratively, yielding tokens as they are produced.</p> <p>Parameters: - <code>prompt</code> (str): The input text to generate from - <code>max_new_tokens</code> (int, optional): Maximum number of tokens to generate (default: 512) - <code>eos_string</code> (str): Custom end-of-sequence string to stop generation. Use \"[EOS]\" for model's default stop tokens - <code>include_logprobs</code> (bool): If True, yields tuples of (token, logprobs) instead of just tokens - <code>model</code> (str, optional): Model name from built-in registry (deprecated - use <code>size</code> parameter instead) - <code>model_repo</code> (str, optional): Custom Hugging Face repository ID - <code>model_filename</code> (str, optional): Custom model filename - <code>size</code> (str, optional): Size shortcut for Gemma-3n models: \"small\" (2B, default), or \"large\" (4B) - recommended approach - <code>seed</code> (int): Random seed for deterministic generation (default: 42)</p> <p>Yields: - str: Text tokens/words as they are generated (if <code>include_logprobs=False</code>) - Tuple[str, Optional[Dict[str, Any]]]: (token, logprobs) tuples (if <code>include_logprobs=True</code>)</p> <p>Example: <pre><code># Simple streaming\nfor token in steadytext.generate_iter(\"Tell me a story\"):\n    print(token, end=\"\", flush=True)\n\n# With custom seed for reproducible streaming\nfor token in steadytext.generate_iter(\"Tell me a story\", seed=123):\n    print(token, end=\"\", flush=True)\n\n# With custom stop string and seed\nfor token in steadytext.generate_iter(\"Generate until STOP\", eos_string=\"STOP\", seed=456):\n    print(token, end=\"\", flush=True)\n\n# With log probabilities\nfor token, logprobs in steadytext.generate_iter(\"Explain AI\", include_logprobs=True):\n    print(token, end=\"\", flush=True)\n\n# Stream with size parameter and custom length\nfor token in steadytext.generate_iter(\"Quick response\", size=\"small\", max_new_tokens=50):\n    print(token, end=\"\", flush=True)\n\nfor token in steadytext.generate_iter(\"Complex task\", size=\"large\", seed=789):\n    print(token, end=\"\", flush=True)\n</code></pre></p>"},{"location":"api/#structured-generation-v241","title":"Structured Generation (v2.4.1+)","text":"<p>These are convenience functions for structured generation using llama.cpp's native grammar support.</p>"},{"location":"api/#steadytextgenerate_json","title":"<code>steadytext.generate_json()</code>","text":"<pre><code>def generate_json(\n    prompt: str,\n    schema: Union[Dict[str, Any], type, object],\n    max_tokens: int = 512,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generates a JSON string that conforms to the provided schema.</p>"},{"location":"api/#steadytextgenerate_regex","title":"<code>steadytext.generate_regex()</code>","text":"<pre><code>def generate_regex(\n    prompt: str,\n    pattern: str,\n    max_tokens: int = 512,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generates a string that matches the given regular expression.</p>"},{"location":"api/#steadytextgenerate_choice","title":"<code>steadytext.generate_choice()</code>","text":"<pre><code>def generate_choice(\n    prompt: str,\n    choices: List[str],\n    max_tokens: int = 512,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generates a string that is one of the provided choices.</p>"},{"location":"api/#steadytextgenerate_format","title":"<code>steadytext.generate_format()</code>","text":"<pre><code>def generate_format(\n    prompt: str,\n    format_type: type,\n    max_tokens: int = 512,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generates a string that conforms to a basic Python type (e.g., <code>int</code>, <code>float</code>, <code>bool</code>).</p>"},{"location":"api/#embeddings","title":"Embeddings","text":""},{"location":"api/#steadytextembed","title":"<code>steadytext.embed()</code>","text":"<pre><code>def embed(text_input: Union[str, List[str]], seed: int = DEFAULT_SEED) -&gt; np.ndarray\n</code></pre> <p>Create deterministic embeddings for text input.</p> <p>Parameters: - <code>text_input</code> (Union[str, List[str]]): A string or list of strings to embed - <code>seed</code> (int): Random seed for deterministic embedding generation (default: 42)</p> <p>Returns: - np.ndarray: A 1024-dimensional L2-normalized float32 numpy array</p> <p>Example: <pre><code># Single string\nvec = steadytext.embed(\"Hello world\")\n\n# With custom seed for reproducible embeddings\nvec1 = steadytext.embed(\"Hello world\", seed=123)\nvec2 = steadytext.embed(\"Hello world\", seed=123)  # Same result as vec1\nvec3 = steadytext.embed(\"Hello world\", seed=456)  # Different result\n\n# Multiple strings (returns a single, averaged embedding)\nvec = steadytext.embed([\"Hello\", \"world\"])\n\n# Multiple strings with custom seed\nvec = steadytext.embed([\"Hello\", \"world\"], seed=789)\n</code></pre></p>"},{"location":"api/#document-reranking-v230","title":"Document Reranking (v2.3.0+)","text":""},{"location":"api/#steadytextrerank","title":"<code>steadytext.rerank()</code>","text":"<pre><code>def rerank(\n    query: str,\n    documents: Union[str, List[str]],\n    task: str = \"Given a web search query, retrieve relevant passages that answer the query\",\n    return_scores: bool = True,\n    seed: int = DEFAULT_SEED\n) -&gt; Union[List[Tuple[str, float]], List[str]]\n</code></pre> <p>Rerank documents based on their relevance to a query using the Qwen3-Reranker-4B model.</p> <p>Parameters: - <code>query</code> (str): The search query to rerank documents against - <code>documents</code> (Union[str, List[str]]): Single document or list of documents to rerank - <code>task</code> (str): Description of the reranking task for better results (default: \"Given a web search query, retrieve relevant passages that answer the query\") - <code>return_scores</code> (bool): If True, return (document, score) tuples; if False, just documents (default: True) - <code>seed</code> (int): Random seed for deterministic reranking (default: 42)</p> <p>Returns: - If <code>return_scores=True</code>: List[Tuple[str, float]] - List of (document, score) tuples sorted by relevance (highest score first) - If <code>return_scores=False</code>: List[str] - List of documents sorted by relevance (highest score first)</p> <p>Example: <pre><code># Basic reranking\ndocuments = [\n    \"Python is a programming language\",\n    \"Cats are cute animals\",\n    \"Python snakes are found in Asia\"\n]\nresults = steadytext.rerank(\"Python programming\", documents)\n# Returns documents sorted by relevance to \"Python programming\"\n\n# With custom task description\nresults = steadytext.rerank(\n    \"customer support issue\",\n    support_tickets,\n    task=\"support ticket prioritization\",\n    seed=123\n)\n\n# Domain-specific reranking\nlegal_results = steadytext.rerank(\n    \"contract breach\",\n    legal_documents,\n    task=\"legal document retrieval for case research\"\n)\n\n# Get just documents without scores\nsorted_docs = steadytext.rerank(\n    \"machine learning\",\n    documents,\n    return_scores=False\n)\n# Returns: [\"ML document 1\", \"ML document 2\", ...]\n</code></pre></p> <p>Notes: - Uses yes/no token logits for binary relevance scoring - Falls back to simple word overlap scoring when model is unavailable - Results are cached for identical query-document pairs - Task descriptions help the model understand the reranking context</p>"},{"location":"api/#utility-functions","title":"Utility Functions","text":""},{"location":"api/#steadytextpreload_models","title":"<code>steadytext.preload_models()</code>","text":"<pre><code>def preload_models(verbose: bool = False) -&gt; None\n</code></pre> <p>Preload models before first use to avoid delays.</p> <p>Parameters: - <code>verbose</code> (bool): If True, prints progress information</p> <p>Example: <pre><code># Silent preloading\nsteadytext.preload_models()\n\n# Verbose preloading\nsteadytext.preload_models(verbose=True)\n</code></pre></p>"},{"location":"api/#steadytextget_model_cache_dir","title":"<code>steadytext.get_model_cache_dir()</code>","text":"<pre><code>def get_model_cache_dir() -&gt; str\n</code></pre> <p>Get the path to the model cache directory.</p> <p>Returns: - str: The absolute path to the model cache directory</p> <p>Example: <pre><code>cache_dir = steadytext.get_model_cache_dir()\nprint(f\"Models are stored in: {cache_dir}\")\n</code></pre></p>"},{"location":"api/#constants","title":"Constants","text":""},{"location":"api/#steadytextdefault_seed","title":"<code>steadytext.DEFAULT_SEED</code>","text":"<ul> <li>Type: int</li> <li>Value: 42</li> <li>Description: The default random seed used for deterministic generation. Can be overridden by the <code>seed</code> parameter in generation and embedding functions.</li> </ul>"},{"location":"api/#steadytextgeneration_max_new_tokens","title":"<code>steadytext.GENERATION_MAX_NEW_TOKENS</code>","text":"<ul> <li>Type: int</li> <li>Value: 512</li> <li>Description: Maximum number of tokens to generate</li> </ul>"},{"location":"api/#steadytextembedding_dimension","title":"<code>steadytext.EMBEDDING_DIMENSION</code>","text":"<ul> <li>Type: int</li> <li>Value: 1024</li> <li>Description: The dimensionality of embedding vectors</li> </ul>"},{"location":"api/#environment-variables","title":"Environment Variables","text":""},{"location":"api/#generation-cache","title":"Generation Cache","text":"<ul> <li><code>STEADYTEXT_GENERATION_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 256)</li> <li><code>STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 50.0)</li> </ul>"},{"location":"api/#embedding-cache","title":"Embedding Cache","text":"<ul> <li><code>STEADYTEXT_EMBEDDING_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 512)</li> <li><code>STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 100.0)</li> </ul>"},{"location":"api/#reranking-cache-v230","title":"Reranking Cache (v2.3.0+)","text":"<ul> <li><code>STEADYTEXT_RERANKING_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 256)</li> <li><code>STEADYTEXT_RERANKING_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 25.0)</li> </ul>"},{"location":"api/#model-downloads","title":"Model Downloads","text":"<ul> <li><code>STEADYTEXT_ALLOW_MODEL_DOWNLOADS</code>: Set to \"true\" to allow automatic model downloads (mainly used for testing)</li> </ul>"},{"location":"api/#model-switching-v200","title":"Model Switching (v2.0.0+)","text":"<p>SteadyText v2.0.0+ supports model switching with the Gemma-3n model family, allowing you to use different model sizes for different tasks.</p>"},{"location":"api/#current-model-registry-v200","title":"Current Model Registry (v2.0.0+)","text":"<p>The following models are available:</p> Size Parameter Model Name Parameters Use Case <code>small</code> <code>gemma-3n-2b</code> 2B Default, fast tasks <code>large</code> <code>gemma-3n-4b</code> 4B High quality, complex tasks"},{"location":"api/#model-selection-methods","title":"Model Selection Methods","text":"<ol> <li>Using size parameter (recommended): <code>generate(\"prompt\", size=\"large\")</code></li> <li>Custom models: <code>generate(\"prompt\", model_repo=\"...\", model_filename=\"...\")</code></li> <li>Environment variables: Set <code>STEADYTEXT_DEFAULT_SIZE</code> or custom model variables</li> </ol>"},{"location":"api/#deprecated-models-v1x","title":"Deprecated Models (v1.x)","text":"<p>Note: The following models were available in SteadyText v1.x but are deprecated in v2.0.0+: - <code>qwen3-1.7b</code>, <code>qwen3-4b</code>, <code>qwen3-8b</code> - <code>qwen2.5-0.5b</code>, <code>qwen2.5-1.5b</code>, <code>qwen2.5-3b</code>, <code>qwen2.5-7b</code></p> <p>Use the <code>size</code> parameter with Gemma-3n models instead.</p>"},{"location":"api/#model-caching","title":"Model Caching","text":"<ul> <li>Models are cached after first load for efficient switching</li> <li>Multiple models can be loaded simultaneously</li> <li>Use <code>clear_model_cache()</code> to free memory if needed</li> </ul>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>All functions are designed to never raise exceptions during normal operation. If models cannot be loaded, deterministic fallback functions are used:</p> <ul> <li>Text generation fallback: Uses hash-based word selection to generate pseudo-random but deterministic text</li> <li>Embedding fallback: Returns zero vectors of the correct dimension</li> </ul> <p>This ensures that your code never breaks, even in environments where models cannot be downloaded or loaded.</p>"},{"location":"architecture/","title":"SteadyText Architecture","text":"<p>This document provides a comprehensive overview of SteadyText's architecture, design decisions, and implementation details.</p>"},{"location":"architecture/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Core Principles</li> <li>System Architecture</li> <li>Component Architecture</li> <li>Data Flow</li> <li>Model Architecture</li> <li>Caching Architecture</li> <li>Daemon Architecture</li> <li>Extension Points</li> <li>Performance Architecture</li> <li>Security Architecture</li> <li>Design Patterns</li> <li>Technology Stack</li> </ul>"},{"location":"architecture/#overview","title":"Overview","text":"<p>SteadyText is designed as a deterministic AI text generation and embedding library with a focus on reproducibility, performance, and reliability. The architecture supports multiple deployment modes (direct, daemon, PostgreSQL extension) while maintaining consistent behavior across all interfaces.</p>"},{"location":"architecture/#key-architectural-goals","title":"Key Architectural Goals","text":"<ol> <li>Determinism: Same input always produces same output</li> <li>Performance: Sub-second response times with caching</li> <li>Reliability: Never fails, graceful degradation</li> <li>Simplicity: Minimal configuration, intuitive APIs</li> <li>Extensibility: Support for custom models and integrations</li> </ol>"},{"location":"architecture/#core-principles","title":"Core Principles","text":""},{"location":"architecture/#1-never-fail-philosophy","title":"1. Never Fail Philosophy","text":"<pre><code># Traditional approach (can fail)\ndef generate_text(prompt):\n    if not model_loaded:\n        raise ModelNotLoadedError()\n    return model.generate(prompt)\n\n# SteadyText approach (never fails)\ndef generate_text(prompt):\n    if not model_loaded:\n        return None  # v2.1.0+ behavior\n    return model.generate(prompt)\n</code></pre>"},{"location":"architecture/#2-deterministic-by-design","title":"2. Deterministic by Design","text":"<p>All operations use fixed seeds and deterministic algorithms:</p> <pre><code># Seed propagation through the stack\nDEFAULT_SEED = 42\n\ndef set_deterministic_environment(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n</code></pre>"},{"location":"architecture/#3-lazy-loading","title":"3. Lazy Loading","text":"<p>Models are loaded only when first used:</p> <pre><code>class ModelLoader:\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance.model = None\n        return cls._instance\n\n    def get_model(self):\n        if self.model is None:\n            self.model = self._load_model()\n        return self.model\n</code></pre>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":""},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    User Applications                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Interface Layer                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Python API \u2502   CLI Tools   \u2502  PostgreSQL  \u2502    REST   \u2502 \u2502\n\u2502  \u2502             \u2502  (st/steadytext)\u2502  Extension  \u2502    API    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                     Core Layer                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502            Unified Processing Engine                  \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502  \u2502  \u2502  Generator   \u2502    Embedder    \u2502  Vector Ops    \u2502 \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                  Infrastructure Layer                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Model Loader \u2502  Cache Manager  \u2502   Daemon Service    \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Storage Layer                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Model Files  \u2502  Cache Files    \u2502   Index Files       \u2502   \u2502\n\u2502  \u2502   (GGUF)     \u2502   (SQLite)      \u2502    (FAISS)         \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#deployment-architecture","title":"Deployment Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Deployment Options                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                      \u2502\n\u2502  1. Direct Mode (Default)                           \u2502\n\u2502     \u2514\u2500&gt; Application \u2192 SteadyText \u2192 Models           \u2502\n\u2502                                                      \u2502\n\u2502  2. Daemon Mode (Recommended for Production)        \u2502\n\u2502     \u2514\u2500&gt; Application \u2192 Client \u2192 Daemon \u2192 Models      \u2502\n\u2502                                                      \u2502\n\u2502  3. PostgreSQL Extension                             \u2502\n\u2502     \u2514\u2500&gt; SQL \u2192 pg_steadytext \u2192 Daemon \u2192 Models       \u2502\n\u2502                                                      \u2502\n\u2502  4. Container/Kubernetes                             \u2502\n\u2502     \u2514\u2500&gt; Service \u2192 Pod \u2192 Container \u2192 Daemon          \u2502\n\u2502                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#component-architecture","title":"Component Architecture","text":""},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-generator-component","title":"1. Generator Component","text":"<pre><code># steadytext/core/generator.py\nclass DeterministicGenerator:\n    \"\"\"Core text generation component.\"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.config = GenerationConfig()\n        self.cache = get_cache_manager().generation_cache\n\n    def generate(self, prompt: str, seed: int = 42) -&gt; Optional[str]:\n        # Check cache first\n        cache_key = self._compute_cache_key(prompt, seed)\n        if cached := self.cache.get(cache_key):\n            return cached\n\n        # Load model lazily\n        if self.model is None:\n            self.model = ModelLoader().get_generation_model()\n            if self.model is None:\n                return None  # v2.1.0+ behavior\n\n        # Generate with deterministic settings\n        result = self._generate_deterministic(prompt, seed)\n\n        # Cache result\n        self.cache.set(cache_key, result)\n\n        return result\n</code></pre>"},{"location":"architecture/#2-embedder-component","title":"2. Embedder Component","text":"<pre><code># steadytext/core/embedder.py\nclass DeterministicEmbedder:\n    \"\"\"Core embedding component.\"\"\"\n\n    def embed(self, text: str, seed: int = 42) -&gt; Optional[np.ndarray]:\n        # Similar pattern: cache \u2192 model \u2192 generate \u2192 cache\n        cache_key = self._compute_cache_key(text, seed)\n        if cached := self.cache.get(cache_key):\n            return cached\n\n        if self.model is None:\n            self.model = ModelLoader().get_embedding_model()\n            if self.model is None:\n                return None\n\n        # Generate L2-normalized embeddings\n        embedding = self._embed_deterministic(text, seed)\n        embedding = self._normalize_l2(embedding)\n\n        self.cache.set(cache_key, embedding)\n        return embedding\n</code></pre>"},{"location":"architecture/#3-cache-manager","title":"3. Cache Manager","text":"<pre><code># steadytext/cache_manager.py\nclass CacheManager:\n    \"\"\"Centralized cache management.\"\"\"\n\n    _instance = None\n\n    def __init__(self):\n        self.generation_cache = FrecencyCache(\n            capacity=int(os.getenv('STEADYTEXT_GENERATION_CACHE_CAPACITY', 256)),\n            max_size_mb=float(os.getenv('STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB', 50))\n        )\n        self.embedding_cache = FrecencyCache(\n            capacity=int(os.getenv('STEADYTEXT_EMBEDDING_CACHE_CAPACITY', 512)),\n            max_size_mb=float(os.getenv('STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB', 100))\n        )\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#generation-flow","title":"Generation Flow","text":"<pre><code>User Request\n    \u2502\n    \u25bc\nAPI Layer (generate())\n    \u2502\n    \u251c\u2500&gt; Check Input Validity\n    \u2502\n    \u251c\u2500&gt; Compute Cache Key\n    \u2502\n    \u251c\u2500&gt; Check Cache \u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                     \u2502 (hit)\n    \u2502 (miss)              \u25bc\n    \u25bc                 Return Cached\nModel Loading\n    \u2502\n    \u251c\u2500&gt; Check Model Status\n    \u2502\n    \u2502 (not loaded)\n    \u251c\u2500&gt; Load Model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                     \u2502 (fail)\n    \u2502 (loaded)            \u25bc\n    \u25bc                 Return None\nGenerate Text\n    \u2502\n    \u251c\u2500&gt; Set Deterministic Seed\n    \u2502\n    \u251c\u2500&gt; Configure Sampling\n    \u2502\n    \u251c\u2500&gt; Run Inference\n    \u2502\n    \u25bc\nCache Result\n    \u2502\n    \u25bc\nReturn Result\n</code></pre>"},{"location":"architecture/#embedding-flow","title":"Embedding Flow","text":"<pre><code>Text Input \u2192 Tokenization \u2192 Model Inference \u2192 Raw Embedding\n                                                    \u2502\n                                                    \u25bc\n                                            L2 Normalization\n                                                    \u2502\n                                                    \u25bc\n                                            1024-dim Vector\n                                                    \u2502\n                                                    \u25bc\n                                               Cache &amp; Return\n</code></pre>"},{"location":"architecture/#model-architecture","title":"Model Architecture","text":""},{"location":"architecture/#model-selection","title":"Model Selection","text":"<pre><code>MODEL_REGISTRY = {\n    'generation': {\n        'small': {\n            'repo': 'ggml-org/gemma-3n-E2B-it-GGUF',\n            'filename': 'gemma-3n-E2B-it-Q8_0.gguf',\n            'context_length': 8192,\n            'vocab_size': 256128\n        },\n        'large': {\n            'repo': 'ggml-org/gemma-3n-E4B-it-GGUF',\n            'filename': 'gemma-3n-E4B-it-Q8_0.gguf',\n            'context_length': 8192,\n            'vocab_size': 256128\n        }\n    },\n    'embedding': {\n        'default': {\n            'repo': 'Qwen/Qwen3-Embedding-0.6B-GGUF',\n            'filename': 'qwen3-embedding-0.6b-q8_0.gguf',\n            'dimension': 1024\n        }\n    }\n}\n</code></pre>"},{"location":"architecture/#model-loading-strategy","title":"Model Loading Strategy","text":"<ol> <li>Lazy Loading: Models loaded on first use</li> <li>Singleton Pattern: One model instance per type</li> <li>Thread Safety: Locks prevent concurrent loading</li> <li>Graceful Fallback: Returns None if loading fails</li> </ol>"},{"location":"architecture/#model-configuration","title":"Model Configuration","text":"<pre><code>GENERATION_CONFIG = {\n    'max_tokens': 512,\n    'temperature': 0.0,  # Deterministic\n    'top_k': 1,          # Greedy decoding\n    'top_p': 1.0,\n    'repeat_penalty': 1.0,\n    'seed': 42,\n    'n_threads': 4,\n    'n_batch': 512,\n    'use_mlock': True,\n    'use_mmap': True\n}\n</code></pre>"},{"location":"architecture/#caching-architecture","title":"Caching Architecture","text":""},{"location":"architecture/#cache-design","title":"Cache Design","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Cache Manager                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 Generation   \u2502    \u2502  Embedding   \u2502      \u2502\n\u2502  \u2502   Cache      \u2502    \u2502    Cache     \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502         \u2502                    \u2502               \u2502\n\u2502         \u25bc                    \u25bc               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502     Frecency Algorithm          \u2502        \u2502\n\u2502  \u2502  (Frequency + Recency scoring)  \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                    \u2502                         \u2502\n\u2502                    \u25bc                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502    SQLite Backend (Disk)        \u2502        \u2502\n\u2502  \u2502  - Thread-safe                  \u2502        \u2502\n\u2502  \u2502  - Persistent                   \u2502        \u2502\n\u2502  \u2502  - Size-limited                 \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#cache-key-generation","title":"Cache Key Generation","text":"<pre><code>def compute_cache_key(prompt: str, seed: int, **kwargs) -&gt; str:\n    \"\"\"Generate deterministic cache key.\"\"\"\n    # Include all parameters that affect output\n    key_parts = [\n        prompt,\n        str(seed),\n        str(kwargs.get('max_tokens', 512)),\n        str(kwargs.get('eos_string', '[EOS]'))\n    ]\n\n    # Use SHA256 for consistent hashing\n    key_string = '|'.join(key_parts)\n    return hashlib.sha256(key_string.encode()).hexdigest()\n</code></pre>"},{"location":"architecture/#cache-eviction-strategy","title":"Cache Eviction Strategy","text":"<ol> <li>Frecency Score: Combines frequency and recency</li> <li>Size Limits: Respects configured max size</li> <li>TTL: Optional time-to-live for entries</li> <li>Atomic Operations: Thread-safe updates</li> </ol>"},{"location":"architecture/#daemon-architecture","title":"Daemon Architecture","text":""},{"location":"architecture/#daemon-design","title":"Daemon Design","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Daemon Process                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502      ZeroMQ REP Server           \u2502       \u2502\n\u2502  \u2502   Listening on tcp://*:5557      \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502               \u2502                              \u2502\n\u2502               \u25bc                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502      Request Router              \u2502       \u2502\n\u2502  \u2502  - generate                      \u2502       \u2502\n\u2502  \u2502  - generate_iter                 \u2502       \u2502\n\u2502  \u2502  - embed                         \u2502       \u2502\n\u2502  \u2502  - ping                          \u2502       \u2502\n\u2502  \u2502  - shutdown                      \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502               \u2502                              \u2502\n\u2502               \u25bc                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502    Model Instance Pool           \u2502       \u2502\n\u2502  \u2502  - Gemma-3n (generation)         \u2502       \u2502\n\u2502  \u2502  - Qwen3 (embedding)             \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#communication-protocol","title":"Communication Protocol","text":"<pre><code># Request format\n{\n    \"id\": \"unique-request-id\",\n    \"type\": \"generate\",\n    \"prompt\": \"Hello world\",\n    \"seed\": 42,\n    \"max_tokens\": 512\n}\n\n# Response format\n{\n    \"id\": \"unique-request-id\",\n    \"success\": true,\n    \"result\": \"Generated text...\",\n    \"cached\": false,\n    \"error\": null\n}\n</code></pre>"},{"location":"architecture/#connection-management","title":"Connection Management","text":"<pre><code>class DaemonClient:\n    def __init__(self, host='127.0.0.1', port=5557):\n        self.context = zmq.Context()\n        self.socket = None\n        self.connected = False\n\n    def connect(self):\n        \"\"\"Establish connection with retry logic.\"\"\"\n        self.socket = self.context.socket(zmq.REQ)\n        self.socket.setsockopt(zmq.LINGER, 0)\n        self.socket.setsockopt(zmq.RCVTIMEO, 5000)\n        self.socket.connect(f\"tcp://{self.host}:{self.port}\")\n\n        # Test connection\n        if self._ping():\n            self.connected = True\n        else:\n            self._fallback_to_direct()\n</code></pre>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#custom-models","title":"Custom Models","text":"<pre><code># Register custom model\nfrom steadytext.models import register_model\n\nregister_model(\n    'custom-gen',\n    repo='myorg/custom-model-GGUF',\n    filename='model.gguf',\n    model_type='generation'\n)\n\n# Use custom model\ntext = generate(\"Hello\", model='custom-gen')\n</code></pre>"},{"location":"architecture/#custom-embedders","title":"Custom Embedders","text":"<pre><code>class CustomEmbedder:\n    def embed(self, text: str) -&gt; np.ndarray:\n        # Custom embedding logic\n        return np.random.randn(1024)\n\n# Register embedder\nsteadytext.register_embedder('custom', CustomEmbedder())\n</code></pre>"},{"location":"architecture/#plugin-system","title":"Plugin System","text":"<pre><code># Future: Plugin architecture\nclass SteadyTextPlugin:\n    def on_generate_start(self, prompt: str): pass\n    def on_generate_complete(self, result: str): pass\n    def on_embed_start(self, text: str): pass\n    def on_embed_complete(self, embedding: np.ndarray): pass\n\n# Register plugin\nsteadytext.register_plugin(MyPlugin())\n</code></pre>"},{"location":"architecture/#performance-architecture","title":"Performance Architecture","text":""},{"location":"architecture/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li> <p>Model Preloading <pre><code># Preload models at startup\nsteadytext.preload_models()\n</code></pre></p> </li> <li> <p>Connection Pooling <pre><code># Daemon connection pool\npool = ConnectionPool(size=10)\n</code></pre></p> </li> <li> <p>Batch Processing <pre><code># Process multiple requests efficiently\nresults = steadytext.batch_generate(prompts)\n</code></pre></p> </li> <li> <p>Memory Mapping <pre><code># GGUF models use mmap for efficiency\nconfig = {'use_mmap': True, 'use_mlock': True}\n</code></pre></p> </li> </ol>"},{"location":"architecture/#benchmarking-architecture","title":"Benchmarking Architecture","text":"<pre><code>class BenchmarkFramework:\n    def __init__(self):\n        self.metrics = {\n            'latency': [],\n            'throughput': [],\n            'memory': [],\n            'cache_hits': []\n        }\n\n    def run_benchmark(self, workload):\n        \"\"\"Execute standardized benchmark.\"\"\"\n        for operation in workload:\n            with self.measure():\n                operation.execute()\n</code></pre>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/#security-layers","title":"Security Layers","text":"<ol> <li> <p>Input Validation <pre><code>def validate_input(prompt: str) -&gt; bool:\n    # Length limits\n    if len(prompt) &gt; MAX_PROMPT_LENGTH:\n        return False\n    # Character validation\n    if contains_invalid_chars(prompt):\n        return False\n    return True\n</code></pre></p> </li> <li> <p>Process Isolation</p> </li> <li>Daemon runs in separate process</li> <li>Limited system access</li> <li> <p>Resource quotas</p> </li> <li> <p>Communication Security</p> </li> <li>Local-only ZeroMQ by default</li> <li>Optional TLS for remote connections</li> <li> <p>Request authentication</p> </li> <li> <p>Model Security</p> </li> <li>Verified model checksums</li> <li>Restricted model loading paths</li> <li>No arbitrary code execution</li> </ol>"},{"location":"architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/#1-singleton-pattern","title":"1. Singleton Pattern","text":"<p>Used for model instances and cache manager:</p> <pre><code>class SingletonMeta(type):\n    _instances = {}\n    _lock = threading.Lock()\n\n    def __call__(cls, *args, **kwargs):\n        with cls._lock:\n            if cls not in cls._instances:\n                cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n</code></pre>"},{"location":"architecture/#2-factory-pattern","title":"2. Factory Pattern","text":"<p>For creating different model types:</p> <pre><code>class ModelFactory:\n    @staticmethod\n    def create_model(model_type: str, size: str):\n        if model_type == 'generation':\n            return GenerationModel(size)\n        elif model_type == 'embedding':\n            return EmbeddingModel()\n</code></pre>"},{"location":"architecture/#3-strategy-pattern","title":"3. Strategy Pattern","text":"<p>For different generation strategies:</p> <pre><code>class GenerationStrategy(ABC):\n    @abstractmethod\n    def generate(self, prompt: str) -&gt; str: pass\n\nclass GreedyStrategy(GenerationStrategy):\n    def generate(self, prompt: str) -&gt; str:\n        # Greedy decoding implementation\n        pass\n\nclass BeamSearchStrategy(GenerationStrategy):\n    def generate(self, prompt: str) -&gt; str:\n        # Beam search implementation\n        pass\n</code></pre>"},{"location":"architecture/#4-observer-pattern","title":"4. Observer Pattern","text":"<p>For event handling:</p> <pre><code>class EventManager:\n    def __init__(self):\n        self.listeners = defaultdict(list)\n\n    def subscribe(self, event: str, callback):\n        self.listeners[event].append(callback)\n\n    def notify(self, event: str, data: Any):\n        for callback in self.listeners[event]:\n            callback(data)\n</code></pre>"},{"location":"architecture/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/#core-technologies","title":"Core Technologies","text":"<ul> <li>Python 3.8+: Primary language</li> <li>llama-cpp-python: GGUF model inference</li> <li>NumPy: Numerical operations</li> <li>SQLite: Cache storage</li> <li>ZeroMQ: IPC for daemon</li> <li>FAISS: Vector indexing</li> </ul>"},{"location":"architecture/#development-tools","title":"Development Tools","text":"<ul> <li>UV: Package management</li> <li>pytest: Testing framework</li> <li>ruff: Linting</li> <li>mypy: Type checking</li> <li>mkdocs: Documentation</li> </ul>"},{"location":"architecture/#model-format","title":"Model Format","text":"<ul> <li>GGUF: Efficient model storage</li> <li>Quantization: INT8 for efficiency</li> <li>Compression: Built-in GGUF compression</li> </ul>"},{"location":"architecture/#future-architecture","title":"Future Architecture","text":""},{"location":"architecture/#planned-enhancements","title":"Planned Enhancements","text":"<ol> <li>Distributed Architecture</li> <li>Multiple daemon instances</li> <li>Load balancing</li> <li> <p>Horizontal scaling</p> </li> <li> <p>GPU Support</p> </li> <li>CUDA acceleration</li> <li>Metal Performance Shaders</li> <li> <p>Vulkan compute</p> </li> <li> <p>Streaming Architecture</p> </li> <li>WebSocket support</li> <li>Server-sent events</li> <li> <p>Real-time generation</p> </li> <li> <p>Cloud Native</p> </li> <li>Kubernetes operators</li> <li>Service mesh integration</li> <li>Cloud-specific optimizations</li> </ol>"},{"location":"architecture/#architecture-evolution","title":"Architecture Evolution","text":"<pre><code>Current (Monolithic)          Future (Microservices)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SteadyText \u2502              \u2502   Gateway   \u2502\n\u2502   Library   \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502                 \u2502\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502Generation\u2502    \u2502 Embedding  \u2502\n                     \u2502  Service \u2502    \u2502  Service   \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#conclusion","title":"Conclusion","text":"<p>SteadyText's architecture prioritizes:</p> <ol> <li>Simplicity: Easy to understand and use</li> <li>Reliability: Predictable behavior</li> <li>Performance: Fast response times</li> <li>Extensibility: Easy to extend and customize</li> </ol> <p>The modular design allows for future enhancements while maintaining backward compatibility and consistent behavior across all deployment modes.</p>"},{"location":"benchmarks/","title":"SteadyText Performance Benchmarks","text":"<p>This document provides detailed performance and accuracy benchmarks for SteadyText v1.3.3.</p>"},{"location":"benchmarks/#quick-summary","title":"Quick Summary","text":"<p>SteadyText delivers 100% deterministic text generation and embeddings with competitive performance:</p> <ul> <li>Text Generation: 21.4 generations/sec (46.7ms mean latency)</li> <li>Embeddings: 104.4 single embeddings/sec, up to 598.7 embeddings/sec in batches</li> <li>Cache Performance: 48x speedup for repeated prompts</li> <li>Memory Usage: ~1.4GB for models, 150-200MB during operation</li> <li>Determinism: 100% consistent outputs across all platforms and runs</li> <li>Accuracy: 69.4% similarity for related texts with correct similarity ordering</li> </ul>"},{"location":"benchmarks/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Speed Benchmarks</li> <li>Accuracy Benchmarks</li> <li>Determinism Tests</li> <li>Hardware &amp; Methodology</li> <li>Comparison with Alternatives</li> </ol>"},{"location":"benchmarks/#speed-benchmarks","title":"Speed Benchmarks","text":""},{"location":"benchmarks/#text-generation-performance","title":"Text Generation Performance","text":"<p>SteadyText v2.0.0+ uses the Gemma-3n-E2B-it-Q8_0.gguf model (Gemma-3n-2B) for deterministic text generation:</p> Metric Value Notes Throughput 21.4 generations/sec Fixed 512 tokens per generation Mean Latency 46.7ms Time to generate 512 tokens Median Latency 45.8ms 50th percentile P95 Latency 58.0ms 95th percentile P99 Latency 69.5ms 99th percentile Memory Usage 154MB During generation"},{"location":"benchmarks/#streaming-generation","title":"Streaming Generation","text":"<p>Streaming provides similar performance with slightly higher memory usage:</p> Metric Value Throughput 20.3 generations/sec Mean Latency 49.3ms Memory Usage 213MB"},{"location":"benchmarks/#embedding-performance","title":"Embedding Performance","text":"<p>SteadyText uses the Qwen3-Embedding-0.6B-Q8_0.gguf model for deterministic embeddings (unchanged in v2.0.0+):</p> Batch Size Throughput Mean Latency Use Case 1 104.4 embeddings/sec 9.6ms Single document 10 432.7 embeddings/sec 23.1ms Small batches 50 598.7 embeddings/sec 83.5ms Bulk processing"},{"location":"benchmarks/#cache-performance","title":"Cache Performance","text":"<p>SteadyText includes a frecency cache that dramatically improves performance for repeated operations:</p> Operation Mean Latency Notes Cache Miss 47.6ms First time generating Cache Hit 1.00ms Repeated prompt Speedup 48x Cache vs no-cache Hit Rate 65% Typical workload"},{"location":"benchmarks/#concurrent-performance","title":"Concurrent Performance","text":"<p>SteadyText scales well with multiple concurrent requests:</p> Workers Throughput Scaling Efficiency 1 21.6 ops/sec 100% 2 84.4 ops/sec 95% 4 312.9 ops/sec 90% 8 840.5 ops/sec 85%"},{"location":"benchmarks/#daemon-mode-performance","title":"Daemon Mode Performance","text":"<p>SteadyText v1.3+ includes a daemon mode that keeps models loaded in memory for instant responses:</p> Operation Direct Mode Daemon Mode Improvement First Request 2.4s 15ms 160x faster Subsequent Requests 46.7ms 46.7ms Same With Cache Hit 1.0ms 1.0ms Same Startup Time 0s 2.4s (once) One-time cost <p>Benefits of daemon mode: - Eliminates model loading overhead for each request - Maintains persistent cache across all operations - Supports concurrent requests efficiently - Graceful fallback to direct mode if daemon unavailable</p>"},{"location":"benchmarks/#model-loading","title":"Model Loading","text":"<p>One-time startup cost:</p> <ul> <li>Loading Time: 2.4 seconds</li> <li>Memory Usage: 1.4GB (both models)</li> <li>Models Download: Automatic on first use (~1.9GB total)</li> </ul>"},{"location":"benchmarks/#accuracy-benchmarks","title":"Accuracy Benchmarks","text":""},{"location":"benchmarks/#standard-nlp-benchmarks","title":"Standard NLP Benchmarks","text":"<p>SteadyText performs competitively for a 1B parameter quantized model:</p> Benchmark SteadyText Baseline (1B) Description TruthfulQA 0.42 0.40 Truthfulness in Q&amp;A GSM8K 0.18 0.15 Grade school math HellaSwag 0.58 0.55 Common sense reasoning ARC-Easy 0.71 0.68 Science questions"},{"location":"benchmarks/#embedding-quality","title":"Embedding Quality","text":"Metric Score Description Semantic Similarity 0.76 Correlation with human judgments (STS-B) Clustering Quality 0.68 Silhouette score on 20newsgroups Related Text Similarity 0.694 Cosine similarity for semantically related texts Different Text Similarity 0.466 Cosine similarity for unrelated texts Similarity Ordering \u2705 PASS Correctly ranks related vs unrelated texts"},{"location":"benchmarks/#determinism-tests","title":"Determinism Tests","text":"<p>SteadyText's core guarantee is 100% deterministic outputs:</p>"},{"location":"benchmarks/#test-results","title":"Test Results","text":"Test Result Details Identical Outputs \u2705 PASS 100% consistency across 100 iterations Seed Consistency \u2705 PASS 10 different seeds tested Platform Consistency \u2705 PASS Linux x86_64 verified Fallback Determinism \u2705 PASS Works without models Generation Determinism \u2705 PASS 100% determinism rate in accuracy tests Code Generation Quality \u2705 PASS Generates valid code snippets"},{"location":"benchmarks/#determinism-guarantees","title":"Determinism Guarantees","text":"<ol> <li>Same Input \u2192 Same Output: Every time, on every machine</li> <li>Customizable Seeds: Always uses <code>DEFAULT_SEED=42</code> by default, but can be overridden.</li> <li>Greedy Decoding: No randomness in token selection</li> <li>Quantized Models: 8-bit precision ensures consistency</li> <li>Fallback Support: Deterministic even without models</li> </ol>"},{"location":"benchmarks/#hardware-methodology","title":"Hardware &amp; Methodology","text":""},{"location":"benchmarks/#test-environment","title":"Test Environment","text":"<ul> <li>CPU: Intel Core i7-8700K @ 3.70GHz</li> <li>RAM: 32GB DDR4</li> <li>OS: Linux 6.14.11 (Fedora 42)</li> <li>Python: 3.13.2</li> <li>Models: Gemma-3n-E2B-it-Q8_0.gguf (v2.0.0+), Qwen3-Embedding-0.6B-Q8_0.gguf</li> </ul>"},{"location":"benchmarks/#benchmark-methodology","title":"Benchmark Methodology","text":""},{"location":"benchmarks/#speed-tests","title":"Speed Tests","text":"<ul> <li>5 warmup iterations before measurement</li> <li>100 iterations for statistical significance</li> <li>High-resolution timing with <code>time.perf_counter()</code></li> <li>Memory tracking with <code>psutil</code></li> <li>Cache cleared between hit/miss tests</li> </ul>"},{"location":"benchmarks/#accuracy-tests","title":"Accuracy Tests","text":"<ul> <li>LightEval framework for standard benchmarks</li> <li>Custom determinism verification suite</li> <li>Multiple seed testing for consistency</li> <li>Platform compatibility checks</li> </ul>"},{"location":"benchmarks/#comparison-with-alternatives","title":"Comparison with Alternatives","text":""},{"location":"benchmarks/#vs-non-deterministic-llms","title":"vs. Non-Deterministic LLMs","text":"Feature SteadyText GPT/Claude APIs Determinism 100% guaranteed Variable Latency 46.7ms (fixed) 500-3000ms Cost Free (local) $0.01-0.15/1K tokens Offline \u2705 Works \u274c Requires internet Privacy \u2705 Local only \u26a0\ufe0f Cloud processing"},{"location":"benchmarks/#vs-caching-solutions","title":"vs. Caching Solutions","text":"Feature SteadyText Redis/Memcached Setup Zero config Requires setup First Run 46.7ms N/A (miss) Cached 1.0ms 0.5-2ms Semantic \u2705 Built-in \u274c Exact match only"},{"location":"benchmarks/#running-benchmarks","title":"Running Benchmarks","text":"<p>To run benchmarks yourself:</p> <p>Using UV (recommended): <pre><code># Run all benchmarks\nuv run python benchmarks/run_all_benchmarks.py\n\n# Quick benchmarks (for CI)\nuv run python benchmarks/run_all_benchmarks.py --quick\n\n# Test framework only\nuv run python benchmarks/test_benchmarks.py\n</code></pre></p> <p>Legacy method: <pre><code># Install benchmark dependencies\npip install steadytext[benchmark]\n\n# Run all benchmarks\npython benchmarks/run_all_benchmarks.py\n</code></pre></p> <p>See benchmarks/README.md for detailed instructions.</p>"},{"location":"benchmarks/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Production Ready: Sub-50ms latency suitable for real-time applications</li> <li>Efficient Caching: 48x speedup for repeated operations</li> <li>Scalable: Good concurrent performance up to 8 workers</li> <li>Quality Trade-off: Slightly lower accuracy than larger models, but 100% deterministic</li> <li>Resource Efficient: Only 1.4GB memory for both models</li> </ol> <p>Perfect for testing, CLI tools, and any application requiring reproducible AI outputs.</p>"},{"location":"cache-backends/","title":"Cache Backends","text":"<p>SteadyText now supports pluggable cache backends, allowing you to choose the best caching solution for your deployment scenario.</p>"},{"location":"cache-backends/#available-backends","title":"Available Backends","text":""},{"location":"cache-backends/#sqlite-default","title":"SQLite (Default)","text":"<p>The SQLite backend provides thread-safe, process-safe caching with automatic frecency-based eviction.</p> <p>Features: - Default backend, no configuration required - Thread-safe and process-safe using WAL mode - Automatic migration from legacy pickle format - Configurable size limits with automatic eviction - Persistent storage with atomic operations</p> <p>Configuration: <pre><code># Optional: explicitly select SQLite backend\nexport STEADYTEXT_CACHE_BACKEND=sqlite\n\n# Configure cache settings\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50.0\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100.0\n</code></pre></p>"},{"location":"cache-backends/#cloudflare-d1","title":"Cloudflare D1","text":"<p>The D1 backend enables distributed caching using Cloudflare's edge SQLite database.</p> <p>Features: - Global distribution across Cloudflare's edge network - Automatic replication and disaster recovery - Serverless with no infrastructure to manage - Pay-per-use pricing model - Frecency-based eviction algorithm</p> <p>Requirements: - Cloudflare account with Workers enabled - Deployed D1 proxy Worker (see setup guide below)</p> <p>Configuration: <pre><code># Select D1 backend\nexport STEADYTEXT_CACHE_BACKEND=d1\n\n# Required: D1 proxy Worker configuration\nexport STEADYTEXT_D1_API_URL=https://your-worker.workers.dev\nexport STEADYTEXT_D1_API_KEY=your-secret-api-key\n\n# Optional: Batch size for operations\nexport STEADYTEXT_D1_BATCH_SIZE=50\n</code></pre></p>"},{"location":"cache-backends/#memory","title":"Memory","text":"<p>The memory backend provides fast, in-memory caching for testing or ephemeral workloads.</p> <p>Features: - Fastest performance (no disk I/O) - Simple FIFO eviction when capacity reached - No persistence (data lost on restart) - Minimal overhead</p> <p>Configuration: <pre><code># Select memory backend\nexport STEADYTEXT_CACHE_BACKEND=memory\n\n# Same capacity settings apply\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512\n</code></pre></p>"},{"location":"cache-backends/#d1-backend-setup-guide","title":"D1 Backend Setup Guide","text":""},{"location":"cache-backends/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Cloudflare account with Workers enabled</li> <li>Node.js 16.17.0 or later</li> <li>Wrangler CLI: <code>npm install -g wrangler</code></li> </ul>"},{"location":"cache-backends/#2-deploy-the-d1-proxy-worker","title":"2. Deploy the D1 Proxy Worker","text":"<pre><code># Navigate to the Worker directory\ncd workers/d1-cache-proxy\n\n# Install dependencies\nnpm install\n\n# Login to Cloudflare\nnpx wrangler login\n\n# Create D1 database\nnpx wrangler d1 create steadytext-cache\n\n# Update wrangler.toml with the database ID from above\n\n# Initialize database schema\nnpx wrangler d1 execute steadytext-cache --file=src/schema.sql\n\n# Generate API key\nopenssl rand -base64 32\n\n# Set API key as secret\nnpx wrangler secret put API_KEY\n# Paste your generated API key when prompted\n\n# Deploy the Worker\nnpm run deploy\n</code></pre>"},{"location":"cache-backends/#3-configure-steadytext","title":"3. Configure SteadyText","text":"<p>After deployment, configure SteadyText to use your D1 Worker:</p> <pre><code>import os\n\n# Configure D1 backend\nos.environ[\"STEADYTEXT_CACHE_BACKEND\"] = \"d1\"\nos.environ[\"STEADYTEXT_D1_API_URL\"] = \"https://d1-cache-proxy.your-subdomain.workers.dev\"\nos.environ[\"STEADYTEXT_D1_API_KEY\"] = \"your-api-key-from-step-2\"\n\n# Now use SteadyText normally\nfrom steadytext import generate, embed\n\ntext = generate(\"Hello world\")  # Uses D1 cache\nembedding = embed(\"Some text\")   # Uses D1 cache\n</code></pre>"},{"location":"cache-backends/#choosing-a-backend","title":"Choosing a Backend","text":""},{"location":"cache-backends/#use-sqlite-default-when","title":"Use SQLite (default) when:","text":"<ul> <li>Running on a single machine or small cluster</li> <li>Need persistent cache that survives restarts</li> <li>Want zero configuration</li> <li>Have moderate traffic levels</li> </ul>"},{"location":"cache-backends/#use-d1-when","title":"Use D1 when:","text":"<ul> <li>Deploying globally distributed applications</li> <li>Need cache shared across multiple regions</li> <li>Want serverless, managed infrastructure</li> <li>Can tolerate slight network latency for cache operations</li> <li>Building on Cloudflare Workers platform</li> </ul>"},{"location":"cache-backends/#use-memory-when","title":"Use Memory when:","text":"<ul> <li>Running tests or development</li> <li>Cache persistence is not important</li> <li>Need maximum performance</li> <li>Have plenty of available RAM</li> </ul>"},{"location":"cache-backends/#performance-considerations","title":"Performance Considerations","text":""},{"location":"cache-backends/#latency-comparison","title":"Latency Comparison","text":"<ul> <li>Memory: ~0.01ms per operation</li> <li>SQLite: ~0.1-1ms per operation</li> <li>D1: ~10-50ms per operation (depends on proximity to edge)</li> </ul>"},{"location":"cache-backends/#throughput","title":"Throughput","text":"<ul> <li>Memory: Highest (limited by CPU)</li> <li>SQLite: High (limited by disk I/O)</li> <li>D1: Moderate (limited by network and API rate limits)</li> </ul>"},{"location":"cache-backends/#recommendations","title":"Recommendations","text":"<ol> <li>For single-machine deployments: Use SQLite (default)</li> <li>For global/edge deployments: Use D1</li> <li>For testing: Use Memory</li> <li>For high-throughput local apps: Consider Memory with periodic persistence</li> </ol>"},{"location":"cache-backends/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"cache-backends/#custom-backend-implementation","title":"Custom Backend Implementation","text":"<p>You can create your own cache backend by implementing the <code>CacheBackend</code> interface:</p> <pre><code>from steadytext.cache.base import CacheBackend\nfrom typing import Any, Dict, Optional\n\nclass MyCustomBackend(CacheBackend):\n    def get(self, key: Any) -&gt; Optional[Any]:\n        # Implement get logic\n        pass\n\n    def set(self, key: Any, value: Any) -&gt; None:\n        # Implement set logic\n        pass\n\n    def clear(self) -&gt; None:\n        # Implement clear logic\n        pass\n\n    def sync(self) -&gt; None:\n        # Implement sync logic (if needed)\n        pass\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        # Return statistics\n        return {\"backend\": \"custom\", \"entries\": 0}\n\n    def __len__(self) -&gt; int:\n        # Return number of entries\n        return 0\n</code></pre>"},{"location":"cache-backends/#programmatic-backend-selection","title":"Programmatic Backend Selection","text":"<pre><code>from steadytext.disk_backed_frecency_cache import DiskBackedFrecencyCache\n\n# Use specific backend programmatically\ncache = DiskBackedFrecencyCache(\n    backend_type=\"d1\",\n    api_url=\"https://your-worker.workers.dev\",\n    api_key=\"your-api-key\",\n    capacity=1000,\n    max_size_mb=100.0\n)\n\n# Or with memory backend\ncache = DiskBackedFrecencyCache(backend_type=\"memory\")\n</code></pre>"},{"location":"cache-backends/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"cache-backends/#cache-statistics","title":"Cache Statistics","text":"<p>All backends provide statistics through the <code>get_stats()</code> method:</p> <pre><code>from steadytext import get_cache_manager\n\ncache_manager = get_cache_manager()\nstats = cache_manager.get_cache_stats()\n\nprint(f\"Generation cache: {stats['generation']}\")\nprint(f\"Embedding cache: {stats['embedding']}\")\n</code></pre>"},{"location":"cache-backends/#d1-worker-monitoring","title":"D1 Worker Monitoring","text":"<p>Monitor your D1 Worker performance:</p> <pre><code># View real-time logs\ncd workers/d1-cache-proxy\nnpm run tail\n\n# Check Worker analytics in Cloudflare dashboard\n</code></pre>"},{"location":"cache-backends/#debug-environment-variables","title":"Debug Environment Variables","text":"<pre><code># Enable debug logging\nexport STEADYTEXT_LOG_LEVEL=DEBUG\n\n# Skip cache initialization (for testing)\nexport STEADYTEXT_SKIP_CACHE_INIT=1\n\n# Disable specific cache\nexport STEADYTEXT_DISABLE_CACHE=1\n</code></pre>"},{"location":"cache-backends/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cache-backends/#d1-backend-issues","title":"D1 Backend Issues","text":"<p>Connection Errors: - Verify Worker is deployed: <code>npx wrangler tail</code> - Check API URL is correct (no trailing slash) - Verify API key matches the secret set in Worker</p> <p>Authentication Errors: - Ensure Bearer token format in API_KEY - Check secret was set correctly: <code>npx wrangler secret list</code></p> <p>Performance Issues: - Monitor Worker CPU usage in Cloudflare dashboard - Consider increasing batch size for bulk operations - Check proximity to nearest Cloudflare edge location</p>"},{"location":"cache-backends/#sqlite-backend-issues","title":"SQLite Backend Issues","text":"<p>Database Corruption: - SteadyText automatically moves corrupted databases to <code>.corrupted.*</code> files - Check logs for corruption warnings - Delete corrupted files if disk space is an issue</p> <p>Lock Timeouts: - Usually indicates high concurrency - Consider using D1 for distributed workloads - Increase timeout values if needed</p>"},{"location":"cache-backends/#memory-backend-issues","title":"Memory Backend Issues","text":"<p>Out of Memory: - Reduce cache capacity settings - Monitor memory usage of your application - Consider using SQLite for overflow</p>"},{"location":"cache-backends/#migration-guide","title":"Migration Guide","text":""},{"location":"cache-backends/#from-pickle-to-sqlite-automatic","title":"From Pickle to SQLite (Automatic)","text":"<p>The SQLite backend automatically migrates legacy pickle caches:</p> <ol> <li>On first use, it detects <code>.pkl</code> files</li> <li>Migrates all entries to SQLite format</li> <li>Removes old pickle files after successful migration</li> <li>No manual intervention required</li> </ol>"},{"location":"cache-backends/#switching-backends","title":"Switching Backends","text":"<p>To switch backends:</p> <ol> <li>Export existing cache data (optional)</li> <li>Change <code>STEADYTEXT_CACHE_BACKEND</code> environment variable</li> <li>Restart application</li> <li>Cache will be empty (unless migrating to same backend type)</li> </ol> <p>Note: Cache data is not automatically transferred between different backend types.</p>"},{"location":"cache-backends/#best-practices","title":"Best Practices","text":"<ol> <li>Start with defaults: SQLite backend works well for most use cases</li> <li>Monitor cache hit rates: Use statistics to optimize capacity</li> <li>Set appropriate size limits: Prevent unbounded cache growth</li> <li>Use batch operations: Reduce round trips for D1 backend</li> <li>Test backend switching: Ensure your app handles empty caches gracefully</li> <li>Secure your API keys: Use environment variables, never commit keys</li> <li>Monitor Worker health: Set up alerts for D1 Worker errors</li> </ol>"},{"location":"cache-backends/#future-backends","title":"Future Backends","text":"<p>Planned backend support: - Redis/Valkey (for traditional distributed caching) - DynamoDB (for AWS deployments) - Cloud Storage (for large value caching)</p> <p>To request a backend, open an issue on GitHub.</p>"},{"location":"contributing/","title":"Contributing to SteadyText","text":"<p>We welcome contributions to SteadyText! This document provides guidelines for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork: <code>git clone https://github.com/your-username/steadytext.git</code></li> <li>Create a feature branch: <code>git checkout -b feature/your-feature-name</code></li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (supports up to Python 3.13)</li> <li>Git</li> <li>Recommended: uv for faster dependency management</li> </ul>"},{"location":"contributing/#installation","title":"Installation","text":"With uv (Recommended)With pip <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext\n\n# Install in development mode\nuv sync --dev\n\n# Activate the virtual environment\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n</code></pre> <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -e .[dev]\n</code></pre>"},{"location":"contributing/#development-commands","title":"Development Commands","text":"<p>SteadyText uses uv for task management:</p> <pre><code># Run tests\nuv run python -m pytest\n\n# Run tests with coverage\nuv run python -m pytest --cov=steadytext\n\n# Run tests with model downloads (slower)\nSTEADYTEXT_ALLOW_MODEL_DOWNLOADS=true uv run python -m pytest\n\n# Run linting\nuvx ruff check .\n\n# Format code\nuvx ruff format .\n\n# Type checking\nuvx mypy .\n\n# Run pre-commit hooks\nuvx pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8: Use <code>uvx ruff format .</code> to auto-format code</li> <li>Use type hints: Add type annotations for function parameters and returns</li> <li>Add docstrings: Document all public functions and classes</li> <li>Keep functions focused: Single responsibility principle</li> </ul> <p>Example:</p> <pre><code>def embed(text_input: Union[str, List[str]]) -&gt; np.ndarray:\n    \"\"\"Create deterministic embeddings for text input.\n\n    Args:\n        text_input: String or list of strings to embed\n\n    Returns:\n        1024-dimensional L2-normalized float32 numpy array\n    \"\"\"\n    # Implementation here\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>SteadyText has comprehensive tests covering:</p> <ul> <li>Deterministic behavior: Same input \u2192 same output</li> <li>Fallback functionality: Works without models</li> <li>Edge cases: Empty inputs, invalid types</li> <li>Performance: Caching behavior</li> </ul>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>def test_your_feature():\n    \"\"\"Test your new feature.\"\"\"\n    # Test deterministic behavior\n    result1 = your_function(\"test input\")\n    result2 = your_function(\"test input\")\n    assert result1 == result2  # Should be identical\n\n    # Test edge cases\n    result3 = your_function(\"\")\n    assert isinstance(result3, expected_type)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run python -m pytest\n\n# Run specific test file\nuv run python -m pytest tests/test_your_feature.py\n\n# Run with coverage\nuv run python -m pytest --cov=steadytext\n\n# Run tests that require model downloads\nSTEADYTEXT_ALLOW_MODEL_DOWNLOADS=true uv run python -m pytest\n\n# Run tests in parallel\nuv run python -m pytest -n auto\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update API docs: Modify files in <code>docs/api/</code> if adding new functions</li> <li>Add examples: Include usage examples in <code>docs/examples/</code></li> <li>Update README: For major features, update the main README.md</li> </ul>"},{"location":"contributing/#architecture-guidelines","title":"Architecture Guidelines","text":"<p>SteadyText follows a layered architecture:</p> <pre><code>steadytext/\n\u251c\u2500\u2500 core/          # Core generation and embedding logic\n\u251c\u2500\u2500 models/        # Model loading and caching\n\u251c\u2500\u2500 cli/           # Command-line interface\n\u2514\u2500\u2500 utils.py       # Shared utilities\n</code></pre>"},{"location":"contributing/#core-principles","title":"Core Principles","text":"<ol> <li>Never fail: Functions should always return valid outputs</li> <li>Deterministic: Same input always produces same output</li> <li>Thread-safe: Support concurrent usage</li> <li>Cached: Use frecency caching for performance</li> </ol>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":"<ol> <li>Core functionality: Add to <code>steadytext/core/</code></li> <li>Model support: Modify <code>steadytext/models/</code></li> <li>CLI commands: Add to <code>steadytext/cli/commands/</code></li> <li>Utilities: Add to <code>steadytext/utils.py</code></li> </ol>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Run all tests: <code>uv run python -m pytest</code></li> <li>Check linting: <code>uvx ruff check .</code></li> <li>Format code: <code>uvx ruff format .</code></li> <li>Type check: <code>uvx mypy .</code></li> <li>Update documentation: Add/update relevant docs</li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create descriptive title: \"Add feature X\" or \"Fix bug Y\"</li> <li>Write clear description: Explain what changes and why</li> <li>Reference issues: Link to related GitHub issues</li> <li>Add tests: Include tests for new functionality</li> <li>Update changelog: Add entry to CHANGELOG.md</li> </ol>"},{"location":"contributing/#pull-request-template","title":"Pull Request Template","text":"<pre><code>## Description\nBrief description of the changes\n\n## Changes Made\n- [ ] Added feature X\n- [ ] Fixed bug Y\n- [ ] Updated documentation\n\n## Testing\n- [ ] All tests pass\n- [ ] Added tests for new functionality\n- [ ] Manually tested edge cases\n\n## Checklist\n- [ ] Code follows project style guidelines\n- [ ] Self-review completed\n- [ ] Documentation updated\n- [ ] Changelog updated\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#typical-development-cycle","title":"Typical Development Cycle","text":"<ol> <li>Pick/create an issue: Find something to work on</li> <li>Create feature branch: <code>git checkout -b feature/issue-123</code></li> <li>Make changes: Implement your feature</li> <li>Test thoroughly: Run tests and manual testing</li> <li>Commit changes: Use descriptive commit messages</li> <li>Push and PR: Create pull request</li> </ol>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits:</p> <pre><code>feat: add new embedding model support\nfix: resolve caching issue with concurrent access\ndocs: update API documentation for generate()\ntest: add tests for edge cases\nchore: update dependencies\n</code></pre>"},{"location":"contributing/#branch-naming","title":"Branch Naming","text":"<ul> <li><code>feature/description</code> - New features</li> <li><code>fix/description</code> - Bug fixes  </li> <li><code>docs/description</code> - Documentation updates</li> <li><code>refactor/description</code> - Code refactoring</li> </ul>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>SteadyText follows semantic versioning:</p> <ul> <li>Major (1.0.0): Breaking changes, new model versions</li> <li>Minor (0.1.0): New features, backward compatible</li> <li>Patch (0.0.1): Bug fixes, small improvements</li> </ul>"},{"location":"contributing/#model-versioning","title":"Model Versioning","text":"<ul> <li>Models are fixed per major version</li> <li>Only major version updates change model outputs</li> <li>This ensures deterministic behavior across patch/minor updates</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>GitHub Discussions: For questions and general discussion</li> <li>Discord: Join our community chat (link in README)</li> </ul>"},{"location":"contributing/#common-issues","title":"Common Issues","text":"<p>Tests failing locally: <pre><code># Clear caches\nrm -rf ~/.cache/steadytext/\n\n# Reinstall dependencies  \npip install -e .[dev]\n\n# Run tests\npoe test\n</code></pre></p> <p>Import errors: <pre><code># Make sure you're in the right directory\ncd steadytext/\n\n# Install in development mode\npip install -e .\n</code></pre></p> <p>Model download issues: <pre><code># Set environment variable\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Run tests\npoe test-models\n</code></pre></p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive in all interactions. We want SteadyText to be a welcoming project for everyone.</p>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - README.md: Major contributors listed - CHANGELOG.md: Contributions noted in releases - GitHub: Contributor graphs and statistics</p> <p>Thank you for contributing to SteadyText! \ud83d\ude80</p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>This guide covers deploying SteadyText in various production environments, from simple servers to cloud-native architectures.</p>"},{"location":"deployment/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Deployment Options</li> <li>System Requirements</li> <li>Basic Server Deployment</li> <li>Docker Deployment</li> <li>Kubernetes Deployment</li> <li>Cloud Deployments</li> <li>AWS</li> <li>Google Cloud</li> <li>Azure</li> <li>PostgreSQL Extension Deployment</li> <li>Production Configuration</li> <li>Monitoring and Observability</li> <li>Security Considerations</li> <li>High Availability</li> <li>Troubleshooting</li> </ul>"},{"location":"deployment/#deployment-options","title":"Deployment Options","text":""},{"location":"deployment/#overview-of-deployment-methods","title":"Overview of Deployment Methods","text":"Method Best For Complexity Scalability Direct Install Development Low Limited Systemd Service Single server Medium Vertical Docker Containerized apps Medium Horizontal Kubernetes Cloud-native High Auto-scaling PostgreSQL Extension Database-integrated Medium With database"},{"location":"deployment/#system-requirements","title":"System Requirements","text":""},{"location":"deployment/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>CPU: 2 cores (4+ recommended)</li> <li>RAM: 4GB (8GB+ recommended)</li> <li>Storage: 10GB (for models and cache)</li> <li>OS: Linux (Ubuntu 20.04+), macOS, Windows Server</li> <li>Python: 3.8+ (3.10+ recommended)</li> </ul>"},{"location":"deployment/#recommended-production-specs","title":"Recommended Production Specs","text":"<pre><code># Production server specifications\nproduction:\n  cpu: 8 cores\n  ram: 16GB\n  storage: 50GB SSD\n  network: 1Gbps\n  os: Ubuntu 22.04 LTS\n</code></pre>"},{"location":"deployment/#resource-planning","title":"Resource Planning","text":"<pre><code># Calculate resource requirements\ndef calculate_resources(concurrent_users, cache_size_gb, model_size):\n    \"\"\"Estimate resource requirements.\"\"\"\n\n    # Memory calculation\n    base_memory_gb = 2  # OS and services\n    model_memory_gb = {\n        'small': 2,\n        'large': 4\n    }[model_size]\n    cache_memory_gb = cache_size_gb\n    worker_memory_gb = concurrent_users * 0.1  # 100MB per concurrent user\n\n    total_memory_gb = (\n        base_memory_gb + \n        model_memory_gb + \n        cache_memory_gb + \n        worker_memory_gb\n    )\n\n    # CPU calculation\n    cpu_cores = max(4, concurrent_users // 10)\n\n    return {\n        'memory_gb': total_memory_gb,\n        'cpu_cores': cpu_cores,\n        'storage_gb': 10 + cache_size_gb * 2  # 2x cache for growth\n    }\n\n# Example: 100 concurrent users, 10GB cache, large model\nresources = calculate_resources(100, 10, 'large')\nprint(f\"Required: {resources['memory_gb']}GB RAM, {resources['cpu_cores']} cores\")\n</code></pre>"},{"location":"deployment/#basic-server-deployment","title":"Basic Server Deployment","text":""},{"location":"deployment/#1-system-setup","title":"1. System Setup","text":"<pre><code># Ubuntu/Debian setup\nsudo apt update\nsudo apt install -y python3.10 python3.10-venv python3-pip\n\n# Create dedicated user\nsudo useradd -m -s /bin/bash steadytext\nsudo mkdir -p /opt/steadytext\nsudo chown steadytext:steadytext /opt/steadytext\n\n# Switch to steadytext user\nsudo su - steadytext\ncd /opt/steadytext\n</code></pre>"},{"location":"deployment/#2-python-environment","title":"2. Python Environment","text":"<pre><code># Create virtual environment\npython3.10 -m venv venv\nsource venv/bin/activate\n\n# Install SteadyText\npip install steadytext\n\n# Preload models\nst models download --all\nst models preload\n</code></pre>"},{"location":"deployment/#3-systemd-service","title":"3. Systemd Service","text":"<p>Create <code>/etc/systemd/system/steadytext.service</code>:</p> <pre><code>[Unit]\nDescription=SteadyText Daemon Service\nAfter=network.target\n\n[Service]\nType=simple\nUser=steadytext\nGroup=steadytext\nWorkingDirectory=/opt/steadytext\nEnvironment=\"PATH=/opt/steadytext/venv/bin\"\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\"\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=1024\"\nEnvironment=\"STEADYTEXT_EMBEDDING_CACHE_CAPACITY=4096\"\nEnvironment=\"STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=2048\"\nExecStart=/opt/steadytext/venv/bin/st daemon start --foreground --host 0.0.0.0 --port 5557\nRestart=always\nRestartSec=10\n\n# Security\nNoNewPrivileges=true\nPrivateTmp=true\nProtectSystem=strict\nProtectHome=true\nReadWritePaths=/opt/steadytext /home/steadytext/.cache\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable steadytext\nsudo systemctl start steadytext\nsudo systemctl status steadytext\n</code></pre>"},{"location":"deployment/#4-nginx-reverse-proxy","title":"4. Nginx Reverse Proxy","text":"<p>Install and configure Nginx:</p> <pre><code># /etc/nginx/sites-available/steadytext\nupstream steadytext_backend {\n    server 127.0.0.1:5557;\n    keepalive 32;\n}\n\nserver {\n    listen 80;\n    server_name steadytext.example.com;\n\n    # Redirect to HTTPS\n    return 301 https://$server_name$request_uri;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name steadytext.example.com;\n\n    # SSL configuration\n    ssl_certificate /etc/letsencrypt/live/steadytext.example.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/steadytext.example.com/privkey.pem;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers HIGH:!aNULL:!MD5;\n\n    # Security headers\n    add_header X-Frame-Options \"SAMEORIGIN\" always;\n    add_header X-Content-Type-Options \"nosniff\" always;\n    add_header X-XSS-Protection \"1; mode=block\" always;\n\n    # WebSocket support for streaming\n    location /ws {\n        proxy_pass http://steadytext_backend;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_read_timeout 3600s;\n    }\n\n    # Regular HTTP API\n    location / {\n        proxy_pass http://steadytext_backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        # Timeouts\n        proxy_connect_timeout 60s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n}\n</code></pre> <p>Enable site:</p> <pre><code>sudo ln -s /etc/nginx/sites-available/steadytext /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl reload nginx\n</code></pre>"},{"location":"deployment/#docker-deployment","title":"Docker Deployment","text":""},{"location":"deployment/#1-dockerfile","title":"1. Dockerfile","text":"<pre><code># Dockerfile\nFROM python:3.10-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create app user\nRUN useradd -m -s /bin/bash steadytext\n\n# Set working directory\nWORKDIR /app\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Download models during build (optional)\nRUN python -c \"import steadytext; steadytext.preload_models()\"\n\n# Copy application code\nCOPY . .\n\n# Change ownership\nRUN chown -R steadytext:steadytext /app\n\n# Switch to non-root user\nUSER steadytext\n\n# Expose port\nEXPOSE 5557\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n    CMD st daemon status || exit 1\n\n# Start daemon\nCMD [\"st\", \"daemon\", \"start\", \"--foreground\", \"--host\", \"0.0.0.0\"]\n</code></pre>"},{"location":"deployment/#2-docker-compose","title":"2. Docker Compose","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  steadytext:\n    build: .\n    image: steadytext:latest\n    container_name: steadytext-daemon\n    ports:\n      - \"5557:5557\"\n    environment:\n      - STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\n      - STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=1024\n      - STEADYTEXT_EMBEDDING_CACHE_CAPACITY=4096\n      - STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=2048\n    volumes:\n      - steadytext-cache:/home/steadytext/.cache\n      - steadytext-models:/app/models\n    restart: unless-stopped\n    deploy:\n      resources:\n        limits:\n          cpus: '4'\n          memory: 8G\n        reservations:\n          cpus: '2'\n          memory: 4G\n\n  nginx:\n    image: nginx:alpine\n    container_name: steadytext-nginx\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./certs:/etc/nginx/certs:ro\n    depends_on:\n      - steadytext\n    restart: unless-stopped\n\nvolumes:\n  steadytext-cache:\n  steadytext-models:\n</code></pre>"},{"location":"deployment/#3-build-and-run","title":"3. Build and Run","text":"<pre><code># Build image\ndocker build -t steadytext:latest .\n\n# Run with docker-compose\ndocker-compose up -d\n\n# Check logs\ndocker-compose logs -f steadytext\n\n# Scale horizontally\ndocker-compose up -d --scale steadytext=3\n</code></pre>"},{"location":"deployment/#kubernetes-deployment","title":"Kubernetes Deployment","text":""},{"location":"deployment/#1-configmap","title":"1. ConfigMap","text":"<pre><code># steadytext-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: steadytext-config\n  namespace: steadytext\ndata:\n  STEADYTEXT_GENERATION_CACHE_CAPACITY: \"2048\"\n  STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB: \"1024\"\n  STEADYTEXT_EMBEDDING_CACHE_CAPACITY: \"4096\"\n  STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB: \"2048\"\n  DAEMON_HOST: \"0.0.0.0\"\n  DAEMON_PORT: \"5557\"\n</code></pre>"},{"location":"deployment/#2-deployment","title":"2. Deployment","text":"<pre><code># steadytext-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: steadytext\n  namespace: steadytext\n  labels:\n    app: steadytext\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: steadytext\n  template:\n    metadata:\n      labels:\n        app: steadytext\n    spec:\n      containers:\n      - name: steadytext\n        image: steadytext:latest\n        ports:\n        - containerPort: 5557\n          name: daemon\n        envFrom:\n        - configMapRef:\n            name: steadytext-config\n        resources:\n          requests:\n            memory: \"4Gi\"\n            cpu: \"2\"\n          limits:\n            memory: \"8Gi\"\n            cpu: \"4\"\n        livenessProbe:\n          exec:\n            command:\n            - st\n            - daemon\n            - status\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - st\n            - daemon\n            - status\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        volumeMounts:\n        - name: cache\n          mountPath: /home/steadytext/.cache\n        - name: models\n          mountPath: /app/models\n      volumes:\n      - name: cache\n        persistentVolumeClaim:\n          claimName: steadytext-cache-pvc\n      - name: models\n        persistentVolumeClaim:\n          claimName: steadytext-models-pvc\n</code></pre>"},{"location":"deployment/#3-service","title":"3. Service","text":"<pre><code># steadytext-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: steadytext-service\n  namespace: steadytext\nspec:\n  selector:\n    app: steadytext\n  ports:\n  - port: 5557\n    targetPort: 5557\n    name: daemon\n  type: ClusterIP\n</code></pre>"},{"location":"deployment/#4-horizontal-pod-autoscaler","title":"4. Horizontal Pod Autoscaler","text":"<pre><code># steadytext-hpa.yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: steadytext-hpa\n  namespace: steadytext\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: steadytext\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n</code></pre>"},{"location":"deployment/#5-ingress","title":"5. Ingress","text":"<pre><code># steadytext-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: steadytext-ingress\n  namespace: steadytext\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  tls:\n  - hosts:\n    - steadytext.example.com\n    secretName: steadytext-tls\n  rules:\n  - host: steadytext.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: steadytext-service\n            port:\n              number: 5557\n</code></pre>"},{"location":"deployment/#6-deploy-to-kubernetes","title":"6. Deploy to Kubernetes","text":"<pre><code># Create namespace\nkubectl create namespace steadytext\n\n# Apply configurations\nkubectl apply -f steadytext-config.yaml\nkubectl apply -f steadytext-pvc.yaml  # Create PVCs first\nkubectl apply -f steadytext-deployment.yaml\nkubectl apply -f steadytext-service.yaml\nkubectl apply -f steadytext-hpa.yaml\nkubectl apply -f steadytext-ingress.yaml\n\n# Check status\nkubectl -n steadytext get pods\nkubectl -n steadytext logs -f deployment/steadytext\n</code></pre>"},{"location":"deployment/#cloud-deployments","title":"Cloud Deployments","text":""},{"location":"deployment/#aws-deployment","title":"AWS Deployment","text":""},{"location":"deployment/#1-ec2-instance","title":"1. EC2 Instance","text":"<pre><code># Launch EC2 instance (via AWS CLI)\naws ec2 run-instances \\\n  --image-id ami-0c55b159cbfafe1f0 \\\n  --instance-type t3.xlarge \\\n  --key-name your-key \\\n  --security-group-ids sg-xxxxxx \\\n  --subnet-id subnet-xxxxxx \\\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=steadytext-server}]' \\\n  --user-data file://setup.sh\n</code></pre> <p>Setup script (<code>setup.sh</code>):</p> <pre><code>#!/bin/bash\n# Update system\napt update &amp;&amp; apt upgrade -y\n\n# Install dependencies\napt install -y python3.10 python3.10-venv python3-pip nginx\n\n# Install SteadyText\npip3 install steadytext\n\n# Configure and start daemon\ncat &gt; /etc/systemd/system/steadytext.service &lt;&lt; EOF\n[Unit]\nDescription=SteadyText Daemon\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/local/bin/st daemon start --foreground\nRestart=always\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\"\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl enable steadytext\nsystemctl start steadytext\n</code></pre>"},{"location":"deployment/#2-ecs-fargate","title":"2. ECS Fargate","text":"<pre><code>// task-definition.json\n{\n  \"family\": \"steadytext\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"2048\",\n  \"memory\": \"8192\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"steadytext\",\n      \"image\": \"your-ecr-repo/steadytext:latest\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 5557,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"STEADYTEXT_GENERATION_CACHE_CAPACITY\",\n          \"value\": \"2048\"\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/steadytext\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      },\n      \"healthCheck\": {\n        \"command\": [\"CMD-SHELL\", \"st daemon status || exit 1\"],\n        \"interval\": 30,\n        \"timeout\": 10,\n        \"retries\": 3,\n        \"startPeriod\": 60\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"deployment/#3-lambda-function","title":"3. Lambda Function","text":"<pre><code># lambda_function.py\nimport json\nimport steadytext\n\ndef lambda_handler(event, context):\n    \"\"\"AWS Lambda handler for SteadyText.\"\"\"\n\n    # Parse request\n    body = json.loads(event.get('body', '{}'))\n    prompt = body.get('prompt', '')\n    seed = body.get('seed', 42)\n\n    # Generate text\n    result = steadytext.generate(prompt, seed=seed)\n\n    if result is None:\n        return {\n            'statusCode': 503,\n            'body': json.dumps({'error': 'Model not available'})\n        }\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps({\n            'text': result,\n            'seed': seed\n        })\n    }\n</code></pre>"},{"location":"deployment/#google-cloud-deployment","title":"Google Cloud Deployment","text":""},{"location":"deployment/#1-compute-engine","title":"1. Compute Engine","text":"<pre><code># Create instance\ngcloud compute instances create steadytext-server \\\n  --machine-type=n2-standard-4 \\\n  --image-family=ubuntu-2204-lts \\\n  --image-project=ubuntu-os-cloud \\\n  --boot-disk-size=50GB \\\n  --metadata-from-file startup-script=setup.sh\n</code></pre>"},{"location":"deployment/#2-cloud-run","title":"2. Cloud Run","text":"<pre><code># Dockerfile for Cloud Run\nFROM python:3.10-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\n# Cloud Run sets PORT environment variable\nCMD exec gunicorn --bind :$PORT --workers 1 --threads 8 app:app\n</code></pre> <p>Deploy:</p> <pre><code># Build and push\ngcloud builds submit --tag gcr.io/PROJECT-ID/steadytext\n\n# Deploy\ngcloud run deploy steadytext \\\n  --image gcr.io/PROJECT-ID/steadytext \\\n  --platform managed \\\n  --memory 8Gi \\\n  --cpu 4 \\\n  --timeout 300 \\\n  --concurrency 10\n</code></pre>"},{"location":"deployment/#3-kubernetes-engine-gke","title":"3. Kubernetes Engine (GKE)","text":"<pre><code># Create cluster\ngcloud container clusters create steadytext-cluster \\\n  --num-nodes=3 \\\n  --machine-type=n2-standard-4 \\\n  --enable-autoscaling \\\n  --min-nodes=2 \\\n  --max-nodes=10\n\n# Deploy application\nkubectl apply -f k8s/\n</code></pre>"},{"location":"deployment/#azure-deployment","title":"Azure Deployment","text":""},{"location":"deployment/#1-virtual-machine","title":"1. Virtual Machine","text":"<pre><code># Create VM\naz vm create \\\n  --resource-group steadytext-rg \\\n  --name steadytext-vm \\\n  --image UbuntuLTS \\\n  --size Standard_D4s_v3 \\\n  --admin-username azureuser \\\n  --generate-ssh-keys \\\n  --custom-data setup.sh\n</code></pre>"},{"location":"deployment/#2-container-instances","title":"2. Container Instances","text":"<pre><code># Deploy container\naz container create \\\n  --resource-group steadytext-rg \\\n  --name steadytext \\\n  --image your-acr.azurecr.io/steadytext:latest \\\n  --cpu 4 \\\n  --memory 8 \\\n  --port 5557 \\\n  --environment-variables \\\n    STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\n</code></pre>"},{"location":"deployment/#3-app-service","title":"3. App Service","text":"<pre><code># Create App Service plan\naz appservice plan create \\\n  --name steadytext-plan \\\n  --resource-group steadytext-rg \\\n  --sku P2v3 \\\n  --is-linux\n\n# Deploy container\naz webapp create \\\n  --resource-group steadytext-rg \\\n  --plan steadytext-plan \\\n  --name steadytext-app \\\n  --deployment-container-image-name your-acr.azurecr.io/steadytext:latest\n</code></pre>"},{"location":"deployment/#postgresql-extension-deployment","title":"PostgreSQL Extension Deployment","text":"<p>The pg_steadytext extension brings production-ready AI capabilities directly to PostgreSQL with features including async operations, AI summarization, and structured generation (v2.4.1+).</p>"},{"location":"deployment/#key-features","title":"Key Features","text":"<ul> <li>Native SQL Functions: Text generation and embeddings</li> <li>Async Operations (v1.1.0+): Non-blocking AI operations with queue-based processing</li> <li>AI Summarization: Aggregate functions with TimescaleDB support</li> <li>Structured Generation (v2.4.1+): JSON schemas, regex patterns, and choice constraints</li> <li>Docker Support: Production-ready containerization</li> </ul>"},{"location":"deployment/#1-standalone-postgresql","title":"1. Standalone PostgreSQL","text":"<pre><code># Install PostgreSQL and dependencies\nsudo apt install postgresql-15 postgresql-server-dev-15 python3-dev python3-pip\nsudo apt install postgresql-15-pgvector  # For pgvector extension\n\n# Install Python dependencies system-wide (for PostgreSQL)\nsudo pip3 install steadytext&gt;=2.1.0 pyzmq numpy\n\n# Clone and install pg_steadytext\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\nmake &amp;&amp; sudo make install\n\n# Configure PostgreSQL\nsudo -u postgres psql &lt;&lt; EOF\nCREATE DATABASE steadytext_db;\n\\c steadytext_db\nCREATE EXTENSION plpython3u CASCADE;\nCREATE EXTENSION pgvector CASCADE;\nCREATE EXTENSION pg_steadytext CASCADE;\n\n-- Verify installation\nSELECT steadytext_version();\nSELECT * FROM steadytext_config;\nEOF\n\n# Start the daemon for better performance\nsudo -u postgres psql steadytext_db -c \"SELECT steadytext_daemon_start();\"\n</code></pre>"},{"location":"deployment/#2-docker-postgresql-recommended","title":"2. Docker PostgreSQL (Recommended)","text":"<p>The pg_steadytext repository includes a production-ready Dockerfile:</p> <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\n\n# Build the Docker image\ndocker build -t pg_steadytext .\n\n# Or build with fallback model support (for compatibility)\ndocker build --build-arg STEADYTEXT_USE_FALLBACK_MODEL=true -t pg_steadytext .\n\n# Run the container\ndocker run -d \\\n  --name pg_steadytext \\\n  -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=mysecretpassword \\\n  -e POSTGRES_DB=steadytext_db \\\n  -v steadytext_data:/var/lib/postgresql/data \\\n  pg_steadytext\n\n# Test the installation\ndocker exec -it pg_steadytext psql -U postgres steadytext_db -c \"SELECT steadytext_version();\"\ndocker exec -it pg_steadytext psql -U postgres steadytext_db -c \"SELECT steadytext_generate('Hello Docker!');\"\n</code></pre>"},{"location":"deployment/#docker-compose-setup","title":"Docker Compose Setup","text":"<p>Create <code>docker-compose.yml</code>:</p> <pre><code>version: '3.8'\n\nservices:\n  postgres-steadytext:\n    build: \n      context: ./pg_steadytext\n      args:\n        - STEADYTEXT_USE_FALLBACK_MODEL=true\n    container_name: pg_steadytext\n    environment:\n      - POSTGRES_PASSWORD=mysecretpassword\n      - POSTGRES_DB=steadytext_db\n      - POSTGRES_USER=steadytext\n    ports:\n      - \"5432:5432\"\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      - ./init-scripts:/docker-entrypoint-initdb.d\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U steadytext &amp;&amp; psql -U steadytext steadytext_db -c 'SELECT steadytext_version();'\"]\n      interval: 30s\n      timeout: 10s\n      retries: 5\n    restart: unless-stopped\n\nvolumes:\n  postgres_data:\n</code></pre>"},{"location":"deployment/#production-docker-configuration","title":"Production Docker Configuration","text":"<p>For production deployments, use environment variables and resource limits:</p> <pre><code># docker-compose.prod.yml\nversion: '3.8'\n\nservices:\n  postgres-steadytext:\n    image: pg_steadytext:latest\n    deploy:\n      resources:\n        limits:\n          cpus: '4.0'\n          memory: 16G\n        reservations:\n          cpus: '2.0'\n          memory: 8G\n    environment:\n      # PostgreSQL settings\n      - POSTGRES_PASSWORD_FILE=/run/secrets/postgres_password\n      - POSTGRES_DB=steadytext_prod\n      - POSTGRES_SHARED_BUFFERS=4GB\n      - POSTGRES_WORK_MEM=256MB\n      - POSTGRES_MAX_CONNECTIONS=200\n\n      # SteadyText settings\n      - STEADYTEXT_GENERATION_CACHE_CAPACITY=4096\n      - STEADYTEXT_EMBEDDING_CACHE_CAPACITY=8192\n      - STEADYTEXT_USE_FALLBACK_MODEL=false\n      - STEADYTEXT_DAEMON_HOST=0.0.0.0\n      - STEADYTEXT_DAEMON_PORT=5557\n\n      # Worker settings for async operations\n      - STEADYTEXT_WORKER_BATCH_SIZE=20\n      - STEADYTEXT_WORKER_POLL_INTERVAL_MS=500\n    secrets:\n      - postgres_password\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      - postgres_logs:/var/log/postgresql\n    networks:\n      - steadytext_network\n\nsecrets:\n  postgres_password:\n    file: ./secrets/postgres_password.txt\n\nnetworks:\n  steadytext_network:\n    driver: bridge\n\nvolumes:\n  postgres_data:\n  postgres_logs:\n</code></pre>"},{"location":"deployment/#3-kubernetes-deployment","title":"3. Kubernetes Deployment","text":"<p>Deploy pg_steadytext on Kubernetes:</p> <pre><code># postgres-steadytext-deployment.yaml\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-steadytext\nspec:\n  serviceName: postgres-steadytext\n  replicas: 1\n  selector:\n    matchLabels:\n      app: postgres-steadytext\n  template:\n    metadata:\n      labels:\n        app: postgres-steadytext\n    spec:\n      containers:\n      - name: postgres\n        image: pg_steadytext:latest\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: password\n        - name: POSTGRES_DB\n          value: steadytext_db\n        - name: STEADYTEXT_GENERATION_CACHE_CAPACITY\n          value: \"4096\"\n        resources:\n          requests:\n            memory: \"8Gi\"\n            cpu: \"2\"\n          limits:\n            memory: \"16Gi\"\n            cpu: \"4\"\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n  volumeClaimTemplates:\n  - metadata:\n      name: postgres-storage\n    spec:\n      accessModes: [\"ReadWriteOnce\"]\n      resources:\n        requests:\n          storage: 100Gi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: postgres-steadytext\nspec:\n  ports:\n  - port: 5432\n  selector:\n    app: postgres-steadytext\n</code></pre>"},{"location":"deployment/#4-feature-configuration","title":"4. Feature Configuration","text":""},{"location":"deployment/#async-operations-v110","title":"Async Operations (v1.1.0+)","text":"<p>Configure and start the background worker for async operations:</p> <pre><code>-- Start the background worker\nSELECT steadytext_worker_start();\n\n-- Configure worker settings\nSELECT steadytext_config_set('worker_batch_size', '20');\nSELECT steadytext_config_set('worker_poll_interval_ms', '500');\nSELECT steadytext_config_set('worker_max_retries', '3');\n\n-- Check worker status\nSELECT * FROM steadytext_worker_status();\n\n-- Example async usage\nSELECT request_id FROM steadytext_generate_async('Write a story', priority := 10);\n</code></pre>"},{"location":"deployment/#ai-summarization-setup","title":"AI Summarization Setup","text":"<p>For TimescaleDB continuous aggregates:</p> <pre><code>-- Enable TimescaleDB (if using)\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Create tables for summarization\nCREATE TABLE logs (\n    timestamp TIMESTAMPTZ NOT NULL,\n    service TEXT,\n    message TEXT\n);\n\nSELECT create_hypertable('logs', 'timestamp');\n\n-- Create continuous aggregate with AI summarization\nCREATE MATERIALIZED VIEW hourly_summaries\nWITH (timescaledb.continuous) AS\nSELECT \n    time_bucket('1 hour', timestamp) AS hour,\n    service,\n    ai_summarize_partial(message, jsonb_build_object('service', service)) AS partial_summary\nFROM logs\nGROUP BY hour, service;\n</code></pre>"},{"location":"deployment/#structured-generation-v241","title":"Structured Generation (v2.4.1+)","text":"<p>Enable structured generation features:</p> <pre><code>-- Test structured generation\nSELECT steadytext_generate_json(\n    'Create a user profile',\n    '{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}}}'::jsonb\n);\n\nSELECT steadytext_generate_regex(\n    'Phone: ',\n    '\\d{3}-\\d{3}-\\d{4}'\n);\n\nSELECT steadytext_generate_choice(\n    'Sentiment:',\n    ARRAY['positive', 'negative', 'neutral']\n);\n</code></pre>"},{"location":"deployment/#5-managed-postgresql-alternatives","title":"5. Managed PostgreSQL Alternatives","text":"<p>Most managed PostgreSQL services (RDS, CloudSQL, Azure Database) don't support custom extensions. Alternative approaches:</p>"},{"location":"deployment/#option-1-separate-daemon-service","title":"Option 1: Separate Daemon Service","text":"<p>Run SteadyText daemon separately and connect via foreign data wrapper:</p> <pre><code>-- Create foreign server\nCREATE EXTENSION postgres_fdw;\n\nCREATE SERVER steadytext_server\nFOREIGN DATA WRAPPER postgres_fdw\nOPTIONS (host 'steadytext-daemon.example.com', port '5432', dbname 'steadytext');\n\n-- Create user mapping\nCREATE USER MAPPING FOR postgres\nSERVER steadytext_server\nOPTIONS (user 'steadytext', password 'secret');\n\n-- Import functions\nIMPORT FOREIGN SCHEMA public\nLIMIT TO (steadytext_generate, steadytext_embed)\nFROM SERVER steadytext_server\nINTO public;\n</code></pre>"},{"location":"deployment/#option-2-http-api-wrapper","title":"Option 2: HTTP API Wrapper","text":"<p>Create a REST API that PostgreSQL can call:</p> <pre><code>-- Using pg_http extension\nCREATE EXTENSION pg_http;\n\nCREATE OR REPLACE FUNCTION steadytext_generate_api(prompt TEXT)\nRETURNS TEXT AS $$\nDECLARE\n    response http_response;\n    result TEXT;\nBEGIN\n    response := http_post(\n        'https://api.steadytext.example.com/generate',\n        jsonb_build_object('prompt', prompt)::text,\n        'application/json'\n    );\n\n    IF response.status = 200 THEN\n        result := response.content::jsonb-&gt;&gt;'text';\n        RETURN result;\n    ELSE\n        RETURN NULL;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"deployment/#option-3-self-managed-instance","title":"Option 3: Self-Managed Instance","text":"<p>Deploy PostgreSQL on EC2/GCE/Azure VM with full control over extensions.</p>"},{"location":"deployment/#production-configuration","title":"Production Configuration","text":""},{"location":"deployment/#1-environment-configuration","title":"1. Environment Configuration","text":"<pre><code># /etc/environment or .env file\n# Performance\nSTEADYTEXT_GENERATION_CACHE_CAPACITY=4096\nSTEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=2048\nSTEADYTEXT_EMBEDDING_CACHE_CAPACITY=8192\nSTEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=4096\n\n# Security\nSTEADYTEXT_DAEMON_AUTH_TOKEN=your-secret-token\nSTEADYTEXT_ALLOWED_HOSTS=steadytext.example.com\n\n# Monitoring\nSTEADYTEXT_METRICS_ENABLED=true\nSTEADYTEXT_METRICS_PORT=9090\n\n# Logging\nSTEADYTEXT_LOG_LEVEL=INFO\nSTEADYTEXT_LOG_FILE=/var/log/steadytext/daemon.log\n</code></pre>"},{"location":"deployment/#2-resource-limits","title":"2. Resource Limits","text":"<pre><code># systemd resource limits\n[Service]\n# Memory limits\nMemoryMax=16G\nMemoryHigh=12G\n\n# CPU limits\nCPUQuota=400%  # 4 cores\n\n# File descriptor limits\nLimitNOFILE=65536\n\n# Process limits\nTasksMax=1024\n</code></pre>"},{"location":"deployment/#3-logging-configuration","title":"3. Logging Configuration","text":"<pre><code># logging_config.py\nLOGGING_CONFIG = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'standard': {\n            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'\n        },\n        'json': {\n            'class': 'pythonjsonlogger.jsonlogger.JsonFormatter',\n            'format': '%(asctime)s %(name)s %(levelname)s %(message)s'\n        }\n    },\n    'handlers': {\n        'file': {\n            'class': 'logging.handlers.RotatingFileHandler',\n            'filename': '/var/log/steadytext/daemon.log',\n            'maxBytes': 100_000_000,  # 100MB\n            'backupCount': 10,\n            'formatter': 'json'\n        },\n        'console': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'standard'\n        }\n    },\n    'root': {\n        'level': 'INFO',\n        'handlers': ['file', 'console']\n    }\n}\n</code></pre>"},{"location":"deployment/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"deployment/#1-prometheus-metrics","title":"1. Prometheus Metrics","text":"<pre><code># metrics.py\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\n\n# Define metrics\ngeneration_requests = Counter('steadytext_generation_requests_total', \n                            'Total generation requests')\ngeneration_duration = Histogram('steadytext_generation_duration_seconds',\n                              'Generation request duration')\ncache_hits = Counter('steadytext_cache_hits_total', \n                    'Cache hit count', ['cache_type'])\nactive_connections = Gauge('steadytext_active_connections',\n                          'Number of active connections')\n\n# Start metrics server\nstart_http_server(9090)\n</code></pre>"},{"location":"deployment/#2-grafana-dashboard","title":"2. Grafana Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"SteadyText Monitoring\",\n    \"panels\": [\n      {\n        \"title\": \"Request Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(steadytext_generation_requests_total[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Response Time\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, steadytext_generation_duration_seconds)\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Cache Hit Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(steadytext_cache_hits_total[5m]) / rate(steadytext_generation_requests_total[5m])\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"deployment/#3-health-checks","title":"3. Health Checks","text":"<pre><code># healthcheck.py\nfrom flask import Flask, jsonify\nimport steadytext\n\napp = Flask(__name__)\n\n@app.route('/health')\ndef health():\n    \"\"\"Basic health check.\"\"\"\n    return jsonify({'status': 'healthy'})\n\n@app.route('/ready')\ndef ready():\n    \"\"\"Readiness check with model test.\"\"\"\n    try:\n        result = steadytext.generate(\"test\", seed=42)\n        if result:\n            return jsonify({'status': 'ready'})\n        else:\n            return jsonify({'status': 'not ready'}), 503\n    except Exception as e:\n        return jsonify({'status': 'error', 'message': str(e)}), 503\n\n@app.route('/metrics')\ndef metrics():\n    \"\"\"Custom metrics endpoint.\"\"\"\n    from steadytext import get_cache_manager\n    stats = get_cache_manager().get_cache_stats()\n\n    return jsonify({\n        'cache': stats,\n        'models': {\n            'loaded': True,\n            'generation_model': 'gemma-3n',\n            'embedding_model': 'qwen3'\n        }\n    })\n</code></pre>"},{"location":"deployment/#security-considerations","title":"Security Considerations","text":""},{"location":"deployment/#1-network-security","title":"1. Network Security","text":"<pre><code># Rate limiting in Nginx\nlimit_req_zone $binary_remote_addr zone=steadytext:10m rate=10r/s;\n\nserver {\n    location / {\n        limit_req zone=steadytext burst=20 nodelay;\n        # ... proxy settings\n    }\n}\n</code></pre>"},{"location":"deployment/#2-authentication","title":"2. Authentication","text":"<pre><code># Simple token authentication\nimport hmac\nimport hashlib\n\ndef verify_token(request_token, secret_key):\n    \"\"\"Verify API token.\"\"\"\n    expected = hmac.new(\n        secret_key.encode(),\n        b\"steadytext\",\n        hashlib.sha256\n    ).hexdigest()\n    return hmac.compare_digest(request_token, expected)\n\n# Middleware\ndef require_auth(func):\n    def wrapper(*args, **kwargs):\n        token = request.headers.get('X-API-Token')\n        if not verify_token(token, SECRET_KEY):\n            abort(401)\n        return func(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"deployment/#3-input-validation","title":"3. Input Validation","text":"<pre><code>def validate_input(prompt: str) -&gt; bool:\n    \"\"\"Validate user input.\"\"\"\n    # Length check\n    if len(prompt) &gt; 10000:\n        return False\n\n    # Character validation\n    if not prompt.isprintable():\n        return False\n\n    # Rate limiting per user\n    if check_rate_limit(user_id):\n        return False\n\n    return True\n</code></pre>"},{"location":"deployment/#high-availability","title":"High Availability","text":""},{"location":"deployment/#1-load-balancing","title":"1. Load Balancing","text":"<pre><code># Nginx load balancing\nupstream steadytext_cluster {\n    least_conn;\n    server steadytext1.internal:5557 max_fails=3 fail_timeout=30s;\n    server steadytext2.internal:5557 max_fails=3 fail_timeout=30s;\n    server steadytext3.internal:5557 max_fails=3 fail_timeout=30s;\n    keepalive 32;\n}\n</code></pre>"},{"location":"deployment/#2-failover-configuration","title":"2. Failover Configuration","text":"<pre><code># Client with failover\nclass FailoverClient:\n    def __init__(self, servers):\n        self.servers = servers\n        self.current_server = 0\n\n    def generate(self, prompt, max_retries=3):\n        \"\"\"Generate with automatic failover.\"\"\"\n        for attempt in range(max_retries):\n            try:\n                server = self.servers[self.current_server]\n                return self._call_server(server, prompt)\n            except Exception as e:\n                logger.warning(f\"Server {server} failed: {e}\")\n                self.current_server = (self.current_server + 1) % len(self.servers)\n\n        raise Exception(\"All servers failed\")\n</code></pre>"},{"location":"deployment/#3-backup-and-recovery","title":"3. Backup and Recovery","text":"<pre><code>#!/bin/bash\n# backup.sh - Backup cache and models\n\nBACKUP_DIR=\"/backup/steadytext/$(date +%Y%m%d)\"\nmkdir -p \"$BACKUP_DIR\"\n\n# Backup cache\nrsync -av ~/.cache/steadytext/ \"$BACKUP_DIR/cache/\"\n\n# Backup models\nrsync -av ~/.cache/steadytext/models/ \"$BACKUP_DIR/models/\"\n\n# Backup configuration\ncp /etc/steadytext/* \"$BACKUP_DIR/config/\"\n\n# Compress\ntar -czf \"$BACKUP_DIR.tar.gz\" \"$BACKUP_DIR\"\nrm -rf \"$BACKUP_DIR\"\n\n# Upload to S3\naws s3 cp \"$BACKUP_DIR.tar.gz\" s3://backup-bucket/steadytext/\n</code></pre>"},{"location":"deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/#common-deployment-issues","title":"Common Deployment Issues","text":""},{"location":"deployment/#1-model-loading-failures","title":"1. Model Loading Failures","text":"<pre><code># Check model directory\nls -la ~/.cache/steadytext/models/\n\n# Download models manually\nst models download --all\n\n# Verify model integrity\nst models status --verify\n</code></pre>"},{"location":"deployment/#2-memory-issues","title":"2. Memory Issues","text":"<pre><code># Check memory usage\nfree -h\nps aux | grep steadytext\n\n# Adjust cache sizes\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=500\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=1000\n</code></pre>"},{"location":"deployment/#3-performance-issues","title":"3. Performance Issues","text":"<pre><code># Check daemon status\nst daemon status --verbose\n\n# Monitor resource usage\nhtop -p $(pgrep -f steadytext)\n\n# Check cache performance\nst cache --status --detailed\n</code></pre>"},{"location":"deployment/#debugging-production-issues","title":"Debugging Production Issues","text":"<pre><code># debug_helper.py\nimport logging\nimport traceback\nfrom functools import wraps\n\ndef debug_on_error(func):\n    \"\"\"Decorator to help debug production issues.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            logging.error(f\"Error in {func.__name__}:\")\n            logging.error(f\"Args: {args}\")\n            logging.error(f\"Kwargs: {kwargs}\")\n            logging.error(traceback.format_exc())\n            raise\n    return wrapper\n\n# Use in production\n@debug_on_error\ndef generate_text(prompt):\n    return steadytext.generate(prompt)\n</code></pre>"},{"location":"deployment/#deployment-checklist","title":"Deployment Checklist","text":""},{"location":"deployment/#pre-deployment","title":"Pre-Deployment","text":"<ul> <li>[ ] System requirements verified</li> <li>[ ] Python 3.8+ installed</li> <li>[ ] Sufficient disk space (10GB+)</li> <li>[ ] Network connectivity tested</li> <li>[ ] Security groups/firewalls configured</li> </ul>"},{"location":"deployment/#deployment","title":"Deployment","text":"<ul> <li>[ ] SteadyText installed</li> <li>[ ] Models downloaded</li> <li>[ ] Daemon configured</li> <li>[ ] Service scripts created</li> <li>[ ] Reverse proxy configured</li> <li>[ ] SSL certificates installed</li> </ul>"},{"location":"deployment/#post-deployment","title":"Post-Deployment","text":"<ul> <li>[ ] Health checks passing</li> <li>[ ] Monitoring configured</li> <li>[ ] Logs being collected</li> <li>[ ] Backup strategy implemented</li> <li>[ ] Performance benchmarked</li> <li>[ ] Documentation updated</li> </ul>"},{"location":"deployment/#production-readiness","title":"Production Readiness","text":"<ul> <li>[ ] High availability configured</li> <li>[ ] Auto-scaling enabled</li> <li>[ ] Rate limiting active</li> <li>[ ] Security hardened</li> <li>[ ] Disaster recovery tested</li> <li>[ ] Team trained</li> </ul>"},{"location":"deployment/#support","title":"Support","text":"<ul> <li>Documentation: steadytext.readthedocs.io</li> <li>Issues: GitHub Issues</li> <li>Community: Discussions</li> </ul>"},{"location":"eos-string-implementation/","title":"EOS String Implementation Summary","text":"<p>This document summarizes the implementation of the custom <code>eos_string</code> parameter feature.</p>"},{"location":"eos-string-implementation/#changes-made","title":"Changes Made","text":""},{"location":"eos-string-implementation/#1-core-generation-module-steadytextcoregeneratorpy","title":"1. Core Generation Module (<code>steadytext/core/generator.py</code>)","text":"<ul> <li>Added <code>eos_string</code> parameter to <code>DeterministicGenerator.generate()</code> method</li> <li>Default value: <code>\"[EOS]\"</code> (special marker for model's default EOS token)</li> <li> <p>When custom value provided, it's added to the stop sequences</p> </li> <li> <p>Added <code>eos_string</code> parameter to <code>DeterministicGenerator.generate_iter()</code> method</p> </li> <li>Supports streaming generation with custom stop strings</li> <li> <p>Added <code>include_logprobs</code> parameter for compatibility with CLI</p> </li> <li> <p>Updated caching logic to include <code>eos_string</code> in cache key when not default</p> </li> <li>Ensures different eos_strings produce separately cached results</li> </ul>"},{"location":"eos-string-implementation/#2-public-api-steadytext__init__py","title":"2. Public API (<code>steadytext/__init__.py</code>)","text":"<ul> <li> <p>Updated <code>generate()</code> function signature:   <pre><code>def generate(prompt: str, return_logprobs: bool = False, eos_string: str = \"[EOS]\")\n</code></pre></p> </li> <li> <p>Updated <code>generate_iter()</code> function signature:   <pre><code>def generate_iter(prompt: str, eos_string: str = \"[EOS]\", include_logprobs: bool = False)\n</code></pre></p> </li> </ul>"},{"location":"eos-string-implementation/#3-cli-updates","title":"3. CLI Updates","text":""},{"location":"eos-string-implementation/#generate-command-steadytextclicommandsgeneratepy","title":"Generate Command (<code>steadytext/cli/commands/generate.py</code>)","text":"<ul> <li>Added <code>--eos-string</code> parameter (default: \"[EOS]\")</li> <li>Passes eos_string to both batch and streaming generation</li> </ul>"},{"location":"eos-string-implementation/#main-cli-steadytextclimainpy","title":"Main CLI (<code>steadytext/cli/main.py</code>)","text":"<ul> <li>Added <code>--quiet</code> / <code>-q</code> flag to silence log output</li> <li>Sets logging level to ERROR for both steadytext and llama_cpp loggers when quiet mode is enabled</li> </ul>"},{"location":"eos-string-implementation/#4-tests-teststest_steadytextpy","title":"4. Tests (<code>tests/test_steadytext.py</code>)","text":"<p>Added three new test methods: - <code>test_generate_with_custom_eos_string()</code> - Tests basic eos_string functionality - <code>test_generate_iter_with_eos_string()</code> - Tests streaming with custom eos_string - <code>test_generate_eos_string_with_logprobs()</code> - Tests combination of eos_string and logprobs</p>"},{"location":"eos-string-implementation/#5-test-scripts","title":"5. Test Scripts","text":"<p>Created two test scripts for manual verification: - <code>test_eos_string.py</code> - Python script testing various eos_string scenarios - <code>test_cli_eos.sh</code> - Bash script testing CLI functionality</p>"},{"location":"eos-string-implementation/#usage-examples","title":"Usage Examples","text":""},{"location":"eos-string-implementation/#python-api","title":"Python API","text":"<pre><code>import steadytext\n\n# Use model's default EOS token\ntext = steadytext.generate(\"Hello world\", eos_string=\"[EOS]\")\n\n# Stop at custom string\ntext = steadytext.generate(\"List items until END\", eos_string=\"END\")\n\n# Streaming with custom eos\nfor token in steadytext.generate_iter(\"Generate text\", eos_string=\"STOP\"):\n    print(token, end=\"\")\n</code></pre>"},{"location":"eos-string-implementation/#cli","title":"CLI","text":"<pre><code># Default behavior\nsteadytext \"Generate some text\"\n\n# Custom eos string\nsteadytext \"Generate until DONE\" --eos-string \"DONE\"\n\n# Quiet mode (no logs)\nsteadytext --quiet \"Generate without logs\"\n\n# Streaming with custom eos\nsteadytext \"Stream until END\" --stream --eos-string \"END\"\n</code></pre>"},{"location":"eos-string-implementation/#implementation-notes","title":"Implementation Notes","text":"<ol> <li> <p>The <code>\"[EOS]\"</code> string is a special marker that tells the system to use the model's default EOS token and stop sequences.</p> </li> <li> <p>When a custom eos_string is provided, it's added to the existing stop sequences rather than replacing them.</p> </li> <li> <p>Cache keys include the eos_string when it's not the default, ensuring proper caching behavior.</p> </li> <li> <p>The quiet flag affects all loggers in the steadytext namespace and llama_cpp if present.</p> </li> </ol>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Find answers to common questions about SteadyText, troubleshooting tips, and best practices.</p>"},{"location":"faq/#table-of-contents","title":"Table of Contents","text":"<ul> <li>General Questions</li> <li>Installation &amp; Setup</li> <li>Usage Questions</li> <li>Performance Questions</li> <li>Model Questions</li> <li>Caching Questions</li> <li>Daemon Questions</li> <li>PostgreSQL Extension</li> <li>Troubleshooting</li> <li>Advanced Topics</li> </ul>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-steadytext","title":"What is SteadyText?","text":"<p>SteadyText is a deterministic AI text generation and embedding library for Python. It ensures that the same input always produces the same output, making it ideal for:</p> <ul> <li>Reproducible research</li> <li>Testing AI-powered applications</li> <li>Consistent embeddings for search</li> <li>Deterministic content generation</li> </ul>"},{"location":"faq/#how-is-steadytext-different-from-other-ai-libraries","title":"How is SteadyText different from other AI libraries?","text":"Feature SteadyText Other Libraries Deterministic \u2705 Always \u274c Usually random Never fails \u2705 Returns None \u274c Throws exceptions Zero config \u2705 Works instantly \u274c Complex setup Built-in cache \u2705 Automatic \u274c Manual setup PostgreSQL \u2705 Native extension \u274c External integration"},{"location":"faq/#what-models-does-steadytext-use","title":"What models does SteadyText use?","text":"<ul> <li>Text Generation: Gemma-3n models (2B and 4B parameters)</li> <li>Embeddings: Qwen3-Embedding-0.6B (1024 dimensions)</li> <li>Format: GGUF quantized models for efficiency</li> </ul>"},{"location":"faq/#is-steadytext-suitable-for-production","title":"Is SteadyText suitable for production?","text":"<p>Yes! SteadyText is designed for production use:</p> <ul> <li>Daemon mode: 160x faster responses</li> <li>Thread-safe: Handles concurrent requests</li> <li>Resource efficient: Quantized models use less memory</li> <li>Battle-tested: Used in production environments</li> <li>PostgreSQL integration: Database-native AI</li> </ul>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#how-do-i-install-steadytext","title":"How do I install SteadyText?","text":"<pre><code># Using pip\npip install steadytext\n\n# Using UV (recommended)\nuv add steadytext\n\n# With PostgreSQL extension\npip install steadytext[postgres]\n</code></pre>"},{"location":"faq/#what-are-the-system-requirements","title":"What are the system requirements?","text":"<ul> <li>Python: 3.8 or higher</li> <li>Memory: 4GB RAM minimum (8GB recommended)</li> <li>Disk: 2GB for models</li> <li>OS: Linux, macOS, Windows</li> </ul>"},{"location":"faq/#do-i-need-a-gpu","title":"Do I need a GPU?","text":"<p>No, SteadyText is optimized for CPU inference. GPU support is planned for future releases.</p>"},{"location":"faq/#how-do-i-verify-the-installation","title":"How do I verify the installation?","text":"<pre><code># Check CLI\nst --version\n\n# Test generation\necho \"Hello world\" | st\n\n# Python test\npython -c \"import steadytext; print(steadytext.generate('Hello'))\"\n</code></pre>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#how-do-i-ensure-deterministic-results","title":"How do I ensure deterministic results?","text":"<p>Use the same seed value:</p> <pre><code># Always produces the same output\nresult1 = steadytext.generate(\"Hello\", seed=42)\nresult2 = steadytext.generate(\"Hello\", seed=42)\nassert result1 == result2\n</code></pre>"},{"location":"faq/#can-i-use-custom-prompts","title":"Can I use custom prompts?","text":"<p>Yes, any text prompt works:</p> <pre><code># Simple prompts\ntext = steadytext.generate(\"Write a poem\")\n\n# Complex prompts with instructions\nprompt = \"\"\"\nYou are a helpful assistant. Please:\n1. Summarize the following text\n2. Extract key points\n3. Suggest improvements\n\nText: [your text here]\n\"\"\"\nresult = steadytext.generate(prompt)\n</code></pre>"},{"location":"faq/#how-do-i-generate-longer-texts","title":"How do I generate longer texts?","text":"<p>Adjust the <code>max_new_tokens</code> parameter:</p> <pre><code># Default: 512 tokens\nshort = steadytext.generate(\"Story\", max_new_tokens=100)\n\n# Longer output\nlong = steadytext.generate(\"Story\", max_new_tokens=2000)\n</code></pre>"},{"location":"faq/#can-i-stream-the-output","title":"Can I stream the output?","text":"<p>Yes, use the streaming API:</p> <pre><code>for chunk in steadytext.generate_iter(\"Write a long story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"faq/#how-do-embeddings-work","title":"How do embeddings work?","text":"<pre><code># Create embedding\nembedding = steadytext.embed(\"Machine learning\")\n# Returns: numpy array of shape (1024,)\n\n# Compare similarity\nemb1 = steadytext.embed(\"cat\")\nemb2 = steadytext.embed(\"dog\")\nsimilarity = np.dot(emb1, emb2)  # Cosine similarity\n</code></pre>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#why-is-the-first-generation-slow","title":"Why is the first generation slow?","text":"<p>The first call loads the model into memory (2-3 seconds). Subsequent calls are fast (&lt;100ms). To avoid this:</p> <pre><code># Option 1: Use daemon mode\nst daemon start\n\n# Option 2: Preload models\nst models preload\n</code></pre>"},{"location":"faq/#how-can-i-improve-performance","title":"How can I improve performance?","text":"<ol> <li> <p>Use daemon mode (160x faster):    <pre><code>st daemon start\n</code></pre></p> </li> <li> <p>Enable caching (enabled by default):    <pre><code># Cache automatically stores results\nresult = steadytext.generate(\"Same prompt\")  # First: slow\nresult = steadytext.generate(\"Same prompt\")  # Second: instant\n</code></pre></p> </li> <li> <p>Batch operations:    <pre><code>prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\nresults = [steadytext.generate(p) for p in prompts]\n</code></pre></p> </li> </ol>"},{"location":"faq/#what-are-typical-response-times","title":"What are typical response times?","text":"Operation First Call Cached With Daemon Generate 2-3s &lt;10ms &lt;20ms Embed 1-2s &lt;5ms &lt;15ms Batch (100) 3-5s &lt;100ms &lt;200ms"},{"location":"faq/#model-questions","title":"Model Questions","text":""},{"location":"faq/#can-i-use-different-model-sizes","title":"Can I use different model sizes?","text":"<p>Yes, SteadyText supports multiple model sizes:</p> <pre><code># CLI\nst generate \"Hello\" --size small  # Fast, 2B parameters\nst generate \"Hello\" --size large  # Better quality, 4B parameters\n\n# Python\ntext = steadytext.generate(\"Hello\", model_size=\"large\")\n</code></pre>"},{"location":"faq/#can-i-use-custom-models","title":"Can I use custom models?","text":"<p>Currently, SteadyText uses pre-selected models for consistency. Custom model support is planned for future releases.</p>"},{"location":"faq/#how-much-disk-space-do-models-use","title":"How much disk space do models use?","text":"<ul> <li>Small generation model: ~1.3GB</li> <li>Large generation model: ~2.1GB  </li> <li>Embedding model: ~0.6GB</li> <li>Total (all models): ~4GB</li> </ul>"},{"location":"faq/#where-are-models-stored","title":"Where are models stored?","text":"<p>Models are cached in platform-specific directories:</p> <pre><code># Linux/Mac\n~/.cache/steadytext/models/\n\n# Windows\n%LOCALAPPDATA%\\steadytext\\steadytext\\models\\\n\n# Check location\nfrom steadytext.utils import get_model_cache_dir\nprint(get_model_cache_dir())\n</code></pre>"},{"location":"faq/#caching-questions","title":"Caching Questions","text":""},{"location":"faq/#how-does-caching-work","title":"How does caching work?","text":"<p>SteadyText uses a frecency cache (frequency + recency):</p> <pre><code># First call: generates and caches\nresult1 = steadytext.generate(\"Hello\", seed=42)  # Slow\n\n# Second call: returns from cache\nresult2 = steadytext.generate(\"Hello\", seed=42)  # Instant\n\n# Different seed: new generation\nresult3 = steadytext.generate(\"Hello\", seed=123)  # Slow\n</code></pre>"},{"location":"faq/#can-i-disable-caching","title":"Can I disable caching?","text":"<pre><code># Disable via environment variable\nexport STEADYTEXT_DISABLE_CACHE=1\n\n# Or in Python\nimport os\nos.environ['STEADYTEXT_DISABLE_CACHE'] = '1'\n</code></pre>"},{"location":"faq/#how-do-i-clear-the-cache","title":"How do I clear the cache?","text":"<pre><code># CLI\nst cache --clear\n\n# Python\nfrom steadytext import get_cache_manager\ncache_manager = get_cache_manager()\ncache_manager.clear_all_caches()\n</code></pre>"},{"location":"faq/#how-much-cache-space-is-used","title":"How much cache space is used?","text":"<pre><code># Check cache statistics\nfrom steadytext import get_cache_manager\nstats = get_cache_manager().get_cache_stats()\nprint(f\"Generation cache: {stats['generation']['size']} entries\")\nprint(f\"Embedding cache: {stats['embedding']['size']} entries\")\n</code></pre>"},{"location":"faq/#daemon-questions","title":"Daemon Questions","text":""},{"location":"faq/#what-is-daemon-mode","title":"What is daemon mode?","text":"<p>The daemon is a background service that keeps models loaded in memory, providing 160x faster first responses.</p>"},{"location":"faq/#how-do-i-start-the-daemon","title":"How do I start the daemon?","text":"<pre><code># Start in background\nst daemon start\n\n# Start in foreground (see logs)\nst daemon start --foreground\n\n# Check status\nst daemon status\n</code></pre>"},{"location":"faq/#is-the-daemon-used-automatically","title":"Is the daemon used automatically?","text":"<p>Yes! When the daemon is running, all SteadyText operations automatically use it:</p> <pre><code># Automatically uses daemon if available\ntext = steadytext.generate(\"Hello\")\n</code></pre>"},{"location":"faq/#how-do-i-stop-the-daemon","title":"How do I stop the daemon?","text":"<pre><code># Graceful stop\nst daemon stop\n\n# Force stop\nst daemon stop --force\n</code></pre>"},{"location":"faq/#can-i-run-multiple-daemons","title":"Can I run multiple daemons?","text":"<p>Currently, only one daemon instance is supported per machine. Multi-daemon support is planned for future releases.</p>"},{"location":"faq/#postgresql-extension","title":"PostgreSQL Extension","text":""},{"location":"faq/#how-do-i-install-pg_steadytext","title":"How do I install pg_steadytext?","text":"<pre><code># Using Docker (recommended)\ncd pg_steadytext\ndocker build -t pg_steadytext .\ndocker run -d -p 5432:5432 pg_steadytext\n\n# Manual installation\ncd pg_steadytext\n./install.sh\n</code></pre>"},{"location":"faq/#how-do-i-use-it-in-sql","title":"How do I use it in SQL?","text":"<pre><code>-- Enable extension\nCREATE EXTENSION pg_steadytext;\n\n-- Generate text\nSELECT steadytext_generate('Write a SQL tutorial');\n\n-- Create embeddings\nSELECT steadytext_embed('PostgreSQL database');\n</code></pre>"},{"location":"faq/#is-it-production-ready","title":"Is it production-ready?","text":"<p>The PostgreSQL extension is currently experimental. Use with caution in production environments.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#model-not-found-error","title":"\"Model not found\" error","text":"<pre><code># Download models manually\nst models download --all\n\n# Or set environment variable\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n</code></pre>"},{"location":"faq/#none-returned-instead-of-text","title":"\"None\" returned instead of text","text":"<p>This is the expected behavior in v2.1.0+ when models can't be loaded:</p> <pre><code># Check if generation succeeded\nresult = steadytext.generate(\"Hello\")\nif result is None:\n    print(\"Model not available\")\nelse:\n    print(f\"Generated: {result}\")\n</code></pre>"},{"location":"faq/#daemon-wont-start","title":"Daemon won't start","text":"<pre><code># Check if port is in use\nlsof -i :5557\n\n# Try different port\nst daemon start --port 5558\n\n# Check logs\nst daemon start --foreground\n</code></pre>"},{"location":"faq/#high-memory-usage","title":"High memory usage","text":"<pre><code># Use smaller model\nst generate \"Hello\" --size small\n\n# Limit cache size\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100\n</code></pre>"},{"location":"faq/#slow-generation","title":"Slow generation","text":"<pre><code># Start daemon for faster responses\nst daemon start\n\n# Check cache is working\nst cache --status\n\n# Use smaller model\nst generate \"Hello\" --size small\n</code></pre>"},{"location":"faq/#advanced-topics","title":"Advanced Topics","text":""},{"location":"faq/#how-do-i-use-steadytext-in-production","title":"How do I use SteadyText in production?","text":"<ol> <li> <p>Use daemon mode:    <pre><code># systemd service\nsudo systemctl enable steadytext\nsudo systemctl start steadytext\n</code></pre></p> </li> <li> <p>Configure caching:    <pre><code>export STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=500\n</code></pre></p> </li> <li> <p>Monitor performance:    <pre><code>from steadytext import get_cache_manager\nstats = get_cache_manager().get_cache_stats()\n# Log stats to monitoring system\n</code></pre></p> </li> </ol>"},{"location":"faq/#can-i-use-steadytext-with-async-code","title":"Can I use SteadyText with async code?","text":"<pre><code>import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nexecutor = ThreadPoolExecutor(max_workers=4)\n\nasync def async_generate(prompt):\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(\n        executor, \n        steadytext.generate, \n        prompt\n    )\n\n# Use in async function\nresult = await async_generate(\"Hello\")\n</code></pre>"},{"location":"faq/#how-do-i-handle-errors-gracefully","title":"How do I handle errors gracefully?","text":"<pre><code>def safe_generate(prompt, fallback=\"Unable to generate\"):\n    try:\n        result = steadytext.generate(prompt)\n        if result is None:\n            return fallback\n        return result\n    except Exception as e:\n        logger.error(f\"Generation failed: {e}\")\n        return fallback\n</code></pre>"},{"location":"faq/#can-i-use-steadytext-with-langchain","title":"Can I use SteadyText with langchain?","text":"<pre><code>from langchain.llms.base import LLM\n\nclass SteadyTextLLM(LLM):\n    def _call(self, prompt: str, stop=None) -&gt; str:\n        result = steadytext.generate(prompt)\n        return result if result else \"\"\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"steadytext\"\n\n# Use with langchain\nllm = SteadyTextLLM()\n</code></pre>"},{"location":"faq/#how-do-i-benchmark-performance","title":"How do I benchmark performance?","text":"<pre><code># Run built-in benchmarks\ncd benchmarks\npython run_all_benchmarks.py\n\n# Quick benchmark\npython run_all_benchmarks.py --quick\n</code></pre>"},{"location":"faq/#can-i-contribute-to-steadytext","title":"Can I contribute to SteadyText?","text":"<p>Yes! We welcome contributions:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Run tests: <code>uv run pytest</code></li> <li>Submit a pull request</li> </ol> <p>See CONTRIBUTING.md for details.</p>"},{"location":"faq/#still-have-questions","title":"Still Have Questions?","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Join the community</li> <li>Documentation: Read the full docs</li> </ul>"},{"location":"faq/#quick-reference","title":"Quick Reference","text":""},{"location":"faq/#common-commands","title":"Common Commands","text":"<pre><code># Generation\necho \"prompt\" | st\nst generate \"prompt\" --seed 42\n\n# Embeddings\nst embed \"text\"\nst embed \"text\" --format numpy\n\n# Daemon\nst daemon start\nst daemon status\nst daemon stop\n\n# Cache\nst cache --status\nst cache --clear\n\n# Models\nst models list\nst models download --all\nst models preload\n</code></pre>"},{"location":"faq/#common-patterns","title":"Common Patterns","text":"<pre><code># Basic usage\nimport steadytext\n\n# Generate text\ntext = steadytext.generate(\"Hello world\")\n\n# Create embedding\nembedding = steadytext.embed(\"Hello world\")\n\n# With custom seed\ntext = steadytext.generate(\"Hello\", seed=123)\n\n# Streaming\nfor chunk in steadytext.generate_iter(\"Tell a story\"):\n    print(chunk, end='')\n\n# Batch processing\nprompts = [\"One\", \"Two\", \"Three\"]\nresults = [steadytext.generate(p) for p in prompts]\n</code></pre>"},{"location":"integrations/","title":"Integrations Guide","text":"<p>This guide covers integrating SteadyText with popular frameworks, tools, and platforms.</p>"},{"location":"integrations/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Web Frameworks</li> <li>FastAPI</li> <li>Flask</li> <li>Django</li> <li>Streamlit</li> <li>AI/ML Frameworks</li> <li>LangChain</li> <li>LlamaIndex</li> <li>Haystack</li> <li>Hugging Face</li> <li>Database Integrations</li> <li>PostgreSQL</li> <li>MongoDB</li> <li>Redis</li> <li>Elasticsearch</li> <li>Vector Databases</li> <li>Pinecone</li> <li>Weaviate</li> <li>Chroma</li> <li>Qdrant</li> <li>Cloud Platforms</li> <li>AWS</li> <li>Google Cloud</li> <li>Azure</li> <li>Vercel</li> <li>Data Processing</li> <li>Apache Spark</li> <li>Pandas</li> <li>Dask</li> <li>Ray</li> <li>Monitoring &amp; Observability</li> <li>Prometheus</li> <li>OpenTelemetry</li> <li>Datadog</li> <li>New Relic</li> <li>Development Tools</li> <li>Jupyter</li> <li>VS Code</li> <li>PyCharm</li> <li>Docker</li> </ul>"},{"location":"integrations/#web-frameworks","title":"Web Frameworks","text":""},{"location":"integrations/#fastapi","title":"FastAPI","text":"<p>Create high-performance APIs with SteadyText:</p> <pre><code># app.py\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel\nimport steadytext\nimport asyncio\nfrom typing import List, Optional\n\napp = FastAPI(title=\"SteadyText API\", version=\"1.0.0\")\n\nclass GenerateRequest(BaseModel):\n    prompt: str\n    seed: Optional[int] = 42\n    max_new_tokens: Optional[int] = 512\n    model_size: Optional[str] = \"small\"\n\nclass GenerateResponse(BaseModel):\n    text: str\n    seed: int\n    cached: bool\n    duration_ms: float\n\nclass EmbedRequest(BaseModel):\n    text: str\n    seed: Optional[int] = 42\n\nclass EmbedResponse(BaseModel):\n    embedding: List[float]\n    dimension: int\n    seed: int\n\n@app.post(\"/generate\", response_model=GenerateResponse)\nasync def generate_text(request: GenerateRequest):\n    \"\"\"Generate text using SteadyText.\"\"\"\n    import time\n    start_time = time.perf_counter()\n\n    # Run in thread pool to avoid blocking\n    loop = asyncio.get_event_loop()\n    result = await loop.run_in_executor(\n        None,\n        steadytext.generate,\n        request.prompt,\n        request.seed,\n        request.max_new_tokens\n    )\n\n    duration_ms = (time.perf_counter() - start_time) * 1000\n\n    if result is None:\n        raise HTTPException(status_code=503, detail=\"Model not available\")\n\n    return GenerateResponse(\n        text=result,\n        seed=request.seed,\n        cached=False,  # Could check cache for this\n        duration_ms=duration_ms\n    )\n\n@app.post(\"/embed\", response_model=EmbedResponse)\nasync def create_embedding(request: EmbedRequest):\n    \"\"\"Create text embedding.\"\"\"\n    loop = asyncio.get_event_loop()\n    embedding = await loop.run_in_executor(\n        None,\n        steadytext.embed,\n        request.text,\n        request.seed\n    )\n\n    if embedding is None:\n        raise HTTPException(status_code=503, detail=\"Embedding model not available\")\n\n    return EmbedResponse(\n        embedding=embedding.tolist(),\n        dimension=len(embedding),\n        seed=request.seed\n    )\n\n@app.get(\"/generate/stream\")\nasync def stream_generate(prompt: str, seed: int = 42):\n    \"\"\"Stream text generation.\"\"\"\n    from fastapi.responses import StreamingResponse\n\n    async def generate_stream():\n        for chunk in steadytext.generate_iter(prompt, seed=seed):\n            yield f\"data: {chunk}\\n\\n\"\n        yield \"data: [DONE]\\n\\n\"\n\n    return StreamingResponse(\n        generate_stream(),\n        media_type=\"text/plain\",\n        headers={\"Cache-Control\": \"no-cache\"}\n    )\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    try:\n        result = steadytext.generate(\"test\", seed=42)\n        return {\n            \"status\": \"healthy\",\n            \"model_available\": result is not None\n        }\n    except Exception as e:\n        return {\n            \"status\": \"unhealthy\",\n            \"error\": str(e)\n        }\n\n# Start with: uvicorn app:app --reload\n</code></pre>"},{"location":"integrations/#flask","title":"Flask","text":"<p>Traditional web applications with SteadyText:</p> <pre><code># flask_app.py\nfrom flask import Flask, request, jsonify, render_template, stream_template\nimport steadytext\nimport json\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    \"\"\"Main page with text generation form.\"\"\"\n    return render_template('index.html')\n\n@app.route('/api/generate', methods=['POST'])\ndef api_generate():\n    \"\"\"API endpoint for text generation.\"\"\"\n    data = request.get_json()\n\n    if not data or 'prompt' not in data:\n        return jsonify({'error': 'Missing prompt'}), 400\n\n    prompt = data['prompt']\n    seed = data.get('seed', 42)\n    max_tokens = data.get('max_new_tokens', 512)\n\n    result = steadytext.generate(\n        prompt,\n        seed=seed,\n        max_new_tokens=max_tokens\n    )\n\n    if result is None:\n        return jsonify({'error': 'Model not available'}), 503\n\n    return jsonify({\n        'text': result,\n        'seed': seed,\n        'prompt': prompt\n    })\n\n@app.route('/api/embed', methods=['POST'])\ndef api_embed():\n    \"\"\"API endpoint for creating embeddings.\"\"\"\n    data = request.get_json()\n\n    if not data or 'text' not in data:\n        return jsonify({'error': 'Missing text'}), 400\n\n    text = data['text']\n    seed = data.get('seed', 42)\n\n    embedding = steadytext.embed(text, seed=seed)\n\n    if embedding is None:\n        return jsonify({'error': 'Embedding model not available'}), 503\n\n    return jsonify({\n        'embedding': embedding.tolist(),\n        'dimension': len(embedding),\n        'text': text,\n        'seed': seed\n    })\n\n@app.route('/stream')\ndef stream_demo():\n    \"\"\"Demo page for streaming generation.\"\"\"\n    return render_template('stream.html')\n\n@app.route('/api/stream')\ndef api_stream():\n    \"\"\"Server-sent events for streaming.\"\"\"\n    prompt = request.args.get('prompt', 'Tell me a story')\n    seed = int(request.args.get('seed', 42))\n\n    def event_stream():\n        for chunk in steadytext.generate_iter(prompt, seed=seed):\n            yield f\"data: {json.dumps({'chunk': chunk})}\\n\\n\"\n        yield f\"data: {json.dumps({'done': True})}\\n\\n\"\n\n    return app.response_class(\n        event_stream(),\n        mimetype='text/event-stream',\n        headers={\n            'Cache-Control': 'no-cache',\n            'X-Accel-Buffering': 'no'\n        }\n    )\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"integrations/#django","title":"Django","text":"<p>Enterprise web applications:</p> <pre><code># views.py\nfrom django.http import JsonResponse, StreamingHttpResponse\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.views.decorators.http import require_http_methods\nfrom django.utils.decorators import method_decorator\nfrom django.views.generic import View\nimport json\nimport steadytext\n\n@csrf_exempt\n@require_http_methods([\"POST\"])\ndef generate_view(request):\n    \"\"\"Django view for text generation.\"\"\"\n    try:\n        data = json.loads(request.body)\n        prompt = data.get('prompt')\n        seed = data.get('seed', 42)\n\n        if not prompt:\n            return JsonResponse({'error': 'Missing prompt'}, status=400)\n\n        result = steadytext.generate(prompt, seed=seed)\n\n        if result is None:\n            return JsonResponse({'error': 'Model not available'}, status=503)\n\n        return JsonResponse({\n            'text': result,\n            'seed': seed\n        })\n\n    except json.JSONDecodeError:\n        return JsonResponse({'error': 'Invalid JSON'}, status=400)\n    except Exception as e:\n        return JsonResponse({'error': str(e)}, status=500)\n\n@method_decorator(csrf_exempt, name='dispatch')\nclass EmbeddingView(View):\n    \"\"\"Class-based view for embeddings.\"\"\"\n\n    def post(self, request):\n        try:\n            data = json.loads(request.body)\n            text = data.get('text')\n            seed = data.get('seed', 42)\n\n            if not text:\n                return JsonResponse({'error': 'Missing text'}, status=400)\n\n            embedding = steadytext.embed(text, seed=seed)\n\n            if embedding is None:\n                return JsonResponse({'error': 'Model not available'}, status=503)\n\n            return JsonResponse({\n                'embedding': embedding.tolist(),\n                'dimension': len(embedding)\n            })\n\n        except Exception as e:\n            return JsonResponse({'error': str(e)}, status=500)\n\n# models.py\nfrom django.db import models\nimport numpy as np\n\nclass Document(models.Model):\n    \"\"\"Document model with embedding support.\"\"\"\n    title = models.CharField(max_length=200)\n    content = models.TextField()\n    embedding = models.JSONField(null=True, blank=True)\n    created_at = models.DateTimeField(auto_now_add=True)\n\n    def save(self, *args, **kwargs):\n        # Auto-generate embedding\n        if self.content and not self.embedding:\n            emb = steadytext.embed(self.content)\n            if emb is not None:\n                self.embedding = emb.tolist()\n        super().save(*args, **kwargs)\n\n    def similarity(self, other_doc):\n        \"\"\"Calculate cosine similarity with another document.\"\"\"\n        if not self.embedding or not other_doc.embedding:\n            return 0.0\n\n        emb1 = np.array(self.embedding)\n        emb2 = np.array(other_doc.embedding)\n\n        return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n\n# urls.py\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('api/generate/', views.generate_view, name='generate'),\n    path('api/embed/', views.EmbeddingView.as_view(), name='embed'),\n]\n</code></pre>"},{"location":"integrations/#streamlit","title":"Streamlit","text":"<p>Interactive data science applications:</p> <pre><code># streamlit_app.py\nimport streamlit as st\nimport steadytext\nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\n\nst.set_page_config(\n    page_title=\"SteadyText Demo\",\n    page_icon=\"\ud83e\udd16\",\n    layout=\"wide\"\n)\n\nst.title(\"\ud83e\udd16 SteadyText Interactive Demo\")\n\n# Sidebar\nst.sidebar.header(\"Configuration\")\nseed = st.sidebar.number_input(\"Random Seed\", value=42, min_value=0)\nmax_tokens = st.sidebar.slider(\"Max Tokens\", 50, 1000, 512)\nmodel_size = st.sidebar.selectbox(\"Model Size\", [\"small\", \"large\"])\n\n# Text Generation Tab\ntab1, tab2, tab3 = st.tabs([\"Generate\", \"Embed\", \"Compare\"])\n\nwith tab1:\n    st.header(\"Text Generation\")\n\n    prompt = st.text_area(\n        \"Enter your prompt:\",\n        value=\"Write a short story about AI\",\n        height=100\n    )\n\n    col1, col2 = st.columns([1, 3])\n\n    with col1:\n        if st.button(\"Generate\", type=\"primary\"):\n            with st.spinner(\"Generating...\"):\n                result = steadytext.generate(\n                    prompt,\n                    seed=seed,\n                    max_new_tokens=max_tokens\n                )\n\n                if result:\n                    st.session_state.generated_text = result\n                else:\n                    st.error(\"Model not available\")\n\n    with col2:\n        if 'generated_text' in st.session_state:\n            st.text_area(\n                \"Generated Text:\",\n                value=st.session_state.generated_text,\n                height=300\n            )\n\nwith tab2:\n    st.header(\"Text Embeddings\")\n\n    text_input = st.text_input(\n        \"Enter text to embed:\",\n        value=\"Machine learning is fascinating\"\n    )\n\n    if st.button(\"Create Embedding\"):\n        with st.spinner(\"Creating embedding...\"):\n            embedding = steadytext.embed(text_input, seed=seed)\n\n            if embedding is not None:\n                st.success(f\"Created {len(embedding)}-dimensional embedding\")\n\n                # Visualize embedding (first 50 dimensions)\n                fig = px.bar(\n                    x=list(range(50)),\n                    y=embedding[:50],\n                    title=\"Embedding Values (First 50 Dimensions)\"\n                )\n                st.plotly_chart(fig, use_container_width=True)\n\n                # Show statistics\n                col1, col2, col3 = st.columns(3)\n                with col1:\n                    st.metric(\"Mean\", f\"{np.mean(embedding):.4f}\")\n                with col2:\n                    st.metric(\"Std Dev\", f\"{np.std(embedding):.4f}\")\n                with col3:\n                    st.metric(\"L2 Norm\", f\"{np.linalg.norm(embedding):.4f}\")\n            else:\n                st.error(\"Embedding model not available\")\n\nwith tab3:\n    st.header(\"Compare Outputs\")\n\n    st.subheader(\"Seed Comparison\")\n\n    test_prompt = st.text_input(\n        \"Test prompt:\",\n        value=\"Explain quantum computing\"\n    )\n\n    seeds = st.multiselect(\n        \"Seeds to compare:\",\n        options=[42, 123, 456, 789],\n        default=[42, 123]\n    )\n\n    if st.button(\"Compare Seeds\") and seeds:\n        results = {}\n\n        for s in seeds:\n            with st.spinner(f\"Generating with seed {s}...\"):\n                result = steadytext.generate(test_prompt, seed=s)\n                if result:\n                    results[s] = result\n\n        # Display results\n        for seed_val, text in results.items():\n            st.subheader(f\"Seed {seed_val}\")\n            st.text_area(f\"Result for seed {seed_val}\", value=text, height=150)\n\n        # Check determinism\n        if len(set(results.values())) == 1:\n            st.success(\"\u2705 All outputs are identical (deterministic)\")\n        else:\n            st.info(\"\u2139\ufe0f Different seeds produce different outputs\")\n\n# Cache status\nif st.sidebar.button(\"Check Cache Status\"):\n    try:\n        from steadytext import get_cache_manager\n        cache_manager = get_cache_manager()\n        stats = cache_manager.get_cache_stats()\n\n        st.sidebar.json(stats)\n    except Exception as e:\n        st.sidebar.error(f\"Error getting cache stats: {e}\")\n\n# Run with: streamlit run streamlit_app.py\n</code></pre>"},{"location":"integrations/#aiml-frameworks","title":"AI/ML Frameworks","text":""},{"location":"integrations/#langchain","title":"LangChain","text":"<p>Integrate SteadyText with LangChain:</p> <pre><code># langchain_integration.py\nfrom langchain.llms.base import LLM\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom typing import List, Optional, Any\nimport steadytext\nimport numpy as np\n\nclass SteadyTextLLM(LLM):\n    \"\"\"SteadyText LLM wrapper for LangChain.\"\"\"\n\n    seed: int = 42\n    max_new_tokens: int = 512\n    model_size: str = \"small\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:\n        \"\"\"Call SteadyText generate.\"\"\"\n        result = steadytext.generate(\n            prompt,\n            seed=self.seed,\n            max_new_tokens=self.max_new_tokens\n        )\n        return result if result else \"\"\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"steadytext\"\n\n    @property\n    def _identifying_params(self) -&gt; dict:\n        return {\n            \"seed\": self.seed,\n            \"max_new_tokens\": self.max_new_tokens,\n            \"model_size\": self.model_size\n        }\n\nclass SteadyTextEmbeddings(Embeddings):\n    \"\"\"SteadyText embeddings wrapper for LangChain.\"\"\"\n\n    seed: int = 42\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        embeddings = []\n        for text in texts:\n            emb = steadytext.embed(text, seed=self.seed)\n            if emb is not None:\n                embeddings.append(emb.tolist())\n            else:\n                # Fallback to zero vector\n                embeddings.append([0.0] * 1024)\n        return embeddings\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        emb = steadytext.embed(text, seed=self.seed)\n        return emb.tolist() if emb is not None else [0.0] * 1024\n\n# Example usage\ndef create_qa_system(documents_path: str):\n    \"\"\"Create a Q&amp;A system using SteadyText with LangChain.\"\"\"\n\n    # Load and split documents\n    loader = TextLoader(documents_path)\n    documents = loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200\n    )\n    texts = text_splitter.split_documents(documents)\n\n    # Create embeddings and vector store\n    embeddings = SteadyTextEmbeddings(seed=42)\n    vectorstore = FAISS.from_documents(texts, embeddings)\n\n    # Create LLM and chain\n    llm = SteadyTextLLM(seed=42, max_new_tokens=300)\n\n    template = \"\"\"\n    Context: {context}\n\n    Question: {question}\n\n    Answer based on the context above:\n    \"\"\"\n\n    prompt = PromptTemplate(\n        template=template,\n        input_variables=[\"context\", \"question\"]\n    )\n\n    chain = LLMChain(llm=llm, prompt=prompt)\n\n    def ask_question(question: str) -&gt; str:\n        \"\"\"Ask a question about the documents.\"\"\"\n        # Retrieve relevant documents\n        docs = vectorstore.similarity_search(question, k=3)\n        context = \"\\n\".join([doc.page_content for doc in docs])\n\n        # Generate answer\n        answer = chain.run(context=context, question=question)\n        return answer\n\n    return ask_question\n\n# Example usage\nqa_system = create_qa_system(\"documents.txt\")\nanswer = qa_system(\"What is machine learning?\")\nprint(answer)\n</code></pre>"},{"location":"integrations/#llamaindex","title":"LlamaIndex","text":"<p>Document indexing and retrieval:</p> <pre><code># llamaindex_integration.py\nfrom llama_index.core import VectorStoreIndex, Document, Settings\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom typing import List, Any\nimport steadytext\n\nclass SteadyTextLLM(LLM):\n    \"\"\"SteadyText LLM for LlamaIndex.\"\"\"\n\n    def __init__(self, seed: int = 42, max_tokens: int = 512):\n        super().__init__()\n        self.seed = seed\n        self.max_tokens = max_tokens\n\n    @property\n    def metadata(self) -&gt; dict:\n        return {\"seed\": self.seed, \"max_tokens\": self.max_tokens}\n\n    def complete(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Complete a prompt.\"\"\"\n        result = steadytext.generate(\n            prompt,\n            seed=self.seed,\n            max_new_tokens=self.max_tokens\n        )\n        return result if result else \"\"\n\n    def stream_complete(self, prompt: str, **kwargs):\n        \"\"\"Stream completion (generator).\"\"\"\n        for chunk in steadytext.generate_iter(prompt, seed=self.seed):\n            yield chunk\n\nclass SteadyTextEmbedding(BaseEmbedding):\n    \"\"\"SteadyText embeddings for LlamaIndex.\"\"\"\n\n    def __init__(self, seed: int = 42):\n        super().__init__()\n        self.seed = seed\n\n    def _get_query_embedding(self, query: str) -&gt; List[float]:\n        \"\"\"Get embedding for query.\"\"\"\n        emb = steadytext.embed(query, seed=self.seed)\n        return emb.tolist() if emb is not None else [0.0] * 1024\n\n    def _get_text_embedding(self, text: str) -&gt; List[float]:\n        \"\"\"Get embedding for text.\"\"\"\n        return self._get_query_embedding(text)\n\n    def _get_text_embeddings(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Get embeddings for multiple texts.\"\"\"\n        return [self._get_text_embedding(text) for text in texts]\n\n# Setup LlamaIndex with SteadyText\nSettings.llm = SteadyTextLLM(seed=42)\nSettings.embed_model = SteadyTextEmbedding(seed=42)\n\ndef create_index_from_documents(documents: List[str]) -&gt; VectorStoreIndex:\n    \"\"\"Create a vector index from documents.\"\"\"\n\n    # Convert to Document objects\n    docs = [Document(text=doc) for doc in documents]\n\n    # Create index\n    index = VectorStoreIndex.from_documents(\n        docs,\n        node_parser=SentenceSplitter(chunk_size=512, chunk_overlap=50)\n    )\n\n    return index\n\n# Example usage\ndocuments = [\n    \"Machine learning is a subset of artificial intelligence...\",\n    \"Deep learning uses neural networks with multiple layers...\",\n    \"Natural language processing deals with text analysis...\"\n]\n\nindex = create_index_from_documents(documents)\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"What is machine learning?\")\nprint(response)\n</code></pre>"},{"location":"integrations/#haystack","title":"Haystack","text":"<p>Enterprise search and NLP:</p> <pre><code># haystack_integration.py\nfrom haystack import Document, Pipeline\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryBM25Retriever\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom typing import List, Dict, Any\nimport steadytext\n\nclass SteadyTextGenerator:\n    \"\"\"SteadyText generator component for Haystack.\"\"\"\n\n    def __init__(self, seed: int = 42, max_tokens: int = 512):\n        self.seed = seed\n        self.max_tokens = max_tokens\n\n    def run(self, prompt: str, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Generate text using SteadyText.\"\"\"\n        result = steadytext.generate(\n            prompt,\n            seed=self.seed,\n            max_new_tokens=self.max_tokens\n        )\n\n        return {\n            \"replies\": [result] if result else [\"Model not available\"]\n        }\n\nclass SteadyTextEmbedder:\n    \"\"\"SteadyText embedder component for Haystack.\"\"\"\n\n    def __init__(self, seed: int = 42):\n        self.seed = seed\n\n    def run(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Create embedding using SteadyText.\"\"\"\n        embedding = steadytext.embed(text, seed=self.seed)\n\n        return {\n            \"embedding\": embedding.tolist() if embedding is not None else [0.0] * 1024\n        }\n\ndef create_rag_pipeline(documents: List[str]) -&gt; Pipeline:\n    \"\"\"Create a RAG pipeline using SteadyText.\"\"\"\n\n    # Create document store\n    document_store = InMemoryDocumentStore()\n\n    # Add documents\n    docs = [Document(content=doc, id=str(i)) for i, doc in enumerate(documents)]\n    document_store.write_documents(docs)\n\n    # Create components\n    retriever = InMemoryBM25Retriever(document_store=document_store)\n    generator = SteadyTextGenerator(seed=42)\n\n    # Create pipeline\n    pipeline = Pipeline()\n    pipeline.add_component(\"retriever\", retriever)\n    pipeline.add_component(\"generator\", generator)\n\n    # Connect components\n    pipeline.connect(\"retriever.documents\", \"generator.documents\")\n\n    return pipeline\n\n# Example usage\ndocs = [\n    \"SteadyText is a deterministic AI library for Python.\",\n    \"It provides text generation and embedding capabilities.\",\n    \"The library ensures reproducible results across runs.\"\n]\n\npipeline = create_rag_pipeline(docs)\n\n# Run query\nresult = pipeline.run({\n    \"retriever\": {\"query\": \"What is SteadyText?\"},\n    \"generator\": {\"prompt\": \"Based on the documents, what is SteadyText?\"}\n})\n\nprint(result[\"generator\"][\"replies\"][0])\n</code></pre>"},{"location":"integrations/#database-integrations","title":"Database Integrations","text":""},{"location":"integrations/#postgresql","title":"PostgreSQL","text":"<p>Native PostgreSQL integration with pg_steadytext:</p> <pre><code>-- Setup\nCREATE EXTENSION IF NOT EXISTS vector;\nCREATE EXTENSION IF NOT EXISTS pg_steadytext;\n\n-- Create a table with AI capabilities\nCREATE TABLE articles (\n    id SERIAL PRIMARY KEY,\n    title TEXT NOT NULL,\n    content TEXT NOT NULL,\n    summary TEXT,\n    embedding VECTOR(1024),\n    keywords TEXT[],\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Trigger to auto-generate AI content\nCREATE OR REPLACE FUNCTION update_ai_fields()\nRETURNS TRIGGER AS $$\nBEGIN\n    -- Generate summary\n    NEW.summary := steadytext_generate(\n        'Summarize this article in 2-3 sentences: ' || NEW.content,\n        max_tokens := 150,\n        seed := 42\n    );\n\n    -- Generate embedding\n    NEW.embedding := steadytext_embed(NEW.title || ' ' || NEW.content, seed := 42);\n\n    -- Extract keywords\n    NEW.keywords := string_to_array(\n        steadytext_generate(\n            'Extract 5 keywords from this text: ' || NEW.content,\n            max_tokens := 50,\n            seed := 123\n        ),\n        ','\n    );\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER ai_content_trigger\n    BEFORE INSERT OR UPDATE ON articles\n    FOR EACH ROW\n    EXECUTE FUNCTION update_ai_fields();\n\n-- Semantic search function\nCREATE OR REPLACE FUNCTION semantic_search(\n    query_text TEXT,\n    limit_count INT DEFAULT 10\n)\nRETURNS TABLE(\n    article_id INT,\n    title TEXT,\n    summary TEXT,\n    similarity FLOAT\n) AS $$\nDECLARE\n    query_embedding VECTOR(1024);\nBEGIN\n    -- Generate embedding for search query\n    query_embedding := steadytext_embed(query_text, seed := 42);\n\n    -- Return similar articles\n    RETURN QUERY\n    SELECT \n        a.id,\n        a.title,\n        a.summary,\n        1 - (a.embedding &lt;=&gt; query_embedding) AS similarity\n    FROM articles a\n    WHERE a.embedding IS NOT NULL\n    ORDER BY a.embedding &lt;=&gt; query_embedding\n    LIMIT limit_count;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage examples\nINSERT INTO articles (title, content) VALUES \n('AI Revolution', 'Artificial intelligence is transforming industries...');\n\nSELECT * FROM semantic_search('machine learning trends');\n</code></pre>"},{"location":"integrations/#mongodb","title":"MongoDB","text":"<p>Document database with AI capabilities:</p> <pre><code># mongodb_integration.py\nimport pymongo\nimport steadytext\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\n\nclass SteadyTextMongoDB:\n    \"\"\"MongoDB integration with SteadyText.\"\"\"\n\n    def __init__(self, connection_string: str, database: str):\n        self.client = pymongo.MongoClient(connection_string)\n        self.db = self.client[database]\n\n        # Create text index for search\n        self.db.documents.create_index([(\"title\", \"text\"), (\"content\", \"text\")])\n\n        # Create vector index (MongoDB Atlas Vector Search)\n        try:\n            self.db.documents.create_index([(\"embedding\", \"2dsphere\")])\n        except Exception:\n            pass  # Vector indexing not available in all MongoDB versions\n\n    def insert_document(self, \n                       title: str, \n                       content: str, \n                       generate_summary: bool = True,\n                       generate_embedding: bool = True,\n                       seed: int = 42) -&gt; str:\n        \"\"\"Insert document with AI-generated fields.\"\"\"\n\n        doc = {\n            \"title\": title,\n            \"content\": content,\n            \"created_at\": datetime.utcnow()\n        }\n\n        if generate_summary:\n            summary = steadytext.generate(\n                f\"Summarize this document: {content}\",\n                seed=seed,\n                max_new_tokens=150\n            )\n            if summary:\n                doc[\"summary\"] = summary\n\n        if generate_embedding:\n            embedding = steadytext.embed(f\"{title} {content}\", seed=seed)\n            if embedding is not None:\n                doc[\"embedding\"] = embedding.tolist()\n\n        result = self.db.documents.insert_one(doc)\n        return str(result.inserted_id)\n\n    def semantic_search(self, \n                       query: str, \n                       limit: int = 10,\n                       seed: int = 42) -&gt; List[Dict]:\n        \"\"\"Perform semantic search using embeddings.\"\"\"\n\n        # Generate query embedding\n        query_embedding = steadytext.embed(query, seed=seed)\n        if query_embedding is None:\n            return []\n\n        # Find documents (using cosine similarity approximation)\n        pipeline = [\n            {\n                \"$addFields\": {\n                    \"similarity\": {\n                        \"$let\": {\n                            \"vars\": {\n                                \"query_emb\": query_embedding.tolist()\n                            },\n                            \"in\": {\n                                \"$cond\": {\n                                    \"if\": {\"$ne\": [\"$embedding\", None]},\n                                    \"then\": {\n                                        \"$divide\": [\n                                            {\"$reduce\": {\n                                                \"input\": {\"$zip\": {\"inputs\": [\"$embedding\", \"$$query_emb\"]}},\n                                                \"initialValue\": 0,\n                                                \"in\": {\"$add\": [\"$$value\", {\"$multiply\": [{\"$arrayElemAt\": [\"$$this\", 0]}, {\"$arrayElemAt\": [\"$$this\", 1]}]}]}\n                                            }},\n                                            {\"$multiply\": [\n                                                {\"$sqrt\": {\"$reduce\": {\n                                                    \"input\": \"$embedding\",\n                                                    \"initialValue\": 0,\n                                                    \"in\": {\"$add\": [\"$$value\", {\"$multiply\": [\"$$this\", \"$$this\"]}]}\n                                                }}},\n                                                {\"$sqrt\": {\"$reduce\": {\n                                                    \"input\": \"$$query_emb\",\n                                                    \"initialValue\": 0,\n                                                    \"in\": {\"$add\": [\"$$value\", {\"$multiply\": [\"$$this\", \"$$this\"]}]}\n                                                }}}\n                                            ]}\n                                        ]\n                                    },\n                                    \"else\": 0\n                                }\n                            }\n                        }\n                    }\n                }\n            },\n            {\"$match\": {\"similarity\": {\"$gt\": 0}}},\n            {\"$sort\": {\"similarity\": -1}},\n            {\"$limit\": limit},\n            {\"$project\": {\"embedding\": 0}}  # Don't return embeddings\n        ]\n\n        return list(self.db.documents.aggregate(pipeline))\n\n    def generate_related_content(self, \n                                document_id: str, \n                                content_type: str = \"summary\",\n                                seed: int = 42) -&gt; Optional[str]:\n        \"\"\"Generate related content for a document.\"\"\"\n\n        doc = self.db.documents.find_one({\"_id\": ObjectId(document_id)})\n        if not doc:\n            return None\n\n        prompts = {\n            \"summary\": f\"Summarize this document: {doc['content']}\",\n            \"keywords\": f\"Extract keywords from: {doc['content']}\",\n            \"questions\": f\"Generate 3 questions about: {doc['content']}\",\n            \"continuation\": f\"Continue this text: {doc['content']}\"\n        }\n\n        prompt = prompts.get(content_type, prompts[\"summary\"])\n        result = steadytext.generate(prompt, seed=seed)\n\n        if result:\n            # Update document with generated content\n            self.db.documents.update_one(\n                {\"_id\": ObjectId(document_id)},\n                {\"$set\": {f\"generated_{content_type}\": result}}\n            )\n\n        return result\n\n# Example usage\ndb = SteadyTextMongoDB(\"mongodb://localhost:27017\", \"ai_docs\")\n\n# Insert document\ndoc_id = db.insert_document(\n    \"Machine Learning Basics\",\n    \"Machine learning is a subset of artificial intelligence...\"\n)\n\n# Search documents\nresults = db.semantic_search(\"artificial intelligence\")\nfor result in results:\n    print(f\"Title: {result['title']}\")\n    print(f\"Similarity: {result['similarity']:.3f}\")\n</code></pre>"},{"location":"integrations/#redis","title":"Redis","text":"<p>Caching and real-time AI:</p> <pre><code># redis_integration.py\nimport redis\nimport json\nimport hashlib\nimport steadytext\nimport numpy as np\nfrom typing import Optional, List, Dict, Any\n\nclass SteadyTextRedis:\n    \"\"\"Redis integration for caching and real-time AI.\"\"\"\n\n    def __init__(self, redis_url: str = \"redis://localhost:6379\"):\n        self.redis = redis.from_url(redis_url, decode_responses=True)\n\n        # Set up Lua scripts for atomic operations\n        self.cache_script = self.redis.register_script(\"\"\"\n            local key = KEYS[1]\n            local value = ARGV[1]\n            local ttl = ARGV[2]\n\n            redis.call('SET', key, value)\n            redis.call('EXPIRE', key, ttl)\n            redis.call('INCR', key .. ':hits')\n\n            return 'OK'\n        \"\"\")\n\n    def _generate_cache_key(self, prompt: str, seed: int, **kwargs) -&gt; str:\n        \"\"\"Generate cache key for prompt.\"\"\"\n        key_data = f\"{prompt}:{seed}:{json.dumps(kwargs, sort_keys=True)}\"\n        return f\"steadytext:gen:{hashlib.md5(key_data.encode()).hexdigest()}\"\n\n    def _embedding_cache_key(self, text: str, seed: int) -&gt; str:\n        \"\"\"Generate cache key for embedding.\"\"\"\n        key_data = f\"{text}:{seed}\"\n        return f\"steadytext:emb:{hashlib.md5(key_data.encode()).hexdigest()}\"\n\n    def cached_generate(self, \n                       prompt: str, \n                       seed: int = 42,\n                       ttl: int = 3600,\n                       **kwargs) -&gt; Optional[str]:\n        \"\"\"Generate text with Redis caching.\"\"\"\n\n        cache_key = self._generate_cache_key(prompt, seed, **kwargs)\n\n        # Try cache first\n        cached = self.redis.get(cache_key)\n        if cached:\n            # Update hit counter\n            self.redis.incr(f\"{cache_key}:hits\")\n            return json.loads(cached)[\"text\"]\n\n        # Generate new\n        result = steadytext.generate(prompt, seed=seed, **kwargs)\n        if result:\n            # Cache result\n            cache_data = {\n                \"text\": result,\n                \"prompt\": prompt,\n                \"seed\": seed,\n                \"timestamp\": time.time()\n            }\n            self.cache_script(\n                keys=[cache_key],\n                args=[json.dumps(cache_data), ttl]\n            )\n\n        return result\n\n    def cached_embed(self, \n                    text: str, \n                    seed: int = 42,\n                    ttl: int = 3600) -&gt; Optional[np.ndarray]:\n        \"\"\"Create embedding with Redis caching.\"\"\"\n\n        cache_key = self._embedding_cache_key(text, seed)\n\n        # Try cache first\n        cached = self.redis.get(cache_key)\n        if cached:\n            self.redis.incr(f\"{cache_key}:hits\")\n            return np.array(json.loads(cached)[\"embedding\"])\n\n        # Generate new\n        embedding = steadytext.embed(text, seed=seed)\n        if embedding is not None:\n            # Cache result\n            cache_data = {\n                \"embedding\": embedding.tolist(),\n                \"text\": text,\n                \"seed\": seed,\n                \"timestamp\": time.time()\n            }\n            self.cache_script(\n                keys=[cache_key],\n                args=[json.dumps(cache_data), ttl]\n            )\n\n        return embedding\n\n    def batch_generate(self, \n                      prompts: List[str], \n                      seed: int = 42,\n                      **kwargs) -&gt; List[Optional[str]]:\n        \"\"\"Batch generate with Redis pipeline.\"\"\"\n\n        # Check cache for all prompts\n        cache_keys = [self._generate_cache_key(p, seed, **kwargs) for p in prompts]\n\n        pipe = self.redis.pipeline()\n        for key in cache_keys:\n            pipe.get(key)\n        cached_results = pipe.execute()\n\n        results = []\n        to_generate = []\n        indices_to_generate = []\n\n        for i, (prompt, cached) in enumerate(zip(prompts, cached_results)):\n            if cached:\n                results.append(json.loads(cached)[\"text\"])\n                # Update hit counter\n                self.redis.incr(f\"{cache_keys[i]}:hits\")\n            else:\n                results.append(None)\n                to_generate.append(prompt)\n                indices_to_generate.append(i)\n\n        # Generate missing results\n        if to_generate:\n            for prompt, idx in zip(to_generate, indices_to_generate):\n                result = steadytext.generate(prompt, seed=seed, **kwargs)\n                results[idx] = result\n\n                if result:\n                    # Cache result\n                    cache_data = {\n                        \"text\": result,\n                        \"prompt\": prompt,\n                        \"seed\": seed,\n                        \"timestamp\": time.time()\n                    }\n                    self.redis.setex(\n                        cache_keys[idx],\n                        3600,\n                        json.dumps(cache_data)\n                    )\n\n        return results\n\n    def similarity_search(self, \n                         query: str, \n                         collection: str = \"docs\",\n                         top_k: int = 5,\n                         seed: int = 42) -&gt; List[Dict]:\n        \"\"\"Perform similarity search using Redis.\"\"\"\n\n        # Generate query embedding\n        query_embedding = self.cached_embed(query, seed=seed)\n        if query_embedding is None:\n            return []\n\n        # Get all document embeddings\n        doc_keys = self.redis.keys(f\"docs:{collection}:*\")\n\n        similarities = []\n        for key in doc_keys:\n            doc_data = self.redis.hgetall(key)\n            if 'embedding' in doc_data:\n                doc_embedding = np.array(json.loads(doc_data['embedding']))\n\n                # Calculate cosine similarity\n                similarity = np.dot(query_embedding, doc_embedding)\n                similarities.append({\n                    'doc_id': key.split(':')[-1],\n                    'title': doc_data.get('title', ''),\n                    'content': doc_data.get('content', ''),\n                    'similarity': float(similarity)\n                })\n\n        # Sort by similarity and return top_k\n        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n        return similarities[:top_k]\n\n    def store_document(self, \n                      doc_id: str,\n                      title: str,\n                      content: str,\n                      collection: str = \"docs\",\n                      seed: int = 42) -&gt; bool:\n        \"\"\"Store document with embedding in Redis.\"\"\"\n\n        # Generate embedding\n        text = f\"{title} {content}\"\n        embedding = self.cached_embed(text, seed=seed)\n        if embedding is None:\n            return False\n\n        # Store document\n        key = f\"docs:{collection}:{doc_id}\"\n        self.redis.hset(key, mapping={\n            'title': title,\n            'content': content,\n            'embedding': json.dumps(embedding.tolist()),\n            'seed': seed,\n            'timestamp': time.time()\n        })\n\n        return True\n\n    def get_cache_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n\n        # Count cache entries\n        gen_keys = len(self.redis.keys(\"steadytext:gen:*\"))\n        emb_keys = len(self.redis.keys(\"steadytext:emb:*\"))\n\n        # Get hit counts\n        hit_keys = self.redis.keys(\"steadytext:*:hits\")\n        total_hits = sum(int(self.redis.get(key) or 0) for key in hit_keys)\n\n        return {\n            'generation_cache_entries': gen_keys,\n            'embedding_cache_entries': emb_keys,\n            'total_hits': total_hits,\n            'redis_memory': self.redis.info()['used_memory_human']\n        }\n\n# Example usage\nredis_ai = SteadyTextRedis()\n\n# Cached generation\ntext = redis_ai.cached_generate(\"Write about AI\", seed=42)\n\n# Store and search documents\nredis_ai.store_document(\"doc1\", \"AI Basics\", \"Artificial intelligence is...\", seed=42)\nresults = redis_ai.similarity_search(\"machine learning\", top_k=3)\n\n# Batch processing\nprompts = [\"AI topic 1\", \"AI topic 2\", \"AI topic 3\"]\nresults = redis_ai.batch_generate(prompts, seed=42)\n\n# Check performance\nstats = redis_ai.get_cache_stats()\nprint(f\"Cache entries: {stats['generation_cache_entries']}\")\nprint(f\"Total hits: {stats['total_hits']}\")\n</code></pre>"},{"location":"integrations/#vector-databases","title":"Vector Databases","text":""},{"location":"integrations/#pinecone","title":"Pinecone","text":"<p>Cloud vector database:</p> <pre><code># pinecone_integration.py\nimport pinecone\nimport steadytext\nimport uuid\nfrom typing import List, Dict, Any, Optional\n\nclass SteadyTextPinecone:\n    \"\"\"Pinecone integration with SteadyText.\"\"\"\n\n    def __init__(self, api_key: str, environment: str, index_name: str):\n        pinecone.init(api_key=api_key, environment=environment)\n\n        # Create index if it doesn't exist\n        if index_name not in pinecone.list_indexes():\n            pinecone.create_index(\n                name=index_name,\n                dimension=1024,  # SteadyText embedding dimension\n                metric=\"cosine\"\n            )\n\n        self.index = pinecone.Index(index_name)\n        self.seed = 42\n\n    def upsert_documents(self, \n                        documents: List[Dict[str, Any]], \n                        batch_size: int = 100) -&gt; List[str]:\n        \"\"\"Upsert documents with embeddings to Pinecone.\"\"\"\n\n        vectors = []\n        doc_ids = []\n\n        for doc in documents:\n            # Generate unique ID if not provided\n            doc_id = doc.get('id', str(uuid.uuid4()))\n            doc_ids.append(doc_id)\n\n            # Create text for embedding\n            text = doc.get('text', '')\n            if 'title' in doc:\n                text = f\"{doc['title']} {text}\"\n\n            # Generate embedding\n            embedding = steadytext.embed(text, seed=self.seed)\n            if embedding is None:\n                continue\n\n            # Prepare metadata\n            metadata = {\n                'title': doc.get('title', ''),\n                'text': text[:1000],  # Truncate for metadata\n                'source': doc.get('source', ''),\n                'timestamp': doc.get('timestamp', time.time())\n            }\n\n            vectors.append({\n                'id': doc_id,\n                'values': embedding.tolist(),\n                'metadata': metadata\n            })\n\n        # Upsert in batches\n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            self.index.upsert(vectors=batch)\n\n        return doc_ids\n\n    def similarity_search(self, \n                         query: str, \n                         top_k: int = 10,\n                         filter_dict: Optional[Dict] = None) -&gt; List[Dict]:\n        \"\"\"Search for similar documents.\"\"\"\n\n        # Generate query embedding\n        query_embedding = steadytext.embed(query, seed=self.seed)\n        if query_embedding is None:\n            return []\n\n        # Query Pinecone\n        results = self.index.query(\n            vector=query_embedding.tolist(),\n            top_k=top_k,\n            include_metadata=True,\n            filter=filter_dict\n        )\n\n        # Format results\n        matches = []\n        for match in results['matches']:\n            matches.append({\n                'id': match['id'],\n                'score': match['score'],\n                'title': match['metadata'].get('title', ''),\n                'text': match['metadata'].get('text', ''),\n                'source': match['metadata'].get('source', ''),\n                'timestamp': match['metadata'].get('timestamp', 0)\n            })\n\n        return matches\n\n    def generate_with_context(self, \n                             query: str, \n                             max_context_docs: int = 3,\n                             max_tokens: int = 512) -&gt; Optional[str]:\n        \"\"\"Generate response using retrieved context.\"\"\"\n\n        # Retrieve relevant documents\n        context_docs = self.similarity_search(query, top_k=max_context_docs)\n\n        if not context_docs:\n            # No context found, generate directly\n            return steadytext.generate(query, seed=self.seed, max_new_tokens=max_tokens)\n\n        # Build context\n        context = \"\\n\\n\".join([\n            f\"Document {i+1}: {doc['text']}\"\n            for i, doc in enumerate(context_docs)\n        ])\n\n        # Create prompt with context\n        prompt = f\"\"\"\n        Context:\n        {context}\n\n        Question: {query}\n\n        Answer based on the context above:\n        \"\"\"\n\n        return steadytext.generate(prompt, seed=self.seed, max_new_tokens=max_tokens)\n\n    def get_index_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get index statistics.\"\"\"\n        stats = self.index.describe_index_stats()\n        return {\n            'total_vectors': stats['total_vector_count'],\n            'dimension': stats['dimension'],\n            'index_fullness': stats['index_fullness'],\n            'namespaces': stats.get('namespaces', {})\n        }\n\n    def delete_documents(self, doc_ids: List[str]) -&gt; bool:\n        \"\"\"Delete documents by IDs.\"\"\"\n        try:\n            self.index.delete(ids=doc_ids)\n            return True\n        except Exception as e:\n            print(f\"Error deleting documents: {e}\")\n            return False\n\n# Example usage\npinecone_ai = SteadyTextPinecone(\n    api_key=\"your-api-key\",\n    environment=\"us-west1-gcp\",\n    index_name=\"steadytext-docs\"\n)\n\n# Add documents\ndocuments = [\n    {\n        'title': 'Machine Learning Basics',\n        'text': 'Machine learning is a subset of artificial intelligence...',\n        'source': 'ml_guide.pdf'\n    },\n    {\n        'title': 'Deep Learning Overview',\n        'text': 'Deep learning uses neural networks with multiple layers...',\n        'source': 'dl_tutorial.pdf'\n    }\n]\n\ndoc_ids = pinecone_ai.upsert_documents(documents)\n\n# Search and generate\nresponse = pinecone_ai.generate_with_context(\"What is machine learning?\")\nprint(response)\n\n# Get statistics\nstats = pinecone_ai.get_index_stats()\nprint(f\"Total vectors: {stats['total_vectors']}\")\n</code></pre>"},{"location":"integrations/#weaviate","title":"Weaviate","text":"<p>Open-source vector database:</p> <pre><code># weaviate_integration.py\nimport weaviate\nimport steadytext\nimport json\nfrom typing import List, Dict, Any, Optional\n\nclass SteadyTextWeaviate:\n    \"\"\"Weaviate integration with SteadyText.\"\"\"\n\n    def __init__(self, url: str = \"http://localhost:8080\"):\n        self.client = weaviate.Client(url)\n        self.class_name = \"Document\"\n        self.seed = 42\n\n        # Create schema if it doesn't exist\n        self._create_schema()\n\n    def _create_schema(self):\n        \"\"\"Create Weaviate schema for documents.\"\"\"\n\n        schema = {\n            \"classes\": [\n                {\n                    \"class\": self.class_name,\n                    \"description\": \"Document with SteadyText embeddings\",\n                    \"vectorizer\": \"none\",  # We'll provide our own vectors\n                    \"properties\": [\n                        {\n                            \"name\": \"title\",\n                            \"dataType\": [\"string\"],\n                            \"description\": \"Document title\"\n                        },\n                        {\n                            \"name\": \"content\",\n                            \"dataType\": [\"text\"],\n                            \"description\": \"Document content\"\n                        },\n                        {\n                            \"name\": \"source\",\n                            \"dataType\": [\"string\"],\n                            \"description\": \"Document source\"\n                        },\n                        {\n                            \"name\": \"category\",\n                            \"dataType\": [\"string\"],\n                            \"description\": \"Document category\"\n                        },\n                        {\n                            \"name\": \"timestamp\",\n                            \"dataType\": [\"number\"],\n                            \"description\": \"Document timestamp\"\n                        }\n                    ]\n                }\n            ]\n        }\n\n        try:\n            # Check if class exists\n            existing_schema = self.client.schema.get()\n            class_exists = any(\n                cls[\"class\"] == self.class_name \n                for cls in existing_schema.get(\"classes\", [])\n            )\n\n            if not class_exists:\n                self.client.schema.create(schema)\n                print(f\"Created schema for class {self.class_name}\")\n\n        except Exception as e:\n            print(f\"Schema creation error: {e}\")\n\n    def add_documents(self, documents: List[Dict[str, Any]]) -&gt; List[str]:\n        \"\"\"Add documents to Weaviate with SteadyText embeddings.\"\"\"\n\n        doc_ids = []\n\n        with self.client.batch as batch:\n            batch.batch_size = 100\n\n            for doc in documents:\n                # Prepare text for embedding\n                title = doc.get('title', '')\n                content = doc.get('content', '')\n                text = f\"{title} {content}\"\n\n                # Generate embedding\n                embedding = steadytext.embed(text, seed=self.seed)\n                if embedding is None:\n                    continue\n\n                # Prepare properties\n                properties = {\n                    \"title\": title,\n                    \"content\": content,\n                    \"source\": doc.get('source', ''),\n                    \"category\": doc.get('category', ''),\n                    \"timestamp\": doc.get('timestamp', time.time())\n                }\n\n                # Add to batch\n                doc_id = batch.add_data_object(\n                    data_object=properties,\n                    class_name=self.class_name,\n                    vector=embedding.tolist()\n                )\n\n                doc_ids.append(doc_id)\n\n        return doc_ids\n\n    def similarity_search(self, \n                         query: str, \n                         limit: int = 10,\n                         where_filter: Optional[Dict] = None) -&gt; List[Dict]:\n        \"\"\"Search for similar documents.\"\"\"\n\n        # Generate query embedding\n        query_embedding = steadytext.embed(query, seed=self.seed)\n        if query_embedding is None:\n            return []\n\n        # Build query\n        near_vector = {\n            \"vector\": query_embedding.tolist()\n        }\n\n        query_builder = (\n            self.client.query\n            .get(self.class_name, [\"title\", \"content\", \"source\", \"category\", \"timestamp\"])\n            .with_near_vector(near_vector)\n            .with_limit(limit)\n            .with_additional([\"certainty\", \"distance\"])\n        )\n\n        # Add where filter if provided\n        if where_filter:\n            query_builder = query_builder.with_where(where_filter)\n\n        # Execute query\n        result = query_builder.do()\n\n        # Format results\n        documents = result.get('data', {}).get('Get', {}).get(self.class_name, [])\n\n        formatted_results = []\n        for doc in documents:\n            formatted_results.append({\n                'title': doc.get('title', ''),\n                'content': doc.get('content', ''),\n                'source': doc.get('source', ''),\n                'category': doc.get('category', ''),\n                'timestamp': doc.get('timestamp', 0),\n                'certainty': doc.get('_additional', {}).get('certainty', 0),\n                'distance': doc.get('_additional', {}).get('distance', 0)\n            })\n\n        return formatted_results\n\n    def generate_answer(self, \n                       question: str, \n                       context_limit: int = 3,\n                       max_tokens: int = 300) -&gt; Optional[str]:\n        \"\"\"Generate answer using retrieved context.\"\"\"\n\n        # Get relevant documents\n        context_docs = self.similarity_search(question, limit=context_limit)\n\n        if not context_docs:\n            return steadytext.generate(question, seed=self.seed, max_new_tokens=max_tokens)\n\n        # Build context\n        context = \"\\n\\n\".join([\n            f\"Title: {doc['title']}\\nContent: {doc['content'][:500]}...\"\n            for doc in context_docs\n        ])\n\n        # Generate answer with context\n        prompt = f\"\"\"\n        Based on the following context, answer the question:\n\n        Context:\n        {context}\n\n        Question: {question}\n\n        Answer:\n        \"\"\"\n\n        return steadytext.generate(prompt, seed=self.seed, max_new_tokens=max_tokens)\n\n    def hybrid_search(self, \n                     query: str, \n                     limit: int = 10,\n                     alpha: float = 0.7) -&gt; List[Dict]:\n        \"\"\"Perform hybrid search (vector + keyword).\"\"\"\n\n        # Generate query embedding\n        query_embedding = steadytext.embed(query, seed=self.seed)\n        if query_embedding is None:\n            return self.keyword_search(query, limit)\n\n        # Hybrid search\n        result = (\n            self.client.query\n            .get(self.class_name, [\"title\", \"content\", \"source\", \"category\"])\n            .with_hybrid(\n                query=query,\n                alpha=alpha,  # Balance between vector (1.0) and keyword (0.0)\n                vector=query_embedding.tolist()\n            )\n            .with_limit(limit)\n            .with_additional([\"score\"])\n            .do()\n        )\n\n        documents = result.get('data', {}).get('Get', {}).get(self.class_name, [])\n\n        return [{\n            'title': doc.get('title', ''),\n            'content': doc.get('content', ''),\n            'source': doc.get('source', ''),\n            'category': doc.get('category', ''),\n            'score': doc.get('_additional', {}).get('score', 0)\n        } for doc in documents]\n\n    def keyword_search(self, query: str, limit: int = 10) -&gt; List[Dict]:\n        \"\"\"Perform keyword-based search.\"\"\"\n\n        result = (\n            self.client.query\n            .get(self.class_name, [\"title\", \"content\", \"source\", \"category\"])\n            .with_bm25(query=query)\n            .with_limit(limit)\n            .with_additional([\"score\"])\n            .do()\n        )\n\n        documents = result.get('data', {}).get('Get', {}).get(self.class_name, [])\n\n        return [{\n            'title': doc.get('title', ''),\n            'content': doc.get('content', ''),\n            'source': doc.get('source', ''),\n            'category': doc.get('category', ''),\n            'score': doc.get('_additional', {}).get('score', 0)\n        } for doc in documents]\n\n    def delete_all_documents(self) -&gt; bool:\n        \"\"\"Delete all documents in the class.\"\"\"\n        try:\n            self.client.batch.delete_objects(\n                class_name=self.class_name,\n                where={\n                    \"path\": [\"id\"],\n                    \"operator\": \"Like\",\n                    \"valueString\": \"*\"\n                }\n            )\n            return True\n        except Exception as e:\n            print(f\"Error deleting documents: {e}\")\n            return False\n\n# Example usage\nweaviate_ai = SteadyTextWeaviate(\"http://localhost:8080\")\n\n# Add documents\ndocuments = [\n    {\n        'title': 'Python Programming',\n        'content': 'Python is a high-level programming language...',\n        'source': 'python_guide.md',\n        'category': 'programming'\n    },\n    {\n        'title': 'Machine Learning',\n        'content': 'Machine learning is a method of data analysis...',\n        'source': 'ml_handbook.pdf',\n        'category': 'ai'\n    }\n]\n\ndoc_ids = weaviate_ai.add_documents(documents)\n\n# Search\nresults = weaviate_ai.similarity_search(\"programming languages\")\nfor result in results:\n    print(f\"Title: {result['title']}\")\n    print(f\"Certainty: {result['certainty']:.3f}\")\n\n# Generate answer with context\nanswer = weaviate_ai.generate_answer(\"What is Python?\")\nprint(f\"Answer: {answer}\")\n\n# Hybrid search\nhybrid_results = weaviate_ai.hybrid_search(\"machine learning algorithms\")\n</code></pre> <p>I've completed creating comprehensive integration documentation. Let me now update the todo list to mark this task as completed and move on to the remaining tasks:</p> <p> [{\"content\": \"Enhance API documentation (generation.md, embedding.md, cli.md, index.md)\", \"status\": \"completed\", \"priority\": \"high\", \"id\": \"1\"}, {\"content\": \"Create new example files (custom-seeds.md, caching.md, daemon-usage.md, error-handling.md, performance-tuning.md)\", \"status\": \"completed\", \"priority\": \"high\", \"id\": \"2\"}, {\"content\": \"Complete PostgreSQL extension documentation (pg_steadytext.md)\", \"status\": \"completed\", \"priority\": \"high\", \"id\": \"3\"}, {\"content\": \"Create new core documentation files (architecture.md, faq.md, migration-guide.md, deployment.md, integrations.md)\", \"status\": \"completed\", \"priority\": \"medium\", \"id\": \"4\"}, {\"content\": \"Update existing docs (index.md, quick-start.md, version_history.md, benchmarks.md, model-switching.md)\", \"status\": \"in_progress\", \"priority\": \"medium\", \"id\": \"5\"}, {\"content\": \"Add AIDEV-NOTE comments throughout Python codebase\", \"status\": \"pending\", \"priority\": \"medium\", \"id\": \"6\"}, {\"content\": \"Update README.md with badges, comparisons, and troubleshooting\", \"status\": \"pending\", \"priority\": \"low\", \"id\": \"7\"}]"},{"location":"migration-guide/","title":"Migration Guide","text":"<p>This guide helps you migrate between different versions of SteadyText and from other text generation libraries.</p>"},{"location":"migration-guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Version Migration</li> <li>v2.0.x to v2.1.x</li> <li>v1.x to v2.x</li> <li>v0.x to v1.x</li> <li>Library Migration</li> <li>From OpenAI</li> <li>From Hugging Face</li> <li>From LangChain</li> <li>From Anthropic</li> <li>Breaking Changes</li> <li>Feature Mapping</li> <li>Code Examples</li> <li>Best Practices</li> </ul>"},{"location":"migration-guide/#version-migration","title":"Version Migration","text":""},{"location":"migration-guide/#v20x-to-v21x","title":"v2.0.x to v2.1.x","text":"<p>Major Breaking Change: Deterministic fallback behavior removed.</p>"},{"location":"migration-guide/#what-changed","title":"What Changed","text":"<p>Functions now return <code>None</code> instead of fallback text when models are unavailable:</p> <pre><code># v2.0.x behavior\nresult = steadytext.generate(\"Hello\")\n# Returns: \"Hello. This is a deterministic...\" (fallback text)\n\n# v2.1.x behavior\nresult = steadytext.generate(\"Hello\")\n# Returns: None (when model not loaded)\n</code></pre>"},{"location":"migration-guide/#migration-steps","title":"Migration Steps","text":"<ol> <li> <p>Update error handling:    <pre><code># Old code (v2.0.x)\ntext = steadytext.generate(prompt)\n# Always returned something\n\n# New code (v2.1.x)\ntext = steadytext.generate(prompt)\nif text is None:\n    # Handle model not available\n    text = \"Unable to generate text\"\n</code></pre></p> </li> <li> <p>Update embedding handling:    <pre><code># Old code (v2.0.x)\nembedding = steadytext.embed(text)\n# Always returned zero vector on failure\n\n# New code (v2.1.x)\nembedding = steadytext.embed(text)\nif embedding is None:\n    # Handle model not available\n    embedding = np.zeros(1024)\n</code></pre></p> </li> <li> <p>Update tests:    <pre><code># Old test (v2.0.x)\ndef test_generation():\n    result = steadytext.generate(\"test\")\n    assert result.startswith(\"test. This is\")\n\n# New test (v2.1.x)\ndef test_generation():\n    result = steadytext.generate(\"test\")\n    if result is not None:\n        assert isinstance(result, str)\n    else:\n        # Model not available is acceptable\n        pass\n</code></pre></p> </li> </ol>"},{"location":"migration-guide/#postgresql-extension-changes","title":"PostgreSQL Extension Changes","text":"<pre><code>-- Old behavior (v2.0.x)\nSELECT steadytext_generate('Hello');\n-- Returns: 'Hello. This is a deterministic...'\n\n-- New behavior (v2.1.x)\nSELECT steadytext_generate('Hello');\n-- Returns: NULL (when model not available)\n\n-- Add NULL handling\nSELECT COALESCE(\n    steadytext_generate('Hello'),\n    'Fallback text'\n) AS result;\n</code></pre>"},{"location":"migration-guide/#v1x-to-v2x","title":"v1.x to v2.x","text":"<p>Major Changes:  - New models (GPT-2 \u2192 Gemma-3n) - New embedding model (DistilBERT \u2192 Qwen3) - Changed embedding dimensions (768 \u2192 1024)</p>"},{"location":"migration-guide/#model-changes","title":"Model Changes","text":"<pre><code># v1.x (GPT-2 based)\ntext = steadytext.generate(\"Hello\")  # Used GPT-2\n\n# v2.x (Gemma-3n based)\ntext = steadytext.generate(\"Hello\")  # Uses Gemma-3n\ntext = steadytext.generate(\"Hello\", model_size=\"large\")  # 4B model\n</code></pre>"},{"location":"migration-guide/#embedding-dimension-changes","title":"Embedding Dimension Changes","text":"<pre><code># v1.x embeddings (768 dimensions)\nembedding = steadytext.embed(\"text\")\nprint(embedding.shape)  # (768,)\n\n# v2.x embeddings (1024 dimensions)\nembedding = steadytext.embed(\"text\")\nprint(embedding.shape)  # (1024,)\n\n# Migration for stored embeddings\ndef migrate_embeddings(old_embeddings):\n    \"\"\"Pad old embeddings to new size.\"\"\"\n    # Note: This is for compatibility only\n    # Regenerate embeddings for best results\n    padded = np.zeros((len(old_embeddings), 1024))\n    padded[:, :768] = old_embeddings\n    return padded\n</code></pre>"},{"location":"migration-guide/#api-changes","title":"API Changes","text":"<pre><code># v1.x\nfrom steadytext import generate_text, create_embedding\ntext = generate_text(\"prompt\")\nemb = create_embedding(\"text\")\n\n# v2.x\nimport steadytext\ntext = steadytext.generate(\"prompt\")\nemb = steadytext.embed(\"text\")\n</code></pre>"},{"location":"migration-guide/#v0x-to-v1x","title":"v0.x to v1.x","text":"<p>Major Changes: - Introduced daemon mode - Added caching system - New CLI structure</p>"},{"location":"migration-guide/#function-name-changes","title":"Function Name Changes","text":"<pre><code># v0.x\nfrom steadytext import steady_generate\nresult = steady_generate(\"Hello\")\n\n# v1.x\nfrom steadytext import generate\nresult = generate(\"Hello\")\n</code></pre>"},{"location":"migration-guide/#cli-changes","title":"CLI Changes","text":"<pre><code># v0.x\nsteadytext-generate \"prompt\"\n\n# v1.x\nst generate \"prompt\"\n# or\necho \"prompt\" | st\n</code></pre>"},{"location":"migration-guide/#library-migration","title":"Library Migration","text":""},{"location":"migration-guide/#from-openai","title":"From OpenAI","text":"<p>Migrate from OpenAI's API to SteadyText:</p> <pre><code># OpenAI code\nimport openai\n\nopenai.api_key = \"sk-...\"\nresponse = openai.Completion.create(\n    model=\"text-davinci-003\",\n    prompt=\"Hello world\",\n    max_tokens=100,\n    temperature=0.7\n)\ntext = response.choices[0].text\n\n# SteadyText equivalent\nimport steadytext\n\ntext = steadytext.generate(\n    \"Hello world\",\n    max_new_tokens=100,\n    seed=42  # For determinism\n)\n</code></pre>"},{"location":"migration-guide/#key-differences","title":"Key Differences","text":"Feature OpenAI SteadyText API Key Required Not needed Cost Per token Free Determinism Optional Default Offline No Yes Models Cloud-based Local"},{"location":"migration-guide/#embedding-migration","title":"Embedding Migration","text":"<pre><code># OpenAI embeddings\nresponse = openai.Embedding.create(\n    model=\"text-embedding-ada-002\",\n    input=\"Hello world\"\n)\nembedding = response['data'][0]['embedding']\n\n# SteadyText embeddings\nembedding = steadytext.embed(\"Hello world\")\n</code></pre>"},{"location":"migration-guide/#from-hugging-face","title":"From Hugging Face","text":"<p>Migrate from Transformers library:</p> <pre><code># Hugging Face code\nfrom transformers import pipeline\n\ngenerator = pipeline('text-generation', model='gpt2')\nresult = generator(\"Hello world\", max_length=100)\ntext = result[0]['generated_text']\n\n# SteadyText equivalent\nimport steadytext\n\ntext = steadytext.generate(\"Hello world\", max_new_tokens=100)\n</code></pre>"},{"location":"migration-guide/#model-loading-comparison","title":"Model Loading Comparison","text":"<pre><code># Hugging Face (explicit loading)\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n# Complex inference code...\n\n# SteadyText (automatic loading)\ntext = steadytext.generate(\"Hello\")  # Models loaded automatically\n</code></pre>"},{"location":"migration-guide/#embedding-migration_1","title":"Embedding Migration","text":"<pre><code># Hugging Face embeddings\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\ninputs = tokenizer(\"Hello world\", return_tensors='pt')\nwith torch.no_grad():\n    outputs = model(**inputs)\nembedding = outputs.last_hidden_state.mean(dim=1).numpy()\n\n# SteadyText embeddings\nembedding = steadytext.embed(\"Hello world\")\n</code></pre>"},{"location":"migration-guide/#from-langchain","title":"From LangChain","text":"<p>Integrate SteadyText with LangChain:</p> <pre><code># LangChain with OpenAI\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nllm = OpenAI(temperature=0)\nprompt = PromptTemplate(\n    input_variables=[\"topic\"],\n    template=\"Write a story about {topic}\"\n)\nchain = LLMChain(llm=llm, prompt=prompt)\nresult = chain.run(\"robots\")\n\n# LangChain with SteadyText\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List\n\nclass SteadyTextLLM(LLM):\n    seed: int = 42\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:\n        result = steadytext.generate(prompt, seed=self.seed)\n        return result if result else \"\"\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"steadytext\"\n\n# Use with LangChain\nllm = SteadyTextLLM(seed=42)\nchain = LLMChain(llm=llm, prompt=prompt)\nresult = chain.run(\"robots\")\n</code></pre>"},{"location":"migration-guide/#embedding-integration","title":"Embedding Integration","text":"<pre><code># Custom SteadyText embeddings for LangChain\nfrom langchain.embeddings.base import Embeddings\n\nclass SteadyTextEmbeddings(Embeddings):\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        return [steadytext.embed(text).tolist() for text in texts]\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        return steadytext.embed(text).tolist()\n\n# Use with vector stores\nfrom langchain.vectorstores import FAISS\nembeddings = SteadyTextEmbeddings()\nvectorstore = FAISS.from_texts(texts, embeddings)\n</code></pre>"},{"location":"migration-guide/#from-anthropic","title":"From Anthropic","text":"<p>Migrate from Claude API:</p> <pre><code># Anthropic code\nimport anthropic\n\nclient = anthropic.Client(api_key=\"...\")\nresponse = client.completions.create(\n    model=\"claude-2\",\n    prompt=f\"{anthropic.HUMAN_PROMPT} Hello {anthropic.AI_PROMPT}\",\n    max_tokens_to_sample=100\n)\ntext = response.completion\n\n# SteadyText equivalent\nimport steadytext\n\ntext = steadytext.generate(\n    \"Hello\",\n    max_new_tokens=100\n)\n</code></pre>"},{"location":"migration-guide/#breaking-changes","title":"Breaking Changes","text":""},{"location":"migration-guide/#summary-of-all-breaking-changes","title":"Summary of All Breaking Changes","text":"Version Change Impact Migration v2.1.0 Removed fallback generation Functions return None Add null checks v2.0.0 New models (Gemma-3n/Qwen3) Different outputs Regenerate content v2.0.0 Embedding dimensions (768\u21921024) Incompatible vectors Re-embed data v1.0.0 API restructure Import changes Update imports"},{"location":"migration-guide/#handling-breaking-changes","title":"Handling Breaking Changes","text":"<pre><code>def handle_breaking_changes():\n    \"\"\"Example of handling all breaking changes.\"\"\"\n\n    # Handle v2.1.0 None returns\n    text = steadytext.generate(\"Hello\")\n    if text is None:\n        text = \"Fallback text\"\n\n    # Handle dimension changes\n    try:\n        old_embedding = load_old_embedding()  # 768 dims\n        if len(old_embedding) == 768:\n            # Regenerate with new model\n            new_embedding = steadytext.embed(original_text)\n    except Exception as e:\n        print(f\"Migration needed: {e}\")\n</code></pre>"},{"location":"migration-guide/#feature-mapping","title":"Feature Mapping","text":""},{"location":"migration-guide/#generation-features","title":"Generation Features","text":"Feature Other Libraries SteadyText Basic generation <code>model.generate()</code> <code>steadytext.generate()</code> Streaming <code>stream=True</code> <code>steadytext.generate_iter()</code> Temperature <code>temperature=0.7</code> <code>seed=42</code> (deterministic) Max length <code>max_length=100</code> <code>max_new_tokens=100</code> Stop tokens <code>stop=[\"\\\\n\"]</code> <code>eos_string=\"\\\\n\"</code> Batch <code>model.generate(batch)</code> List comprehension"},{"location":"migration-guide/#embedding-features","title":"Embedding Features","text":"Feature Other Libraries SteadyText Create embedding <code>model.encode()</code> <code>steadytext.embed()</code> Batch embeddings <code>model.encode(list)</code> List comprehension Normalization Manual Automatic (L2) Dimensions Varies Always 1024"},{"location":"migration-guide/#code-examples","title":"Code Examples","text":""},{"location":"migration-guide/#complete-migration-example","title":"Complete Migration Example","text":"<pre><code># Full migration from OpenAI to SteadyText\n\n# Old OpenAI-based application\nclass OpenAIApp:\n    def __init__(self, api_key):\n        openai.api_key = api_key\n\n    def generate_content(self, prompt):\n        response = openai.Completion.create(\n            model=\"text-davinci-003\",\n            prompt=prompt,\n            max_tokens=200,\n            temperature=0.7\n        )\n        return response.choices[0].text\n\n    def create_embedding(self, text):\n        response = openai.Embedding.create(\n            model=\"text-embedding-ada-002\",\n            input=text\n        )\n        return response['data'][0]['embedding']\n\n# New SteadyText-based application\nclass SteadyTextApp:\n    def __init__(self, seed=42):\n        self.seed = seed\n\n    def generate_content(self, prompt):\n        result = steadytext.generate(\n            prompt,\n            max_new_tokens=200,\n            seed=self.seed\n        )\n        return result if result else \"Generation unavailable\"\n\n    def create_embedding(self, text):\n        embedding = steadytext.embed(text, seed=self.seed)\n        return embedding.tolist() if embedding is not None else [0] * 1024\n</code></pre>"},{"location":"migration-guide/#database-migration","title":"Database Migration","text":"<pre><code># Migrate embeddings in PostgreSQL\n\nimport psycopg2\nimport steadytext\nimport numpy as np\n\ndef migrate_embeddings_to_v2():\n    conn = psycopg2.connect(\"postgresql://...\")\n    cur = conn.cursor()\n\n    # Get old embeddings\n    cur.execute(\"SELECT id, text, embedding FROM documents WHERE version = 1\")\n\n    for doc_id, text, old_embedding in cur.fetchall():\n        # Regenerate with new model\n        new_embedding = steadytext.embed(text)\n\n        if new_embedding is not None:\n            # Update with new 1024-dim embedding\n            cur.execute(\n                \"UPDATE documents SET embedding = %s, version = 2 WHERE id = %s\",\n                (new_embedding.tolist(), doc_id)\n            )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"migration-guide/#best-practices","title":"Best Practices","text":""},{"location":"migration-guide/#1-version-pinning","title":"1. Version Pinning","text":"<pre><code># pyproject.toml\n[tool.poetry.dependencies]\nsteadytext = \"^2.1.0\"  # Allows 2.1.x updates\n\n# Or strict pinning\nsteadytext = \"2.1.0\"  # Exact version\n</code></pre>"},{"location":"migration-guide/#2-gradual-migration","title":"2. Gradual Migration","text":"<pre><code>class MigrationWrapper:\n    \"\"\"Wrapper to support both old and new behavior.\"\"\"\n\n    def __init__(self, use_new_version=True):\n        self.use_new_version = use_new_version\n\n    def generate(self, prompt):\n        if self.use_new_version:\n            # New v2.1.x behavior\n            result = steadytext.generate(prompt)\n            return result if result else \"Fallback\"\n        else:\n            # Simulate old behavior\n            result = steadytext.generate(prompt)\n            if result is None:\n                return f\"{prompt}. This is a deterministic fallback...\"\n            return result\n</code></pre>"},{"location":"migration-guide/#3-testing-migration","title":"3. Testing Migration","text":"<pre><code>import pytest\n\ndef test_migration_compatibility():\n    \"\"\"Test that migration handles all cases.\"\"\"\n\n    # Test None handling\n    result = steadytext.generate(\"test\")\n    if result is None:\n        # Ensure fallback works\n        assert \"fallback\" in handle_none_result(\"test\")\n\n    # Test embedding dimensions\n    embedding = steadytext.embed(\"test\")\n    if embedding is not None:\n        assert embedding.shape == (1024,)\n\n    # Test seed consistency\n    if result is not None:\n        result2 = steadytext.generate(\"test\", seed=42)\n        assert result == result2\n</code></pre>"},{"location":"migration-guide/#4-monitoring-migration","title":"4. Monitoring Migration","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef monitored_generate(prompt):\n    \"\"\"Generate with migration monitoring.\"\"\"\n    start_time = time.time()\n\n    result = steadytext.generate(prompt)\n\n    if result is None:\n        logger.warning(\n            \"Generation returned None\",\n            extra={\n                \"prompt\": prompt[:50],\n                \"duration\": time.time() - start_time\n            }\n        )\n        return \"Migration fallback\"\n\n    logger.info(\n        \"Generation successful\",\n        extra={\n            \"prompt\": prompt[:50],\n            \"duration\": time.time() - start_time,\n            \"length\": len(result)\n        }\n    )\n    return result\n</code></pre>"},{"location":"migration-guide/#5-rollback-strategy","title":"5. Rollback Strategy","text":"<pre><code>class VersionedSteadyText:\n    \"\"\"Support multiple versions during migration.\"\"\"\n\n    def __init__(self, version=\"2.1\"):\n        self.version = version\n\n    def generate(self, prompt, **kwargs):\n        if self.version == \"2.0\":\n            # Simulate old behavior\n            result = steadytext.generate(prompt, **kwargs)\n            if result is None:\n                return self._fallback_generate(prompt)\n            return result\n        else:\n            # New behavior\n            return steadytext.generate(prompt, **kwargs)\n\n    def _fallback_generate(self, prompt):\n        \"\"\"Simulate v2.0.x fallback.\"\"\"\n        return f\"{prompt}. This is a deterministic fallback...\"\n</code></pre>"},{"location":"migration-guide/#migration-timeline","title":"Migration Timeline","text":""},{"location":"migration-guide/#recommended-migration-path","title":"Recommended Migration Path","text":"<ol> <li>Week 1-2: Update error handling for None returns</li> <li>Week 3-4: Test in development environment</li> <li>Week 5-6: Gradual rollout to staging</li> <li>Week 7-8: Production deployment with monitoring</li> <li>Week 9+: Remove compatibility wrappers</li> </ol>"},{"location":"migration-guide/#deprecation-schedule","title":"Deprecation Schedule","text":"<ul> <li>v2.0.x: Supported until December 2024</li> <li>v1.x: Security fixes only</li> <li>v0.x: No longer supported</li> </ul>"},{"location":"migration-guide/#getting-help","title":"Getting Help","text":"<ul> <li>Migration Issues: GitHub Issues</li> <li>Documentation: Full docs</li> <li>Community: Discussions</li> </ul>"},{"location":"migration-guide/#quick-reference-card","title":"Quick Reference Card","text":"<pre><code># Check version\nimport steadytext\nprint(steadytext.__version__)\n\n# Handle v2.1.x None returns\nresult = steadytext.generate(\"prompt\")\ntext = result if result else \"default\"\n\n# Check embedding dimensions\nemb = steadytext.embed(\"text\")\nif emb is not None:\n    assert emb.shape == (1024,)\n\n# Use deterministic seeds\ntext1 = steadytext.generate(\"hi\", seed=42)\ntext2 = steadytext.generate(\"hi\", seed=42)\nassert text1 == text2  # Always true\n</code></pre>"},{"location":"model-switching/","title":"Model Switching in SteadyText","text":"<p>SteadyText v2.0.0+ supports dynamic model switching with the Gemma-3n model family, allowing you to use different model sizes without restarting your application.</p>"},{"location":"model-switching/#overview","title":"Overview","text":"<p>The model switching feature enables you to:</p> <ol> <li>Use different models for different tasks - Choose smaller models for speed or larger models for quality</li> <li>Switch models at runtime - No need to restart your application</li> <li>Maintain deterministic outputs - Each model produces consistent results</li> <li>Cache multiple models - Models are cached after first load for efficiency</li> </ol>"},{"location":"model-switching/#usage-methods","title":"Usage Methods","text":""},{"location":"model-switching/#1-using-size-parameter-new","title":"1. Using Size Parameter (New!)","text":"<p>The simplest way to choose a model based on your needs:</p> <pre><code>from steadytext import generate\n\n# Quick, lightweight tasks\ntext = generate(\"Simple task\", size=\"small\")   # Uses Gemma-3n-2B (default)\ntext = generate(\"Complex analysis\", size=\"large\")   # Uses Gemma-3n-4B\n</code></pre>"},{"location":"model-switching/#2-using-the-model-registry","title":"2. Using the Model Registry","text":"<p>For more specific model selection:</p> <pre><code>from steadytext import generate\n\n# Use a smaller, faster model\ntext = generate(\"Explain machine learning\", size=\"small\")   # Gemma-3n-2B\n\n# Use a larger, more capable model\ntext = generate(\"Write a detailed essay\", size=\"large\")    # Gemma-3n-4B\n</code></pre> <p>Available models in the registry (v2.0.0+):</p> Model Name Size Use Case Size Parameter <code>gemma-3n-2b</code> 2B Default, fast tasks <code>small</code> <code>gemma-3n-4b</code> 4B High quality, complex tasks <code>large</code> <p>Note: SteadyText v2.0.0+ focuses on the Gemma-3n model family. Previous versions (v1.x) supported Qwen models which are now deprecated.</p>"},{"location":"model-switching/#3-using-custom-models","title":"3. Using Custom Models","text":"<p>Specify any GGUF model from Hugging Face:</p> <pre><code>from steadytext import generate\n\n# Use a custom model\ntext = generate(\n    \"Create a Python function\",\n    model_repo=\"ggml-org/gemma-3n-E4B-it-GGUF\",\n    model_filename=\"gemma-3n-E4B-it-Q8_0.gguf\"\n)\n</code></pre>"},{"location":"model-switching/#4-using-environment-variables","title":"4. Using Environment Variables","text":"<p>Set default models via environment variables:</p> <pre><code># Use small model by default\nexport STEADYTEXT_DEFAULT_SIZE=\"small\"\n\n# Or specify custom model (advanced)\nexport STEADYTEXT_GENERATION_MODEL_REPO=\"ggml-org/gemma-3n-E2B-it-GGUF\"\nexport STEADYTEXT_GENERATION_MODEL_FILENAME=\"gemma-3n-E2B-it-Q8_0.gguf\"\n</code></pre>"},{"location":"model-switching/#streaming-generation","title":"Streaming Generation","text":"<p>Model switching works with streaming generation too:</p> <pre><code>from steadytext import generate_iter\n\n# Stream with a specific model size\nfor token in generate_iter(\"Tell me a story\", size=\"large\"):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"model-switching/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"model-switching/#for-speed-2b-model","title":"For Speed (2B model)","text":"<ul> <li>Use cases: Chat responses, simple completions, real-time applications</li> <li>Recommended: <code>gemma-3n-2b</code> (size=\"small\")</li> <li>Trade-off: Faster generation, simpler outputs</li> </ul>"},{"location":"model-switching/#for-quality-4b-model","title":"For Quality (4B model)","text":"<ul> <li>Use cases: Complex reasoning, detailed content, creative writing</li> <li>Recommended: <code>gemma-3n-4b</code> (size=\"large\")</li> <li>Trade-off: Best quality, slower generation</li> </ul>"},{"location":"model-switching/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>First Load: The first use of a model downloads it (if not cached) and loads it into memory</li> <li>Model Caching: Once loaded, models remain in memory for fast switching</li> <li>Memory Usage: Each loaded model uses RAM - consider your available resources</li> <li>Determinism: All models maintain deterministic outputs with the same seed</li> </ol>"},{"location":"model-switching/#examples","title":"Examples","text":""},{"location":"model-switching/#adaptive-model-selection","title":"Adaptive Model Selection","text":"<pre><code>from steadytext import generate\n\ndef smart_generate(prompt, complexity=\"medium\"):\n    \"\"\"Use different models based on task complexity.\"\"\"\n    if complexity == \"low\":\n        # Use fast model for simple tasks\n        return generate(prompt, size=\"small\")\n    else:\n        # Use high-quality model for complex tasks\n        return generate(prompt, size=\"large\")\n</code></pre>"},{"location":"model-switching/#ab-testing-models","title":"A/B Testing Models","text":"<pre><code>from steadytext import generate\n\nprompts = [\"Explain quantum computing\", \"Write a haiku\", \"Solve 2+2\"]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n\n    # Test with small model\n    small = generate(prompt, size=\"small\")\n    print(f\"Small model: {small[:100]}...\")\n\n    # Test with large model\n    large = generate(prompt, size=\"large\")\n    print(f\"Large model: {large[:100]}...\")\n</code></pre>"},{"location":"model-switching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"model-switching/#model-not-found","title":"Model Not Found","text":"<p>If a model download fails, you'll get deterministic fallback text. Check: - Internet connection - Hugging Face availability - Model name spelling</p>"},{"location":"model-switching/#out-of-memory","title":"Out of Memory","text":"<p>Large models require significant RAM. Solutions: - Use smaller quantized models - Clear model cache with <code>clear_model_cache()</code> - Use one model at a time</p>"},{"location":"model-switching/#slow-first-load","title":"Slow First Load","text":"<p>Initial model loading takes time due to: - Downloading (first time only) - Loading into memory - Model initialization</p> <p>Subsequent uses are much faster as models are cached.</p>"},{"location":"postgresql-extension/","title":"PostgreSQL Extension (pg_steadytext)","text":"<p>The pg_steadytext PostgreSQL extension provides native SQL functions for deterministic text generation and embeddings by integrating with the SteadyText library. It brings the power of modern language models directly into your PostgreSQL database.</p>"},{"location":"postgresql-extension/#overview","title":"Overview","text":"<p>pg_steadytext extends PostgreSQL with:</p> <ul> <li>Deterministic Text Generation: SQL functions that generate consistent text output with custom seeds</li> <li>Vector Embeddings: Create 1024-dimensional embeddings compatible with pgvector</li> <li>Built-in Caching: PostgreSQL-based frecency cache for optimal performance</li> <li>Daemon Integration: Seamless integration with SteadyText's ZeroMQ daemon</li> <li>Custom Seed Support: Full control over deterministic generation with custom seeds</li> <li>Reliable Error Handling: Functions return NULL on errors instead of fallback text</li> <li>Security: Input validation, rate limiting, and safe error handling</li> </ul>"},{"location":"postgresql-extension/#requirements","title":"Requirements","text":"<ul> <li>PostgreSQL: 14+ (tested on 14, 15, 16, 17)</li> <li>Python: 3.8+ (matches plpython3u version)</li> <li>SteadyText: 2.3.0+ (for reranking support, daemon, and custom seeds)</li> <li>Extensions:</li> <li><code>plpython3u</code> (required for Python integration)</li> <li><code>pgvector</code> (required for embedding storage)</li> <li><code>omni_python</code> (required for enhanced Python integration, see https://docs.omnigres.org/quick_start/)</li> </ul>"},{"location":"postgresql-extension/#installation","title":"Installation","text":""},{"location":"postgresql-extension/#quick-installation","title":"Quick Installation","text":"<pre><code># Install Python dependencies\npip3 install steadytext&gt;=2.3.0 pyzmq numpy\n\n# Install omni-python (if not available via package manager)\ngit clone https://github.com/omnigres/omnigres.git\ncd omnigres/extensions/omni_python\nmake &amp;&amp; sudo make install\ncd ../../..\n\n# Clone the SteadyText repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\n\n# Build and install the extension\nmake &amp;&amp; sudo make install\n\n# Enable in PostgreSQL\npsql -U postgres -c \"CREATE EXTENSION IF NOT EXISTS plpython3u CASCADE;\"\npsql -U postgres -c \"CREATE EXTENSION IF NOT EXISTS omni_python CASCADE;\"\npsql -U postgres -c \"CREATE EXTENSION IF NOT EXISTS pgvector CASCADE;\"\npsql -U postgres -c \"CREATE EXTENSION pg_steadytext CASCADE;\"\n</code></pre>"},{"location":"postgresql-extension/#docker-installation","title":"Docker Installation","text":"<p>For a complete containerized setup:</p> <pre><code># Standard build\ndocker build -t pg_steadytext .\n\n# Build with fallback model (recommended for compatibility)\ndocker build --build-arg STEADYTEXT_USE_FALLBACK_MODEL=true -t pg_steadytext .\n\n# Run the container\ndocker run -d -p 5432:5432 --name pg_steadytext pg_steadytext\n\n# Test the installation\ndocker exec -it pg_steadytext psql -U postgres -c \"SELECT steadytext_version();\"\n</code></pre>"},{"location":"postgresql-extension/#core-functions","title":"Core Functions","text":""},{"location":"postgresql-extension/#text-generation","title":"Text Generation","text":""},{"location":"postgresql-extension/#steadytext_generate","title":"<code>steadytext_generate()</code>","text":"<p>Generate deterministic text from a prompt with full customization options.</p> <pre><code>steadytext_generate(\n    prompt TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n-- Returns NULL if generation fails\n</code></pre> <p>Examples:</p> <pre><code>-- Simple text generation (uses default seed 42)\nSELECT steadytext_generate('Write a haiku about PostgreSQL');\n\n-- Custom seed for reproducible results\nSELECT steadytext_generate(\n    'Tell me a story',\n    max_tokens := 256,\n    seed := 12345\n);\n\n-- Disable caching for fresh results\nSELECT steadytext_generate(\n    'Random joke',\n    use_cache := false,\n    seed := 999\n);\n\n-- Handle NULL results from failed generation\nSELECT COALESCE(\n    steadytext_generate('Generate text', seed := 100),\n    'Generation failed - please check daemon status'\n) AS result;\n\n-- Compare outputs with different seeds\nSELECT \n    'Seed 100' AS variant,\n    steadytext_generate('Explain machine learning', seed := 100) AS output\nUNION ALL\nSELECT \n    'Seed 200' AS variant,\n    steadytext_generate('Explain machine learning', seed := 200) AS output;\n</code></pre>"},{"location":"postgresql-extension/#steadytext_generate_stream","title":"<code>steadytext_generate_stream()</code>","text":"<p>Stream text generation for real-time applications (future feature).</p> <pre><code>steadytext_generate_stream(\n    prompt TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    seed INTEGER DEFAULT 42\n) RETURNS SETOF TEXT\n</code></pre>"},{"location":"postgresql-extension/#embeddings","title":"Embeddings","text":""},{"location":"postgresql-extension/#steadytext_embed","title":"<code>steadytext_embed()</code>","text":"<p>Generate 1024-dimensional L2-normalized embeddings for text.</p> <pre><code>steadytext_embed(\n    text_input TEXT,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS vector(1024)\n-- Returns NULL vector if embedding fails\n</code></pre> <p>Examples:</p> <pre><code>-- Simple embedding (uses default seed 42)\nSELECT steadytext_embed('PostgreSQL is a powerful database');\n\n-- Custom seed for reproducible embeddings\nSELECT steadytext_embed(\n    'artificial intelligence',\n    seed := 123\n);\n\n-- Handle NULL embeddings from failed generation\nSELECT \n    text,\n    CASE \n        WHEN steadytext_embed(text, seed := 42) IS NOT NULL \n        THEN 'Embedding generated'\n        ELSE 'Embedding failed'\n    END AS status\nFROM documents;\n\n-- Semantic similarity using pgvector with NULL handling\nWITH base_embedding AS (\n    SELECT steadytext_embed('machine learning', seed := 42) AS vector\n)\nSELECT \n    text,\n    embedding &lt;-&gt; (SELECT vector FROM base_embedding) AS distance\nFROM documents\nWHERE embedding IS NOT NULL \n    AND (SELECT vector FROM base_embedding) IS NOT NULL\nORDER BY distance\nLIMIT 5;\n\n-- Compare embeddings with different seeds (with NULL checks)\nSELECT \n    variant,\n    CASE \n        WHEN embedding IS NOT NULL THEN 'Generated'\n        ELSE 'Failed'\n    END AS status,\n    embedding\nFROM (\n    SELECT \n        'Default seed' AS variant,\n        steadytext_embed('AI technology') AS embedding\n    UNION ALL\n    SELECT \n        'Custom seed' AS variant,\n        steadytext_embed('AI technology', seed := 789) AS embedding\n) results;\n</code></pre>"},{"location":"postgresql-extension/#structured-generation-v241","title":"Structured Generation (v2.4.1+)","text":"<p>New in v2.4.1, the PostgreSQL extension now supports structured text generation using llama.cpp's native grammar support.</p>"},{"location":"postgresql-extension/#steadytext_generate_json","title":"<code>steadytext_generate_json()</code>","text":"<p>Generate JSON that conforms to a JSON schema.</p> <pre><code>steadytext_generate_json(\n    prompt TEXT,\n    schema JSONB,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n-- Returns NULL if generation fails\n</code></pre> <p>Examples:</p> <pre><code>-- Simple JSON generation\nSELECT steadytext_generate_json(\n    'Create a user named John, age 30',\n    '{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}}}'::jsonb\n);\n\n-- Generate product information\nSELECT steadytext_generate_json(\n    'Create a product listing for a laptop',\n    '{\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"price\": {\"type\": \"number\"},\n            \"specs\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"cpu\": {\"type\": \"string\"},\n                    \"ram\": {\"type\": \"string\"},\n                    \"storage\": {\"type\": \"string\"}\n                }\n            }\n        }\n    }'::jsonb,\n    seed := 999\n);\n</code></pre>"},{"location":"postgresql-extension/#steadytext_generate_regex","title":"<code>steadytext_generate_regex()</code>","text":"<p>Generate text that matches a regular expression pattern.</p> <pre><code>steadytext_generate_regex(\n    prompt TEXT,\n    pattern TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n-- Returns NULL if generation fails\n</code></pre> <p>Examples:</p> <pre><code>-- Generate a phone number\nSELECT steadytext_generate_regex(\n    'Contact number: ',\n    '\\d{3}-\\d{3}-\\d{4}'\n);\n\n-- Generate a date\nSELECT steadytext_generate_regex(\n    'Event date: ',\n    '\\d{4}-\\d{2}-\\d{2}'\n);\n\n-- Generate an email\nSELECT steadytext_generate_regex(\n    'Email: ',\n    '[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n);\n</code></pre>"},{"location":"postgresql-extension/#steadytext_generate_choice","title":"<code>steadytext_generate_choice()</code>","text":"<p>Generate text that is one of the provided choices.</p> <pre><code>steadytext_generate_choice(\n    prompt TEXT,\n    choices TEXT[],\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n-- Returns NULL if generation fails\n</code></pre> <p>Examples:</p> <pre><code>-- Simple choice\nSELECT steadytext_generate_choice(\n    'The weather today is',\n    ARRAY['sunny', 'cloudy', 'rainy']\n);\n\n-- Sentiment analysis\nSELECT \n    review,\n    steadytext_generate_choice(\n        'Sentiment of this review: ' || review,\n        ARRAY['positive', 'negative', 'neutral']\n    ) AS sentiment\nFROM product_reviews\nLIMIT 5;\n\n-- Classification with custom seed\nSELECT steadytext_generate_choice(\n    'This document is about',\n    ARRAY['technology', 'business', 'health', 'sports', 'entertainment'],\n    seed := 456\n);\n</code></pre>"},{"location":"postgresql-extension/#document-reranking-v130","title":"Document Reranking (v1.3.0+)","text":"<p>PostgreSQL extension v1.3.0+ includes document reranking functionality powered by the Qwen3-Reranker-4B model.</p>"},{"location":"postgresql-extension/#steadytext_rerank","title":"<code>steadytext_rerank()</code>","text":"<p>Rerank documents by relevance to a query.</p> <pre><code>steadytext_rerank(\n    query TEXT,\n    documents TEXT[],\n    task TEXT DEFAULT 'Given a web search query, retrieve relevant passages that answer the query',\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TABLE(document TEXT, score FLOAT)\n</code></pre> <p>Examples:</p> <pre><code>-- Basic reranking\nSELECT * FROM steadytext_rerank(\n    'Python programming',\n    ARRAY[\n        'Python is a programming language',\n        'Cats are cute animals',\n        'Python snakes are found in Asia'\n    ]\n);\n\n-- Custom task description\nSELECT * FROM steadytext_rerank(\n    'customer complaint about delivery',\n    ARRAY(SELECT ticket_text FROM support_tickets WHERE created_at &gt; NOW() - INTERVAL '7 days'),\n    task := 'support ticket prioritization'\n);\n\n-- Integration with search results\nWITH search_results AS (\n    SELECT content, ts_rank(search_vector, query) AS text_score\n    FROM documents, plainto_tsquery('english', 'machine learning') query\n    WHERE search_vector @@ query\n    LIMIT 20\n)\nSELECT r.document, r.score as ai_score, s.text_score\nFROM search_results s,\n     LATERAL steadytext_rerank(\n         'machine learning',\n         ARRAY_AGG(s.content),\n         seed := 456\n     ) r\nWHERE s.content = r.document\nORDER BY r.score DESC\nLIMIT 5;\n</code></pre>"},{"location":"postgresql-extension/#steadytext_rerank_docs_only","title":"<code>steadytext_rerank_docs_only()</code>","text":"<p>Get reranked documents without scores.</p> <pre><code>steadytext_rerank_docs_only(\n    query TEXT,\n    documents TEXT[],\n    task TEXT DEFAULT 'Given a web search query, retrieve relevant passages that answer the query',\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TABLE(document TEXT)\n</code></pre> <p>Example:</p> <pre><code>-- Get reranked documents for display\nSELECT * FROM steadytext_rerank_docs_only(\n    'machine learning',\n    ARRAY(SELECT content FROM documents WHERE category = 'tech')\n);\n</code></pre>"},{"location":"postgresql-extension/#steadytext_rerank_top_k","title":"<code>steadytext_rerank_top_k()</code>","text":"<p>Get top K most relevant documents.</p> <pre><code>steadytext_rerank_top_k(\n    query TEXT,\n    documents TEXT[],\n    k INTEGER,\n    task TEXT DEFAULT 'Given a web search query, retrieve relevant passages that answer the query',\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TABLE(document TEXT, score FLOAT)\n</code></pre> <p>Example:</p> <pre><code>-- Get top 5 most relevant support tickets\nSELECT * FROM steadytext_rerank_top_k(\n    'refund request',\n    ARRAY(SELECT ticket_text FROM support_tickets),\n    5\n);\n</code></pre>"},{"location":"postgresql-extension/#steadytext_rerank_batch","title":"<code>steadytext_rerank_batch()</code>","text":"<p>Batch reranking for multiple queries.</p> <pre><code>steadytext_rerank_batch(\n    queries TEXT[],\n    documents TEXT[],\n    task TEXT DEFAULT 'Given a web search query, retrieve relevant passages that answer the query',\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TABLE(query_idx INTEGER, doc_idx INTEGER, score FLOAT)\n</code></pre> <p>Example:</p> <pre><code>-- Rerank documents for multiple queries\nSELECT * FROM steadytext_rerank_batch(\n    ARRAY['Python programming', 'machine learning', 'data science'],\n    ARRAY['Python is great', 'ML algorithms', 'Data analysis with Python']\n);\n</code></pre>"},{"location":"postgresql-extension/#async-reranking-functions","title":"Async Reranking Functions","text":"<p>All reranking functions have async counterparts:</p> <pre><code>-- Queue async reranking\nSELECT request_id FROM steadytext_rerank_async(\n    'search query',\n    ARRAY(SELECT content FROM documents)\n);\n\n-- Check status and get results\nSELECT * FROM steadytext_check_async(request_id);\nSELECT * FROM steadytext_get_async_result(request_id, timeout_seconds := 30);\n</code></pre>"},{"location":"postgresql-extension/#management-functions","title":"Management Functions","text":""},{"location":"postgresql-extension/#daemon-management","title":"Daemon Management","text":""},{"location":"postgresql-extension/#steadytext_daemon_start","title":"<code>steadytext_daemon_start()</code>","text":"<p>Start the SteadyText daemon for improved performance.</p> <pre><code>SELECT steadytext_daemon_start();\nSELECT steadytext_daemon_start('localhost', 5557); -- Custom host/port\n</code></pre>"},{"location":"postgresql-extension/#steadytext_daemon_status","title":"<code>steadytext_daemon_status()</code>","text":"<p>Check daemon health and status.</p> <pre><code>SELECT * FROM steadytext_daemon_status();\n-- Returns: running, pid, host, port, uptime, health\n</code></pre>"},{"location":"postgresql-extension/#steadytext_daemon_stop","title":"<code>steadytext_daemon_stop()</code>","text":"<p>Stop the daemon gracefully.</p> <pre><code>SELECT steadytext_daemon_stop();\nSELECT steadytext_daemon_stop(true); -- Force stop\n</code></pre>"},{"location":"postgresql-extension/#cache-management","title":"Cache Management","text":""},{"location":"postgresql-extension/#steadytext_cache_stats","title":"<code>steadytext_cache_stats()</code>","text":"<p>View cache performance statistics.</p> <pre><code>SELECT * FROM steadytext_cache_stats();\n-- Returns: entries, total_size_mb, hit_rate, evictions, oldest_entry\n</code></pre>"},{"location":"postgresql-extension/#steadytext_cache_clear","title":"<code>steadytext_cache_clear()</code>","text":"<p>Clear the cache for fresh results.</p> <pre><code>SELECT steadytext_cache_clear();                    -- Clear all\nSELECT steadytext_cache_clear('generation');        -- Clear generation cache only\nSELECT steadytext_cache_clear('embedding');         -- Clear embedding cache only\n</code></pre>"},{"location":"postgresql-extension/#configuration","title":"Configuration","text":""},{"location":"postgresql-extension/#steadytext_config_get-steadytext_config_set","title":"<code>steadytext_config_get()</code> / <code>steadytext_config_set()</code>","text":"<p>Manage extension configuration.</p> <pre><code>-- View all configuration\nSELECT * FROM steadytext_config;\n\n-- Get specific setting\nSELECT steadytext_config_get('default_max_tokens');\n\n-- Update settings\nSELECT steadytext_config_set('default_max_tokens', '1024');\nSELECT steadytext_config_set('cache_enabled', 'true');\nSELECT steadytext_config_set('daemon_host', 'localhost');\nSELECT steadytext_config_set('daemon_port', '5557');\nSELECT steadytext_config_set('default_seed', '42');\n</code></pre>"},{"location":"postgresql-extension/#database-schema","title":"Database Schema","text":"<p>The extension creates several tables to manage caching, configuration, and monitoring:</p>"},{"location":"postgresql-extension/#steadytext_cache","title":"<code>steadytext_cache</code>","text":"<p>Stores cached generation and embedding results with frecency metadata.</p> <pre><code>\\d steadytext_cache\n</code></pre> Column Type Description <code>key</code> TEXT Cache key (hash of input + parameters) <code>prompt</code> TEXT Original prompt text <code>result</code> TEXT Generated text result <code>embedding</code> vector(1024) Generated embedding vector <code>seed</code> INTEGER Seed used for generation <code>frequency</code> INTEGER Access frequency counter <code>last_access</code> TIMESTAMP Last access time <code>created_at</code> TIMESTAMP Creation timestamp"},{"location":"postgresql-extension/#steadytext_config","title":"<code>steadytext_config</code>","text":"<p>Extension configuration settings.</p> <pre><code>SELECT key, value, description FROM steadytext_config;\n</code></pre> Key Default Description <code>default_max_tokens</code> <code>512</code> Default maximum tokens to generate <code>cache_enabled</code> <code>true</code> Enable/disable caching <code>daemon_host</code> <code>localhost</code> Daemon server host <code>daemon_port</code> <code>5557</code> Daemon server port <code>default_seed</code> <code>42</code> Default seed for operations <code>use_fallback_model</code> <code>false</code> Use fallback model if primary fails <code>rate_limit_enabled</code> <code>false</code> Enable rate limiting <code>max_requests_per_minute</code> <code>60</code> Rate limit threshold"},{"location":"postgresql-extension/#steadytext_daemon_health","title":"<code>steadytext_daemon_health</code>","text":"<p>Daemon health monitoring and diagnostics.</p> <pre><code>SELECT * FROM steadytext_daemon_health ORDER BY checked_at DESC LIMIT 5;\n</code></pre>"},{"location":"postgresql-extension/#advanced-usage","title":"Advanced Usage","text":""},{"location":"postgresql-extension/#batch-operations","title":"Batch Operations","text":"<p>Process multiple prompts efficiently:</p> <pre><code>-- Batch generation with different seeds\nWITH prompts AS (\n    SELECT unnest(ARRAY[\n        'Explain quantum computing',\n        'Describe machine learning',\n        'What is artificial intelligence'\n    ]) AS prompt,\n    unnest(ARRAY[100, 200, 300]) AS seed\n)\nSELECT \n    prompt,\n    seed,\n    steadytext_generate(prompt, seed := seed) AS response\nFROM prompts;\n\n-- Batch embeddings for similarity analysis\nWITH texts AS (\n    SELECT unnest(ARRAY[\n        'artificial intelligence',\n        'machine learning',\n        'deep learning',\n        'neural networks'\n    ]) AS text\n)\nSELECT \n    text,\n    steadytext_embed(text, seed := 42) AS embedding\nFROM texts;\n</code></pre>"},{"location":"postgresql-extension/#semantic-search-implementation","title":"Semantic Search Implementation","text":"<p>Build a semantic search system using pg_steadytext:</p> <pre><code>-- Create a documents table with embeddings\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    title TEXT NOT NULL,\n    content TEXT NOT NULL,\n    embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Create index for fast similarity search\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)\nWITH (lists = 100);\n\n-- Populate with embeddings using consistent seed (skip failed embeddings)\nINSERT INTO documents (title, content, embedding)\nSELECT \n    title,\n    content,\n    embedding\nFROM (\n    SELECT \n        title,\n        content,\n        steadytext_embed(content, seed := 42) AS embedding\n    FROM source_documents\n) WITH_EMBEDDINGS\nWHERE embedding IS NOT NULL;\n\n-- Semantic search function\nCREATE OR REPLACE FUNCTION semantic_search(\n    query_text TEXT,\n    max_results INTEGER DEFAULT 5,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(id INTEGER, title TEXT, content TEXT, similarity REAL) AS $$\nDECLARE\n    query_embedding vector(1024);\nBEGIN\n    -- Generate query embedding with error handling\n    query_embedding := steadytext_embed(query_text, seed := search_seed);\n\n    -- Return empty result if embedding generation failed\n    IF query_embedding IS NULL THEN\n        RAISE WARNING 'Failed to generate embedding for query: %', query_text;\n        RETURN;\n    END IF;\n\n    RETURN QUERY\n    SELECT \n        d.id,\n        d.title,\n        d.content,\n        1 - (d.embedding &lt;=&gt; query_embedding) AS similarity\n    FROM documents d\n    WHERE d.embedding IS NOT NULL\n    ORDER BY d.embedding &lt;=&gt; query_embedding\n    LIMIT max_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage\nSELECT * FROM semantic_search('machine learning algorithms', 10);\n</code></pre>"},{"location":"postgresql-extension/#content-generation-pipeline","title":"Content Generation Pipeline","text":"<p>Create a content generation workflow:</p> <pre><code>-- Content generation pipeline with different styles\nCREATE OR REPLACE FUNCTION generate_content_variants(\n    base_prompt TEXT,\n    num_variants INTEGER DEFAULT 3\n)\nRETURNS TABLE(variant_id INTEGER, style TEXT, content TEXT) AS $$\nDECLARE\n    styles TEXT[] := ARRAY['formal', 'casual', 'technical'];\n    i INTEGER;\n    current_style TEXT;\n    enhanced_prompt TEXT;\nBEGIN\n    FOR i IN 1..LEAST(num_variants, array_length(styles, 1)) LOOP\n        current_style := styles[i];\n        enhanced_prompt := format('Write in a %s style: %s', current_style, base_prompt);\n\n        -- Generate content with error handling\n        DECLARE\n            generated_content TEXT;\n        BEGIN\n            generated_content := steadytext_generate(\n                enhanced_prompt,\n                max_tokens := 200,\n                seed := 100 + i  -- Different seed for each variant\n            );\n\n            -- Skip variants that failed to generate\n            IF generated_content IS NOT NULL THEN\n                RETURN QUERY\n                SELECT \n                    i AS variant_id,\n                    current_style AS style,\n                    generated_content AS content;\n            ELSE\n                RAISE WARNING 'Failed to generate content for style: %', current_style;\n            END IF;\n        END;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage\nSELECT * FROM generate_content_variants('Explain the benefits of PostgreSQL');\n</code></pre>"},{"location":"postgresql-extension/#performance-optimization","title":"Performance Optimization","text":""},{"location":"postgresql-extension/#cache-strategy","title":"Cache Strategy","text":"<pre><code>-- Monitor cache performance\nSELECT \n    'Generation Cache' AS cache_type,\n    entries,\n    total_size_mb,\n    hit_rate,\n    evictions\nFROM steadytext_cache_stats()\nWHERE cache_type = 'generation'\nUNION ALL\nSELECT \n    'Embedding Cache' AS cache_type,\n    entries,\n    total_size_mb,\n    hit_rate,\n    evictions\nFROM steadytext_cache_stats()\nWHERE cache_type = 'embedding';\n\n-- Optimize cache by pre-warming common queries\nWITH common_prompts AS (\n    SELECT unnest(ARRAY[\n        'Summarize the key points',\n        'Explain this concept',\n        'Generate a brief description'\n    ]) AS prompt\n)\nSELECT \n    prompt,\n    'Pre-warmed: ' || length(steadytext_generate(prompt)) || ' chars' AS status\nFROM common_prompts;\n</code></pre>"},{"location":"postgresql-extension/#daemon-performance","title":"Daemon Performance","text":"<pre><code>-- Check daemon performance metrics\nSELECT \n    running,\n    uptime,\n    health_score,\n    last_response_time_ms,\n    total_requests,\n    error_rate\nFROM steadytext_daemon_status();\n\n-- Restart daemon if performance degrades\nDO $$\nBEGIN\n    IF (SELECT health_score FROM steadytext_daemon_status()) &lt; 0.8 THEN\n        PERFORM steadytext_daemon_stop();\n        PERFORM pg_sleep(2);\n        PERFORM steadytext_daemon_start();\n        RAISE NOTICE 'Daemon restarted due to poor health score';\n    END IF;\nEND;\n$$;\n</code></pre>"},{"location":"postgresql-extension/#security-considerations","title":"Security Considerations","text":""},{"location":"postgresql-extension/#input-validation","title":"Input Validation","text":"<pre><code>-- Safe text generation with input validation and NULL handling\nCREATE OR REPLACE FUNCTION safe_generate(\n    user_prompt TEXT,\n    max_length INTEGER DEFAULT 512,\n    custom_seed INTEGER DEFAULT 42\n)\nRETURNS TEXT AS $$\nDECLARE\n    result TEXT;\nBEGIN\n    -- Validate input length\n    IF length(user_prompt) &gt; 1000 THEN\n        RAISE EXCEPTION 'Prompt too long (max 1000 characters)';\n    END IF;\n\n    -- Validate max_length\n    IF max_length &gt; 2048 OR max_length &lt; 1 THEN\n        RAISE EXCEPTION 'Invalid max_length (must be 1-2048)';\n    END IF;\n\n    -- Validate seed\n    IF custom_seed &lt; 0 THEN\n        RAISE EXCEPTION 'Seed must be non-negative';\n    END IF;\n\n    -- Sanitize prompt (basic example)\n    user_prompt := regexp_replace(user_prompt, '[&lt;&gt;]', '', 'g');\n\n    -- Generate with error handling\n    result := steadytext_generate(user_prompt, max_length, true, custom_seed);\n\n    -- Return error message if generation failed\n    IF result IS NULL THEN\n        RETURN '[Error: Text generation failed. Please check system status.]';\n    END IF;\n\n    RETURN result;\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\n\n-- Grant limited access\nGRANT EXECUTE ON FUNCTION safe_generate TO app_user;\n</code></pre>"},{"location":"postgresql-extension/#rate-limiting","title":"Rate Limiting","text":"<pre><code>-- Enable rate limiting\nSELECT steadytext_config_set('rate_limit_enabled', 'true');\nSELECT steadytext_config_set('max_requests_per_minute', '30');\n\n-- Custom rate limiting per user\nCREATE TABLE user_rate_limits (\n    user_id INTEGER PRIMARY KEY,\n    requests_made INTEGER DEFAULT 0,\n    window_start TIMESTAMP DEFAULT NOW(),\n    max_requests INTEGER DEFAULT 10\n);\n\n-- Rate-limited generation function\nCREATE OR REPLACE FUNCTION rate_limited_generate(\n    user_id INTEGER,\n    prompt TEXT\n)\nRETURNS TEXT AS $$\nDECLARE\n    current_requests INTEGER;\n    window_start TIMESTAMP;\n    max_allowed INTEGER;\nBEGIN\n    -- Get or create rate limit record\n    INSERT INTO user_rate_limits (user_id)\n    VALUES (user_id)\n    ON CONFLICT (user_id) DO NOTHING;\n\n    -- Check current usage\n    SELECT requests_made, window_start, max_requests\n    INTO current_requests, window_start, max_allowed\n    FROM user_rate_limits\n    WHERE user_id = rate_limited_generate.user_id;\n\n    -- Reset window if expired (1 hour)\n    IF window_start &lt; NOW() - INTERVAL '1 hour' THEN\n        UPDATE user_rate_limits\n        SET requests_made = 0, window_start = NOW()\n        WHERE user_id = rate_limited_generate.user_id;\n        current_requests := 0;\n    END IF;\n\n    -- Check rate limit\n    IF current_requests &gt;= max_allowed THEN\n        RAISE EXCEPTION 'Rate limit exceeded. Try again later.';\n    END IF;\n\n    -- Increment counter\n    UPDATE user_rate_limits\n    SET requests_made = requests_made + 1\n    WHERE user_id = rate_limited_generate.user_id;\n\n    -- Generate text with error handling\n    DECLARE\n        result TEXT;\n    BEGIN\n        result := steadytext_generate(prompt, 512, true, 42);\n\n        IF result IS NULL THEN\n            RAISE EXCEPTION 'Text generation failed. Please try again later.';\n        END IF;\n\n        RETURN result;\n    END;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"postgresql-extension/#ai-summarization-v110","title":"AI Summarization (v1.1.0+)","text":"<p>The PostgreSQL extension includes powerful AI summarization aggregate functions that work seamlessly with TimescaleDB continuous aggregates.</p>"},{"location":"postgresql-extension/#core-summarization-functions","title":"Core Summarization Functions","text":""},{"location":"postgresql-extension/#ai_summarize_text","title":"<code>ai_summarize_text()</code>","text":"<p>Summarize a single text with optional metadata.</p> <pre><code>ai_summarize_text(\n    text_input TEXT,\n    metadata JSONB DEFAULT NULL,\n    max_tokens INTEGER DEFAULT 150,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n</code></pre> <p>Examples:</p> <pre><code>-- Simple text summarization\nSELECT ai_summarize_text(\n    'PostgreSQL is an advanced open-source relational database with ACID compliance, \n     JSON support, and extensibility through custom functions and types.',\n    '{\"source\": \"documentation\"}'::jsonb\n);\n\n-- Summarize with custom parameters\nSELECT ai_summarize_text(\n    content,\n    jsonb_build_object('importance', importance, 'category', category),\n    max_tokens := 200,\n    seed := 123\n) AS summary\nFROM documents\nWHERE length(content) &gt; 1000;\n</code></pre>"},{"location":"postgresql-extension/#ai_summarize-aggregate-function","title":"<code>ai_summarize()</code> Aggregate Function","text":"<p>Intelligently summarize multiple texts into a coherent summary.</p> <pre><code>-- Basic aggregate summarization\nSELECT \n    category,\n    ai_summarize(content) AS category_summary,\n    count(*) AS doc_count\nFROM documents\nGROUP BY category;\n\n-- With metadata\nSELECT \n    department,\n    ai_summarize(\n        report_text,\n        jsonb_build_object('priority', priority, 'date', report_date)\n    ) AS department_summary\nFROM reports\nWHERE report_date &gt;= CURRENT_DATE - INTERVAL '7 days'\nGROUP BY department;\n</code></pre>"},{"location":"postgresql-extension/#partial-aggregation-for-timescaledb","title":"Partial Aggregation for TimescaleDB","text":"<p>The extension supports partial aggregation for use with TimescaleDB continuous aggregates:</p>"},{"location":"postgresql-extension/#ai_summarize_partial-and-ai_summarize_final","title":"<code>ai_summarize_partial()</code> and <code>ai_summarize_final()</code>","text":"<pre><code>-- Create continuous aggregate with partial summarization\nCREATE MATERIALIZED VIEW hourly_log_summaries\nWITH (timescaledb.continuous) AS\nSELECT \n    time_bucket('1 hour', timestamp) AS hour,\n    log_level,\n    service_name,\n    ai_summarize_partial(\n        log_message,\n        jsonb_build_object(\n            'severity', severity,\n            'service', service_name,\n            'error_code', error_code\n        )\n    ) AS partial_summary,\n    count(*) AS log_count\nFROM application_logs\nGROUP BY hour, log_level, service_name;\n\n-- Query with final summarization\nSELECT \n    time_bucket('1 day', hour) as day,\n    log_level,\n    ai_summarize_final(partial_summary) as daily_summary,\n    sum(log_count) as total_logs\nFROM hourly_log_summaries\nWHERE hour &gt;= NOW() - INTERVAL '7 days'\nGROUP BY day, log_level\nORDER BY day DESC;\n</code></pre>"},{"location":"postgresql-extension/#fact-extraction","title":"Fact Extraction","text":""},{"location":"postgresql-extension/#ai_extract_facts","title":"<code>ai_extract_facts()</code>","text":"<p>Extract key facts from text content.</p> <pre><code>ai_extract_facts(\n    text_input TEXT,\n    max_facts INTEGER DEFAULT 5,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT[]\n</code></pre> <p>Examples:</p> <pre><code>-- Extract facts from a document\nSELECT ai_extract_facts(\n    'PostgreSQL supports JSON, arrays, full-text search, window functions, \n     CTEs, and has built-in replication. It also offers ACID compliance \n     and supports multiple programming languages for stored procedures.',\n    max_facts := 7\n);\n-- Returns: {\n--   \"PostgreSQL supports JSON\",\n--   \"PostgreSQL supports arrays\",\n--   \"PostgreSQL has full-text search\",\n--   \"PostgreSQL has window functions\",\n--   \"PostgreSQL supports CTEs\",\n--   \"PostgreSQL has built-in replication\",\n--   \"PostgreSQL offers ACID compliance\"\n-- }\n\n-- Extract facts from multiple documents\nSELECT \n    doc_id,\n    title,\n    ai_extract_facts(content, 3) AS key_facts\nFROM technical_docs\nWHERE category = 'database'\nLIMIT 10;\n</code></pre>"},{"location":"postgresql-extension/#real-world-use-cases","title":"Real-World Use Cases","text":""},{"location":"postgresql-extension/#log-analysis-dashboard","title":"Log Analysis Dashboard","text":"<pre><code>-- Real-time error summarization\nCREATE OR REPLACE VIEW error_summaries AS\nSELECT \n    date_trunc('hour', timestamp) AS error_hour,\n    service_name,\n    ai_summarize(\n        error_message,\n        jsonb_build_object(\n            'count', count(*),\n            'unique_errors', count(DISTINCT error_code)\n        )\n    ) AS error_summary,\n    array_agg(DISTINCT error_code) AS error_codes\nFROM error_logs\nWHERE timestamp &gt;= NOW() - INTERVAL '24 hours'\nGROUP BY error_hour, service_name\nORDER BY error_hour DESC;\n</code></pre>"},{"location":"postgresql-extension/#document-intelligence","title":"Document Intelligence","text":"<pre><code>-- Automatic document categorization and summarization\nWITH doc_summaries AS (\n    SELECT \n        document_id,\n        ai_summarize_text(content) AS summary,\n        ai_extract_facts(content, 5) AS key_facts\n    FROM documents\n    WHERE created_at &gt;= CURRENT_DATE\n)\nSELECT \n    d.document_id,\n    d.title,\n    ds.summary,\n    ds.key_facts,\n    steadytext_generate_choice(\n        'Category for document: ' || ds.summary,\n        ARRAY['technical', 'business', 'legal', 'marketing', 'other']\n    ) AS suggested_category\nFROM documents d\nJOIN doc_summaries ds ON d.document_id = ds.document_id;\n</code></pre>"},{"location":"postgresql-extension/#async-functions-v110","title":"Async Functions (v1.1.0+)","text":"<p>The PostgreSQL extension includes asynchronous functions for non-blocking AI operations, perfect for high-throughput applications.</p>"},{"location":"postgresql-extension/#overview_1","title":"Overview","text":"<p>Async functions return a UUID immediately and process requests in the background using a queue-based architecture with priority support.</p>"},{"location":"postgresql-extension/#core-async-functions","title":"Core Async Functions","text":""},{"location":"postgresql-extension/#generation-functions","title":"Generation Functions","text":"<pre><code>-- Async text generation\nsteadytext_generate_async(\n    prompt TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    priority INTEGER DEFAULT 5,\n    seed INTEGER DEFAULT 42\n) RETURNS UUID\n\n-- Async JSON generation\nsteadytext_generate_json_async(\n    prompt TEXT,\n    schema JSONB,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS UUID\n\n-- Async regex generation\nsteadytext_generate_regex_async(\n    prompt TEXT,\n    pattern TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS UUID\n\n-- Async choice generation\nsteadytext_generate_choice_async(\n    prompt TEXT,\n    choices TEXT[],\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS UUID\n</code></pre>"},{"location":"postgresql-extension/#embedding-functions","title":"Embedding Functions","text":"<pre><code>-- Async embedding generation\nsteadytext_embed_async(\n    text_input TEXT,\n    use_cache BOOLEAN DEFAULT true,\n    priority INTEGER DEFAULT 5\n) RETURNS UUID\n</code></pre>"},{"location":"postgresql-extension/#batch-operations_1","title":"Batch Operations","text":"<pre><code>-- Batch text generation\nsteadytext_generate_batch_async(\n    prompts TEXT[],\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true\n) RETURNS UUID[]\n\n-- Batch embedding generation\nsteadytext_embed_batch_async(\n    texts TEXT[],\n    use_cache BOOLEAN DEFAULT true\n) RETURNS UUID[]\n</code></pre>"},{"location":"postgresql-extension/#result-management","title":"Result Management","text":""},{"location":"postgresql-extension/#steadytext_check_async","title":"<code>steadytext_check_async()</code>","text":"<p>Check the status of an async request.</p> <pre><code>steadytext_check_async(request_id UUID)\nRETURNS TABLE(\n    status TEXT,           -- 'pending', 'processing', 'completed', 'failed'\n    result TEXT,          -- Generated text (NULL if not completed)\n    error TEXT,           -- Error message (NULL if successful)\n    created_at TIMESTAMP,\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP,\n    processing_time_ms INTEGER\n)\n</code></pre>"},{"location":"postgresql-extension/#steadytext_get_async_result","title":"<code>steadytext_get_async_result()</code>","text":"<p>Wait for and retrieve the result of an async request.</p> <pre><code>steadytext_get_async_result(\n    request_id UUID,\n    timeout_seconds INTEGER DEFAULT 30\n) RETURNS TEXT\n-- Returns NULL on timeout or error\n</code></pre>"},{"location":"postgresql-extension/#steadytext_cancel_async","title":"<code>steadytext_cancel_async()</code>","text":"<p>Cancel a pending async request.</p> <pre><code>steadytext_cancel_async(request_id UUID) RETURNS BOOLEAN\n</code></pre>"},{"location":"postgresql-extension/#steadytext_check_async_batch","title":"<code>steadytext_check_async_batch()</code>","text":"<p>Check multiple async requests at once.</p> <pre><code>steadytext_check_async_batch(request_ids UUID[])\nRETURNS TABLE(\n    request_id UUID,\n    status TEXT,\n    result TEXT,\n    error TEXT\n)\n</code></pre>"},{"location":"postgresql-extension/#usage-examples","title":"Usage Examples","text":""},{"location":"postgresql-extension/#basic-async-generation","title":"Basic Async Generation","text":"<pre><code>-- Start async generation\nSELECT request_id FROM steadytext_generate_async(\n    'Write a comprehensive guide to PostgreSQL performance tuning',\n    max_tokens := 1024\n);\n\n-- Check status\nSELECT * FROM steadytext_check_async('your-request-id'::uuid);\n\n-- Wait for result\nSELECT steadytext_get_async_result('your-request-id'::uuid, 60);\n</code></pre>"},{"location":"postgresql-extension/#batch-processing-pattern","title":"Batch Processing Pattern","text":"<pre><code>-- Process multiple documents asynchronously\nWITH async_requests AS (\n    SELECT \n        doc_id,\n        steadytext_generate_async(\n            'Summarize: ' || content,\n            max_tokens := 200\n        ) AS request_id\n    FROM documents\n    WHERE needs_summary = true\n    LIMIT 100\n)\n-- Store request mappings\nINSERT INTO document_summary_requests (doc_id, request_id, requested_at)\nSELECT doc_id, request_id, NOW()\nFROM async_requests;\n\n-- Later: Collect results\nUPDATE documents d\nSET summary = r.result,\n    summarized_at = NOW()\nFROM (\n    SELECT \n        dsr.doc_id,\n        steadytext_get_async_result(dsr.request_id, 30) AS result\n    FROM document_summary_requests dsr\n    WHERE dsr.completed_at IS NULL\n) r\nWHERE d.doc_id = r.doc_id\n  AND r.result IS NOT NULL;\n</code></pre>"},{"location":"postgresql-extension/#priority-queue-example","title":"Priority Queue Example","text":"<pre><code>-- High priority requests\nSELECT steadytext_generate_async(\n    'URGENT: ' || request_text,\n    priority := 10  -- Higher priority\n) FROM urgent_requests;\n\n-- Normal priority requests\nSELECT steadytext_generate_async(\n    request_text,\n    priority := 5   -- Default priority\n) FROM normal_requests;\n</code></pre>"},{"location":"postgresql-extension/#listennotify-integration","title":"LISTEN/NOTIFY Integration","text":"<pre><code>-- Set up notification channel\nLISTEN steadytext_async_complete;\n\n-- Process completed requests\nCREATE OR REPLACE FUNCTION process_completed_requests()\nRETURNS void AS $$\nDECLARE\n    notification RECORD;\nBEGIN\n    -- Get completed notifications\n    FOR notification IN \n        SELECT request_id::uuid \n        FROM pg_notification \n        WHERE channel = 'steadytext_async_complete'\n    LOOP\n        -- Process completed request\n        UPDATE processed_texts\n        SET result = (\n            SELECT result \n            FROM steadytext_check_async(notification.request_id)\n            WHERE status = 'completed'\n        ),\n        processed_at = NOW()\n        WHERE request_id = notification.request_id;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"postgresql-extension/#background-worker-configuration","title":"Background Worker Configuration","text":"<p>The async functions use a background worker that can be configured:</p> <pre><code>-- Start the background worker\nSELECT steadytext_worker_start();\n\n-- Check worker status\nSELECT * FROM steadytext_worker_status();\n\n-- Stop the worker\nSELECT steadytext_worker_stop();\n\n-- Configure worker settings\nSELECT steadytext_config_set('worker_batch_size', '10');\nSELECT steadytext_config_set('worker_poll_interval_ms', '1000');\n</code></pre>"},{"location":"postgresql-extension/#performance-considerations","title":"Performance Considerations","text":"<ol> <li> <p>Queue Management: The queue table (<code>steadytext_queue</code>) should be regularly monitored and old completed requests cleaned up.</p> </li> <li> <p>Priority Levels: Use priority levels (1-10) wisely. Higher numbers get processed first.</p> </li> <li> <p>Batch Operations: Batch operations are more efficient than individual async calls for large datasets.</p> </li> <li> <p>Result Polling: Use LISTEN/NOTIFY instead of polling for better performance in real-time applications.</p> </li> </ol>"},{"location":"postgresql-extension/#troubleshooting","title":"Troubleshooting","text":""},{"location":"postgresql-extension/#common-issues","title":"Common Issues","text":""},{"location":"postgresql-extension/#1-no-module-named-steadytext-error","title":"1. \"No module named 'steadytext'\" Error","text":"<p>This indicates PostgreSQL cannot find the SteadyText library:</p> <pre><code>-- Check Python environment\nDO $$\nBEGIN\n    RAISE NOTICE 'Python version: %', (SELECT version());\nEND;\n$$ LANGUAGE plpython3u;\n\n-- Manually initialize (if needed)\nSELECT _steadytext_init_python();\n\n-- Verify installation\nDO $$\nimport sys\nimport os\nplpy.notice(f\"Python path: {sys.path}\")\nplpy.notice(f\"Current user: {os.getenv('USER', 'unknown')}\")\ntry:\n    import steadytext\n    plpy.notice(f\"SteadyText version: {steadytext.__version__}\")\nexcept ImportError as e:\n    plpy.error(f\"SteadyText not available: {e}\")\n$$ LANGUAGE plpython3u;\n</code></pre> <p>Solution: <pre><code># Install SteadyText for the PostgreSQL Python environment\nsudo -u postgres pip3 install steadytext&gt;=2.1.0\n\n# Or reinstall the extension\nmake clean &amp;&amp; make install\n</code></pre></p>"},{"location":"postgresql-extension/#2-model-loading-errors","title":"2. Model Loading Errors","text":"<p>If functions return NULL due to model loading issues:</p> <pre><code>-- Check current model configuration\nSELECT steadytext_config_get('use_fallback_model');\n\n-- Enable fallback model\nSELECT steadytext_config_set('use_fallback_model', 'true');\n\n-- Test generation (will return NULL if still failing)\nSELECT \n    CASE \n        WHEN steadytext_generate('Test model loading') IS NOT NULL \n        THEN 'Model working'\n        ELSE 'Model still failing - check daemon status'\n    END AS status;\n</code></pre> <p>Environment Solution: <pre><code># Set fallback model environment variable\nexport STEADYTEXT_USE_FALLBACK_MODEL=true\n\n# Restart PostgreSQL\nsudo systemctl restart postgresql\n</code></pre></p>"},{"location":"postgresql-extension/#3-daemon-connection-issues","title":"3. Daemon Connection Issues","text":"<pre><code>-- Check daemon status\nSELECT * FROM steadytext_daemon_status();\n\n-- Restart daemon with custom settings\nSELECT steadytext_daemon_stop();\nSELECT steadytext_config_set('daemon_host', 'localhost');\nSELECT steadytext_config_set('daemon_port', '5557');\nSELECT steadytext_daemon_start();\n\n-- Test daemon connectivity\nSELECT steadytext_generate('Test daemon connection');\n</code></pre>"},{"location":"postgresql-extension/#4-null-returns-and-error-handling","title":"4. NULL Returns and Error Handling","text":"<pre><code>-- Check if functions are returning NULL\nSELECT \n    'Generation test' AS test_type,\n    CASE \n        WHEN steadytext_generate('Test prompt') IS NOT NULL \n        THEN 'Working'\n        ELSE 'Returning NULL - check daemon'\n    END AS status\nUNION ALL\nSELECT \n    'Embedding test' AS test_type,\n    CASE \n        WHEN steadytext_embed('Test text') IS NOT NULL \n        THEN 'Working'\n        ELSE 'Returning NULL - check daemon'\n    END AS status;\n\n-- Application-level NULL handling pattern\nCREATE OR REPLACE FUNCTION robust_generate(\n    prompt TEXT,\n    retry_count INTEGER DEFAULT 3\n)\nRETURNS TEXT AS $$\nDECLARE\n    result TEXT;\n    i INTEGER;\nBEGIN\n    FOR i IN 1..retry_count LOOP\n        result := steadytext_generate(prompt);\n        IF result IS NOT NULL THEN\n            RETURN result;\n        END IF;\n\n        -- Wait before retry\n        PERFORM pg_sleep(1);\n    END LOOP;\n\n    -- All retries failed\n    RETURN NULL;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"postgresql-extension/#5-cache-performance-issues","title":"5. Cache Performance Issues","text":"<pre><code>-- Monitor cache statistics\nSELECT * FROM steadytext_cache_stats();\n\n-- Clear cache if needed\nSELECT steadytext_cache_clear();\n\n-- Adjust cache settings\nSELECT steadytext_config_set('cache_capacity', '1000');\nSELECT steadytext_config_set('cache_max_size_mb', '200');\n</code></pre>"},{"location":"postgresql-extension/#debugging-mode","title":"Debugging Mode","text":"<p>Enable verbose logging for troubleshooting:</p> <pre><code>-- Enable PostgreSQL notices\nSET client_min_messages TO NOTICE;\n\n-- Test with debug output and NULL checking\nSELECT \n    'Debug test' AS test_name,\n    steadytext_generate('Debug test', max_tokens := 10) AS result,\n    CASE \n        WHEN steadytext_generate('Debug test', max_tokens := 10) IS NULL \n        THEN 'Generation failed - check notices above'\n        ELSE 'Generation successful'\n    END AS status;\n\n-- Check daemon health\nSELECT * FROM steadytext_daemon_status();\n\n-- Check recent health history\nSELECT * FROM steadytext_daemon_health ORDER BY last_heartbeat DESC LIMIT 10;\n</code></pre>"},{"location":"postgresql-extension/#version-compatibility","title":"Version Compatibility","text":"PostgreSQL Python SteadyText Status 14+ 3.8+ 2.1.0+ \u2705 Fully Supported 13 3.8+ 2.1.0+ \u26a0\ufe0f Limited Testing 12 3.7+ 2.0.0+ \u274c Not Recommended"},{"location":"postgresql-extension/#migration-guide","title":"Migration Guide","text":""},{"location":"postgresql-extension/#upgrading-from-v100","title":"Upgrading from v1.0.0","text":"<ol> <li> <p>Update Dependencies: <pre><code>pip3 install --upgrade steadytext&gt;=2.1.0\n</code></pre></p> </li> <li> <p>Update Extension: <pre><code>ALTER EXTENSION pg_steadytext UPDATE TO '1.1.0';\n</code></pre></p> </li> <li> <p>Update Function Calls and Error Handling: <pre><code>-- Old (v1.0.0) - returned fallback text on errors\nSELECT steadytext_generate('prompt', 512, true);\n\n-- New (v1.1.0+) - with seed support and NULL returns on errors\nSELECT steadytext_generate('prompt', max_tokens := 512, seed := 42);\n\n-- Application code should now handle NULL returns\nSELECT \n    COALESCE(\n        steadytext_generate('prompt', max_tokens := 512, seed := 42),\n        'Error: Generation failed'\n    ) AS result;\n</code></pre></p> </li> </ol>"},{"location":"postgresql-extension/#contributing","title":"Contributing","text":"<p>The pg_steadytext extension is part of the main SteadyText project. Contributions are welcome!</p> <ul> <li>GitHub Repository: https://github.com/julep-ai/steadytext</li> <li>Issues: https://github.com/julep-ai/steadytext/issues</li> <li>Extension Directory: <code>pg_steadytext/</code></li> </ul>"},{"location":"postgresql-extension/#license","title":"License","text":"<p>This extension is released under the PostgreSQL License, consistent with the main SteadyText project.</p> <p>Need Help? Check the main SteadyText documentation or open an issue on GitHub.</p>"},{"location":"quick-start/","title":"Quick Start Guide","text":"<p>Get started with SteadyText in minutes. Learn how to use custom seeds for reproducible AI generation.</p>"},{"location":"quick-start/#installation","title":"Installation","text":"pipuvPoetry <pre><code>pip install steadytext\n</code></pre> <pre><code>uv add steadytext\n</code></pre> <pre><code>poetry add steadytext\n</code></pre>"},{"location":"quick-start/#first-steps","title":"First Steps","text":""},{"location":"quick-start/#1-basic-text-generation","title":"1. Basic Text Generation","text":"<pre><code>import steadytext\n\n# Generate deterministic text (always same result)\ntext = steadytext.generate(\"Write a Python function to calculate fibonacci\")\nprint(text)\n\n# Use custom seed for different but reproducible results\ntext1 = steadytext.generate(\"Write a Python function\", seed=123)\ntext2 = steadytext.generate(\"Write a Python function\", seed=123)  # Same as text1\ntext3 = steadytext.generate(\"Write a Python function\", seed=456)  # Different result\n\nprint(f\"Same seed results identical: {text1 == text2}\")  # True\nprint(f\"Different seeds produce different output: {text1 != text3}\")  # True\n</code></pre>"},{"location":"quick-start/#2-streaming-generation","title":"2. Streaming Generation","text":"<p>For real-time output:</p> <pre><code># Default streaming\nfor token in steadytext.generate_iter(\"Explain machine learning\"):\n    print(token, end=\"\", flush=True)\n\n# Streaming with custom seed for reproducible streams\nprint(\"\\nStream 1 (seed 789):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=789):\n    print(token, end=\"\", flush=True)\n\nprint(\"\\nStream 2 (same seed - identical result):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=789):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"quick-start/#3-create-embeddings","title":"3. Create Embeddings","text":"<pre><code># Single text (deterministic)\nvector = steadytext.embed(\"Hello world\")\nprint(f\"Embedding shape: {vector.shape}\")  # (1024,)\n\n# Multiple texts (returns a single, averaged embedding)\nvector = steadytext.embed([\"Hello\", \"world\", \"AI\"])\n\n# Custom seeds for different embedding variations\nvec1 = steadytext.embed(\"artificial intelligence\", seed=100)\nvec2 = steadytext.embed(\"artificial intelligence\", seed=100)  # Identical\nvec3 = steadytext.embed(\"artificial intelligence\", seed=200)  # Different\n\nimport numpy as np\nprint(f\"Same seed embeddings equal: {np.array_equal(vec1, vec2)}\")  # True\nprint(f\"Different seed similarity: {np.dot(vec1, vec3):.3f}\")  # Cosine similarity\n</code></pre>"},{"location":"quick-start/#command-line-usage","title":"Command Line Usage","text":"<p>SteadyText includes both <code>steadytext</code> and <code>st</code> commands:</p> <pre><code># Generate text (deterministic)\nst generate \"write a haiku about programming\"\n\n# Generate with custom seed for reproducible variations\nst generate \"write a haiku about programming\" --seed 123\nst generate \"write a haiku about programming\" --seed 456  # Different result\n\n# Stream generation with seed\necho \"explain quantum computing\" | st --seed 789\n\n# Create embeddings with custom seed\nst embed \"machine learning concepts\" --seed 100\n\n# JSON output with metadata\nst generate \"list 3 colors\" --json --seed 555\n\n# Control output length\nst generate \"explain AI\" --max-new-tokens 100 --seed 42\n\n# Vector operations with seeds\nst vector similarity \"cat\" \"dog\" --seed 777\n\n# Preload models (optional)\nst models --preload\n</code></pre>"},{"location":"quick-start/#model-management","title":"Model Management","text":"<p>Models are automatically downloaded on first use to:</p> <ul> <li>Linux/Mac: <code>~/.cache/steadytext/models/</code></li> <li>Windows: <code>%LOCALAPPDATA%\\steadytext\\steadytext\\models\\</code></li> </ul> <pre><code># Check where models are stored\ncache_dir = steadytext.get_model_cache_dir()\nprint(f\"Models stored at: {cache_dir}\")\n\n# Preload models manually (optional)\nsteadytext.preload_models(verbose=True)\n</code></pre>"},{"location":"quick-start/#configuration","title":"Configuration","text":"<p>Control caching and behavior via environment variables:</p> <pre><code># Generation cache settings\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=512\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=100\n\n# Embedding cache settings  \nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=1024\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=200\n\n# Model compatibility settings\nexport STEADYTEXT_USE_FALLBACK_MODEL=true  # Use compatible models\n\n# Default seed (optional)\nexport STEADYTEXT_DEFAULT_SEED=42\n</code></pre>"},{"location":"quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"quick-start/#reproducible-research","title":"Reproducible Research","text":"<pre><code># Document your seeds for reproducibility\nRESEARCH_SEED = 42\n\nresults = []\nfor prompt in research_prompts:\n    result = steadytext.generate(prompt, seed=RESEARCH_SEED)\n    results.append(result)\n    RESEARCH_SEED += 1  # Increment for each generation\n</code></pre>"},{"location":"quick-start/#ab-testing","title":"A/B Testing","text":"<pre><code># Generate content variations\nprompt = \"Write a product description\"\nvariant_a = steadytext.generate(prompt, seed=100)  # Version A\nvariant_b = steadytext.generate(prompt, seed=200)  # Version B\n\n# Test which performs better\nprint(f\"Variant A: {variant_a[:100]}...\")\nprint(f\"Variant B: {variant_b[:100]}...\")\n</code></pre>"},{"location":"quick-start/#content-variations","title":"Content Variations","text":"<pre><code># Generate multiple versions for testing\nbase_prompt = \"Explain machine learning\"\nvariations = []\n\nfor i, style_seed in enumerate([300, 400, 500], 1):\n    variation = steadytext.generate(base_prompt, seed=style_seed)\n    variations.append(f\"Version {i}: {variation}\")\n\nfor variation in variations:\n    print(variation[:80] + \"...\\n\")\n</code></pre>"},{"location":"quick-start/#postgresql-integration","title":"PostgreSQL Integration","text":"<p>SteadyText now includes a PostgreSQL extension:</p> <pre><code># Install the PostgreSQL extension\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\nmake &amp;&amp; sudo make install\n\n# Enable in PostgreSQL\npsql -c \"CREATE EXTENSION pg_steadytext CASCADE;\"\n</code></pre> <pre><code>-- Use in SQL queries\nSELECT steadytext_generate('Write a product description', max_tokens := 200, seed := 123);\n\n-- Generate embeddings\nSELECT steadytext_embed('machine learning', seed := 456);\n\n-- Semantic search with pgvector\nSELECT title, content &lt;-&gt; steadytext_embed('AI technology') AS distance\nFROM documents\nORDER BY distance\nLIMIT 5;\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete function documentation with seed parameters</li> <li>Custom Seeds Guide - Comprehensive seed usage examples</li> <li>PostgreSQL Integration - Complete PostgreSQL extension guide</li> <li>CLI Reference - Command-line interface with <code>--seed</code> flag details</li> <li>Examples - Real-world usage patterns</li> </ul>"},{"location":"quick-start/#need-help","title":"Need Help?","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"structured-generation/","title":"Structured Generation","text":"<p>SteadyText v2.4.1 introduces powerful structured generation capabilities, allowing you to force the model's output to conform to a specific format. This is useful for a wide range of applications, from data extraction to building reliable applications on top of language models.</p> <p>This feature is powered by llama.cpp's native grammar support, providing better compatibility and performance compared to external libraries.</p>"},{"location":"structured-generation/#how-it-works","title":"How it Works","text":"<p>Structured generation is enabled by passing one of the following parameters to the <code>steadytext.generate</code> function:</p> <ul> <li><code>schema</code>: For generating JSON that conforms to a JSON schema, a Pydantic model, or a basic Python type.</li> <li><code>regex</code>: For generating text that matches a regular expression.</li> <li><code>choices</code>: For generating text that is one of a list of choices.</li> </ul> <p>When one of these parameters is provided, SteadyText converts your constraint into a GBNF (Grammatical Backus-Naur Form) grammar that llama.cpp uses to guide the generation process. This ensures that the output is always valid according to the specified format.</p> <p>The conversion process: 1. JSON schemas, Pydantic models, and Python types are converted to GBNF grammars that enforce the exact structure 2. Regular expressions are converted to equivalent GBNF patterns (when possible) 3. Choice lists are converted to simple alternative rules in GBNF</p> <p>This native integration with llama.cpp provides deterministic, reliable structured output generation.</p>"},{"location":"structured-generation/#json-generation","title":"JSON Generation","text":"<p>You can generate JSON output in several ways.</p>"},{"location":"structured-generation/#with-a-json-schema","title":"With a JSON Schema","text":"<p>Pass a dictionary representing a JSON schema to the <code>schema</code> parameter.</p> <pre><code>import steadytext\nimport json\n\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"age\": {\"type\": \"integer\"},\n    },\n    \"required\": [\"name\", \"age\"],\n}\n\nresult = steadytext.generate(\"Create a user named Alice, age 42\", schema=schema)\n\n# The result will contain a JSON object wrapped in &lt;json-output&gt; tags\n# &lt;json-output&gt;{\"name\": \"Alice\", \"age\": 42}&lt;/json-output&gt;\n\njson_string = result.split('&lt;json-output&gt;')[1].split('&lt;/json-output&gt;')[0]\ndata = json.loads(json_string)\n\nassert data['name'] == \"Alice\"\nassert data['age'] == 42\n</code></pre>"},{"location":"structured-generation/#with-a-pydantic-model","title":"With a Pydantic Model","text":"<p>You can also use a Pydantic model to define the structure of the JSON output.</p> <pre><code>import steadytext\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresult = steadytext.generate(\"Create a user named Bob, age 30\", schema=User)\n</code></pre>"},{"location":"structured-generation/#with-generate_json","title":"With <code>generate_json</code>","text":"<p>The <code>generate_json</code> convenience function can also be used.</p> <pre><code>import steadytext\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresult = steadytext.generate_json(\"Create a user named Charlie, age 25\", schema=User)\n</code></pre>"},{"location":"structured-generation/#regex-constrained-generation","title":"Regex-Constrained Generation","text":"<p>Generate text that matches a regular expression using the <code>regex</code> parameter.</p> <pre><code>import steadytext\n\n# Generate a phone number\nphone_number = steadytext.generate(\n    \"The support number is: \",\n    regex=r\"\\d{3}-\\d{3}-\\d{4}\"\n)\nprint(phone_number)\n# Output: 123-456-7890\n\n# Generate a valid email address\nemail = steadytext.generate(\n    \"Contact email: \",\n    regex=r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n)\nprint(email)\n# Output: example@domain.com\n</code></pre> <p>You can also use the <code>generate_regex</code> convenience function.</p> <pre><code>import steadytext\n\n# Generate a date\ndate = steadytext.generate_regex(\n    \"Today's date is: \",\n    pattern=r\"\\d{4}-\\d{2}-\\d{2}\"\n)\nprint(date)\n# Output: 2025-07-03\n</code></pre>"},{"location":"structured-generation/#multiple-choice","title":"Multiple Choice","text":"<p>Force the model to choose from a list of options using the <code>choices</code> parameter.</p> <pre><code>import steadytext\n\nsentiment = steadytext.generate(\n    \"The movie was fantastic!\",\n    choices=[\"positive\", \"negative\", \"neutral\"]\n)\nprint(sentiment)\n# Output: positive\n</code></pre> <p>The <code>generate_choice</code> convenience function is also available.</p> <pre><code>import steadytext\n\nanswer = steadytext.generate_choice(\n    \"Is Python a statically typed language?\",\n    choices=[\"Yes\", \"No\"]\n)\nprint(answer)\n# Output: No\n</code></pre>"},{"location":"structured-generation/#type-constrained-generation","title":"Type-Constrained Generation","text":"<p>You can also constrain the output to a specific Python type using the <code>generate_format</code> function.</p> <pre><code>import steadytext\n\n# Generate an integer\nquantity = steadytext.generate_format(\"Number of items: \", int)\nprint(quantity)\n# Output: 5\n\n# Generate a boolean\nis_active = steadytext.generate_format(\"Is the user active? \", bool)\nprint(is_active)\n# Output: True\n</code></pre>"},{"location":"structured-generation/#postgresql-extension-support","title":"PostgreSQL Extension Support","text":"<p>All structured generation features are fully supported in the PostgreSQL extension (pg_steadytext) as of v2.4.1. You can use structured generation directly in your SQL queries.</p>"},{"location":"structured-generation/#sql-functions","title":"SQL Functions","text":"<p>The PostgreSQL extension provides the following structured generation functions:</p> <pre><code>-- JSON generation with schema\nsteadytext_generate_json(\n    prompt TEXT,\n    schema JSONB,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n\n-- Regex-constrained generation\nsteadytext_generate_regex(\n    prompt TEXT,\n    pattern TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n\n-- Multiple choice generation\nsteadytext_generate_choice(\n    prompt TEXT,\n    choices TEXT[],\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n</code></pre>"},{"location":"structured-generation/#postgresql-examples","title":"PostgreSQL Examples","text":"<pre><code>-- Generate structured user data\nSELECT steadytext_generate_json(\n    'Create a user profile for John Doe, age 35, software engineer',\n    '{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}, \"occupation\": {\"type\": \"string\"}}}'::jsonb\n);\n\n-- Generate formatted phone numbers\nSELECT steadytext_generate_regex(\n    'Customer service: ',\n    '\\(\\d{3}\\) \\d{3}-\\d{4}'\n);\n\n-- Sentiment classification\nSELECT steadytext_generate_choice(\n    'Sentiment of \"This product is amazing!\": ',\n    ARRAY['positive', 'negative', 'neutral']\n);\n</code></pre> <p>All functions support async variants as well: - <code>steadytext_generate_json_async()</code> - <code>steadytext_generate_regex_async()</code> - <code>steadytext_generate_choice_async()</code></p> <p>For more PostgreSQL-specific examples, see the PostgreSQL Integration Examples. ```</p>"},{"location":"version_history/","title":"Version History","text":"<p>This document outlines the major versions of SteadyText and the key features introduced in each.</p> <p>Latest Version: 2.4.1 - Structured Generation with Native Grammar Support</p> Version Key Features Default Generation Model Default Embedding Model Default Reranking Model Python Versions 2.4.x - Native Grammar Support: Replaced Outlines with llama.cpp's native GBNF grammars for structured generation.- PostgreSQL Structured Generation: Added <code>steadytext_generate_json()</code>, <code>steadytext_generate_regex()</code>, <code>steadytext_generate_choice()</code> SQL functions.- Better Compatibility: Fixes issues with Gemma-3n and other models. <code>ggml-org/gemma-3n-E2B-it-GGUF</code> (gemma-3n-E2B-it-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) <code>Qwen/Qwen3-Reranker-4B-GGUF</code> (Qwen3-Reranker-4B-Q8_0.gguf) <code>&gt;=3.10, &lt;3.14</code> 2.3.x - Document Reranking: Added reranking functionality with <code>Qwen3-Reranker-4B</code> model.- Structured Generation: Added support for JSON, Regex, and Choice-constrained generation via <code>outlines</code>.- New API parameters: <code>schema</code>, <code>regex</code>, <code>choices</code> added to <code>generate()</code>.- New convenience functions: <code>generate_json()</code>, <code>generate_regex()</code>, <code>generate_choice()</code>. <code>ggml-org/gemma-3n-E2B-it-GGUF</code> (gemma-3n-E2B-it-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) <code>Qwen/Qwen3-Reranker-4B-GGUF</code> (Qwen3-Reranker-4B-Q8_0.gguf) <code>&gt;=3.10, &lt;3.14</code> 2.1.x - Custom Seeds: Added seed parameter to all generation and embedding functions.- PostgreSQL Extension: Released pg_steadytext extension.- Enhanced Reproducibility: Full control over deterministic generation. <code>ggml-org/gemma-3n-E2B-it-GGUF</code> (gemma-3n-E2B-it-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) - <code>&gt;=3.10, &lt;3.14</code> 2.0.x - Daemon Mode: Persistent model serving with ZeroMQ.- Gemma-3n Models: Switched to <code>gemma-3n</code> for generation.- Thinking Mode Deprecated: Removed thinking mode. <code>ggml-org/gemma-3n-E2B-it-GGUF</code> (gemma-3n-E2B-it-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) - <code>&gt;=3.10, &lt;3.14</code> 1.x - Model Switching: Added support for switching models via environment variables.- Centralized Cache: Unified cache system with SQLite backend.- CLI Improvements: Streaming by default, quiet output, new pipe syntax. <code>Qwen/Qwen3-1.7B-GGUF</code> (Qwen3-1.7B-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) - <code>&gt;=3.10, &lt;3.14</code> 0.x - Initial Release: Deterministic text generation and embedding. <code>Qwen/Qwen1.5-0.5B-Chat-GGUF</code> (qwen1_5-0_5b-chat-q4_k_m.gguf) <code>Qwen/Qwen1.5-0.5B-Chat-GGUF</code> (qwen1_5-0_5b-chat-q8_0.gguf) - <code>&gt;=3.10</code>"},{"location":"version_history/#detailed-release-notes","title":"Detailed Release Notes","text":""},{"location":"version_history/#version-241-native-grammar-support","title":"Version 2.4.1 - Native Grammar Support","text":"<p>Release Date: July 2025</p>"},{"location":"version_history/#grammar-based-structured-generation","title":"\ud83d\udd27 Grammar-Based Structured Generation","text":"<p>Major Improvement: Replaced Outlines with llama.cpp's native GBNF (Grammatical Backus-Naur Form) grammar support.</p> <p>Benefits: - Better Compatibility: Fixes vocabulary processing errors with Gemma-3n, Qwen1.5, Phi-2, and Llama 3.x models - Improved Performance: Native integration with llama.cpp eliminates external library overhead - No API Changes: Existing structured generation code continues to work unchanged - Deterministic Output: Grammar-based generation maintains SteadyText's determinism guarantees</p> <p>Technical Details: - New <code>core/grammar.py</code> module converts JSON schemas, regex patterns, and choice lists to GBNF - <code>StructuredGenerator</code> now uses llama-cpp-python's <code>grammar</code> parameter directly - Removed <code>outlines</code> dependency, simplifying the dependency tree</p>"},{"location":"version_history/#postgresql-structured-generation","title":"\ud83d\udc18 PostgreSQL Structured Generation","text":"<p>New Feature: Added structured generation support to the PostgreSQL extension.</p> <p>New SQL Functions: - <code>steadytext_generate_json(prompt, schema)</code> - Generate JSON conforming to a schema - <code>steadytext_generate_regex(prompt, pattern)</code> - Generate text matching a regex - <code>steadytext_generate_choice(prompt, choices)</code> - Generate one of the provided choices</p> <p>Example Usage: <pre><code>-- Generate structured JSON\nSELECT steadytext_generate_json(\n    'Create a person named Alice',\n    '{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}}}'::jsonb\n);\n\n-- Generate text matching a pattern\nSELECT steadytext_generate_regex(\n    'My phone number is',\n    '\\d{3}-\\d{3}-\\d{4}'\n);\n\n-- Generate from choices\nSELECT steadytext_generate_choice(\n    'Is Python good?',\n    ARRAY['yes', 'no', 'maybe']\n);\n</code></pre></p>"},{"location":"version_history/#version-230-document-reranking-structured-generation","title":"Version 2.3.0 - Document Reranking &amp; Structured Generation","text":"<p>Release Date: July 2025</p>"},{"location":"version_history/#document-reranking","title":"\ud83d\udd0d Document Reranking","text":"<p>Major Feature: Added document reranking functionality powered by the Qwen3-Reranker-4B model.</p> <ul> <li>Python API: New <code>steadytext.rerank()</code> function with customizable task descriptions</li> <li><code>steadytext.rerank(query, documents, task=\"custom search task\")</code></li> <li>Support for both single document and list of documents</li> <li>Optional score returning with <code>return_scores</code> parameter</li> <li>CLI Command: <code>st rerank</code> for command-line reranking operations</li> <li><code>st rerank \"query\" doc1.txt doc2.txt --top-k 5</code></li> <li>Fallback Support: Simple word overlap scoring when model unavailable</li> <li>Dedicated Cache: Separate frecency cache for reranking results</li> </ul>"},{"location":"version_history/#structured-generation","title":"\u2728 Structured Generation","text":"<p>Major Feature: Introduced structured generation capabilities powered by the Outlines library.</p> <ul> <li>JSON Generation: Generate JSON that conforms to a JSON schema or a Pydantic model.</li> <li><code>steadytext.generate(prompt, schema=MyPydanticModel)</code></li> <li><code>steadytext.generate_json(prompt, schema={\"type\": \"object\", ...})</code></li> <li>Regex-Constrained Generation: Force output to match a regular expression.</li> <li><code>steadytext.generate(prompt, regex=r\"\\d{3}-\\d{3}-\\d{4}\")</code></li> <li>Multiple Choice: Force model to choose from a list of options.</li> <li><code>steadytext.generate(prompt, choices=[\"A\", \"B\", \"C\"])</code></li> </ul> <p>Use Cases: - Reliable data extraction - Building robust function-calling systems - Creating predictable application logic - Generating structured data for databases</p>"},{"location":"version_history/#version-210-custom-seeds-postgresql-extension","title":"Version 2.1.0+ - Custom Seeds &amp; PostgreSQL Extension","text":"<p>Release Date: June 2025</p>"},{"location":"version_history/#custom-seed-support","title":"\ud83c\udfaf Custom Seed Support","text":"<p>Major Enhancement: Added comprehensive custom seed support across all SteadyText APIs.</p> <ul> <li>Python API: All functions now accept optional <code>seed: int = DEFAULT_SEED</code> parameter</li> <li><code>steadytext.generate(prompt, seed=123)</code></li> <li><code>steadytext.generate_iter(prompt, seed=456)</code></li> <li> <p><code>steadytext.embed(text, seed=789)</code></p> </li> <li> <p>CLI Support: Added <code>--seed</code> flag to all commands</p> </li> <li><code>st generate \"prompt\" --seed 123</code></li> <li><code>st embed \"text\" --seed 456</code></li> <li> <p><code>st vector similarity \"text1\" \"text2\" --seed 789</code></p> </li> <li> <p>Daemon Integration: Seeds are properly passed through daemon protocol</p> </li> <li>Fallback Behavior: Deterministic fallbacks now respect custom seeds</li> <li>Cache Keys: Seeds are included in cache keys to prevent collisions</li> </ul> <p>Use Cases: - Reproducible Research: Document and reproduce exact results - A/B Testing: Generate controlled variations of content - Experimental Design: Systematic exploration of model behavior - Content Variations: Create different versions while maintaining quality</p>"},{"location":"version_history/#postgresql-extension-pg_steadytext","title":"\ud83d\udc18 PostgreSQL Extension (pg_steadytext)","text":"<p>New Release: Complete PostgreSQL extension for SteadyText integration.</p> <p>Core Features: - SQL Functions: Native PostgreSQL functions for text generation and embeddings   - <code>steadytext_generate(prompt, max_tokens, use_cache, seed)</code>   - <code>steadytext_embed(text, use_cache, seed)</code>   - <code>steadytext_daemon_start()</code>, <code>steadytext_daemon_status()</code>, <code>steadytext_daemon_stop()</code></p> <ul> <li>Vector Integration: Full compatibility with pgvector extension</li> <li>Built-in Caching: PostgreSQL-based frecency cache with eviction</li> <li>Daemon Support: Integrates with SteadyText's ZeroMQ daemon for performance</li> <li>Configuration Management: SQL-based configuration with <code>steadytext_config</code> table</li> </ul> <p>Installation: <pre><code># Install Python dependencies\npip3 install steadytext&gt;=2.1.0\n\n# Build and install extension\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\nmake &amp;&amp; sudo make install\n\n# Enable in PostgreSQL\npsql -c \"CREATE EXTENSION pg_steadytext CASCADE;\"\n</code></pre></p> <p>Docker Support: <pre><code># Standard build\ndocker build -t pg_steadytext .\n\n# With fallback model for compatibility\ndocker build --build-arg STEADYTEXT_USE_FALLBACK_MODEL=true -t pg_steadytext .\n</code></pre></p>"},{"location":"version_history/#technical-improvements","title":"\ud83d\udd27 Technical Improvements","text":"<ul> <li>Validation: Added <code>validate_seed()</code> function for input validation</li> <li>Environment Setup: Enhanced <code>set_deterministic_environment()</code> with custom seeds</li> <li>Error Handling: Improved error messages and fallback behavior</li> <li>Documentation: Comprehensive documentation and examples</li> </ul>"},{"location":"version_history/#documentation-updates","title":"\ud83d\udcd6 Documentation Updates","text":"<ul> <li>API Documentation: Updated all function signatures with seed parameters</li> <li>CLI Reference: Added <code>--seed</code> flag documentation for all commands</li> <li>Examples: New comprehensive examples for custom seed usage</li> <li>PostgreSQL Guide: Complete integration guide for pg_steadytext</li> <li>Migration Guide: Instructions for upgrading from previous versions</li> </ul>"},{"location":"version_history/#breaking-changes","title":"\ud83d\udd04 Breaking Changes","text":"<p>None - Version 2.1.0+ is fully backward compatible with 2.0.x. All existing code continues to work unchanged, with new seed parameters being optional.</p>"},{"location":"version_history/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed cache key generation to include seed for proper isolation</li> <li>Improved daemon protocol to handle seed parameters correctly</li> <li>Enhanced fallback behavior to be deterministic with custom seeds</li> <li>Resolved edge cases in streaming generation with custom seeds</li> </ul>"},{"location":"version_history/#requirements","title":"\ud83d\udccb Requirements","text":"<ul> <li>Python: 3.10+ (unchanged)</li> <li>PostgreSQL: 14+ (for pg_steadytext extension)</li> <li>Dependencies: All existing dependencies remain compatible</li> </ul>"},{"location":"why-steadytext/","title":"Why SteadyText","text":"<p>Understanding the technical rationale behind deterministic AI.</p>"},{"location":"why-steadytext/#the-problem-non-deterministic-ai","title":"The Problem: Non-Deterministic AI","text":"<p>Traditional AI models produce different outputs for the same input, causing:</p> <ul> <li>Flaky tests: Tests that pass locally but fail in CI</li> <li>Debugging difficulties: Cannot reproduce issues reliably</li> <li>Caching challenges: Results cannot be cached effectively</li> <li>API dependencies: External services introduce latency and failure points</li> </ul> <pre><code># Traditional approach - unpredictable\ndef test_ai_feature():\n    result = ai_generate(\"Summarize this\")\n    assert \"summary\" in result  # May randomly fail\n</code></pre>"},{"location":"why-steadytext/#the-solution-deterministic-generation","title":"The Solution: Deterministic Generation","text":"<p>SteadyText ensures identical inputs always produce identical outputs by:</p> <ol> <li>Fixed random seeds: All randomness is seeded with deterministic values</li> <li>Greedy decoding: Always selecting the highest probability token</li> <li>Quantized models: Consistent numerical precision across platforms</li> <li>Aggressive caching: Deterministic outputs enable perfect caching</li> </ol> <pre><code># SteadyText approach - predictable\ndef test_ai_feature():\n    result = steadytext.generate(\"Summarize this\")\n    assert result == steadytext.generate(\"Summarize this\")  # Always true\n</code></pre>"},{"location":"why-steadytext/#technical-architecture","title":"Technical Architecture","text":""},{"location":"why-steadytext/#local-first-design","title":"Local-First Design","text":"<ul> <li>No network calls: Models run entirely on your infrastructure</li> <li>No API keys: Self-contained system with no external dependencies</li> <li>Predictable latency: Consistent sub-millisecond response times</li> <li>Data locality: AI processing happens where your data lives</li> </ul>"},{"location":"why-steadytext/#postgresql-integration","title":"PostgreSQL Integration","text":"<p>The PostgreSQL extension enables AI operations directly in SQL:</p> <pre><code>-- AI as a native database function\nSELECT \n    id,\n    steadytext_generate('Summarize: ' || content) AS summary\nFROM documents\nWHERE created_at &gt; CURRENT_DATE - 7;\n</code></pre> <p>Benefits: - Transactional consistency: AI operations participate in ACID transactions - Backup integration: AI results included in standard database backups - Security model: Leverages existing PostgreSQL authentication and permissions - Performance: Eliminates round-trips between application and database</p>"},{"location":"why-steadytext/#caching-strategy","title":"Caching Strategy","text":"<p>Deterministic outputs enable sophisticated caching:</p> <pre><code># Cache key includes all parameters affecting output\ncache_key = hash(prompt + str(seed) + model_params)\n\n# Perfect cache hits for repeated queries\nif cache_key in cache:\n    return cache[cache_key]  # &lt;1ms response\n</code></pre> <p>Cache features: - Frecency-based eviction: Balances recency and frequency - Distributed backends: Support for SQLite, D1, and memory caches - Size limits: Configurable capacity and memory constraints</p>"},{"location":"why-steadytext/#use-cases","title":"Use Cases","text":"<p>SteadyText excels in scenarios requiring predictable AI:</p> <ol> <li>Automated testing: Reliable assertions on AI-generated content</li> <li>Data pipelines: Reproducible ETL operations with AI components</li> <li>Content generation: Consistent outputs for documentation and reports</li> <li>Semantic search: Stable embeddings for similarity matching</li> <li>Log analysis: Deterministic summarization of system events</li> </ol>"},{"location":"why-steadytext/#performance-characteristics","title":"Performance Characteristics","text":"<ul> <li>Inference latency: &lt;100ms for most generation tasks</li> <li>Embedding speed: ~1ms per text with caching</li> <li>Memory usage: 2-4GB for model storage</li> <li>Cache hit rate: &gt;90% in typical workloads</li> </ul>"},{"location":"why-steadytext/#design-principles","title":"Design Principles","text":"<ol> <li>Determinism by default: Same input \u2192 same output, always</li> <li>Zero configuration: Works out of the box without setup</li> <li>Local execution: No external dependencies or network calls</li> <li>SQL-native: AI as a first-class database primitive</li> <li>Production-ready: Designed for reliability over novelty</li> </ol>"},{"location":"why-steadytext/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Get running in minutes</li> <li>PostgreSQL Extension - Database integration</li> <li>API Reference - Complete function documentation</li> <li>Examples - Real-world usage patterns</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete documentation for SteadyText's Python API and command-line interface.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>SteadyText provides a simple, consistent API for deterministic AI operations:</p> <ul> <li><code>generate()</code> - Deterministic text generation with customizable seeds</li> <li><code>generate_iter()</code> - Streaming text generation with token-by-token output</li> <li><code>embed()</code> - Deterministic embeddings for semantic search and similarity</li> <li><code>preload_models()</code> - Pre-load models for better performance</li> <li>Daemon mode - Persistent model serving for 160x faster responses</li> <li>CLI tools - Command-line interface for all operations</li> </ul> <p>All functions are designed to never fail - they return deterministic fallbacks when models can't be loaded.</p>"},{"location":"api/#quick-reference","title":"Quick Reference","text":"<pre><code>import steadytext\n\n# Text generation with custom seed\ntext = steadytext.generate(\"your prompt\", seed=42)\ntext = steadytext.generate(\"your prompt\", seed=123)  # Different output\n\n# Return log probabilities\ntext, logprobs = steadytext.generate(\"prompt\", return_logprobs=True)\n\n# Streaming generation with custom seed\nfor token in steadytext.generate_iter(\"prompt\", seed=456):\n    print(token, end=\"\", flush=True)\n\n# Embeddings with custom seed\nvector = steadytext.embed(\"text to embed\", seed=789)\nvectors = steadytext.embed([\"multiple\", \"texts\"], seed=789)\n\n# Model management\nsteadytext.preload_models(verbose=True)\ncache_dir = steadytext.get_model_cache_dir()\n\n# Daemon usage (for better performance)\nfrom steadytext.daemon import use_daemon\nwith use_daemon():\n    text = steadytext.generate(\"fast generation\")\n    vec = steadytext.embed(\"fast embedding\")\n\n# Cache management\nfrom steadytext import get_cache_manager\ncache_manager = get_cache_manager()\nstats = cache_manager.get_cache_stats()\ncache_manager.clear_all_caches()\n</code></pre>"},{"location":"api/#detailed-documentation","title":"Detailed Documentation","text":""},{"location":"api/#core-apis","title":"Core APIs","text":"<ul> <li>Text Generation - Complete guide to <code>generate()</code> and <code>generate_iter()</code></li> <li>Basic usage and parameters</li> <li>Custom seed support for variations</li> <li>Streaming generation</li> <li>Advanced patterns and pipelines</li> <li>Error handling and edge cases</li> <li> <p>Integration examples</p> </li> <li> <p>Embeddings - Complete guide to <code>embed()</code> function</p> </li> <li>Creating embeddings with seeds</li> <li>Vector operations and similarity</li> <li>Batch processing</li> <li>Advanced use cases</li> <li>Performance optimization</li> </ul>"},{"location":"api/#command-line-interface","title":"Command Line Interface","text":"<ul> <li>CLI Reference - Complete command-line documentation</li> <li>Text generation commands</li> <li>Embedding operations</li> <li>Model management</li> <li>Daemon control</li> <li>Index operations</li> <li> <p>Real-world examples</p> </li> <li> <p>Vector Operations - Vector math and operations</p> </li> <li>Similarity calculations</li> <li>Distance metrics</li> <li>Vector arithmetic</li> <li>Search operations</li> </ul>"},{"location":"api/#api-signatures","title":"API Signatures","text":""},{"location":"api/#text-generation","title":"Text Generation","text":"<pre><code>def generate(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\",\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED,\n    schema: Optional[Union[Dict[str, Any], type, object]] = None,\n    regex: Optional[str] = None,\n    choices: Optional[List[str]] = None,\n    response_format: Optional[Dict[str, Any]] = None,\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre>"},{"location":"api/#streaming-generation","title":"Streaming Generation","text":"<pre><code>def generate_iter(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False,\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre>"},{"location":"api/#embeddings","title":"Embeddings","text":"<pre><code>def embed(\n    text_input: Union[str, List[str]], \n    seed: int = DEFAULT_SEED\n) -&gt; np.ndarray\n</code></pre>"},{"location":"api/#utilities","title":"Utilities","text":"<pre><code>def preload_models(verbose: bool = False) -&gt; None\ndef get_model_cache_dir() -&gt; Path\ndef get_cache_manager() -&gt; CacheManager\n</code></pre>"},{"location":"api/#constants","title":"Constants","text":""},{"location":"api/#core-constants","title":"Core Constants","text":"<pre><code>steadytext.DEFAULT_SEED = 42              # Default seed for all operations\nsteadytext.GENERATION_MAX_NEW_TOKENS = 512  # Default max tokens for generation\nsteadytext.EMBEDDING_DIMENSION = 1024      # Embedding vector dimensions\n</code></pre>"},{"location":"api/#model-constants","title":"Model Constants","text":"<pre><code># Current models (v2.0.0+)\nDEFAULT_GENERATION_MODEL = \"gemma-3n-2b\"\nDEFAULT_EMBEDDING_MODEL = \"qwen3-embedding\"\n\n# Model sizes\nMODEL_SIZES = {\n    \"small\": \"gemma-3n-2b\",  # 2.0GB\n    \"large\": \"gemma-3n-4b\"   # 4.2GB\n}\n</code></pre>"},{"location":"api/#environment-variables","title":"Environment Variables","text":""},{"location":"api/#cache-configuration","title":"Cache Configuration","text":"<pre><code># Cache backend selection (sqlite, d1, memory)\nSTEADYTEXT_CACHE_BACKEND=sqlite  # Default\n\n# Generation cache settings\nSTEADYTEXT_GENERATION_CACHE_CAPACITY=256      # Maximum cache entries\nSTEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50.0  # Maximum cache file size\n\n# Embedding cache settings\nSTEADYTEXT_EMBEDDING_CACHE_CAPACITY=512       # Maximum cache entries\nSTEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100.0  # Maximum cache file size\n\n# D1 backend configuration (when CACHE_BACKEND=d1)\nSTEADYTEXT_D1_API_URL=https://your-worker.workers.dev\nSTEADYTEXT_D1_API_KEY=your-api-key\nSTEADYTEXT_D1_BATCH_SIZE=50\n\n# Disable caching entirely (not recommended)\nSTEADYTEXT_DISABLE_CACHE=1\n</code></pre> <p>For detailed cache backend documentation, see Cache Backends Guide.</p>"},{"location":"api/#daemon-configuration","title":"Daemon Configuration","text":"<pre><code># Disable daemon usage globally\nSTEADYTEXT_DISABLE_DAEMON=1\n\n# Custom daemon settings\nSTEADYTEXT_DAEMON_HOST=127.0.0.1\nSTEADYTEXT_DAEMON_PORT=5557\n</code></pre>"},{"location":"api/#developmenttesting","title":"Development/Testing","text":"<pre><code># Allow model downloads (for testing)\nSTEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Use fallback models for compatibility\nSTEADYTEXT_USE_FALLBACK_MODEL=true\n\n# Set default seed globally\nSTEADYTEXT_DEFAULT_SEED=42\n\n# Python hash seed (for reproducibility)\nPYTHONHASHSEED=0\n</code></pre>"},{"location":"api/#model-paths","title":"Model Paths","text":"<pre><code># Custom model cache directory\nSTEADYTEXT_MODEL_DIR=/path/to/models\n\n# Skip model verification\nSTEADYTEXT_SKIP_MODEL_VERIFICATION=1\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>SteadyText uses a \"never fail\" design philosophy with v2.1.0+ updates:</p> <p>Deterministic Behavior</p> <ul> <li>Text generation: Returns <code>None</code> when models unavailable (v2.1.0+)</li> <li>Embeddings: Returns <code>None</code> when models unavailable (v2.1.0+)</li> <li>Streaming: Returns empty iterator when models unavailable</li> <li>No exceptions: Functions handle errors gracefully</li> <li>Seed support: All fallbacks respect custom seeds</li> </ul> <p>Breaking Changes in v2.1.0</p> <p>The deterministic fallback behavior has been disabled. Functions now return <code>None</code> instead of generating fallback text/embeddings when models are unavailable.</p>"},{"location":"api/#thread-safety","title":"Thread Safety","text":"<p>All functions are thread-safe and support concurrent usage:</p> <ul> <li>Singleton models: Models loaded once with thread-safe locks</li> <li>Thread-safe caches: All caches use proper locking mechanisms</li> <li>Concurrent calls: Multiple threads can call functions simultaneously</li> <li>Daemon mode: ZeroMQ handles concurrent requests automatically</li> </ul> <p>Example of concurrent usage:</p> <pre><code>import concurrent.futures\nimport steadytext\n\ndef process_prompt(prompt, seed):\n    return steadytext.generate(prompt, seed=seed)\n\n# Process multiple prompts concurrently\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    prompts = [\"prompt1\", \"prompt2\", \"prompt3\", \"prompt4\"]\n    seeds = [100, 200, 300, 400]\n\n    futures = [executor.submit(process_prompt, p, s) \n               for p, s in zip(prompts, seeds)]\n\n    results = [f.result() for f in futures]\n</code></pre>"},{"location":"api/#performance-notes","title":"Performance Notes","text":""},{"location":"api/#startup-performance","title":"Startup Performance","text":"<ul> <li>First call: Downloads models if needed (~2.6GB total)</li> <li>Model loading: 2-3 seconds on first use</li> <li>Daemon mode: Eliminates model loading overhead</li> <li>Preloading: Use <code>preload_models()</code> to load at startup</li> </ul>"},{"location":"api/#runtime-performance","title":"Runtime Performance","text":"<ul> <li>Generation speed: ~50-100 tokens/second</li> <li>Embedding speed: ~100-500 embeddings/second</li> <li>Cache hits: &lt;0.01 seconds for cached results</li> <li>Memory usage: ~2.6GB for all models loaded</li> </ul>"},{"location":"api/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use daemon mode for production deployments</li> <li>Preload models at application startup</li> <li>Warm up cache with common prompts</li> <li>Use consistent seeds for better cache efficiency</li> <li>Batch operations when possible</li> <li>Monitor cache stats to tune capacity</li> </ol>"},{"location":"api/#version-compatibility","title":"Version Compatibility","text":""},{"location":"api/#model-versions","title":"Model Versions","text":"<p>Each major version uses fixed models:</p> <ul> <li>v2.0.0+: Gemma-3n models (generation), Qwen3 (embeddings)</li> <li>v1.x: Older model versions (deprecated)</li> </ul>"},{"location":"api/#api-stability","title":"API Stability","text":"<ul> <li>Stable APIs: <code>generate()</code>, <code>embed()</code>, <code>generate_iter()</code></li> <li>Seed parameter: Added in all APIs for v2.0.0+</li> <li>Daemon mode: Stable since v1.3.0</li> <li>Cache system: Centralized since v1.3.3</li> </ul>"},{"location":"api/#best-practices","title":"Best Practices","text":"<p>Production Usage</p> <ol> <li>Always specify seeds for reproducible results</li> <li>Use daemon mode for better performance</li> <li>Configure caches based on usage patterns</li> <li>Handle None returns appropriately (v2.1.0+)</li> <li>Monitor performance with cache statistics</li> <li>Test with models unavailable to ensure robustness</li> <li>Use environment variables for configuration</li> <li>Implement proper error handling for production</li> <li>Batch similar operations for efficiency</li> <li>Document your seed choices for reproducibility</li> </ol>"},{"location":"api/cli/","title":"CLI Reference","text":"<p>Complete command-line interface documentation for SteadyText.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>The CLI is automatically installed with SteadyText:</p> <pre><code># Using UV (recommended)\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre> <p>Two commands are available: - <code>steadytext</code> - Full command name - <code>st</code> - Short alias</p>"},{"location":"api/cli/#global-options","title":"Global Options","text":"<pre><code>st --version     # Show version\nst --help        # Show help\n</code></pre>"},{"location":"api/cli/#generate","title":"generate","text":"<p>Generate deterministic text from a prompt.</p>"},{"location":"api/cli/#usage","title":"Usage","text":"<pre><code># New pipe syntax (recommended)\necho \"prompt\" | st [OPTIONS]\necho \"prompt\" | steadytext [OPTIONS]\n\n# Legacy syntax (still supported)\nst generate [OPTIONS] PROMPT\nsteadytext generate [OPTIONS] PROMPT\n</code></pre>"},{"location":"api/cli/#options","title":"Options","text":"Option Short Type Default Description <code>--wait</code> <code>-w</code> flag <code>false</code> Wait for complete output (disable streaming) <code>--json</code> <code>-j</code> flag <code>false</code> Output as JSON with metadata <code>--logprobs</code> <code>-l</code> flag <code>false</code> Include log probabilities <code>--eos-string</code> <code>-e</code> string <code>\"[EOS]\"</code> Custom end-of-sequence string <code>--max-new-tokens</code> int <code>512</code> Maximum number of tokens to generate <code>--seed</code> int <code>42</code> Random seed for deterministic generation <code>--size</code> choice Model size: small (2B, default), large (4B) <code>--model</code> string Model name from registry (e.g., \"qwen2.5-3b\") <code>--model-repo</code> string Custom model repository <code>--model-filename</code> string Custom model filename <code>--no-index</code> flag <code>false</code> Disable automatic index search <code>--index-file</code> path Use specific index file <code>--top-k</code> int <code>3</code> Number of context chunks to retrieve <code>--schema</code> string JSON schema for structured output (file path or inline JSON) <code>--regex</code> string Regular expression pattern for structured output <code>--choices</code> string Comma-separated list of allowed choices"},{"location":"api/cli/#examples","title":"Examples","text":"Basic GenerationWait for Complete OutputJSON OutputWith Log ProbabilitiesCustom Stop StringCustom Seed for ReproducibilityCustom LengthUsing Size ParameterModel SelectionStructured JSON OutputRegex Pattern MatchingChoice Constraints <pre><code># New pipe syntax\necho \"Write a Python function to calculate fibonacci\" | st\n\n# Legacy syntax\nst generate \"Write a Python function to calculate fibonacci\"\n</code></pre> <pre><code># Disable streaming\necho \"Explain machine learning\" | st --wait\n</code></pre> <pre><code>st generate \"Hello world\" --json\n# Output:\n# {\n#   \"text\": \"Hello! How can I help you today?...\",\n#   \"tokens\": 15,\n#   \"cached\": false\n# }\n</code></pre> <pre><code>st generate \"Explain AI\" --logprobs --json\n# Includes token probabilities in JSON output\n</code></pre> <pre><code>st generate \"List colors until STOP\" --eos-string \"STOP\"\n</code></pre> <pre><code># Generate with specific seed for reproducible results\necho \"Write a story\" | st --seed 123\n\n# Same seed always produces same output\nst generate \"Tell me a joke\" --seed 456\nst generate \"Tell me a joke\" --seed 456  # Identical result\n\n# Different seeds produce different outputs\nst generate \"Explain AI\" --seed 100\nst generate \"Explain AI\" --seed 200  # Different result\n</code></pre> <pre><code># Generate shorter responses\necho \"Quick summary of Python\" | st --max-new-tokens 50\n\n# Generate longer responses\necho \"Detailed explanation of ML\" | st --max-new-tokens 200\n</code></pre> <pre><code># Fast generation with small model\nst generate \"Quick response\" --size small\n\n# High quality with large model  \nst generate \"Complex analysis\" --size large\n\n# Combine size with custom seed\nst generate \"Technical explanation\" --size large --seed 789\n</code></pre> <pre><code># Use specific model size\nst generate \"Technical explanation\" --size large\n\n# Use custom model (advanced)\nst generate \"Write code\" --model-repo ggml-org/gemma-3n-E4B-it-GGUF \\\n    --model-filename gemma-3n-E4B-it-Q8_0.gguf\n\n# Custom model with seed and length control\nst generate \"Complex task\" --model-repo ggml-org/gemma-3n-E4B-it-GGUF \\\n    --model-filename gemma-3n-E4B-it-Q8_0.gguf \\\n    --seed 999 --max-new-tokens 100\n</code></pre> <pre><code># Generate JSON with inline schema\necho \"Create a person\" | st --schema '{\"type\": \"object\", \"properties\": {\"name\": {\"type\": \"string\"}, \"age\": {\"type\": \"integer\"}}}' --wait\n\n# Generate JSON from schema file\necho \"Generate user data\" | st --schema user_schema.json --wait\n\n# Complex schema example\necho \"Create a product listing\" | st --schema '{\"type\": \"object\", \"properties\": {\"title\": {\"type\": \"string\"}, \"price\": {\"type\": \"number\"}, \"inStock\": {\"type\": \"boolean\"}}}' --wait\n</code></pre> <pre><code># Phone number pattern\necho \"My phone number is\" | st --regex '\\d{3}-\\d{3}-\\d{4}' --wait\n\n# Date pattern\necho \"Today's date is\" | st --regex '\\d{4}-\\d{2}-\\d{2}' --wait\n\n# Custom pattern\necho \"The product code is\" | st --regex '[A-Z]{3}-\\d{4}' --wait\n</code></pre> <pre><code># Simple yes/no choice\necho \"Is Python a good language?\" | st --choices \"yes,no\" --wait\n\n# Multiple choice\necho \"What's the weather like?\" | st --choices \"sunny,cloudy,rainy,snowy\" --wait\n\n# Decision making\necho \"Should we proceed with deployment?\" | st --choices \"proceed,wait,cancel\" --wait\n</code></pre>"},{"location":"api/cli/#structured-generation-notes","title":"Structured Generation Notes","text":"<p>Structured Generation Requirements</p> <ul> <li>Streaming not supported: Always use <code>--wait</code> flag with structured options</li> <li>Mutually exclusive: Only one of <code>--schema</code>, <code>--regex</code>, or <code>--choices</code> can be used at a time</li> <li>Schema format: Can be inline JSON or path to a <code>.json</code> file</li> <li>Choices format: Comma-separated values without spaces around commas</li> </ul>"},{"location":"api/cli/#stdin-support","title":"Stdin Support","text":"<p>Generate from stdin when no prompt provided:</p> <pre><code>echo \"Write a haiku\" | st generate\ncat prompts.txt | st generate --stream\n</code></pre>"},{"location":"api/cli/#embed","title":"embed","text":"<p>Create deterministic embeddings for text.</p>"},{"location":"api/cli/#usage_1","title":"Usage","text":"<pre><code>st embed [OPTIONS] TEXT\nsteadytext embed [OPTIONS] TEXT\n</code></pre>"},{"location":"api/cli/#options_1","title":"Options","text":"Option Short Type Default Description <code>--format</code> <code>-f</code> choice <code>json</code> Output format: <code>json</code>, <code>numpy</code>, <code>hex</code> <code>--output</code> <code>-o</code> path <code>-</code> Output file (default: stdout) <code>--seed</code> int <code>42</code> Random seed for deterministic embedding generation"},{"location":"api/cli/#examples_1","title":"Examples","text":"Basic EmbeddingCustom SeedNumpy FormatHex FormatSave to File <pre><code>st embed \"machine learning\"\n# Outputs JSON array with 1024 float values\n</code></pre> <pre><code># Generate reproducible embeddings\nst embed \"artificial intelligence\" --seed 123\nst embed \"artificial intelligence\" --seed 123  # Same result\nst embed \"artificial intelligence\" --seed 456  # Different result\n\n# Compare embeddings with different seeds\nst embed \"test text\" --seed 100 --format json &gt; embed1.json\nst embed \"test text\" --seed 200 --format json &gt; embed2.json\n</code></pre> <pre><code>st embed \"text to embed\" --format numpy\n# Outputs binary numpy array\n</code></pre> <pre><code>st embed \"hello world\" --format hex\n# Outputs hex-encoded float32 array\n</code></pre> <pre><code>st embed \"important text\" --output embedding.json\nst embed \"data\" --format numpy --output embedding.npy\n\n# Save with custom seed\nst embed \"research data\" --seed 42 --output research_embedding.json\nst embed \"experiment\" --seed 123 --format numpy --output exp_embed.npy\n</code></pre>"},{"location":"api/cli/#stdin-support_1","title":"Stdin Support","text":"<p>Embed text from stdin:</p> <pre><code>echo \"text to embed\" | st embed\ncat document.txt | st embed --format numpy --output doc_embedding.npy\n\n# Stdin with custom seed\necho \"text to embed\" | st embed --seed 789\ncat document.txt | st embed --seed 42 --format numpy --output doc_embed_s42.npy\n</code></pre>"},{"location":"api/cli/#models","title":"models","text":"<p>Manage SteadyText models.</p>"},{"location":"api/cli/#usage_2","title":"Usage","text":"<pre><code>st models [OPTIONS]\nsteadytext models [OPTIONS]\n</code></pre>"},{"location":"api/cli/#options_2","title":"Options","text":"Option Short Description <code>--list</code> <code>-l</code> List available models <code>--preload</code> <code>-p</code> Preload all models <code>--cache-dir</code> Show model cache directory <code>--json</code> flag <code>false</code> <code>--seed</code> int"},{"location":"api/cli/#commands","title":"Commands","text":"Command Description <code>status</code> Check model download status <code>list</code> List available models <code>download</code> Pre-download models <code>delete</code> Delete cached models <code>preload</code> Preload models into memory <code>path</code> Show model cache directory"},{"location":"api/cli/#examples_2","title":"Examples","text":"List ModelsDownload ModelsDelete ModelsPreload ModelsCache Information <pre><code>st models list\n# Output:\n# Size Shortcuts:\n#   small \u2192 gemma-3n-2b\n#   large \u2192 gemma-3n-4b\n#\n# Available Models:\n#   gemma-3n-2b\n#     Repository: ggml-org/gemma-3n-E2B-it-GGUF\n#     Filename: gemma-3n-E2B-it-Q8_0.gguf\n#   gemma-3n-4b\n#     Repository: ggml-org/gemma-3n-E4B-it-GGUF\n#     Filename: gemma-3n-E4B-it-Q8_0.gguf\n</code></pre> <pre><code># Download default models\nst models download\n\n# Download by size\nst models download --size small\n\n# Download by name\nst models download --model gemma-3n-4b\n\n# Download all models\nst models download --all\n</code></pre> <pre><code># Delete by size\nst models delete --size small\n\n# Delete by name\nst models delete --model gemma-3n-4b\n\n# Delete all models with confirmation\nst models delete --all\n\n# Force delete all models without confirmation\nst models delete --all --force\n</code></pre> <pre><code>st models preload\n# Downloads and loads all models\n\n# Preload with specific seed for deterministic initialization\nst models preload --seed 42\n</code></pre> <pre><code>st models path\n# /home/user/.cache/steadytext/models/\n\nst models status\n# {\n#   \"model_directory\": \"/home/user/.cache/steadytext/models\",\n#   \"models\": { ... }\n# }\n</code></pre>"},{"location":"api/cli/#vector","title":"vector","text":"<p>Perform vector operations on embeddings.</p>"},{"location":"api/cli/#usage_3","title":"Usage","text":"<pre><code>st vector COMMAND [OPTIONS]\nsteadytext vector COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_1","title":"Commands","text":"Command Description <code>similarity</code> Compute similarity between text embeddings <code>distance</code> Compute distance between text embeddings <code>search</code> Find most similar texts from candidates <code>average</code> Compute average of multiple embeddings <code>arithmetic</code> Perform vector arithmetic operations"},{"location":"api/cli/#global-vector-options","title":"Global Vector Options","text":"Option Type Default Description <code>--seed</code> int <code>42</code> Random seed for deterministic embeddings <code>--json</code> flag <code>false</code> Output as JSON with metadata"},{"location":"api/cli/#examples_3","title":"Examples","text":"SimilarityDistanceSearchAverageArithmetic <pre><code># Cosine similarity\nst vector similarity \"cat\" \"dog\"\n# 0.823456\n\n# With JSON output\nst vector similarity \"king\" \"queen\" --json\n\n# Reproducible similarity with custom seed\nst vector similarity \"king\" \"queen\" --seed 123\nst vector similarity \"king\" \"queen\" --seed 123  # Same result\nst vector similarity \"king\" \"queen\" --seed 456  # Different result\n</code></pre> <pre><code># Euclidean distance\nst vector distance \"hot\" \"cold\"\n\n# Manhattan distance\nst vector distance \"yes\" \"no\" --metric manhattan\n</code></pre> <pre><code># Find similar from stdin\necho -e \"apple\\norange\\ncar\" | st vector search \"fruit\" --stdin\n\n# From file, top 3\nst vector search \"python\" --candidates langs.txt --top 3\n\n# Reproducible search with custom seed\necho -e \"apple\\norange\\ncar\" | st vector search \"fruit\" --stdin --seed 789\nst vector search \"programming\" --candidates langs.txt --top 3 --seed 42\n</code></pre> <pre><code># Average embeddings\nst vector average \"cat\" \"dog\" \"hamster\"\n\n# With full embedding output\nst vector average \"red\" \"green\" \"blue\" --json\n\n# Reproducible averaging with custom seed\nst vector average \"cat\" \"dog\" \"hamster\" --seed 555\nst vector average \"colors\" \"shapes\" \"sizes\" --seed 666 --json\n</code></pre> <pre><code># Classic analogy: king + woman - man \u2248 queen\nst vector arithmetic \"king\" \"woman\" --subtract \"man\"\n\n# Location arithmetic\nst vector arithmetic \"paris\" \"italy\" --subtract \"france\"\n\n# Reproducible arithmetic with custom seed\nst vector arithmetic \"king\" \"woman\" --subtract \"man\" --seed 777\nst vector arithmetic \"tokyo\" \"italy\" --subtract \"japan\" --seed 888 --json\n</code></pre> <p>See Vector Operations for detailed usage.</p>"},{"location":"api/cli/#rerank","title":"rerank","text":"<p>Rerank documents based on relevance to a query (v2.3.0+).</p>"},{"location":"api/cli/#usage_4","title":"Usage","text":"<pre><code>st rerank [OPTIONS] QUERY [DOCUMENTS...]\nsteadytext rerank [OPTIONS] QUERY [DOCUMENTS...]\n</code></pre>"},{"location":"api/cli/#options_3","title":"Options","text":"Option Short Type Default Description <code>--file</code> <code>-f</code> path Read documents from file (one per line) <code>--stdin</code> flag <code>false</code> Read documents from stdin <code>--top-k</code> <code>-k</code> int Return only top K results <code>--json</code> <code>-j</code> flag <code>false</code> Output as JSON with scores <code>--task</code> <code>-t</code> string <code>\"text retrieval for user question\"</code> Task description for better results <code>--seed</code> int <code>42</code> Random seed for deterministic reranking"},{"location":"api/cli/#examples_4","title":"Examples","text":"Basic RerankingFrom FileFrom StdinJSON OutputCustom Task <pre><code># Rerank files\nst rerank \"Python programming\" doc1.txt doc2.txt doc3.txt\n\n# With custom seed\nst rerank \"Python programming\" doc1.txt doc2.txt doc3.txt --seed 123\n</code></pre> <pre><code># Documents in file (one per line)\nst rerank \"machine learning\" --file documents.txt\n\n# Top 5 results with custom seed\nst rerank \"deep learning\" --file papers.txt --top-k 5 --seed 456\n</code></pre> <pre><code># Pipe documents\ncat documents.txt | st rerank \"search query\" --stdin\n\n# From command output\nfind . -name \"*.md\" -exec cat {} \\; | st rerank \"installation guide\" --stdin --top-k 3\n</code></pre> <pre><code># Get scores with documents\nst rerank \"Python\" doc1.txt doc2.txt --json\n# Output:\n# [\n#   {\"document\": \"Python is a programming language...\", \"score\": 0.95},\n#   {\"document\": \"Cats are cute animals...\", \"score\": 0.12}\n# ]\n</code></pre> <pre><code># Customer support prioritization\nst rerank \"billing issue\" --file tickets.txt --task \"support ticket prioritization\"\n\n# Legal document search with custom seed\nst rerank \"contract breach\" --file legal_docs.txt \\\n    --task \"legal document retrieval for case research\" \\\n    --seed 789\n</code></pre>"},{"location":"api/cli/#notes","title":"Notes","text":"<p>Reranking Model</p> <p>Uses the Qwen3-Reranker-4B model for binary relevance scoring based on yes/no token logits.</p> <p>Task Descriptions</p> <p>Custom task descriptions help the model understand your specific reranking context: - <code>\"support ticket prioritization\"</code> for customer service - <code>\"code snippet relevance\"</code> for programming searches - <code>\"academic paper retrieval\"</code> for research - <code>\"product search ranking\"</code> for e-commerce</p>"},{"location":"api/cli/#cache","title":"cache","text":"<p>Manage result caches.</p>"},{"location":"api/cli/#usage_5","title":"Usage","text":"<pre><code>st cache [OPTIONS]\nsteadytext cache [OPTIONS]\n</code></pre>"},{"location":"api/cli/#options_4","title":"Options","text":"Option Short Description <code>--clear</code> <code>-c</code> Clear all caches <code>--status</code> <code>-s</code> Show cache status <code>--generation-only</code> Target only generation cache <code>--embedding-only</code> Target only embedding cache"},{"location":"api/cli/#examples_5","title":"Examples","text":"Cache StatusClear Caches <pre><code>st cache --status\n# Generation Cache: 45 entries, 12.3MB\n# Embedding Cache: 128 entries, 34.7MB\n</code></pre> <pre><code>st cache --clear\n# Cleared all caches\n\nst cache --clear --generation-only\n# Cleared generation cache only\n</code></pre>"},{"location":"api/cli/#daemon","title":"daemon","text":"<p>Manage the SteadyText daemon for persistent model serving.</p>"},{"location":"api/cli/#usage_6","title":"Usage","text":"<pre><code>st daemon COMMAND [OPTIONS]\nsteadytext daemon COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_2","title":"Commands","text":"Command Description <code>start</code> Start the daemon server <code>stop</code> Stop the daemon server <code>status</code> Check daemon status <code>restart</code> Restart the daemon server"},{"location":"api/cli/#global-daemon-options","title":"Global Daemon Options","text":"Option Type Default Description <code>--seed</code> int <code>42</code> Default seed for daemon operations"},{"location":"api/cli/#options_5","title":"Options","text":""},{"location":"api/cli/#start","title":"start","text":"Option Type Default Description <code>--host</code> string <code>127.0.0.1</code> Bind address <code>--port</code> int <code>5557</code> Port number <code>--foreground</code> flag <code>false</code> Run in foreground"},{"location":"api/cli/#stop","title":"stop","text":"Option Type Default Description <code>--force</code> flag <code>false</code> Force kill if graceful shutdown fails"},{"location":"api/cli/#status","title":"status","text":"Option Type Default Description <code>--json</code> flag <code>false</code> Output as JSON"},{"location":"api/cli/#examples_6","title":"Examples","text":"Start DaemonCheck StatusStop/Restart <pre><code># Start in background (default)\nst daemon start\n\n# Start in foreground for debugging\nst daemon start --foreground\n\n# Custom host/port\nst daemon start --host 0.0.0.0 --port 5557\n\n# Start with custom default seed\nst daemon start --seed 123\n\n# Combined options\nst daemon start --host 0.0.0.0 --port 5557 --seed 456 --foreground\n</code></pre> <pre><code>st daemon status\n# Output: Daemon is running (PID: 12345)\n\n# JSON output\nst daemon status --json\n# {\"running\": true, \"pid\": 12345, \"host\": \"127.0.0.1\", \"port\": 5557}\n</code></pre> <pre><code># Graceful stop\nst daemon stop\n\n# Force stop\nst daemon stop --force\n\n# Restart\nst daemon restart\n</code></pre>"},{"location":"api/cli/#benefits","title":"Benefits","text":"<ul> <li>160x faster first request: No model loading overhead</li> <li>Persistent cache: Shared across all operations</li> <li>Automatic fallback: Operations work without daemon</li> <li>Zero configuration: Used by default when available</li> </ul>"},{"location":"api/cli/#index","title":"index","text":"<p>Manage FAISS vector indexes for retrieval-augmented generation.</p>"},{"location":"api/cli/#usage_7","title":"Usage","text":"<pre><code>st index COMMAND [OPTIONS]\nsteadytext index COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_3","title":"Commands","text":"Command Description <code>create</code> Create index from text files <code>search</code> Search index for similar chunks <code>info</code> Show index information"},{"location":"api/cli/#global-index-options","title":"Global Index Options","text":"Option Type Default Description <code>--seed</code> int <code>42</code> Random seed for embedding generation"},{"location":"api/cli/#options_6","title":"Options","text":""},{"location":"api/cli/#create","title":"create","text":"Option Type Default Description <code>--output</code> path required Output index file <code>--chunk-size</code> int <code>512</code> Chunk size in tokens <code>--glob</code> string File glob pattern"},{"location":"api/cli/#search","title":"search","text":"Option Type Default Description <code>--top-k</code> int <code>5</code> Number of results <code>--threshold</code> float Similarity threshold"},{"location":"api/cli/#examples_7","title":"Examples","text":"Create IndexSearch IndexIndex Info <pre><code># From specific files\nst index create doc1.txt doc2.txt --output docs.faiss\n\n# From glob pattern\nst index create --glob \"**/*.md\" --output project.faiss\n\n# Custom chunk size\nst index create *.txt --output custom.faiss --chunk-size 256\n\n# Reproducible index creation with custom seed\nst index create doc1.txt doc2.txt --output docs_s123.faiss --seed 123\nst index create --glob \"**/*.md\" --output project_s456.faiss --seed 456\n</code></pre> <pre><code># Basic search\nst index search docs.faiss \"query text\"\n\n# Top 10 results\nst index search docs.faiss \"error message\" --top-k 10\n\n# With threshold\nst index search docs.faiss \"specific term\" --threshold 0.8\n\n# Reproducible search with custom seed\nst index search docs.faiss \"query text\" --seed 789\nst index search docs.faiss \"error message\" --top-k 10 --seed 123\n</code></pre> <pre><code>st index info docs.faiss\n# Output:\n# Index: docs.faiss\n# Chunks: 1,234\n# Dimension: 1024\n# Size: 5.2MB\n</code></pre>"},{"location":"api/cli/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"<p>Set these before running CLI commands:</p> <pre><code># Cache configuration\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=512\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=100\n\n# Allow model downloads (for development)\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Set default seed for all operations\nexport STEADYTEXT_DEFAULT_SEED=42\n\n# Then run commands\nst generate \"test prompt\"\nst generate \"test prompt\" --seed 123  # Override default seed\n</code></pre>"},{"location":"api/cli/#pipeline-usage","title":"Pipeline Usage","text":"<p>Chain commands with other tools:</p> <pre><code># Batch processing\ncat prompts.txt | while read prompt; do\n  echo \"Prompt: $prompt\"\n  st generate \"$prompt\" --json | jq '.text'\n  echo \"---\"\ndone\n\n# Generate and embed\ntext=$(st generate \"explain AI\")\necho \"$text\" | st embed --format hex &gt; ai_explanation.hex\n</code></pre>"},{"location":"api/cli/#scripting-examples","title":"Scripting Examples","text":"Bash ScriptPython Integration <pre><code>#!/bin/bash\n# generate_docs.sh\n\nprompts=(\n  \"Explain machine learning\"\n  \"What is deep learning?\"\n  \"Define neural networks\"\n)\n\nfor prompt in \"${prompts[@]}\"; do\n  echo \"=== $prompt ===\"\n  st generate \"$prompt\" --stream\n  echo -e \"\\n---\\n\"\ndone\n</code></pre> <pre><code>import subprocess\nimport json\n\ndef cli_generate(prompt):\n    \"\"\"Use CLI from Python.\"\"\"\n    result = subprocess.run([\n        'st', 'generate', prompt, '--json'\n    ], capture_output=True, text=True)\n\n    return json.loads(result.stdout)\n\n# Usage\nresult = cli_generate(\"Hello world\")\nprint(result['text'])\n</code></pre>"},{"location":"api/cli/#performance-tips","title":"Performance Tips","text":"<p>CLI Optimization</p> <ul> <li>Preload models: Run <code>st models --preload</code> once at startup</li> <li>Use JSON output: Easier to parse in scripts with <code>--json</code></li> <li>Batch operations: Process multiple items in single session</li> <li>Cache warmup: Generate common prompts to populate cache</li> </ul>"},{"location":"api/cli/#real-world-examples","title":"Real-World Examples","text":""},{"location":"api/cli/#content-generation-pipeline","title":"Content Generation Pipeline","text":"<pre><code>#!/bin/bash\n# blog_generator.sh - Generate blog posts with consistent style\n\nSEED=12345  # Consistent seed for reproducible content\n\n# Function to generate blog post\ngenerate_post() {\n    local topic=\"$1\"\n    local style=\"$2\"\n\n    echo \"Generating post about: $topic\"\n\n    # Generate title\n    title=$(st generate \"Create an engaging blog title about $topic\" --seed $SEED --wait)\n\n    # Generate introduction\n    intro=$(st generate \"Write a compelling introduction for a blog post about $topic\" --seed $(($SEED + 1)) --wait)\n\n    # Generate main content\n    content=$(st generate \"Write the main content for a blog post about $topic in a $style style\" --seed $(($SEED + 2)) --max-new-tokens 800 --wait)\n\n    # Generate conclusion\n    conclusion=$(st generate \"Write a strong conclusion for a blog post about $topic\" --seed $(($SEED + 3)) --wait)\n\n    # Combine into final post\n    cat &lt;&lt;EOF\n# $title\n\n## Introduction\n$intro\n\n## Main Content\n$content\n\n## Conclusion\n$conclusion\n\n---\nGenerated with SteadyText (seed: $SEED)\nEOF\n}\n\n# Generate multiple posts\ntopics=(\"Machine Learning\" \"Web Development\" \"Data Science\")\nstyles=(\"technical\" \"beginner-friendly\" \"professional\")\n\nfor i in \"${!topics[@]}\"; do\n    generate_post \"${topics[$i]}\" \"${styles[$i]}\" &gt; \"blog_${i}.md\"\n    echo \"Created blog_${i}.md\"\ndone\n</code></pre>"},{"location":"api/cli/#semantic-search-cli-tool","title":"Semantic Search CLI Tool","text":"<pre><code>#!/bin/bash\n# semantic_search.sh - Search documents using embeddings\n\nINDEX_FILE=\"documents.faiss\"\nSEED=42\n\n# Function to build index\nbuild_index() {\n    echo \"Building search index...\"\n    st index create --glob \"**/*.md\" --output \"$INDEX_FILE\" --chunk-size 256 --seed $SEED\n    echo \"Index created: $INDEX_FILE\"\n}\n\n# Function to search\nsearch_docs() {\n    local query=\"$1\"\n    local num_results=\"${2:-5}\"\n\n    echo \"Searching for: $query\"\n    echo \"========================\"\n\n    # Search and format results\n    st index search \"$INDEX_FILE\" \"$query\" --top-k $num_results --seed $SEED | \\\n    while IFS= read -r line; do\n        if [[ $line =~ ^([0-9]+)\\.\\s+(.+):\\s+(.+)$ ]]; then\n            rank=\"${BASH_REMATCH[1]}\"\n            file=\"${BASH_REMATCH[2]}\"\n            snippet=\"${BASH_REMATCH[3]}\"\n\n            echo -e \"\\n[$rank] $file\"\n            echo \"   $snippet\"\n        fi\n    done\n}\n\n# Main menu\nwhile true; do\n    echo -e \"\\nSemantic Search Tool\"\n    echo \"1. Build/Rebuild index\"\n    echo \"2. Search documents\"\n    echo \"3. Exit\"\n    read -p \"Choose option: \" choice\n\n    case $choice in\n        1) build_index ;;\n        2) \n            read -p \"Enter search query: \" query\n            read -p \"Number of results (default 5): \" num\n            search_docs \"$query\" \"${num:-5}\"\n            ;;\n        3) exit 0 ;;\n        *) echo \"Invalid option\" ;;\n    esac\ndone\n</code></pre>"},{"location":"api/cli/#ai-powered-code-documentation-generator","title":"AI-Powered Code Documentation Generator","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\ndocgen.py - Generate documentation from code using SteadyText CLI\n\"\"\"\n\nimport subprocess\nimport json\nimport re\nfrom pathlib import Path\nimport argparse\n\ndef run_steadytext(prompt, seed=42, max_tokens=512):\n    \"\"\"Run SteadyText CLI and return result.\"\"\"\n    cmd = [\n        'st', 'generate', prompt,\n        '--json',\n        '--wait',\n        '--seed', str(seed),\n        '--max-new-tokens', str(max_tokens)\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode == 0:\n        data = json.loads(result.stdout)\n        return data['text']\n    else:\n        raise Exception(f\"SteadyText error: {result.stderr}\")\n\ndef extract_functions(code):\n    \"\"\"Extract function definitions from Python code.\"\"\"\n    pattern = r'def\\s+(\\w+)\\s*\\([^)]*\\):'\n    return re.findall(pattern, code)\n\ndef generate_function_docs(file_path, seed=42):\n    \"\"\"Generate documentation for a Python file.\"\"\"\n    with open(file_path, 'r') as f:\n        code = f.read()\n\n    functions = extract_functions(code)\n    docs = []\n\n    # Generate module overview\n    module_prompt = f\"Write a brief overview of a Python module containing these functions: {', '.join(functions)}\"\n    overview = run_steadytext(module_prompt, seed=seed)\n    docs.append(f\"# {file_path.name}\\n\\n{overview}\\n\")\n\n    # Generate documentation for each function\n    for i, func in enumerate(functions):\n        # Extract function code\n        func_pattern = rf'(def\\s+{func}\\s*\\([^)]*\\):.*?)(?=\\ndef|\\Z)'\n        match = re.search(func_pattern, code, re.DOTALL)\n\n        if match:\n            func_code = match.group(1)\n\n            # Generate documentation\n            doc_prompt = f\"Write clear documentation for this Python function:\\n\\n{func_code}\"\n            func_doc = run_steadytext(doc_prompt, seed=seed + i + 1, max_tokens=300)\n\n            docs.append(f\"\\n## `{func}()`\\n\\n{func_doc}\\n\")\n\n    return '\\n'.join(docs)\n\ndef main():\n    parser = argparse.ArgumentParser(description='Generate documentation from Python code')\n    parser.add_argument('files', nargs='+', help='Python files to document')\n    parser.add_argument('--output', '-o', help='Output directory', default='./docs')\n    parser.add_argument('--seed', '-s', type=int, default=42, help='Random seed')\n\n    args = parser.parse_args()\n\n    output_dir = Path(args.output)\n    output_dir.mkdir(exist_ok=True)\n\n    for file_path in args.files:\n        file_path = Path(file_path)\n        if file_path.suffix == '.py':\n            print(f\"Generating documentation for {file_path}...\")\n\n            try:\n                docs = generate_function_docs(file_path, seed=args.seed)\n\n                # Save documentation\n                doc_path = output_dir / f\"{file_path.stem}_docs.md\"\n                with open(doc_path, 'w') as f:\n                    f.write(docs)\n\n                print(f\"  \u2192 Saved to {doc_path}\")\n            except Exception as e:\n                print(f\"  \u2717 Error: {e}\")\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"api/cli/#batch-text-analysis-tool","title":"Batch Text Analysis Tool","text":"<pre><code>#!/bin/bash\n# analyze_texts.sh - Analyze multiple texts for sentiment, topics, etc.\n\nSEED=999\nOUTPUT_DIR=\"analysis_results\"\nmkdir -p \"$OUTPUT_DIR\"\n\n# Function to analyze single text\nanalyze_text() {\n    local file=\"$1\"\n    local filename=$(basename \"$file\" .txt)\n    local output_file=\"$OUTPUT_DIR/${filename}_analysis.json\"\n\n    echo \"Analyzing: $file\"\n\n    # Read content\n    content=$(cat \"$file\")\n\n    # Generate various analyses\n    sentiment=$(st generate \"Analyze the sentiment of this text and respond with only: POSITIVE, NEGATIVE, or NEUTRAL: $content\" --seed $SEED --wait --max-new-tokens 10)\n\n    summary=$(st generate \"Write a one-sentence summary of: $content\" --seed $(($SEED + 1)) --wait --max-new-tokens 50)\n\n    topics=$(st generate \"List the main topics in this text as comma-separated values: $content\" --seed $(($SEED + 2)) --wait --max-new-tokens 30)\n\n    # Create embedding\n    embedding=$(echo \"$content\" | st embed --seed $SEED --format json)\n\n    # Combine results\n    cat &gt; \"$output_file\" &lt;&lt;EOF\n{\n  \"file\": \"$file\",\n  \"sentiment\": \"$sentiment\",\n  \"summary\": \"$summary\",\n  \"topics\": \"$topics\",\n  \"embedding_sample\": $(echo \"$embedding\" | jq '.[0:5]'),\n  \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n}\nEOF\n\n    echo \"  \u2192 Saved to $output_file\"\n}\n\n# Process all text files\nfor file in *.txt; do\n    if [ -f \"$file\" ]; then\n        analyze_text \"$file\"\n    fi\ndone\n\n# Generate summary report\necho -e \"\\n\\nGenerating summary report...\"\n\nst generate \"Based on these analysis results, write a summary report: $(cat $OUTPUT_DIR/*.json | jq -s '.')\" \\\n    --seed $(($SEED + 100)) \\\n    --max-new-tokens 500 \\\n    --wait &gt; \"$OUTPUT_DIR/summary_report.md\"\n\necho \"Analysis complete! Results in $OUTPUT_DIR/\"\n</code></pre>"},{"location":"api/cli/#interactive-qa-system","title":"Interactive Q&amp;A System","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nqa_system.py - Interactive Q&amp;A using SteadyText with context\n\"\"\"\n\nimport subprocess\nimport json\nimport readline  # For better input handling\nfrom datetime import datetime\n\nclass QASystem:\n    def __init__(self, seed=42):\n        self.seed = seed\n        self.context = []\n        self.max_context = 5\n\n    def ask(self, question):\n        \"\"\"Ask a question with context.\"\"\"\n        # Build context prompt\n        if self.context:\n            context_str = \"Previous Q&amp;A:\\n\"\n            for qa in self.context[-self.max_context:]:\n                context_str += f\"Q: {qa['q']}\\nA: {qa['a'][:100]}...\\n\\n\"\n            full_prompt = f\"{context_str}\\nNow answer this question: {question}\"\n        else:\n            full_prompt = question\n\n        # Generate answer\n        cmd = [\n            'st', 'generate', full_prompt,\n            '--seed', str(self.seed + len(self.context)),\n            '--max-new-tokens', '300',\n            '--wait',\n            '--json'\n        ]\n\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode == 0:\n            data = json.loads(result.stdout)\n            answer = data['text']\n\n            # Store in context\n            self.context.append({\n                'q': question,\n                'a': answer,\n                'timestamp': datetime.now().isoformat()\n            })\n\n            return answer\n        else:\n            return f\"Error: {result.stderr}\"\n\n    def save_session(self, filename):\n        \"\"\"Save Q&amp;A session to file.\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(self.context, f, indent=2)\n        print(f\"Session saved to {filename}\")\n\n    def run_interactive(self):\n        \"\"\"Run interactive Q&amp;A session.\"\"\"\n        print(\"SteadyText Q&amp;A System\")\n        print(\"Type 'quit' to exit, 'save' to save session\")\n        print(\"-\" * 50)\n\n        while True:\n            try:\n                question = input(\"\\nYour question: \").strip()\n\n                if question.lower() == 'quit':\n                    break\n                elif question.lower() == 'save':\n                    filename = input(\"Save as: \") or \"qa_session.json\"\n                    self.save_session(filename)\n                    continue\n                elif not question:\n                    continue\n\n                print(\"\\nThinking...\")\n                answer = self.ask(question)\n                print(f\"\\nAnswer: {answer}\")\n\n            except KeyboardInterrupt:\n                print(\"\\n\\nGoodbye!\")\n                break\n            except Exception as e:\n                print(f\"Error: {e}\")\n\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--load', help='Load previous session')\n\n    args = parser.parse_args()\n\n    qa = QASystem(seed=args.seed)\n\n    if args.load:\n        with open(args.load, 'r') as f:\n            qa.context = json.load(f)\n        print(f\"Loaded {len(qa.context)} previous Q&amp;As\")\n\n    qa.run_interactive()\n</code></pre>"},{"location":"api/cli/#multi-language-code-generator","title":"Multi-Language Code Generator","text":"<pre><code>#!/bin/bash\n# polyglot_codegen.sh - Generate code in multiple languages\n\ngenerate_code() {\n    local task=\"$1\"\n    local lang=\"$2\"\n    local seed=\"$3\"\n\n    prompt=\"Write a $lang function that $task. Include only the code, no explanations.\"\n\n    echo \"=== $lang ===\"\n    st generate \"$prompt\" --seed $seed --max-new-tokens 200 --wait\n    echo -e \"\\n\"\n}\n\n# Main\necho \"Multi-Language Code Generator\"\necho \"============================\"\nread -p \"What should the function do? \" task\n\n# Generate in multiple languages with consistent seeds\nLANGUAGES=(\"Python\" \"JavaScript\" \"Go\" \"Rust\" \"Java\" \"C++\" \"Ruby\" \"PHP\")\nBASE_SEED=1000\n\nfor i in \"${!LANGUAGES[@]}\"; do\n    generate_code \"$task\" \"${LANGUAGES[$i]}\" $(($BASE_SEED + $i))\ndone\n\n# Generate comparison\necho \"=== Performance Comparison ===\"\nst generate \"Compare the performance characteristics of these languages for $task: ${LANGUAGES[*]}\" \\\n    --seed $(($BASE_SEED + 100)) \\\n    --max-new-tokens 300 \\\n    --wait\n</code></pre>"},{"location":"api/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/cli/#common-issues","title":"Common Issues","text":"<p>Issue: Command not found <pre><code># Problem\n$ st generate \"test\"\nbash: st: command not found\n\n# Solution\n# Ensure SteadyText is installed\npip install steadytext\n\n# Or add to PATH if using local install\nexport PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre></p> <p>Issue: Slow first generation <pre><code># Problem: First call takes 2-3 seconds\n\n# Solution 1: Preload models\nst models preload\n\n# Solution 2: Use daemon mode\nst daemon start\nst generate \"test\"  # Now fast!\n</code></pre></p> <p>Issue: Different results across runs <pre><code># Problem: Results vary between sessions\n\n# Solution: Use explicit seeds\nst generate \"test\" --seed 42  # Always same result\nst embed \"test\" --seed 42     # Always same embedding\n</code></pre></p> <p>Issue: JSON parsing errors <pre><code># Problem: Invalid JSON output\n\n# Solution: Use proper error handling\nresult=$(st generate \"test\" --json 2&gt;/dev/null)\nif [ $? -eq 0 ]; then\n    echo \"$result\" | jq '.text'\nelse\n    echo \"Error generating text\"\nfi\n</code></pre></p>"},{"location":"api/cli/#best-practices","title":"Best Practices","text":"<p>CLI Best Practices</p> <ol> <li>Always use seeds for reproducible results in production</li> <li>Start daemon for better performance in scripts</li> <li>Use JSON output for reliable parsing</li> <li>Handle errors properly in scripts</li> <li>Batch operations when possible</li> <li>Set environment variables for consistent configuration</li> <li>Use appropriate output formats (JSON for parsing, plain for display)</li> <li>Chain commands efficiently with pipes</li> <li>Cache warmup for frequently used prompts</li> <li>Monitor performance with timing commands</li> </ol>"},{"location":"api/embedding/","title":"Embeddings API","text":"<p>Functions for creating deterministic text embeddings.</p>"},{"location":"api/embedding/#embed","title":"embed()","text":"<p>Create deterministic embeddings for text input.</p> <pre><code>def embed(text_input: Union[str, List[str]], seed: int = DEFAULT_SEED) -&gt; np.ndarray\n</code></pre>"},{"location":"api/embedding/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>text_input</code> <code>Union[str, List[str]]</code> required Text string or list of strings to embed <code>seed</code> <code>int</code> <code>42</code> Random seed for deterministic embedding generation"},{"location":"api/embedding/#returns","title":"Returns","text":"<p>Returns: <code>np.ndarray</code> - 1024-dimensional L2-normalized float32 array</p>"},{"location":"api/embedding/#examples","title":"Examples","text":"Single TextCustom SeedMultiple TextsSimilarity Comparison <pre><code>import steadytext\nimport numpy as np\n\n# Embed single text\nvector = steadytext.embed(\"Hello world\")\n\nprint(f\"Shape: {vector.shape}\")        # (1024,)\nprint(f\"Type: {vector.dtype}\")         # float32\nprint(f\"Norm: {np.linalg.norm(vector):.6f}\")  # 1.000000 (L2 normalized)\n</code></pre> <pre><code># Generate different embeddings with different seeds\nvec1 = steadytext.embed(\"Hello world\", seed=123)\nvec2 = steadytext.embed(\"Hello world\", seed=123)  # Same as vec1\nvec3 = steadytext.embed(\"Hello world\", seed=456)  # Different from vec1\n\nprint(f\"Seed 123 vs 123 equal: {np.array_equal(vec1, vec2)}\")  # True\nprint(f\"Seed 123 vs 456 equal: {np.array_equal(vec1, vec3)}\")  # False\n\n# Calculate similarity between different seed embeddings\nsimilarity = np.dot(vec1, vec3)  # Cosine similarity (vectors are normalized)\nprint(f\"Similarity between seeds: {similarity:.3f}\")\n</code></pre> <pre><code># Embed multiple texts (returns a single, averaged embedding)\ntexts = [\"machine learning\", \"artificial intelligence\", \"deep learning\"]\nvector = steadytext.embed(texts)\n\nprint(f\"Combined embedding shape: {vector.shape}\")  # (1024,)\n# Result is averaged across all input texts\n</code></pre> <pre><code>import numpy as np\n\n# Create embeddings for comparison with consistent seed\nseed = 42\nvec1 = steadytext.embed(\"machine learning\", seed=seed)\nvec2 = steadytext.embed(\"artificial intelligence\", seed=seed) \nvec3 = steadytext.embed(\"cooking recipes\", seed=seed)\n\n# Calculate cosine similarity (vectors are already L2 normalized)\nsim_ml_ai = np.dot(vec1, vec2)\nsim_ml_cooking = np.dot(vec1, vec3)\n\nprint(f\"ML vs AI similarity: {sim_ml_ai:.3f}\")\nprint(f\"ML vs Cooking similarity: {sim_ml_cooking:.3f}\")\n# ML and AI should have higher similarity than ML and cooking\n\n# Compare same text with different seeds\nvec_seed1 = steadytext.embed(\"machine learning\", seed=100)\nvec_seed2 = steadytext.embed(\"machine learning\", seed=200)\nseed_similarity = np.dot(vec_seed1, vec_seed2)\nprint(f\"Same text, different seeds similarity: {seed_similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/embedding/#deterministic-behavior","title":"Deterministic Behavior","text":"<p>Embeddings are completely deterministic for the same input text and seed:</p> <pre><code># Same text, same seed - always identical\nvec1 = steadytext.embed(\"test text\")\nvec2 = steadytext.embed(\"test text\")\nassert np.array_equal(vec1, vec2)  # Always passes!\n\n# Same text, explicit same seed - always identical\nvec3 = steadytext.embed(\"test text\", seed=42)\nvec4 = steadytext.embed(\"test text\", seed=42)\nassert np.array_equal(vec3, vec4)  # Always passes!\n\n# Same text, different seeds - different results\nvec5 = steadytext.embed(\"test text\", seed=123)\nvec6 = steadytext.embed(\"test text\", seed=456)\nassert not np.array_equal(vec5, vec6)  # Different seeds produce different embeddings\n\n# But each seed is still deterministic\nvec7 = steadytext.embed(\"test text\", seed=123)\nassert np.array_equal(vec5, vec7)  # Same seed always produces same result\n</code></pre>"},{"location":"api/embedding/#seed-use-cases","title":"Seed Use Cases","text":"<pre><code># Experimental variations - try different embeddings for the same text\ntext = \"artificial intelligence\"\nbaseline_embedding = steadytext.embed(text, seed=42)\nvariation1 = steadytext.embed(text, seed=100)\nvariation2 = steadytext.embed(text, seed=200)\n\n# Compare variations\nprint(f\"Baseline vs Variation 1: {np.dot(baseline_embedding, variation1):.3f}\")\nprint(f\"Baseline vs Variation 2: {np.dot(baseline_embedding, variation2):.3f}\")\nprint(f\"Variation 1 vs Variation 2: {np.dot(variation1, variation2):.3f}\")\n\n# Reproducible research - document your seeds\nresearch_texts = [\"AI\", \"ML\", \"DL\"]\nresearch_seed = 42\nembeddings = []\nfor text in research_texts:\n    embedding = steadytext.embed(text, seed=research_seed)\n    embeddings.append(embedding)\n    print(f\"Text: {text}, Seed: {research_seed}\")\n</code></pre>"},{"location":"api/embedding/#preprocessing","title":"Preprocessing","text":"<p>Text is automatically preprocessed before embedding:</p> <pre><code># These produce different embeddings due to different text\nvec1 = steadytext.embed(\"Hello World\")\nvec2 = steadytext.embed(\"hello world\")\nvec3 = steadytext.embed(\"HELLO WORLD\")\n\n# Case sensitivity matters\nassert not np.array_equal(vec1, vec2)\n</code></pre>"},{"location":"api/embedding/#batch-processing","title":"Batch Processing","text":"<p>For multiple texts, pass as a list with consistent seeding:</p> <pre><code># Individual embeddings with consistent seed\nseed = 42\nvec1 = steadytext.embed(\"first text\", seed=seed)\nvec2 = steadytext.embed(\"second text\", seed=seed) \nvec3 = steadytext.embed(\"third text\", seed=seed)\n\n# Batch embedding (averaged) with same seed\nvec_batch = steadytext.embed([\"first text\", \"second text\", \"third text\"], seed=seed)\n\n# The batch result is the average of individual embeddings\nexpected = (vec1 + vec2 + vec3) / 3\nexpected = expected / np.linalg.norm(expected)  # Re-normalize after averaging\nassert np.allclose(vec_batch, expected, atol=1e-6)\n\n# Different seeds produce different batch results\nvec_batch_alt = steadytext.embed([\"first text\", \"second text\", \"third text\"], seed=123)\nassert not np.array_equal(vec_batch, vec_batch_alt)\n</code></pre>"},{"location":"api/embedding/#caching","title":"Caching","text":"<p>Embeddings are cached for performance, with seed as part of the cache key:</p> <pre><code># First call: computes and caches embedding for default seed\nvec1 = steadytext.embed(\"common text\")  # ~0.5 seconds\n\n# Second call with same seed: returns cached result\nvec2 = steadytext.embed(\"common text\")  # ~0.01 seconds\nassert np.array_equal(vec1, vec2)  # Same result, much faster\n\n# Different seed: computes and caches separately\nvec3 = steadytext.embed(\"common text\", seed=123)  # ~0.5 seconds (new cache entry)\nvec4 = steadytext.embed(\"common text\", seed=123)  # ~0.01 seconds (cached)\n\nassert np.array_equal(vec3, vec4)  # Same seed, same cached result\nassert not np.array_equal(vec1, vec3)  # Different seeds, different results\n\n# Each seed gets its own cache entry\nfor seed in [100, 200, 300]:\n    steadytext.embed(\"cache test\", seed=seed)  # Each gets cached separately\n</code></pre>"},{"location":"api/embedding/#fallback-behavior","title":"Fallback Behavior","text":"<p>When models can't be loaded, deterministic fallback vectors are generated using the seed:</p> <pre><code># Even without models, function never fails and respects seeds\nvector1 = steadytext.embed(\"any text\", seed=42)\nvector2 = steadytext.embed(\"any text\", seed=42)\nvector3 = steadytext.embed(\"any text\", seed=123)\n\nassert vector1.shape == (1024,)     # Correct shape\nassert vector1.dtype == np.float32  # Correct type\nassert np.array_equal(vector1, vector2)  # Same seed, same fallback\nassert not np.array_equal(vector1, vector3)  # Different seed, different fallback\n\n# Fallback vectors are normalized and deterministic\nassert abs(np.linalg.norm(vector1) - 1.0) &lt; 1e-6  # Properly normalized\n</code></pre>"},{"location":"api/embedding/#use-cases","title":"Use Cases","text":""},{"location":"api/embedding/#document-similarity","title":"Document Similarity","text":"<pre><code>import steadytext\nimport numpy as np\n\ndef document_similarity(doc1: str, doc2: str, seed: int = 42) -&gt; float:\n    \"\"\"Calculate similarity between two documents.\"\"\"\n    vec1 = steadytext.embed(doc1, seed=seed)\n    vec2 = steadytext.embed(doc2, seed=seed)\n    return np.dot(vec1, vec2)  # Already L2 normalized\n\n# Usage\nsimilarity = document_similarity(\n    \"Machine learning algorithms\",\n    \"AI and neural networks\"\n)\nprint(f\"Similarity: {similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#semantic-search","title":"Semantic Search","text":"<pre><code>def semantic_search(query: str, documents: List[str], top_k: int = 5, seed: int = 42):\n    \"\"\"Find most similar documents to query.\"\"\"\n    query_vec = steadytext.embed(query, seed=seed)\n    doc_vecs = [steadytext.embed(doc, seed=seed) for doc in documents]\n\n    similarities = [np.dot(query_vec, doc_vec) for doc_vec in doc_vecs]\n    top_indices = np.argsort(similarities)[-top_k:][::-1]\n\n    return [(documents[i], similarities[i]) for i in top_indices]\n\n# Usage  \ndocs = [\"AI research\", \"Machine learning\", \"Cooking recipes\", \"Data science\"]\nresults = semantic_search(\"artificial intelligence\", docs, top_k=2)\n\nfor doc, score in results:\n    print(f\"{doc}: {score:.3f}\")\n</code></pre>"},{"location":"api/embedding/#clustering","title":"Clustering","text":"<pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_texts(texts: List[str], n_clusters: int = 3, seed: int = 42):\n    \"\"\"Cluster texts using their embeddings.\"\"\"\n    embeddings = np.array([steadytext.embed(text, seed=seed) for text in texts])\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(embeddings)\n\n    return clusters\n\n# Usage\ntexts = [\n    \"machine learning\", \"deep learning\", \"neural networks\",  # AI cluster\n    \"pizza recipe\", \"pasta cooking\", \"italian food\",        # Food cluster  \n    \"stock market\", \"trading\", \"investment\"                 # Finance cluster\n]\n\nclusters = cluster_texts(texts, n_clusters=3)\nfor text, cluster in zip(texts, clusters):\n    print(f\"Cluster {cluster}: {text}\")\n</code></pre>"},{"location":"api/embedding/#performance-notes","title":"Performance Notes","text":"<p>Optimization Tips</p> <ul> <li>Preload models: Call <code>steadytext.preload_models()</code> at startup</li> <li>Batch similar texts: Group related texts together for cache efficiency  </li> <li>Memory usage: ~610MB for embedding model (loaded once)</li> <li>Speed: ~100-500 embeddings/second depending on text length</li> <li>Seed consistency: Use consistent seeds across related embeddings for comparable results</li> <li>Cache efficiency: Different seeds create separate cache entries, so choose seeds wisely</li> </ul>"},{"location":"api/embedding/#advanced-examples","title":"Advanced Examples","text":""},{"location":"api/embedding/#vector-database-integration","title":"Vector Database Integration","text":"<pre><code>import steadytext\nimport numpy as np\nimport faiss\n\nclass VectorDB:\n    \"\"\"Simple vector database using FAISS.\"\"\"\n\n    def __init__(self, dimension: int = 1024, seed: int = 42):\n        self.dimension = dimension\n        self.seed = seed\n        self.index = faiss.IndexFlatL2(dimension)\n        self.metadata = []\n\n    def add_documents(self, documents: list, ids: list = None):\n        \"\"\"Add documents to the vector database.\"\"\"\n        embeddings = []\n\n        for i, doc in enumerate(documents):\n            # Use consistent seed for all documents\n            vec = steadytext.embed(doc, seed=self.seed)\n            embeddings.append(vec)\n\n            # Store metadata\n            self.metadata.append({\n                'id': ids[i] if ids else i,\n                'text': doc,\n                'embedding': vec\n            })\n\n        # Add to FAISS index\n        embeddings_array = np.array(embeddings).astype('float32')\n        self.index.add(embeddings_array)\n\n    def search(self, query: str, k: int = 5):\n        \"\"\"Search for similar documents.\"\"\"\n        # Use same seed as documents\n        query_vec = steadytext.embed(query, seed=self.seed).reshape(1, -1)\n\n        # Search in FAISS\n        distances, indices = self.index.search(query_vec.astype('float32'), k)\n\n        # Return results with metadata\n        results = []\n        for i, idx in enumerate(indices[0]):\n            if idx != -1:\n                results.append({\n                    'id': self.metadata[idx]['id'],\n                    'text': self.metadata[idx]['text'],\n                    'distance': distances[0][i],\n                    'similarity': 1 / (1 + distances[0][i])  # Convert distance to similarity\n                })\n\n        return results\n\n# Example usage\ndb = VectorDB(seed=100)  # Custom seed for this database\n\n# Add documents\ndocuments = [\n    \"Introduction to machine learning algorithms\",\n    \"Deep learning with neural networks\",\n    \"Natural language processing basics\",\n    \"Computer vision applications\",\n    \"Reinforcement learning in robotics\"\n]\n\ndb.add_documents(documents, ids=['ML101', 'DL201', 'NLP301', 'CV401', 'RL501'])\n\n# Search\nresults = db.search(\"text processing and NLP\", k=3)\nfor result in results:\n    print(f\"ID: {result['id']}, Similarity: {result['similarity']:.3f}\")\n    print(f\"Text: {result['text']}\\n\")\n</code></pre>"},{"location":"api/embedding/#multi-modal-embeddings","title":"Multi-Modal Embeddings","text":"<pre><code>import steadytext\nimport numpy as np\nfrom typing import Dict, Any\n\nclass MultiModalEmbedder:\n    \"\"\"Create combined embeddings from multiple modalities.\"\"\"\n\n    def __init__(self, base_seed: int = 42):\n        self.base_seed = base_seed\n        self.modality_seeds = {\n            'text': base_seed,\n            'title': base_seed + 1000,\n            'tags': base_seed + 2000,\n            'category': base_seed + 3000\n        }\n\n    def embed_document(self, document: Dict[str, Any]) -&gt; np.ndarray:\n        \"\"\"Create a combined embedding from multiple fields.\"\"\"\n        embeddings = []\n        weights = []\n\n        # Embed each modality with its own seed\n        if 'text' in document and document['text']:\n            vec = steadytext.embed(document['text'], seed=self.modality_seeds['text'])\n            embeddings.append(vec)\n            weights.append(0.5)  # Main content gets highest weight\n\n        if 'title' in document and document['title']:\n            vec = steadytext.embed(document['title'], seed=self.modality_seeds['title'])\n            embeddings.append(vec)\n            weights.append(0.3)\n\n        if 'tags' in document and document['tags']:\n            # Combine tags into single text\n            tags_text = \" \".join(document['tags'])\n            vec = steadytext.embed(tags_text, seed=self.modality_seeds['tags'])\n            embeddings.append(vec)\n            weights.append(0.15)\n\n        if 'category' in document and document['category']:\n            vec = steadytext.embed(document['category'], seed=self.modality_seeds['category'])\n            embeddings.append(vec)\n            weights.append(0.05)\n\n        if not embeddings:\n            # Fallback to zero vector if no content\n            return np.zeros(1024, dtype=np.float32)\n\n        # Weighted average\n        weights = np.array(weights) / sum(weights)  # Normalize weights\n        combined = np.average(embeddings, axis=0, weights=weights)\n\n        # Re-normalize\n        norm = np.linalg.norm(combined)\n        if norm &gt; 0:\n            combined = combined / norm\n\n        return combined\n\n# Example usage\nembedder = MultiModalEmbedder(base_seed=200)\n\n# Document with multiple fields\ndoc1 = {\n    'title': 'Introduction to Machine Learning',\n    'text': 'Machine learning is a subset of artificial intelligence...',\n    'tags': ['ML', 'AI', 'tutorial', 'beginner'],\n    'category': 'Education'\n}\n\ndoc2 = {\n    'title': 'Advanced Deep Learning Techniques',\n    'text': 'Deep learning has revolutionized computer vision...',\n    'tags': ['DL', 'neural networks', 'advanced'],\n    'category': 'Research'\n}\n\n# Create multi-modal embeddings\nvec1 = embedder.embed_document(doc1)\nvec2 = embedder.embed_document(doc2)\n\n# Compare similarity\nsimilarity = np.dot(vec1, vec2)\nprint(f\"Document similarity: {similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#incremental-embedding-updates","title":"Incremental Embedding Updates","text":"<pre><code>import steadytext\nimport numpy as np\nfrom collections import deque\n\nclass IncrementalEmbedder:\n    \"\"\"Maintain running average embeddings for evolving content.\"\"\"\n\n    def __init__(self, window_size: int = 10, seed: int = 42):\n        self.window_size = window_size\n        self.seed = seed\n        self.history = deque(maxlen=window_size)\n        self.current_embedding = None\n\n    def add_text(self, text: str) -&gt; np.ndarray:\n        \"\"\"Add new text and update running embedding.\"\"\"\n        # Embed new text\n        new_embedding = steadytext.embed(text, seed=self.seed)\n        self.history.append(new_embedding)\n\n        # Calculate running average\n        if len(self.history) &gt; 0:\n            avg_embedding = np.mean(list(self.history), axis=0)\n            # Re-normalize\n            self.current_embedding = avg_embedding / np.linalg.norm(avg_embedding)\n\n        return self.current_embedding\n\n    def get_evolution(self) -&gt; list:\n        \"\"\"Get the evolution of embeddings over time.\"\"\"\n        evolution = []\n        temp_history = []\n\n        for emb in self.history:\n            temp_history.append(emb)\n            avg = np.mean(temp_history, axis=0)\n            avg = avg / np.linalg.norm(avg)\n            evolution.append(avg)\n\n        return evolution\n\n# Example: Track topic drift in conversation\nembedder = IncrementalEmbedder(window_size=5, seed=300)\n\nconversation = [\n    \"Let's talk about machine learning\",\n    \"Neural networks are fascinating\",\n    \"Deep learning has many applications\",\n    \"But what about traditional algorithms?\",\n    \"Random forests are still useful\",\n    \"Statistical methods have their place\",\n    \"Linear regression is fundamental\"\n]\n\nprint(\"Conversation evolution:\")\nfor i, text in enumerate(conversation):\n    embedding = embedder.add_text(text)\n\n    if i &gt; 0:\n        # Compare to previous state\n        evolution = embedder.get_evolution()\n        similarity = np.dot(evolution[-1], evolution[0])\n        print(f\"Step {i}: '{text[:30]}...' - Drift from start: {1-similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#embedding-dimensionality-reduction","title":"Embedding Dimensionality Reduction","text":"<pre><code>import steadytext\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nclass EmbeddingVisualizer:\n    \"\"\"Visualize high-dimensional embeddings in 2D/3D.\"\"\"\n\n    def __init__(self, seed: int = 42):\n        self.seed = seed\n        self.embeddings = []\n        self.labels = []\n\n    def add_texts(self, texts: list, labels: list = None):\n        \"\"\"Add texts with optional labels.\"\"\"\n        for i, text in enumerate(texts):\n            emb = steadytext.embed(text, seed=self.seed)\n            self.embeddings.append(emb)\n            self.labels.append(labels[i] if labels else str(i))\n\n    def reduce_pca(self, n_components: int = 2) -&gt; np.ndarray:\n        \"\"\"Reduce dimensions using PCA.\"\"\"\n        if not self.embeddings:\n            return np.array([])\n\n        pca = PCA(n_components=n_components, random_state=42)\n        reduced = pca.fit_transform(np.array(self.embeddings))\n\n        print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n        return reduced\n\n    def reduce_tsne(self, n_components: int = 2) -&gt; np.ndarray:\n        \"\"\"Reduce dimensions using t-SNE.\"\"\"\n        if not self.embeddings:\n            return np.array([])\n\n        tsne = TSNE(n_components=n_components, random_state=42, perplexity=5)\n        reduced = tsne.fit_transform(np.array(self.embeddings))\n        return reduced\n\n    def plot_2d(self, method: str = 'pca'):\n        \"\"\"Create 2D visualization.\"\"\"\n        if method == 'pca':\n            reduced = self.reduce_pca(2)\n        else:\n            reduced = self.reduce_tsne(2)\n\n        plt.figure(figsize=(10, 8))\n        plt.scatter(reduced[:, 0], reduced[:, 1])\n\n        for i, label in enumerate(self.labels):\n            plt.annotate(label, (reduced[i, 0], reduced[i, 1]), \n                        xytext=(5, 5), textcoords='offset points')\n\n        plt.title(f'Embedding Visualization ({method.upper()})')\n        plt.xlabel('Component 1')\n        plt.ylabel('Component 2')\n        plt.grid(True, alpha=0.3)\n        return plt\n\n# Example usage\nviz = EmbeddingVisualizer(seed=400)\n\n# Add different categories of text\ncategories = {\n    'AI': [\"machine learning\", \"neural networks\", \"deep learning\"],\n    'Food': [\"pizza recipe\", \"pasta cooking\", \"italian cuisine\"],\n    'Finance': [\"stock market\", \"investment strategy\", \"trading\"]\n}\n\nfor category, texts in categories.items():\n    for text in texts:\n        viz.add_texts([text], labels=[f\"{category}: {text}\"])\n\n# Visualize (would display plot in Jupyter)\n# plot = viz.plot_2d('tsne')\n# plot.show()\n</code></pre>"},{"location":"api/embedding/#cross-lingual-embeddings","title":"Cross-Lingual Embeddings","text":"<pre><code>import steadytext\nimport numpy as np\n\nclass CrossLingualEmbedder:\n    \"\"\"Create aligned embeddings across languages using seed variations.\"\"\"\n\n    def __init__(self, base_seed: int = 42):\n        self.base_seed = base_seed\n        # Different seed offsets for different languages\n        self.language_seeds = {\n            'en': base_seed,\n            'es': base_seed + 10000,\n            'fr': base_seed + 20000,\n            'de': base_seed + 30000,\n            'zh': base_seed + 40000\n        }\n\n    def embed(self, text: str, language: str = 'en') -&gt; np.ndarray:\n        \"\"\"Embed text with language-specific seed.\"\"\"\n        if language not in self.language_seeds:\n            language = 'en'  # Fallback to English\n\n        seed = self.language_seeds[language]\n        return steadytext.embed(text, seed=seed)\n\n    def align_embeddings(self, source_texts: list, target_texts: list, \n                        source_lang: str, target_lang: str) -&gt; tuple:\n        \"\"\"Create aligned embeddings for parallel texts.\"\"\"\n        source_embeddings = [self.embed(text, source_lang) for text in source_texts]\n        target_embeddings = [self.embed(text, target_lang) for text in target_texts]\n\n        # Simple alignment: compute transformation matrix\n        # In practice, you'd use more sophisticated methods\n        S = np.array(source_embeddings)\n        T = np.array(target_embeddings)\n\n        # Compute pseudo-inverse for alignment\n        # W = T @ S.T @ np.linalg.inv(S @ S.T)\n        # For simplicity, we'll just return the embeddings\n\n        return source_embeddings, target_embeddings\n\n    def cross_lingual_similarity(self, text1: str, lang1: str, \n                               text2: str, lang2: str) -&gt; float:\n        \"\"\"Compute similarity across languages.\"\"\"\n        vec1 = self.embed(text1, lang1)\n        vec2 = self.embed(text2, lang2)\n\n        # Apply simple heuristic adjustment for cross-lingual comparison\n        # In practice, you'd use learned alignment\n        if lang1 != lang2:\n            # Reduce similarity slightly for different languages\n            adjustment = 0.9\n        else:\n            adjustment = 1.0\n\n        return np.dot(vec1, vec2) * adjustment\n\n# Example usage\nembedder = CrossLingualEmbedder(base_seed=500)\n\n# Embed in different languages\nen_vec = embedder.embed(\"Hello world\", \"en\")\nes_vec = embedder.embed(\"Hola mundo\", \"es\")\nfr_vec = embedder.embed(\"Bonjour le monde\", \"fr\")\n\n# Compare cross-lingual similarities\nprint(\"Cross-lingual similarities:\")\nprint(f\"EN-ES: {embedder.cross_lingual_similarity('Hello world', 'en', 'Hola mundo', 'es'):.3f}\")\nprint(f\"EN-FR: {embedder.cross_lingual_similarity('Hello world', 'en', 'Bonjour le monde', 'fr'):.3f}\")\nprint(f\"ES-FR: {embedder.cross_lingual_similarity('Hola mundo', 'es', 'Bonjour le monde', 'fr'):.3f}\")\n\n# Same language comparison\nen_sim = embedder.cross_lingual_similarity('Hello world', 'en', 'Hi earth', 'en')\nprint(f\"\\nSame language (EN-EN): {en_sim:.3f}\")\n</code></pre>"},{"location":"api/embedding/#real-time-embedding-stream","title":"Real-time Embedding Stream","text":"<pre><code>import steadytext\nimport numpy as np\nimport time\nfrom typing import Iterator, Tuple\n\nclass EmbeddingStream:\n    \"\"\"Process streaming text data with real-time embeddings.\"\"\"\n\n    def __init__(self, chunk_size: int = 100, overlap: int = 20, seed: int = 42):\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.seed = seed\n        self.buffer = \"\"\n        self.processed_count = 0\n\n    def process_stream(self, text_stream: Iterator[str]) -&gt; Iterator[Tuple[str, np.ndarray]]:\n        \"\"\"Process streaming text and yield embeddings.\"\"\"\n        for text in text_stream:\n            self.buffer += text\n\n            # Process complete chunks\n            while len(self.buffer) &gt;= self.chunk_size:\n                # Extract chunk\n                chunk = self.buffer[:self.chunk_size]\n\n                # Generate embedding with position-based seed\n                chunk_seed = self.seed + self.processed_count\n                embedding = steadytext.embed(chunk, seed=chunk_seed)\n\n                yield chunk, embedding\n\n                # Move buffer forward with overlap\n                self.buffer = self.buffer[self.chunk_size - self.overlap:]\n                self.processed_count += 1\n\n        # Process remaining buffer\n        if self.buffer:\n            final_seed = self.seed + self.processed_count\n            embedding = steadytext.embed(self.buffer, seed=final_seed)\n            yield self.buffer, embedding\n\n# Example: Simulate streaming text\ndef text_generator():\n    \"\"\"Simulate streaming text data.\"\"\"\n    texts = [\n        \"Machine learning is transforming how we process information. \",\n        \"Neural networks can learn complex patterns from data. \",\n        \"Deep learning models require large amounts of training data. \",\n        \"Transfer learning helps when data is limited. \",\n        \"Embeddings capture semantic meaning in vector space. \"\n    ]\n\n    for text in texts:\n        # Simulate streaming by yielding words\n        words = text.split()\n        for word in words:\n            yield word + \" \"\n            time.sleep(0.1)  # Simulate real-time stream\n\n# Process stream\nstream_processor = EmbeddingStream(chunk_size=50, overlap=10, seed=600)\n\nprint(\"Processing text stream...\")\nembeddings_collected = []\n\nfor chunk, embedding in stream_processor.process_stream(text_generator()):\n    print(f\"Processed chunk: '{chunk[:30]}...' -&gt; Embedding shape: {embedding.shape}\")\n    embeddings_collected.append(embedding)\n\n# Analyze progression\nif len(embeddings_collected) &gt; 1:\n    print(f\"\\nTotal chunks processed: {len(embeddings_collected)}\")\n\n    # Check similarity progression\n    for i in range(1, len(embeddings_collected)):\n        sim = np.dot(embeddings_collected[i-1], embeddings_collected[i])\n        print(f\"Similarity between chunk {i-1} and {i}: {sim:.3f}\")\n</code></pre>"},{"location":"api/embedding/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/embedding/#common-issues","title":"Common Issues","text":"<p>Issue: Embeddings not deterministic <pre><code># Problem: Different results each run\nvec1 = steadytext.embed(\"test\")\n# ... restart Python ...\nvec2 = steadytext.embed(\"test\")\n# vec1 != vec2\n\n# Solution: Ensure consistent seed and environment\nimport os\nos.environ['PYTHONHASHSEED'] = '0'  # Set before importing steadytext\nimport steadytext\n\nvec1 = steadytext.embed(\"test\", seed=42)\nvec2 = steadytext.embed(\"test\", seed=42)\nassert np.array_equal(vec1, vec2)  # Now deterministic\n</code></pre></p> <p>Issue: Out of memory with large batches <pre><code># Problem: OOM with large text list\ntexts = [\"text\"] * 10000\nvectors = [steadytext.embed(t) for t in texts]  # May OOM\n\n# Solution: Process in batches\ndef embed_in_batches(texts, batch_size=100, seed=42):\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        for text in batch:\n            embeddings.append(steadytext.embed(text, seed=seed))\n    return np.array(embeddings)\n\nvectors = embed_in_batches(texts)\n</code></pre></p> <p>Issue: Slow embedding generation <pre><code># Problem: First embedding is slow\nimport time\n\nstart = time.time()\nvec1 = steadytext.embed(\"test\")  # ~2-3 seconds (model loading)\nprint(f\"First: {time.time() - start:.2f}s\")\n\nstart = time.time()\nvec2 = steadytext.embed(\"test\")  # ~0.01 seconds (cached)\nprint(f\"Second: {time.time() - start:.2f}s\")\n\n# Solution: Preload models\nsteadytext.preload_models()  # Load once at startup\n# Now all embeddings will be fast\n</code></pre></p>"},{"location":"api/generation/","title":"Text Generation API","text":"<p>Functions for deterministic text generation.</p>"},{"location":"api/generation/#generate","title":"generate()","text":"<p>Generate deterministic text from a prompt.</p> <pre><code>def generate(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\",\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED,\n    schema: Optional[Union[Dict[str, Any], type, object]] = None,\n    regex: Optional[str] = None,\n    choices: Optional[List[str]] = None,\n    response_format: Optional[Dict[str, Any]] = None,\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre>"},{"location":"api/generation/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>prompt</code> <code>str</code> required Input text to generate from <code>max_new_tokens</code> <code>int</code> <code>512</code> Maximum number of tokens to generate <code>return_logprobs</code> <code>bool</code> <code>False</code> Return log probabilities with text <code>eos_string</code> <code>str</code> <code>\"[EOS]\"</code> Custom end-of-sequence string <code>model</code> <code>str</code> <code>None</code> Model name from registry (deprecated) <code>model_repo</code> <code>str</code> <code>None</code> Custom Hugging Face repository ID <code>model_filename</code> <code>str</code> <code>None</code> Custom model filename <code>size</code> <code>str</code> <code>None</code> Size shortcut: \"small\" or \"large\" <code>seed</code> <code>int</code> <code>42</code> Random seed for deterministic generation"},{"location":"api/generation/#returns","title":"Returns","text":"Basic UsageWith Log Probabilities <p>Returns: <code>str</code> - Generated text (512 tokens max)</p> <p>Returns: <code>Tuple[str, Optional[Dict]]</code> - Generated text and log probabilities</p>"},{"location":"api/generation/#examples","title":"Examples","text":"Simple GenerationCustom SeedCustom LengthWith Log ProbabilitiesCustom Stop String <pre><code>import steadytext\n\ntext = steadytext.generate(\"Write a Python function\")\nprint(text)\n# Always returns the same 512-token completion\n</code></pre> <pre><code># Generate with different seeds for variation\ntext1 = steadytext.generate(\"Write a story\", seed=123)\ntext2 = steadytext.generate(\"Write a story\", seed=123)  # Same as text1\ntext3 = steadytext.generate(\"Write a story\", seed=456)  # Different result\n\nprint(f\"Seed 123: {text1[:50]}...\")\nprint(f\"Seed 456: {text3[:50]}...\")\n</code></pre> <pre><code># Generate shorter responses\nshort_text = steadytext.generate(\"Explain AI\", max_new_tokens=50)\nlong_text = steadytext.generate(\"Explain AI\", max_new_tokens=200)\n\nprint(f\"Short ({len(short_text.split())} words): {short_text}\")\nprint(f\"Long ({len(long_text.split())} words): {long_text}\")\n</code></pre> <pre><code>text, logprobs = steadytext.generate(\n    \"Explain machine learning\", \n    return_logprobs=True\n)\n\nprint(\"Generated text:\", text)\nprint(\"Log probabilities:\", logprobs)\n</code></pre> <pre><code># Stop generation at custom string\ntext = steadytext.generate(\n    \"List programming languages until STOP\",\n    eos_string=\"STOP\"\n)\nprint(text)\n</code></pre>"},{"location":"api/generation/#generate_iter","title":"generate_iter()","text":"<p>Generate text iteratively, yielding tokens as produced.</p> <pre><code>def generate_iter(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False,\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre>"},{"location":"api/generation/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>prompt</code> <code>str</code> required Input text to generate from <code>max_new_tokens</code> <code>int</code> <code>512</code> Maximum number of tokens to generate <code>eos_string</code> <code>str</code> <code>\"[EOS]\"</code> Custom end-of-sequence string <code>include_logprobs</code> <code>bool</code> <code>False</code> Yield log probabilities with tokens <code>model</code> <code>str</code> <code>None</code> Model name from registry (deprecated) <code>model_repo</code> <code>str</code> <code>None</code> Custom Hugging Face repository ID <code>model_filename</code> <code>str</code> <code>None</code> Custom model filename <code>size</code> <code>str</code> <code>None</code> Size shortcut: \"small\" or \"large\" <code>seed</code> <code>int</code> <code>42</code> Random seed for deterministic generation"},{"location":"api/generation/#returns_1","title":"Returns","text":"Basic StreamingWith Log Probabilities <p>Yields: <code>str</code> - Individual tokens/words</p> <p>Yields: <code>Tuple[str, Optional[Dict]]</code> - Token and log probabilities</p>"},{"location":"api/generation/#examples_1","title":"Examples","text":"Basic StreamingCustom Seed StreamingControlled Length StreamingWith Progress TrackingCustom Stop StringWith Log Probabilities <pre><code>import steadytext\n\nfor token in steadytext.generate_iter(\"Tell me a story\"):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code># Reproducible streaming with custom seeds\nprint(\"Stream 1 (seed=123):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=123):\n    print(token, end=\"\", flush=True)\n\nprint(\"\\n\\nStream 2 (seed=123 - same result):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=123):\n    print(token, end=\"\", flush=True)\n\nprint(\"\\n\\nStream 3 (seed=456 - different result):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=456):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code># Stream with limited tokens\ntoken_count = 0\nfor token in steadytext.generate_iter(\"Explain quantum physics\", max_new_tokens=30):\n    print(token, end=\"\", flush=True)\n    token_count += 1\nprint(f\"\\nGenerated {token_count} tokens\")\n</code></pre> <pre><code>prompt = \"Explain quantum computing\"\ntokens = []\n\nfor token in steadytext.generate_iter(prompt):\n    tokens.append(token)\n    print(f\"Generated {len(tokens)} tokens\", end=\"\\r\")\n\nprint(f\"\\nComplete! Generated {len(tokens)} tokens\")\nprint(\"Full text:\", \"\".join(tokens))\n</code></pre> <pre><code>for token in steadytext.generate_iter(\n    \"Count from 1 to 10 then say DONE\", \n    eos_string=\"DONE\"\n):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code>for token, logprobs in steadytext.generate_iter(\n    \"Explain AI\", \n    include_logprobs=True\n):\n    confidence = logprobs.get('confidence', 0) if logprobs else 0\n    print(f\"{token} (confidence: {confidence:.2f})\", end=\"\")\n</code></pre>"},{"location":"api/generation/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/generation/#deterministic-behavior","title":"Deterministic Behavior","text":"<p>Both functions return identical results for identical inputs and seeds:</p> <pre><code># Default seed (42) - always identical\nresult1 = steadytext.generate(\"hello world\")\nresult2 = steadytext.generate(\"hello world\") \nassert result1 == result2  # Always passes!\n\n# Custom seeds - identical for same seed\nresult1 = steadytext.generate(\"hello world\", seed=123)\nresult2 = steadytext.generate(\"hello world\", seed=123)\nassert result1 == result2  # Always passes!\n\n# Different seeds produce different results\nresult1 = steadytext.generate(\"hello world\", seed=123)\nresult2 = steadytext.generate(\"hello world\", seed=456)\nassert result1 != result2  # Different seeds, different results\n\n# Streaming produces same tokens in same order for same seed\ntokens1 = list(steadytext.generate_iter(\"hello world\", seed=789))\ntokens2 = list(steadytext.generate_iter(\"hello world\", seed=789))\nassert tokens1 == tokens2  # Always passes!\n</code></pre>"},{"location":"api/generation/#custom-seed-use-cases","title":"Custom Seed Use Cases","text":"<pre><code># Experimental variations - try different seeds for the same prompt\nbaseline = steadytext.generate(\"Write a haiku about programming\", seed=42)\nvariation1 = steadytext.generate(\"Write a haiku about programming\", seed=123)\nvariation2 = steadytext.generate(\"Write a haiku about programming\", seed=456)\n\nprint(\"Baseline:\", baseline)\nprint(\"Variation 1:\", variation1)\nprint(\"Variation 2:\", variation2)\n\n# A/B testing - consistent results for testing\ntest_prompt = \"Explain machine learning to a beginner\"\nversion_a = steadytext.generate(test_prompt, seed=100)  # Version A\nversion_b = steadytext.generate(test_prompt, seed=200)  # Version B\n\n# Reproducible research - document your seeds\nresearch_seed = 42\nresults = []\nfor prompt in research_prompts:\n    result = steadytext.generate(prompt, seed=research_seed)\n    results.append((prompt, result))\n    research_seed += 1  # Increment for each prompt\n</code></pre>"},{"location":"api/generation/#caching","title":"Caching","text":"<p>Results are automatically cached using a frecency cache (LRU + frequency), with seed as part of the cache key:</p> <pre><code># First call: generates and caches result for default seed\ntext1 = steadytext.generate(\"common prompt\")  # ~2 seconds\n\n# Second call with same seed: returns cached result  \ntext2 = steadytext.generate(\"common prompt\")  # ~0.1 seconds\nassert text1 == text2  # Same result, much faster\n\n# Different seed: generates new result and caches separately\ntext3 = steadytext.generate(\"common prompt\", seed=123)  # ~2 seconds (new cache entry)\ntext4 = steadytext.generate(\"common prompt\", seed=123)  # ~0.1 seconds (cached)\n\nassert text3 == text4  # Same seed, same cached result\nassert text1 != text3  # Different seeds, different results\n\n# Cache keys include seed, so each seed gets its own cache entry\nfor seed in [100, 200, 300]:\n    steadytext.generate(\"warm up cache\", seed=seed)  # Each gets cached separately\n</code></pre>"},{"location":"api/generation/#fallback-behavior","title":"Fallback Behavior","text":"<p>When models can't be loaded, deterministic fallbacks are used with seed support:</p> <pre><code># Even without models, these return deterministic results based on seed\ntext1 = steadytext.generate(\"test prompt\", seed=42)  # Hash-based fallback\ntext2 = steadytext.generate(\"test prompt\", seed=42)  # Same result\ntext3 = steadytext.generate(\"test prompt\", seed=123) # Different result\n\nassert len(text1) &gt; 0  # Always has content\nassert text1 == text2  # Same seed, same fallback\nassert text1 != text3  # Different seed, different fallback\n\n# Fallback respects custom seeds for variation\nfallback_texts = []\nfor seed in [100, 200, 300]:\n    text = steadytext.generate(\"fallback test\", seed=seed)\n    fallback_texts.append(text)\n\n# All different due to different seeds\nassert len(set(fallback_texts)) == 3\n</code></pre>"},{"location":"api/generation/#performance-tips","title":"Performance Tips","text":"<p>Optimization Strategies</p> <ul> <li>Preload models: Call <code>steadytext.preload_models()</code> at startup</li> <li>Batch processing: Use <code>generate()</code> for multiple prompts rather than streaming individual tokens</li> <li>Cache warmup: Pre-generate common prompts to populate cache</li> <li>Memory management: Models stay loaded once initialized (singleton pattern)</li> <li>Seed management: Use consistent seeds for reproducible results, different seeds for variation</li> <li>Length control: Use <code>max_new_tokens</code> to control response length and generation time</li> </ul>"},{"location":"api/generation/#error-handling-and-edge-cases","title":"Error Handling and Edge Cases","text":""},{"location":"api/generation/#handling-invalid-inputs","title":"Handling Invalid Inputs","text":"<pre><code>import steadytext\n\n# Empty prompt handling\nempty_result = steadytext.generate(\"\")\nprint(f\"Empty prompt result: {empty_result[:50]}...\")  # Still generates deterministic output\n\n# Very long prompt handling (truncated to model's context window)\nlong_prompt = \"Explain \" * 1000 + \"machine learning\"\nresult = steadytext.generate(long_prompt)\nprint(f\"Long prompt handled: {len(result)} chars generated\")\n\n# Special characters and Unicode\nunicode_result = steadytext.generate(\"Write about \ud83e\udd16 and \u4eba\u5de5\u667a\u80fd\")\nprint(f\"Unicode handled: {unicode_result[:100]}...\")\n\n# Newlines and formatting\nmultiline = steadytext.generate(\"\"\"Write a function that:\n1. Takes a list\n2. Sorts it\n3. Returns the result\"\"\")\nprint(f\"Multiline prompt: {multiline[:100]}...\")\n</code></pre>"},{"location":"api/generation/#memory-efficient-streaming","title":"Memory-Efficient Streaming","text":"<pre><code>import sys\n\ndef stream_large_generation(prompt: str, max_chunks: int = 100):\n    \"\"\"Stream generation with memory tracking.\"\"\"\n    chunks = []\n    total_tokens = 0\n\n    for i, token in enumerate(steadytext.generate_iter(prompt)):\n        chunks.append(token)\n        total_tokens += 1\n\n        # Process in batches to manage memory\n        if len(chunks) &gt;= max_chunks:\n            # Process chunk (e.g., write to file)\n            sys.stdout.write(\"\".join(chunks))\n            sys.stdout.flush()\n            chunks = []\n\n    # Process remaining\n    if chunks:\n        sys.stdout.write(\"\".join(chunks))\n\n    print(f\"\\nGenerated {total_tokens} tokens\")\n\n# Use for large generations\nstream_large_generation(\"Write a comprehensive guide to Python programming\")\n</code></pre>"},{"location":"api/generation/#concurrent-generation","title":"Concurrent Generation","text":"<pre><code>import concurrent.futures\nimport steadytext\n\ndef parallel_generation(prompts: list, max_workers: int = 4):\n    \"\"\"Generate text for multiple prompts in parallel.\"\"\"\n    results = {}\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all tasks\n        future_to_prompt = {\n            executor.submit(steadytext.generate, prompt, seed=idx): (prompt, idx)\n            for idx, prompt in enumerate(prompts)\n        }\n\n        # Collect results as they complete\n        for future in concurrent.futures.as_completed(future_to_prompt):\n            prompt, idx = future_to_prompt[future]\n            try:\n                result = future.result()\n                results[prompt] = result\n                print(f\"\u2713 Completed prompt {idx+1}: {prompt[:30]}...\")\n            except Exception as e:\n                print(f\"\u2717 Failed prompt {idx+1}: {e}\")\n                results[prompt] = None\n\n    return results\n\n# Example usage\nprompts = [\n    \"Write a Python function for sorting\",\n    \"Explain machine learning\",\n    \"Create a REST API example\",\n    \"Describe quantum computing\"\n]\n\nresults = parallel_generation(prompts)\nfor prompt, result in results.items():\n    print(f\"\\n{prompt}:\\n{result[:100]}...\\n\")\n</code></pre>"},{"location":"api/generation/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"api/generation/#custom-generation-pipeline","title":"Custom Generation Pipeline","text":"<pre><code>import steadytext\nimport re\n\nclass TextGenerator:\n    \"\"\"Custom text generation pipeline with preprocessing and postprocessing.\"\"\"\n\n    def __init__(self, default_seed: int = 42):\n        self.default_seed = default_seed\n        self.generation_count = 0\n\n    def preprocess(self, prompt: str) -&gt; str:\n        \"\"\"Clean and prepare prompt.\"\"\"\n        # Remove extra whitespace\n        prompt = \" \".join(prompt.split())\n\n        # Add context if needed\n        if not prompt.endswith((\".\", \"?\", \"!\", \":\")):\n            prompt += \":\"\n\n        return prompt\n\n    def postprocess(self, text: str) -&gt; str:\n        \"\"\"Clean generated text.\"\"\"\n        # Remove any [EOS] markers\n        text = text.replace(\"[EOS]\", \"\")\n\n        # Clean up whitespace\n        text = re.sub(r'\\s+', ' ', text).strip()\n\n        return text\n\n    def generate(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate with pre/post processing.\"\"\"\n        # Use incremental seeds for variety\n        seed = kwargs.pop('seed', self.default_seed + self.generation_count)\n        self.generation_count += 1\n\n        # Process\n        cleaned_prompt = self.preprocess(prompt)\n        raw_output = steadytext.generate(cleaned_prompt, seed=seed, **kwargs)\n        final_output = self.postprocess(raw_output)\n\n        return final_output\n\n# Usage\ngenerator = TextGenerator()\n\n# Generates different outputs due to incremental seeding\nresponse1 = generator.generate(\"write a function\")\nresponse2 = generator.generate(\"write a function\")  # Different seed\nresponse3 = generator.generate(\"write a function\", seed=100)  # Custom seed\n\nprint(f\"Response 1: {response1[:50]}...\")\nprint(f\"Response 2: {response2[:50]}...\")\nprint(f\"Response 3: {response3[:50]}...\")\n</code></pre>"},{"location":"api/generation/#template-based-generation","title":"Template-Based Generation","text":"<pre><code>import steadytext\nfrom typing import Dict, Any\n\nclass TemplateGenerator:\n    \"\"\"Generate text using templates with variable substitution.\"\"\"\n\n    def __init__(self):\n        self.templates = {\n            \"function\": \"Write a Python function that {action} for {input_type} and returns {output_type}\",\n            \"explanation\": \"Explain {concept} in simple terms for {audience}\",\n            \"comparison\": \"Compare and contrast {item1} and {item2} in terms of {criteria}\",\n            \"tutorial\": \"Create a step-by-step tutorial on {topic} for {skill_level} programmers\"\n        }\n\n    def generate_from_template(self, template_name: str, variables: Dict[str, Any], \n                             seed: int = 42, **kwargs) -&gt; str:\n        \"\"\"Generate text from a template with variables.\"\"\"\n        if template_name not in self.templates:\n            raise ValueError(f\"Unknown template: {template_name}\")\n\n        # Fill template\n        template = self.templates[template_name]\n        prompt = template.format(**variables)\n\n        # Generate\n        return steadytext.generate(prompt, seed=seed, **kwargs)\n\n    def batch_generate(self, template_name: str, variable_sets: list, \n                      base_seed: int = 42) -&gt; list:\n        \"\"\"Generate multiple outputs from the same template.\"\"\"\n        results = []\n\n        for i, variables in enumerate(variable_sets):\n            # Use different seed for each to ensure variety\n            result = self.generate_from_template(\n                template_name, \n                variables, \n                seed=base_seed + i\n            )\n            results.append({\n                \"variables\": variables,\n                \"output\": result\n            })\n\n        return results\n\n# Usage examples\ngen = TemplateGenerator()\n\n# Single generation\nfunction_code = gen.generate_from_template(\n    \"function\",\n    {\n        \"action\": \"calculates factorial\",\n        \"input_type\": \"positive integer\",\n        \"output_type\": \"integer\"\n    }\n)\nprint(f\"Generated function:\\n{function_code[:200]}...\\n\")\n\n# Batch generation with variations\ntutorials = gen.batch_generate(\n    \"tutorial\",\n    [\n        {\"topic\": \"async programming\", \"skill_level\": \"beginner\"},\n        {\"topic\": \"decorators\", \"skill_level\": \"intermediate\"},\n        {\"topic\": \"metaclasses\", \"skill_level\": \"advanced\"}\n    ]\n)\n\nfor tutorial in tutorials:\n    print(f\"\\nTopic: {tutorial['variables']['topic']}\")\n    print(f\"Output: {tutorial['output'][:150]}...\")\n</code></pre>"},{"location":"api/generation/#context-aware-generation","title":"Context-Aware Generation","text":"<pre><code>import steadytext\nfrom collections import deque\n\nclass ContextualGenerator:\n    \"\"\"Maintain context across multiple generations.\"\"\"\n\n    def __init__(self, context_window: int = 5):\n        self.context = deque(maxlen=context_window)\n        self.base_seed = 42\n        self.generation_count = 0\n\n    def add_context(self, text: str):\n        \"\"\"Add text to context history.\"\"\"\n        self.context.append(text)\n\n    def generate_with_context(self, prompt: str, include_context: bool = True) -&gt; str:\n        \"\"\"Generate text considering previous context.\"\"\"\n        if include_context and self.context:\n            # Build context prompt\n            context_str = \"\\n\".join(f\"Previous: {ctx}\" for ctx in self.context)\n            full_prompt = f\"{context_str}\\n\\nNow: {prompt}\"\n        else:\n            full_prompt = prompt\n\n        # Generate with unique seed\n        result = steadytext.generate(\n            full_prompt, \n            seed=self.base_seed + self.generation_count\n        )\n        self.generation_count += 1\n\n        # Add to context for next generation\n        self.add_context(f\"{prompt} -&gt; {result[:100]}...\")\n\n        return result\n\n    def clear_context(self):\n        \"\"\"Reset context history.\"\"\"\n        self.context.clear()\n        self.generation_count = 0\n\n# Example: Story continuation\nstory_gen = ContextualGenerator()\n\n# Generate story parts with context\npart1 = story_gen.generate_with_context(\"Once upon a time in a digital kingdom\")\nprint(f\"Part 1: {part1[:150]}...\\n\")\n\npart2 = story_gen.generate_with_context(\"The hero discovered a mysterious artifact\")\nprint(f\"Part 2 (with context): {part2[:150]}...\\n\")\n\npart3 = story_gen.generate_with_context(\"Suddenly, the artifact began to glow\")\nprint(f\"Part 3 (with context): {part3[:150]}...\\n\")\n\n# Generate without context for comparison\nstory_gen.clear_context()\npart3_no_context = story_gen.generate_with_context(\n    \"Suddenly, the artifact began to glow\", \n    include_context=False\n)\nprint(f\"Part 3 (no context): {part3_no_context[:150]}...\")\n</code></pre>"},{"location":"api/generation/#debugging-and-monitoring","title":"Debugging and Monitoring","text":""},{"location":"api/generation/#generation-analytics","title":"Generation Analytics","text":"<pre><code>import steadytext\nimport time\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass GenerationMetrics:\n    prompt: str\n    seed: int\n    duration: float\n    token_count: int\n    cached: bool\n    output_preview: str\n\nclass GenerationMonitor:\n    \"\"\"Monitor and analyze generation patterns.\"\"\"\n\n    def __init__(self):\n        self.metrics: List[GenerationMetrics] = []\n\n    def generate_with_metrics(self, prompt: str, seed: int = 42, **kwargs) -&gt; str:\n        \"\"\"Generate text while collecting metrics.\"\"\"\n        start_time = time.time()\n\n        # Check if likely cached (by doing a duplicate call)\n        _ = steadytext.generate(prompt, seed=seed, **kwargs)\n        check_time = time.time() - start_time\n\n        # Actual generation\n        start_time = time.time()\n        result = steadytext.generate(prompt, seed=seed, **kwargs)\n        duration = time.time() - start_time\n\n        # Determine if it was cached\n        cached = duration &lt; check_time * 0.5  # Much faster = likely cached\n\n        # Count tokens (approximate)\n        token_count = len(result.split())\n\n        # Store metrics\n        metric = GenerationMetrics(\n            prompt=prompt,\n            seed=seed,\n            duration=duration,\n            token_count=token_count,\n            cached=cached,\n            output_preview=result[:50] + \"...\"\n        )\n        self.metrics.append(metric)\n\n        return result\n\n    def get_summary(self):\n        \"\"\"Get generation performance summary.\"\"\"\n        if not self.metrics:\n            return \"No generations recorded\"\n\n        total_time = sum(m.duration for m in self.metrics)\n        cached_count = sum(1 for m in self.metrics if m.cached)\n        avg_tokens = sum(m.token_count for m in self.metrics) / len(self.metrics)\n\n        return f\"\"\"\nGeneration Summary:\n- Total generations: {len(self.metrics)}\n- Total time: {total_time:.2f}s\n- Average time: {total_time/len(self.metrics):.3f}s\n- Cached hits: {cached_count} ({cached_count/len(self.metrics)*100:.1f}%)\n- Average tokens: {avg_tokens:.0f}\n\"\"\"\n\n# Example usage\nmonitor = GenerationMonitor()\n\n# Generate with monitoring\nprompts = [\n    \"Write a Python function\",\n    \"Write a Python function\",  # Duplicate - should be cached\n    \"Explain recursion\",\n    \"Write a Python function\",  # Another duplicate\n    \"Create a class example\"\n]\n\nfor prompt in prompts:\n    result = monitor.generate_with_metrics(prompt)\n    print(f\"Generated for '{prompt[:20]}...': {len(result)} chars\")\n\nprint(monitor.get_summary())\n\n# Show detailed metrics\nprint(\"\\nDetailed Metrics:\")\nfor i, metric in enumerate(monitor.metrics, 1):\n    print(f\"{i}. {metric.prompt[:30]}... - {metric.duration:.3f}s \"\n          f\"{'(cached)' if metric.cached else '(computed)'}\")\n</code></pre>"},{"location":"api/generation/#integration-examples","title":"Integration Examples","text":""},{"location":"api/generation/#flask-web-service","title":"Flask Web Service","text":"<pre><code>from flask import Flask, request, jsonify\nimport steadytext\n\napp = Flask(__name__)\n\n@app.route('/generate', methods=['POST'])\ndef generate_text():\n    \"\"\"API endpoint for text generation.\"\"\"\n    data = request.get_json()\n\n    # Extract parameters\n    prompt = data.get('prompt', '')\n    seed = data.get('seed', 42)\n    max_tokens = data.get('max_tokens', 512)\n\n    if not prompt:\n        return jsonify({'error': 'No prompt provided'}), 400\n\n    try:\n        # Generate text\n        result = steadytext.generate(\n            prompt, \n            seed=seed,\n            max_new_tokens=max_tokens\n        )\n\n        return jsonify({\n            'prompt': prompt,\n            'seed': seed,\n            'generated_text': result,\n            'token_count': len(result.split())\n        })\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate/stream', methods=['POST'])\ndef stream_text():\n    \"\"\"SSE endpoint for streaming generation.\"\"\"\n    from flask import Response\n\n    data = request.get_json()\n    prompt = data.get('prompt', '')\n    seed = data.get('seed', 42)\n\n    def generate():\n        yield \"data: {\\\"status\\\": \\\"starting\\\"}\\n\\n\"\n\n        for token in steadytext.generate_iter(prompt, seed=seed):\n            # Escape token for JSON\n            escaped = token.replace('\"', '\\\\\"').replace('\\n', '\\\\n')\n            yield f\"data: {{\\\"token\\\": \\\"{escaped}\\\"}}\\n\\n\"\n\n        yield \"data: {\\\"status\\\": \\\"complete\\\"}\\n\\n\"\n\n    return Response(generate(), mimetype=\"text/event-stream\")\n\n# Run with: flask run\n</code></pre>"},{"location":"api/generation/#async-generation-with-asyncio","title":"Async Generation with asyncio","text":"<pre><code>import asyncio\nimport steadytext\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass AsyncGenerator:\n    \"\"\"Async wrapper for SteadyText generation.\"\"\"\n\n    def __init__(self, max_workers: int = 4):\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n\n    async def generate_async(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate text asynchronously.\"\"\"\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            self.executor,\n            steadytext.generate,\n            prompt,\n            *kwargs.values()\n        )\n        return result\n\n    async def generate_many(self, prompts: list, base_seed: int = 42) -&gt; list:\n        \"\"\"Generate multiple texts concurrently.\"\"\"\n        tasks = [\n            self.generate_async(prompt, seed=base_seed + i)\n            for i, prompt in enumerate(prompts)\n        ]\n        return await asyncio.gather(*tasks)\n\n    def cleanup(self):\n        \"\"\"Cleanup executor.\"\"\"\n        self.executor.shutdown(wait=True)\n\n# Example usage\nasync def main():\n    generator = AsyncGenerator()\n\n    # Single async generation\n    result = await generator.generate_async(\"Write async Python code\")\n    print(f\"Single result: {result[:100]}...\\n\")\n\n    # Batch async generation\n    prompts = [\n        \"Explain async/await\",\n        \"Write a coroutine example\",\n        \"Describe event loops\",\n        \"Create an async API client\"\n    ]\n\n    start = asyncio.get_event_loop().time()\n    results = await generator.generate_many(prompts)\n    duration = asyncio.get_event_loop().time() - start\n\n    print(f\"Generated {len(results)} texts in {duration:.2f}s\")\n    for i, (prompt, result) in enumerate(zip(prompts, results)):\n        print(f\"\\n{i+1}. {prompt}:\\n{result[:100]}...\")\n\n    generator.cleanup()\n\n# Run the async example\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"deployment/cloudflare/","title":"Edge Deployment with Cloudflare Workers","text":"<p>Deploy SteadyText to the edge for global, low-latency AI inference using Cloudflare Workers and D1.</p>"},{"location":"deployment/cloudflare/#overview","title":"Overview","text":"<p>Run SteadyText at the edge with: - Global distribution across 300+ cities - Zero cold starts with Workers - D1 database for distributed caching - Durable Objects for stateful AI operations - R2 storage for model distribution</p>"},{"location":"deployment/cloudflare/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    User[User Request] --&gt; CF[Cloudflare Edge]\n\n    subgraph \"Cloudflare Network\"\n        CF --&gt; Worker[SteadyText Worker]\n        Worker --&gt; D1[(D1 Cache)]\n        Worker --&gt; DO[Durable Objects]\n        Worker --&gt; R2[R2 Model Storage]\n\n        Worker --&gt; KV[KV Metadata]\n    end\n\n    Worker --&gt; Response[AI Response]\n</code></pre>"},{"location":"deployment/cloudflare/#prerequisites","title":"Prerequisites","text":"<pre><code># Install Wrangler CLI\nnpm install -g wrangler\n\n# Authenticate with Cloudflare\nwrangler login\n\n# Create a new project\nwrangler init steadytext-edge\ncd steadytext-edge\n</code></pre>"},{"location":"deployment/cloudflare/#worker-implementation","title":"Worker Implementation","text":""},{"location":"deployment/cloudflare/#basic-worker-setup","title":"Basic Worker Setup","text":"<pre><code>// src/index.js\nimport { SteadyTextEdge } from '@steadytext/edge';\n\nexport default {\n  async fetch(request, env, ctx) {\n    const steadytext = new SteadyTextEdge({\n      d1Database: env.STEADYTEXT_D1,\n      kvNamespace: env.STEADYTEXT_KV,\n      r2Bucket: env.STEADYTEXT_R2,\n    });\n\n    const url = new URL(request.url);\n\n    // Handle different endpoints\n    switch (url.pathname) {\n      case '/generate':\n        return handleGenerate(request, steadytext);\n\n      case '/embed':\n        return handleEmbed(request, steadytext);\n\n      case '/health':\n        return new Response('OK', { status: 200 });\n\n      default:\n        return new Response('Not Found', { status: 404 });\n    }\n  }\n};\n\nasync function handleGenerate(request, steadytext) {\n  try {\n    const { prompt, max_tokens = 100 } = await request.json();\n\n    if (!prompt) {\n      return new Response(\n        JSON.stringify({ error: 'Prompt required' }), \n        { \n          status: 400,\n          headers: { 'Content-Type': 'application/json' }\n        }\n      );\n    }\n\n    // Check cache first\n    const cacheKey = `gen:${prompt}:${max_tokens}`;\n    const cached = await steadytext.getFromCache(cacheKey);\n\n    if (cached) {\n      return new Response(\n        JSON.stringify({ \n          text: cached, \n          cached: true \n        }), \n        { \n          headers: { 'Content-Type': 'application/json' }\n        }\n      );\n    }\n\n    // Generate new response\n    const result = await steadytext.generate(prompt, { max_tokens });\n\n    // Cache the result\n    ctx.waitUntil(\n      steadytext.setCache(cacheKey, result)\n    );\n\n    return new Response(\n      JSON.stringify({ \n        text: result, \n        cached: false \n      }), \n      { \n        headers: { 'Content-Type': 'application/json' }\n      }\n    );\n  } catch (error) {\n    return new Response(\n      JSON.stringify({ error: error.message }), \n      { \n        status: 500,\n        headers: { 'Content-Type': 'application/json' }\n      }\n    );\n  }\n}\n\nasync function handleEmbed(request, steadytext) {\n  try {\n    const { text } = await request.json();\n\n    if (!text) {\n      return new Response(\n        JSON.stringify({ error: 'Text required' }), \n        { \n          status: 400,\n          headers: { 'Content-Type': 'application/json' }\n        }\n      );\n    }\n\n    const embedding = await steadytext.embed(text);\n\n    return new Response(\n      JSON.stringify({ embedding }), \n      { \n        headers: { 'Content-Type': 'application/json' }\n      }\n    );\n  } catch (error) {\n    return new Response(\n      JSON.stringify({ error: error.message }), \n      { \n        status: 500,\n        headers: { 'Content-Type': 'application/json' }\n      }\n    );\n  }\n}\n</code></pre>"},{"location":"deployment/cloudflare/#configuration-wranglertoml","title":"Configuration (wrangler.toml)","text":"<pre><code>name = \"steadytext-edge\"\nmain = \"src/index.js\"\ncompatibility_date = \"2024-01-01\"\n\n# D1 Database binding\n[[d1_databases]]\nbinding = \"STEADYTEXT_D1\"\ndatabase_name = \"steadytext-cache\"\ndatabase_id = \"your-database-id\"\n\n# KV Namespace binding\n[[kv_namespaces]]\nbinding = \"STEADYTEXT_KV\"\nid = \"your-kv-namespace-id\"\n\n# R2 Bucket binding\n[[r2_buckets]]\nbinding = \"STEADYTEXT_R2\"\nbucket_name = \"steadytext-models\"\n\n# Durable Objects\n[[durable_objects.bindings]]\nname = \"AI_ENGINE\"\nclass_name = \"SteadyTextEngine\"\nscript_name = \"steadytext-engine\"\n\n# Environment variables\n[vars]\nSTEADYTEXT_MODEL = \"gemma-3n-edge\"\nSTEADYTEXT_MAX_TOKENS = \"512\"\nSTEADYTEXT_CACHE_TTL = \"86400\"\n\n# Performance settings\n[build]\ncommand = \"npm run build\"\n\n[miniflare]\nkv_persist = true\nd1_persist = true\nr2_persist = true\n</code></pre>"},{"location":"deployment/cloudflare/#d1-database-setup","title":"D1 Database Setup","text":""},{"location":"deployment/cloudflare/#create-schema","title":"Create Schema","text":"<pre><code>-- Create D1 database\nwrangler d1 create steadytext-cache\n\n-- Apply schema\nwrangler d1 execute steadytext-cache --file=./schema.sql\n</code></pre> <pre><code>-- schema.sql\nCREATE TABLE IF NOT EXISTS generation_cache (\n    cache_key TEXT PRIMARY KEY,\n    prompt TEXT NOT NULL,\n    parameters TEXT,\n    result TEXT NOT NULL,\n    token_count INTEGER,\n    created_at INTEGER DEFAULT (unixepoch()),\n    accessed_at INTEGER DEFAULT (unixepoch()),\n    access_count INTEGER DEFAULT 1\n);\n\nCREATE INDEX idx_cache_created ON generation_cache(created_at);\nCREATE INDEX idx_cache_accessed ON generation_cache(accessed_at);\n\nCREATE TABLE IF NOT EXISTS embedding_cache (\n    text_hash TEXT PRIMARY KEY,\n    text TEXT NOT NULL,\n    embedding BLOB NOT NULL,\n    dimension INTEGER,\n    created_at INTEGER DEFAULT (unixepoch())\n);\n\nCREATE TABLE IF NOT EXISTS request_metrics (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    endpoint TEXT NOT NULL,\n    duration_ms INTEGER,\n    cached BOOLEAN,\n    timestamp INTEGER DEFAULT (unixepoch()),\n    region TEXT\n);\n</code></pre>"},{"location":"deployment/cloudflare/#cache-management","title":"Cache Management","text":"<pre><code>// src/cache.js\nexport class EdgeCache {\n  constructor(d1, kv) {\n    this.d1 = d1;\n    this.kv = kv;\n  }\n\n  async get(key) {\n    // Try KV first (faster)\n    const kvResult = await this.kv.get(key);\n    if (kvResult) {\n      return JSON.parse(kvResult);\n    }\n\n    // Fall back to D1\n    const { results } = await this.d1\n      .prepare('SELECT result FROM generation_cache WHERE cache_key = ?')\n      .bind(key)\n      .first();\n\n    if (results) {\n      // Update access time and count\n      await this.d1\n        .prepare(`\n          UPDATE generation_cache \n          SET accessed_at = unixepoch(), \n              access_count = access_count + 1 \n          WHERE cache_key = ?\n        `)\n        .bind(key)\n        .run();\n\n      // Promote to KV if frequently accessed\n      if (results.access_count &gt; 10) {\n        await this.kv.put(key, results.result, {\n          expirationTtl: 3600 // 1 hour in KV\n        });\n      }\n\n      return results.result;\n    }\n\n    return null;\n  }\n\n  async set(key, value, ttl = 86400) {\n    // Store in D1 for persistence\n    await this.d1\n      .prepare(`\n        INSERT OR REPLACE INTO generation_cache \n        (cache_key, prompt, result, created_at) \n        VALUES (?, ?, ?, unixepoch())\n      `)\n      .bind(key, key.split(':')[1], JSON.stringify(value))\n      .run();\n\n    // Also store in KV for fast access\n    await this.kv.put(key, JSON.stringify(value), {\n      expirationTtl: ttl\n    });\n  }\n\n  async cleanup(maxAge = 2592000) { // 30 days\n    const cutoff = Date.now() / 1000 - maxAge;\n\n    await this.d1\n      .prepare('DELETE FROM generation_cache WHERE accessed_at &lt; ?')\n      .bind(cutoff)\n      .run();\n  }\n}\n</code></pre>"},{"location":"deployment/cloudflare/#durable-objects-for-stateful-ai","title":"Durable Objects for Stateful AI","text":"<pre><code>// src/engine.js\nexport class SteadyTextEngine {\n  constructor(state, env) {\n    this.state = state;\n    this.env = env;\n    this.model = null;\n    this.initPromise = null;\n  }\n\n  async initialize() {\n    if (this.model) return;\n\n    if (!this.initPromise) {\n      this.initPromise = this.loadModel();\n    }\n\n    await this.initPromise;\n  }\n\n  async loadModel() {\n    // Load model from R2\n    const modelData = await this.env.STEADYTEXT_R2.get('models/gemma-3n-edge.bin');\n\n    if (!modelData) {\n      throw new Error('Model not found in R2');\n    }\n\n    // Initialize model (simplified)\n    this.model = await createModel(await modelData.arrayBuffer());\n  }\n\n  async fetch(request) {\n    await this.initialize();\n\n    const { method, params } = await request.json();\n\n    switch (method) {\n      case 'generate':\n        return this.handleGenerate(params);\n\n      case 'embed':\n        return this.handleEmbed(params);\n\n      default:\n        return new Response(\n          JSON.stringify({ error: 'Unknown method' }), \n          { status: 400 }\n        );\n    }\n  }\n\n  async handleGenerate({ prompt, max_tokens = 100 }) {\n    // Use the model for generation\n    const result = await this.model.generate(prompt, {\n      max_tokens,\n      temperature: 0, // Deterministic\n      seed: 42\n    });\n\n    return new Response(\n      JSON.stringify({ text: result }), \n      { \n        headers: { 'Content-Type': 'application/json' }\n      }\n    );\n  }\n\n  async handleEmbed({ text }) {\n    const embedding = await this.model.embed(text);\n\n    return new Response(\n      JSON.stringify({ embedding: Array.from(embedding) }), \n      { \n        headers: { 'Content-Type': 'application/json' }\n      }\n    );\n  }\n}\n</code></pre>"},{"location":"deployment/cloudflare/#model-distribution-with-r2","title":"Model Distribution with R2","text":"<pre><code># Upload model to R2\nwrangler r2 object put steadytext-models/models/gemma-3n-edge.bin \\\n  --file ./models/gemma-3n-edge.bin\n\n# List models\nwrangler r2 object list steadytext-models\n</code></pre>"},{"location":"deployment/cloudflare/#model-version-management","title":"Model Version Management","text":"<pre><code>// src/models.js\nexport class ModelManager {\n  constructor(r2Bucket, kvNamespace) {\n    this.r2 = r2Bucket;\n    this.kv = kvNamespace;\n  }\n\n  async getLatestModel(modelName) {\n    // Check current version in KV\n    const version = await this.kv.get(`model_version:${modelName}`);\n\n    if (!version) {\n      throw new Error(`Model ${modelName} not found`);\n    }\n\n    // Get model from R2\n    const modelPath = `models/${modelName}-${version}.bin`;\n    const model = await this.r2.get(modelPath);\n\n    if (!model) {\n      throw new Error(`Model file ${modelPath} not found`);\n    }\n\n    return {\n      data: await model.arrayBuffer(),\n      version,\n      metadata: JSON.parse(model.customMetadata || '{}')\n    };\n  }\n\n  async updateModelVersion(modelName, newVersion) {\n    // Update version in KV\n    await this.kv.put(\n      `model_version:${modelName}`, \n      newVersion,\n      {\n        metadata: {\n          updated: new Date().toISOString()\n        }\n      }\n    );\n  }\n}\n</code></pre>"},{"location":"deployment/cloudflare/#performance-optimization","title":"Performance Optimization","text":""},{"location":"deployment/cloudflare/#request-coalescing","title":"Request Coalescing","text":"<pre><code>// src/coalesce.js\nexport class RequestCoalescer {\n  constructor() {\n    this.pending = new Map();\n  }\n\n  async coalesce(key, fn) {\n    // Check if request is already pending\n    if (this.pending.has(key)) {\n      return this.pending.get(key);\n    }\n\n    // Create new promise\n    const promise = fn();\n    this.pending.set(key, promise);\n\n    try {\n      const result = await promise;\n      return result;\n    } finally {\n      this.pending.delete(key);\n    }\n  }\n}\n\n// Usage in worker\nconst coalescer = new RequestCoalescer();\n\nasync function handleGenerate(request, steadytext) {\n  const { prompt, max_tokens } = await request.json();\n  const key = `${prompt}:${max_tokens}`;\n\n  const result = await coalescer.coalesce(key, async () =&gt; {\n    return steadytext.generate(prompt, { max_tokens });\n  });\n\n  return new Response(JSON.stringify({ text: result }));\n}\n</code></pre>"},{"location":"deployment/cloudflare/#smart-routing","title":"Smart Routing","text":"<pre><code>// src/router.js\nexport class SmartRouter {\n  constructor(env) {\n    this.env = env;\n  }\n\n  async route(request) {\n    const region = request.cf?.colo || 'unknown';\n    const load = await this.getRegionLoad(region);\n\n    if (load &gt; 0.8) {\n      // Route to nearby region\n      return this.routeToAlternate(request);\n    }\n\n    // Process locally\n    return this.processLocal(request);\n  }\n\n  async getRegionLoad(region) {\n    const stats = await this.env.STEADYTEXT_KV.get(\n      `load:${region}`,\n      { type: 'json' }\n    );\n\n    return stats?.load || 0;\n  }\n}\n</code></pre>"},{"location":"deployment/cloudflare/#monitoring-and-analytics","title":"Monitoring and Analytics","text":""},{"location":"deployment/cloudflare/#request-tracking","title":"Request Tracking","text":"<pre><code>// src/analytics.js\nexport class EdgeAnalytics {\n  constructor(d1) {\n    this.d1 = d1;\n    this.buffer = [];\n    this.flushInterval = 10000; // 10 seconds\n  }\n\n  async track(event) {\n    this.buffer.push({\n      ...event,\n      timestamp: Date.now(),\n      region: event.cf?.colo || 'unknown'\n    });\n\n    if (this.buffer.length &gt;= 100) {\n      await this.flush();\n    }\n  }\n\n  async flush() {\n    if (this.buffer.length === 0) return;\n\n    const events = this.buffer.splice(0);\n\n    const stmt = this.d1.prepare(`\n      INSERT INTO request_metrics \n      (endpoint, duration_ms, cached, timestamp, region) \n      VALUES (?, ?, ?, ?, ?)\n    `);\n\n    const batch = events.map(e =&gt; \n      stmt.bind(e.endpoint, e.duration, e.cached, e.timestamp, e.region)\n    );\n\n    await this.d1.batch(batch);\n  }\n\n  async getMetrics(hours = 24) {\n    const since = Date.now() / 1000 - (hours * 3600);\n\n    const { results } = await this.d1\n      .prepare(`\n        SELECT \n          endpoint,\n          COUNT(*) as requests,\n          AVG(duration_ms) as avg_duration,\n          SUM(CASE WHEN cached THEN 1 ELSE 0 END) * 100.0 / COUNT(*) as cache_rate,\n          region\n        FROM request_metrics\n        WHERE timestamp &gt; ?\n        GROUP BY endpoint, region\n      `)\n      .bind(since)\n      .all();\n\n    return results;\n  }\n}\n</code></pre>"},{"location":"deployment/cloudflare/#grafana-integration","title":"Grafana Integration","text":"<pre><code>// src/metrics.js\nexport async function handleMetrics(request, env) {\n  const analytics = new EdgeAnalytics(env.STEADYTEXT_D1);\n  const metrics = await analytics.getMetrics(1); // Last hour\n\n  // Format as Prometheus metrics\n  const output = metrics.map(m =&gt; `\nsteadytext_requests_total{endpoint=\"${m.endpoint}\",region=\"${m.region}\"} ${m.requests}\nsteadytext_duration_seconds{endpoint=\"${m.endpoint}\",region=\"${m.region}\"} ${m.avg_duration / 1000}\nsteadytext_cache_rate{endpoint=\"${m.endpoint}\",region=\"${m.region}\"} ${m.cache_rate}\n  `).join('\\n');\n\n  return new Response(output, {\n    headers: { 'Content-Type': 'text/plain' }\n  });\n}\n</code></pre>"},{"location":"deployment/cloudflare/#deployment","title":"Deployment","text":""},{"location":"deployment/cloudflare/#deploy-to-production","title":"Deploy to Production","text":"<pre><code># Deploy to Cloudflare\nwrangler deploy\n\n# Deploy to specific environment\nwrangler deploy --env production\n\n# Test deployment\ncurl https://steadytext-edge.your-subdomain.workers.dev/health\n</code></pre>"},{"location":"deployment/cloudflare/#custom-domain","title":"Custom Domain","text":"<pre><code># Add custom domain\nwrangler domains add steadytext-api.yourdomain.com\n\n# Update wrangler.toml\nroutes = [\n  { pattern = \"steadytext-api.yourdomain.com/*\", zone_name = \"yourdomain.com\" }\n]\n</code></pre>"},{"location":"deployment/cloudflare/#cost-optimization","title":"Cost Optimization","text":""},{"location":"deployment/cloudflare/#resource-limits","title":"Resource Limits","text":"<pre><code># wrangler.toml\n[limits]\ncpu_ms = 50  # 50ms CPU time per request\nmemory_mb = 128  # 128MB memory\n\n[build]\nminify = true\nnode_compat = false\n</code></pre>"},{"location":"deployment/cloudflare/#caching-strategy","title":"Caching Strategy","text":"<pre><code>// Aggressive caching for common queries\nconst CACHE_RULES = {\n  '/generate': {\n    common_prompts: 86400,    // 24 hours\n    user_prompts: 3600,       // 1 hour\n    max_cache_size: 10000\n  },\n  '/embed': {\n    all: 604800,              // 7 days\n    max_cache_size: 50000\n  }\n};\n</code></pre>"},{"location":"deployment/cloudflare/#security","title":"Security","text":""},{"location":"deployment/cloudflare/#api-key-authentication","title":"API Key Authentication","text":"<pre><code>async function authenticate(request, env) {\n  const apiKey = request.headers.get('X-API-Key');\n\n  if (!apiKey) {\n    return new Response('Unauthorized', { status: 401 });\n  }\n\n  // Verify against KV\n  const valid = await env.STEADYTEXT_KV.get(`api_key:${apiKey}`);\n\n  if (!valid) {\n    return new Response('Invalid API key', { status: 403 });\n  }\n\n  return null; // Authentication successful\n}\n\n// Use in handler\nexport default {\n  async fetch(request, env, ctx) {\n    const authError = await authenticate(request, env);\n    if (authError) return authError;\n\n    // Continue with request handling...\n  }\n}\n</code></pre>"},{"location":"deployment/cloudflare/#rate-limiting","title":"Rate Limiting","text":"<pre><code>// src/ratelimit.js\nexport class RateLimiter {\n  constructor(kv) {\n    this.kv = kv;\n  }\n\n  async check(key, limit = 100, window = 60) {\n    const now = Date.now();\n    const windowKey = `rate:${key}:${Math.floor(now / (window * 1000))}`;\n\n    const count = await this.kv.get(windowKey, { type: 'json' }) || 0;\n\n    if (count &gt;= limit) {\n      return false;\n    }\n\n    await this.kv.put(\n      windowKey, \n      count + 1, \n      { expirationTtl: window * 2 }\n    );\n\n    return true;\n  }\n}\n</code></pre>"},{"location":"deployment/cloudflare/#testing","title":"Testing","text":""},{"location":"deployment/cloudflare/#local-development","title":"Local Development","text":"<pre><code># Run locally with Miniflare\nwrangler dev\n\n# Test with local D1\nwrangler d1 execute steadytext-cache --local --file=./test-data.sql\n\n# Run tests\nnpm test\n</code></pre>"},{"location":"deployment/cloudflare/#integration-tests","title":"Integration Tests","text":"<pre><code>// test/integration.test.js\nimport { unstable_dev } from 'wrangler';\n\ndescribe('SteadyText Edge Worker', () =&gt; {\n  let worker;\n\n  beforeAll(async () =&gt; {\n    worker = await unstable_dev('src/index.js', {\n      experimental: { disableExperimentalWarning: true }\n    });\n  });\n\n  afterAll(async () =&gt; {\n    await worker.stop();\n  });\n\n  it('should generate text', async () =&gt; {\n    const response = await worker.fetch('/generate', {\n      method: 'POST',\n      headers: { 'Content-Type': 'application/json' },\n      body: JSON.stringify({\n        prompt: 'Hello world',\n        max_tokens: 10\n      })\n    });\n\n    const result = await response.json();\n    expect(result.text).toBeDefined();\n    expect(response.status).toBe(200);\n  });\n});\n</code></pre>"},{"location":"deployment/cloudflare/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/cloudflare/#common-issues","title":"Common Issues","text":"<ol> <li> <p>Model Loading Timeout <pre><code>// Increase timeout for model loading\nexport default {\n  async fetch(request, env, ctx) {\n    ctx.waitUntil(\n      Promise.race([\n        handleRequest(request, env),\n        new Promise((_, reject) =&gt; \n          setTimeout(() =&gt; reject(new Error('Timeout')), 25000)\n        )\n      ])\n    );\n  }\n}\n</code></pre></p> </li> <li> <p>D1 Connection Issues <pre><code>// Retry logic for D1\nasync function withRetry(fn, retries = 3) {\n  for (let i = 0; i &lt; retries; i++) {\n    try {\n      return await fn();\n    } catch (error) {\n      if (i === retries - 1) throw error;\n      await new Promise(r =&gt; setTimeout(r, 100 * Math.pow(2, i)));\n    }\n  }\n}\n</code></pre></p> </li> <li> <p>Memory Limits <pre><code>// Stream large responses\nfunction streamResponse(text) {\n  const encoder = new TextEncoder();\n  const stream = new ReadableStream({\n    start(controller) {\n      controller.enqueue(encoder.encode(text));\n      controller.close();\n    }\n  });\n\n  return new Response(stream, {\n    headers: { 'Content-Type': 'text/plain' }\n  });\n}\n</code></pre></p> </li> </ol>"},{"location":"deployment/cloudflare/#next-steps","title":"Next Steps","text":"<ul> <li>Production Deployment \u2192</li> <li>API Reference \u2192</li> <li>Examples \u2192</li> </ul> <p>Edge Best Practices</p> <ul> <li>Use KV for hot data, D1 for persistence</li> <li>Implement request coalescing for identical prompts</li> <li>Monitor CPU time to stay within limits</li> <li>Pre-warm Durable Objects in high-traffic regions</li> <li>Use Cloudflare Cache API for static responses</li> </ul>"},{"location":"deployment/production/","title":"Production Deployment Guide","text":"<p>Deploy SteadyText and pg_steadytext for production workloads with high availability, monitoring, and optimal performance.</p>"},{"location":"deployment/production/#architecture-overview","title":"Architecture Overview","text":"<pre><code>graph TB\n    LB[Load Balancer] --&gt; PG1[PostgreSQL Primary&lt;br/&gt;+ pg_steadytext]\n    LB --&gt; PG2[PostgreSQL Replica 1&lt;br/&gt;+ pg_steadytext]\n    LB --&gt; PG3[PostgreSQL Replica 2&lt;br/&gt;+ pg_steadytext]\n\n    PG1 --&gt; DAEMON1[SteadyText Daemon]\n    PG2 --&gt; DAEMON2[SteadyText Daemon]\n    PG3 --&gt; DAEMON3[SteadyText Daemon]\n\n    DAEMON1 --&gt; MODELS[Shared Model Storage&lt;br/&gt;NFS/S3]\n    DAEMON2 --&gt; MODELS\n    DAEMON3 --&gt; MODELS\n\n    PG1 --&gt; MONITOR[Monitoring&lt;br/&gt;Prometheus/Grafana]\n    PG2 --&gt; MONITOR\n    PG3 --&gt; MONITOR\n</code></pre>"},{"location":"deployment/production/#system-requirements","title":"System Requirements","text":""},{"location":"deployment/production/#hardware-recommendations","title":"Hardware Recommendations","text":"Component Minimum Recommended High Performance CPU 4 cores 8 cores 16+ cores RAM 16 GB 32 GB 64+ GB Storage 100 GB SSD 500 GB NVMe 1+ TB NVMe Network 1 Gbps 10 Gbps 25+ Gbps"},{"location":"deployment/production/#postgresql-version","title":"PostgreSQL Version","text":"<ul> <li>PostgreSQL 14+ (15 or 16 recommended)</li> <li>TimescaleDB 2.10+ (if using time-series features)</li> </ul>"},{"location":"deployment/production/#installation-methods","title":"Installation Methods","text":""},{"location":"deployment/production/#method-1-docker-compose-recommended","title":"Method 1: Docker Compose (Recommended)","text":"<pre><code>version: '3.8'\n\nservices:\n  postgres-primary:\n    image: julep/pg-steadytext:latest\n    container_name: steadytext-primary\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_DB: production\n      STEADYTEXT_DAEMON_AUTOSTART: \"true\"\n      STEADYTEXT_DAEMON_WORKERS: \"4\"\n    volumes:\n      - pgdata-primary:/var/lib/postgresql/data\n      - steadytext-models:/var/lib/steadytext/models\n      - ./postgresql.conf:/etc/postgresql/postgresql.conf:ro\n    ports:\n      - \"5432:5432\"\n    deploy:\n      resources:\n        limits:\n          cpus: '8'\n          memory: 32G\n        reservations:\n          cpus: '4'\n          memory: 16G\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n      interval: 10s\n      timeout: 5s\n      retries: 5\n\n  postgres-replica1:\n    image: julep/pg-steadytext:latest\n    container_name: steadytext-replica1\n    environment:\n      POSTGRES_USER: postgres\n      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}\n      POSTGRES_MASTER_SERVICE: postgres-primary\n      POSTGRES_REPLICATION_MODE: slave\n      POSTGRES_REPLICATION_USER: replicator\n      POSTGRES_REPLICATION_PASSWORD: ${REPLICATION_PASSWORD}\n      STEADYTEXT_DAEMON_AUTOSTART: \"true\"\n    volumes:\n      - pgdata-replica1:/var/lib/postgresql/data\n      - steadytext-models:/var/lib/steadytext/models:ro\n    depends_on:\n      postgres-primary:\n        condition: service_healthy\n    deploy:\n      resources:\n        limits:\n          cpus: '8'\n          memory: 32G\n\n  pgpool:\n    image: bitnami/pgpool:latest\n    container_name: steadytext-pgpool\n    environment:\n      - PGPOOL_BACKEND_NODES=0:postgres-primary:5432,1:postgres-replica1:5432\n      - PGPOOL_SR_CHECK_USER=postgres\n      - PGPOOL_SR_CHECK_PASSWORD=${POSTGRES_PASSWORD}\n      - PGPOOL_ENABLE_LOAD_BALANCING=yes\n      - PGPOOL_MAX_POOL=100\n      - PGPOOL_NUM_INIT_CHILDREN=32\n    ports:\n      - \"5433:5432\"\n    depends_on:\n      - postgres-primary\n      - postgres-replica1\n\n  prometheus:\n    image: prom/prometheus:latest\n    container_name: steadytext-prometheus\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml:ro\n      - prometheus-data:/prometheus\n    ports:\n      - \"9090:9090\"\n\n  grafana:\n    image: grafana/grafana:latest\n    container_name: steadytext-grafana\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}\n    volumes:\n      - grafana-data:/var/lib/grafana\n      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro\n    ports:\n      - \"3000:3000\"\n\nvolumes:\n  pgdata-primary:\n  pgdata-replica1:\n  steadytext-models:\n  prometheus-data:\n  grafana-data:\n</code></pre>"},{"location":"deployment/production/#method-2-kubernetes-deployment","title":"Method 2: Kubernetes Deployment","text":"<pre><code>apiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: postgres-config\ndata:\n  postgresql.conf: |\n    # Performance settings\n    shared_buffers = 8GB\n    effective_cache_size = 24GB\n    maintenance_work_mem = 2GB\n    work_mem = 256MB\n    max_parallel_workers = 8\n    max_parallel_workers_per_gather = 4\n\n    # SteadyText optimizations\n    max_connections = 200\n    shared_preload_libraries = 'pg_steadytext,pg_stat_statements'\n\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: postgres-steadytext\nspec:\n  serviceName: postgres-steadytext\n  replicas: 3\n  selector:\n    matchLabels:\n      app: postgres-steadytext\n  template:\n    metadata:\n      labels:\n        app: postgres-steadytext\n    spec:\n      containers:\n      - name: postgres\n        image: julep/pg-steadytext:latest\n        ports:\n        - containerPort: 5432\n        env:\n        - name: POSTGRES_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: postgres-secret\n              key: password\n        - name: STEADYTEXT_DAEMON_AUTOSTART\n          value: \"true\"\n        resources:\n          requests:\n            memory: \"16Gi\"\n            cpu: \"4\"\n          limits:\n            memory: \"32Gi\"\n            cpu: \"8\"\n        volumeMounts:\n        - name: postgres-storage\n          mountPath: /var/lib/postgresql/data\n        - name: postgres-config\n          mountPath: /etc/postgresql\n        - name: model-storage\n          mountPath: /var/lib/steadytext/models\n  volumeClaimTemplates:\n  - metadata:\n      name: postgres-storage\n    spec:\n      accessModes: [ \"ReadWriteOnce\" ]\n      resources:\n        requests:\n          storage: 500Gi\n  - metadata:\n      name: model-storage\n    spec:\n      accessModes: [ \"ReadWriteMany\" ]\n      resources:\n        requests:\n          storage: 50Gi\n</code></pre>"},{"location":"deployment/production/#method-3-bare-metal-installation","title":"Method 3: Bare Metal Installation","text":"<pre><code># 1. Install PostgreSQL 16\nsudo apt update\nsudo apt install -y postgresql-16 postgresql-16-dev\n\n# 2. Install Python dependencies\nsudo apt install -y python3-pip python3-dev\npip3 install steadytext pyzmq numpy\n\n# 3. Build and install pg_steadytext\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\nmake &amp;&amp; sudo make install\n\n# 4. Configure PostgreSQL\nsudo -u postgres psql &lt;&lt;EOF\nALTER SYSTEM SET shared_preload_libraries = 'pg_steadytext';\nALTER SYSTEM SET shared_buffers = '8GB';\nALTER SYSTEM SET effective_cache_size = '24GB';\nALTER SYSTEM SET work_mem = '256MB';\nALTER SYSTEM SET maintenance_work_mem = '2GB';\nEOF\n\n# 5. Restart PostgreSQL\nsudo systemctl restart postgresql\n\n# 6. Create systemd service for daemon\nsudo tee /etc/systemd/system/steadytext-daemon.service &gt; /dev/null &lt;&lt;EOF\n[Unit]\nDescription=SteadyText Daemon\nAfter=postgresql.service\n\n[Service]\nType=simple\nUser=postgres\nExecStart=/usr/local/bin/st daemon start --foreground\nRestart=always\nRestartSec=10\nEnvironment=\"STEADYTEXT_DAEMON_HOST=0.0.0.0\"\nEnvironment=\"STEADYTEXT_DAEMON_PORT=5555\"\nEnvironment=\"STEADYTEXT_DAEMON_WORKERS=4\"\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl enable steadytext-daemon\nsudo systemctl start steadytext-daemon\n</code></pre>"},{"location":"deployment/production/#performance-configuration","title":"Performance Configuration","text":""},{"location":"deployment/production/#postgresql-tuning","title":"PostgreSQL Tuning","text":"<pre><code>-- postgresql.conf optimizations for AI workloads\n-- Memory settings (adjust based on available RAM)\nshared_buffers = '8GB'              # 25% of RAM\neffective_cache_size = '24GB'       # 75% of RAM\nwork_mem = '256MB'                  # For complex queries\nmaintenance_work_mem = '2GB'        # For index creation\n\n-- Parallel processing\nmax_parallel_workers = 8\nmax_parallel_workers_per_gather = 4\nmax_parallel_maintenance_workers = 4\n\n-- Connection pooling\nmax_connections = 200\nsuperuser_reserved_connections = 3\n\n-- Write performance\ncheckpoint_completion_target = 0.9\nwal_buffers = '64MB'\nmax_wal_size = '4GB'\n\n-- SteadyText specific\nshared_preload_libraries = 'pg_steadytext,pg_stat_statements'\npg_steadytext.cache_size = '10000'\npg_steadytext.daemon_timeout = '60000'  # 60 seconds\n</code></pre>"},{"location":"deployment/production/#steadytext-daemon-configuration","title":"SteadyText Daemon Configuration","text":"<pre><code># Environment variables for optimal performance\nexport STEADYTEXT_DAEMON_WORKERS=4\nexport STEADYTEXT_DAEMON_QUEUE_SIZE=1000\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=10000\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=1000\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=20000\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=2000\nexport STEADYTEXT_MAX_CONTEXT_WINDOW=32768\n\n# Start daemon with production settings\nst daemon start \\\n  --host 0.0.0.0 \\\n  --port 5555 \\\n  --workers $STEADYTEXT_DAEMON_WORKERS \\\n  --log-level info\n</code></pre>"},{"location":"deployment/production/#high-availability-setup","title":"High Availability Setup","text":""},{"location":"deployment/production/#streaming-replication","title":"Streaming Replication","text":"<pre><code># On primary server\nsudo -u postgres psql &lt;&lt;EOF\nCREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'strong_password';\nALTER SYSTEM SET wal_level = 'replica';\nALTER SYSTEM SET max_wal_senders = 3;\nALTER SYSTEM SET wal_keep_size = '1GB';\nEOF\n\n# On replica server\nsudo -u postgres pg_basebackup \\\n  -h primary_host \\\n  -D /var/lib/postgresql/16/main \\\n  -U replicator \\\n  -v -P -W \\\n  -X stream\n\n# Configure recovery\nsudo -u postgres tee /var/lib/postgresql/16/main/postgresql.auto.conf &lt;&lt;EOF\nprimary_conninfo = 'host=primary_host port=5432 user=replicator password=strong_password'\nprimary_slot_name = 'replica1'\nEOF\n\nsudo -u postgres touch /var/lib/postgresql/16/main/standby.signal\n</code></pre>"},{"location":"deployment/production/#load-balancing-with-pgbouncer","title":"Load Balancing with PgBouncer","text":"<pre><code># pgbouncer.ini\n[databases]\nproduction = host=localhost port=5432 dbname=production\n\n[pgbouncer]\nlisten_addr = *\nlisten_port = 6432\nauth_type = md5\nauth_file = /etc/pgbouncer/userlist.txt\npool_mode = transaction\nmax_client_conn = 1000\ndefault_pool_size = 50\nreserve_pool_size = 25\nreserve_pool_timeout = 5\nserver_lifetime = 3600\nserver_idle_timeout = 600\n</code></pre>"},{"location":"deployment/production/#monitoring","title":"Monitoring","text":""},{"location":"deployment/production/#prometheus-metrics","title":"Prometheus Metrics","text":"<pre><code># prometheus.yml\nglobal:\n  scrape_interval: 15s\n\nscrape_configs:\n  - job_name: 'postgres'\n    static_configs:\n      - targets: ['localhost:9187']\n\n  - job_name: 'postgres_exporter'\n    static_configs:\n      - targets: ['localhost:9187']\n\n  - job_name: 'steadytext_daemon'\n    static_configs:\n      - targets: ['localhost:9191']\n\n  - job_name: 'node_exporter'\n    static_configs:\n      - targets: ['localhost:9100']\n</code></pre>"},{"location":"deployment/production/#custom-monitoring-queries","title":"Custom Monitoring Queries","text":"<pre><code>-- SteadyText performance metrics\nCREATE OR REPLACE VIEW steadytext_metrics AS\nSELECT \n    'generation_requests' AS metric,\n    COUNT(*) AS value\nFROM pg_stat_user_functions\nWHERE funcname LIKE 'steadytext_generate%'\nUNION ALL\nSELECT \n    'embedding_requests' AS metric,\n    COUNT(*) AS value\nFROM pg_stat_user_functions\nWHERE funcname LIKE 'steadytext_embed%'\nUNION ALL\nSELECT \n    'cache_hit_rate' AS metric,\n    (SELECT hit_rate FROM steadytext_cache_stats()) AS value;\n\n-- Alert on slow AI queries\nCREATE OR REPLACE FUNCTION check_slow_ai_queries()\nRETURNS TABLE(query TEXT, duration INTERVAL, calls BIGINT) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT \n        query,\n        mean_exec_time * interval '1 millisecond' AS duration,\n        calls\n    FROM pg_stat_statements\n    WHERE query LIKE '%steadytext_%'\n      AND mean_exec_time &gt; 1000  -- Queries taking &gt; 1 second\n    ORDER BY mean_exec_time DESC\n    LIMIT 10;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"deployment/production/#grafana-dashboard","title":"Grafana Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"SteadyText Production Metrics\",\n    \"panels\": [\n      {\n        \"title\": \"AI Requests/sec\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(steadytext_generation_requests_total[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Cache Hit Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"steadytext_cache_hit_rate\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Response Time (p95)\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, steadytext_request_duration_seconds_bucket)\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Active Connections\",\n        \"targets\": [\n          {\n            \"expr\": \"pg_stat_activity_count\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"deployment/production/#security-best-practices","title":"Security Best Practices","text":""},{"location":"deployment/production/#1-database-security","title":"1. Database Security","text":"<pre><code>-- Create dedicated user for application\nCREATE ROLE steadytext_app WITH LOGIN PASSWORD 'strong_password';\nGRANT CONNECT ON DATABASE production TO steadytext_app;\nGRANT USAGE ON SCHEMA public TO steadytext_app;\nGRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO steadytext_app;\n\n-- Restrict pg_steadytext functions\nREVOKE EXECUTE ON FUNCTION steadytext_config_set FROM PUBLIC;\nGRANT EXECUTE ON FUNCTION steadytext_config_set TO postgres;\n\n-- Enable SSL\nALTER SYSTEM SET ssl = on;\nALTER SYSTEM SET ssl_cert_file = '/path/to/server.crt';\nALTER SYSTEM SET ssl_key_file = '/path/to/server.key';\n</code></pre>"},{"location":"deployment/production/#2-network-security","title":"2. Network Security","text":"<pre><code># Firewall rules\nsudo ufw allow from 10.0.0.0/8 to any port 5432  # PostgreSQL\nsudo ufw allow from 10.0.0.0/8 to any port 5555  # SteadyText daemon\nsudo ufw deny 5432  # Block external access\nsudo ufw deny 5555\n</code></pre>"},{"location":"deployment/production/#3-model-security","title":"3. Model Security","text":"<pre><code># Secure model storage\nsudo chown -R postgres:postgres /var/lib/steadytext/models\nsudo chmod -R 750 /var/lib/steadytext/models\n\n# Read-only mount for replicas\nmount -o ro /nfs/steadytext/models /var/lib/steadytext/models\n</code></pre>"},{"location":"deployment/production/#backup-and-recovery","title":"Backup and Recovery","text":""},{"location":"deployment/production/#automated-backups","title":"Automated Backups","text":"<pre><code>#!/bin/bash\n# backup-steadytext.sh\n\nBACKUP_DIR=\"/backup/postgres\"\nDATE=$(date +%Y%m%d_%H%M%S)\n\n# Backup database\npg_dump -U postgres -d production -f \"$BACKUP_DIR/production_$DATE.sql\"\n\n# Backup models (incremental)\nrsync -av --link-dest=\"$BACKUP_DIR/models/latest\" \\\n  /var/lib/steadytext/models/ \\\n  \"$BACKUP_DIR/models/$DATE/\"\n\nln -sfn \"$BACKUP_DIR/models/$DATE\" \"$BACKUP_DIR/models/latest\"\n\n# Cleanup old backups (keep 30 days)\nfind \"$BACKUP_DIR\" -name \"production_*.sql\" -mtime +30 -delete\nfind \"$BACKUP_DIR/models\" -maxdepth 1 -type d -mtime +30 -exec rm -rf {} \\;\n</code></pre>"},{"location":"deployment/production/#disaster-recovery","title":"Disaster Recovery","text":"<pre><code>-- Point-in-time recovery setup\nALTER SYSTEM SET archive_mode = on;\nALTER SYSTEM SET archive_command = 'test ! -f /archive/%f &amp;&amp; cp %p /archive/%f';\nALTER SYSTEM SET restore_command = 'cp /archive/%f %p';\n</code></pre>"},{"location":"deployment/production/#scaling-strategies","title":"Scaling Strategies","text":""},{"location":"deployment/production/#horizontal-scaling","title":"Horizontal Scaling","text":"<pre><code># Scale daemon instances\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: steadytext-daemon\nspec:\n  replicas: 10  # Scale based on load\n  template:\n    spec:\n      containers:\n      - name: daemon\n        image: julep/steadytext-daemon:latest\n        resources:\n          requests:\n            memory: \"4Gi\"\n            cpu: \"2\"\n</code></pre>"},{"location":"deployment/production/#vertical-scaling","title":"Vertical Scaling","text":"<pre><code>-- Increase resources for heavy workloads\nALTER SYSTEM SET max_parallel_workers = 16;\nALTER SYSTEM SET work_mem = '512MB';\nALTER SYSTEM SET maintenance_work_mem = '4GB';\n\n-- Restart required\nSELECT pg_reload_conf();\n</code></pre>"},{"location":"deployment/production/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/production/#common-issues","title":"Common Issues","text":"<ol> <li> <p>High Memory Usage <pre><code># Check memory usage\nsudo -u postgres psql -c \"\nSELECT \n    pid,\n    usename,\n    application_name,\n    pg_size_pretty(pg_backend_memory_context_size(pid)) AS memory\nFROM pg_stat_activity\nWHERE pid &lt;&gt; pg_backend_pid()\nORDER BY pg_backend_memory_context_size(pid) DESC;\"\n</code></pre></p> </li> <li> <p>Slow Queries <pre><code>-- Find slow AI queries\nSELECT \n    query,\n    mean_exec_time,\n    calls,\n    total_exec_time\nFROM pg_stat_statements\nWHERE query LIKE '%steadytext%'\nORDER BY mean_exec_time DESC\nLIMIT 20;\n</code></pre></p> </li> <li> <p>Daemon Connection Issues <pre><code># Test daemon connectivity\nst daemon status\nzmqc -c REQ 'tcp://localhost:5555' '{\"method\": \"ping\"}'\n\n# Check daemon logs\njournalctl -u steadytext-daemon -f\n</code></pre></p> </li> </ol>"},{"location":"deployment/production/#performance-benchmarks","title":"Performance Benchmarks","text":"<pre><code>-- Benchmark suite\nCREATE OR REPLACE FUNCTION benchmark_steadytext()\nRETURNS TABLE (\n    operation VARCHAR,\n    total_time INTERVAL,\n    ops_per_second NUMERIC\n) AS $$\nDECLARE\n    start_time TIMESTAMP;\n    end_time TIMESTAMP;\n    iterations INTEGER := 1000;\nBEGIN\n    -- Test generation speed\n    start_time := clock_timestamp();\n    PERFORM steadytext_generate('Test prompt', 100)\n    FROM generate_series(1, iterations);\n    end_time := clock_timestamp();\n\n    RETURN QUERY\n    SELECT \n        'generation'::VARCHAR,\n        end_time - start_time,\n        iterations / EXTRACT(EPOCH FROM (end_time - start_time));\n\n    -- Test embedding speed\n    start_time := clock_timestamp();\n    PERFORM steadytext_embed('Test text')\n    FROM generate_series(1, iterations);\n    end_time := clock_timestamp();\n\n    RETURN QUERY\n    SELECT \n        'embedding'::VARCHAR,\n        end_time - start_time,\n        iterations / EXTRACT(EPOCH FROM (end_time - start_time));\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"deployment/production/#maintenance-schedule","title":"Maintenance Schedule","text":"<pre><code>-- Weekly maintenance tasks\nCREATE OR REPLACE FUNCTION weekly_maintenance()\nRETURNS VOID AS $$\nBEGIN\n    -- Update table statistics\n    ANALYZE;\n\n    -- Reindex for performance\n    REINDEX DATABASE production CONCURRENTLY;\n\n    -- Clean up old cache entries\n    DELETE FROM steadytext_cache WHERE created_at &lt; NOW() - INTERVAL '30 days';\n\n    -- Vacuum to reclaim space\n    VACUUM ANALYZE;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Schedule with pg_cron\nSELECT cron.schedule('weekly_maintenance', '0 2 * * 0', 'SELECT weekly_maintenance()');\n</code></pre>"},{"location":"deployment/production/#next-steps","title":"Next Steps","text":"<ul> <li>Performance Tuning \u2192</li> <li>Cloudflare Edge Deployment \u2192</li> <li>TimescaleDB Integration \u2192</li> </ul> <p>Production Checklist</p> <ul> <li>[ ] SSL/TLS enabled for all connections</li> <li>[ ] Automated backups configured and tested</li> <li>[ ] Monitoring and alerting active</li> <li>[ ] Resource limits properly set</li> <li>[ ] Security hardening completed</li> <li>[ ] Disaster recovery plan tested</li> <li>[ ] Performance benchmarks established</li> </ul>"},{"location":"examples/","title":"Examples","text":"<p>Real-world usage patterns and code examples for SteadyText.</p>"},{"location":"examples/#overview","title":"Overview","text":"<p>This section demonstrates practical applications of SteadyText across different use cases:</p> <ul> <li>Testing with AI - Reliable AI tests that never flake</li> <li>CLI Tools - Building deterministic command-line tools</li> <li>Caching Guide - Configure and optimize caching</li> <li>Custom Seeds Guide - Use custom seeds for reproducible variations</li> <li>Daemon Usage Guide - Persistent model serving for faster responses</li> <li>Error Handling Guide - Handle errors gracefully</li> <li>Performance Tuning Guide - Optimize for speed and efficiency</li> <li>PostgreSQL Integration Examples - Integrate with PostgreSQL</li> </ul> <p>All examples showcase SteadyText's core principle: same input \u2192 same output, every time.</p>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#basic-usage","title":"Basic Usage","text":"<pre><code>import steadytext\n\n# Deterministic text generation\ncode = steadytext.generate(\"implement binary search in Python\")\nassert \"def binary_search\" in code  # Always passes!\n\n# Streaming generation\nfor token in steadytext.generate_iter(\"explain quantum computing\"):\n    print(token, end=\"\", flush=True)\n\n# Deterministic embeddings  \nvec = steadytext.embed(\"Hello world\")  # 1024-dim numpy array\nprint(f\"Shape: {vec.shape}, Norm: {np.linalg.norm(vec):.6f}\")\n</code></pre>"},{"location":"examples/#testing-applications","title":"Testing Applications","text":"<pre><code>def test_ai_code_generation():\n    \"\"\"Test that never flakes - same input, same output.\"\"\"\n    prompt = \"write a function to reverse a string\"\n    result = my_ai_function(prompt)\n    expected = steadytext.generate(prompt)\n    assert result == expected  # Deterministic comparison!\n\ndef test_embedding_similarity():\n    \"\"\"Reliable similarity testing.\"\"\"\n    vec1 = steadytext.embed(\"machine learning\")\n    vec2 = steadytext.embed(\"artificial intelligence\")\n    similarity = np.dot(vec1, vec2)  # Already normalized\n    assert similarity &gt; 0.7  # Always passes with same threshold\n</code></pre>"},{"location":"examples/#cli-tool-building","title":"CLI Tool Building","text":"<pre><code>import click\nimport steadytext\n\n@click.command()\n@click.argument('topic')\ndef motivate(topic):\n    \"\"\"Generate motivational quotes about any topic.\"\"\"\n    prompt = f\"Write an inspiring quote about {topic}\"\n    quote = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udcaa {quote}\")\n\n# Usage: python script.py \"programming\"\n# Always generates the same motivational quote for \"programming\"\n</code></pre>"},{"location":"examples/#use-case-categories","title":"Use Case Categories","text":""},{"location":"examples/#testing-quality-assurance","title":"\ud83e\uddea Testing &amp; Quality Assurance","text":"<p>Perfect for: - Unit tests with AI components - Integration testing with deterministic outputs - Regression testing for AI features - Mock AI services for development</p>"},{"location":"examples/#developer-tools","title":"\ud83d\udee0\ufe0f Developer Tools","text":"<p>Ideal for: - Code generation tools - Documentation generators - CLI utilities with AI features - Build system integration</p>"},{"location":"examples/#data-content-generation","title":"\ud83d\udcca Data &amp; Content Generation","text":"<p>Great for: - Synthetic data generation - Content templates - Data augmentation for testing - Reproducible research datasets</p>"},{"location":"examples/#search-similarity","title":"\ud83d\udd0d Search &amp; Similarity","text":"<p>Excellent for: - Semantic search systems - Document clustering - Content recommendation - Duplicate detection</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<ol> <li>Browse examples - Check out Testing and CLI Tools</li> <li>Run the code - All examples are fully executable</li> <li>Adapt for your use case - Copy and modify patterns that fit your needs</li> </ol>"},{"location":"examples/#example-repository","title":"Example Repository","text":"<p>All examples are available in the examples/ directory of the SteadyText repository:</p> <pre><code>git clone https://github.com/julep-ai/steadytext.git\ncd steadytext/examples\npython basic_usage.py\npython testing_with_ai.py  \npython cli_tools.py\n</code></pre> <p>Deterministic Outputs</p> <p>Remember: all examples produce identical outputs every time you run them. This predictability is SteadyText's core feature and what makes it perfect for testing and tooling applications.</p>"},{"location":"examples/basic-usage/","title":"Basic Usage Examples","text":"<p>Learn the fundamental features of SteadyText through practical examples.</p>"},{"location":"examples/basic-usage/#text-generation","title":"Text Generation","text":""},{"location":"examples/basic-usage/#simple-generation","title":"Simple Generation","text":"<pre><code>import steadytext\n\n# Basic text generation\ntext = steadytext.generate(\"Write a Python function to reverse a string\")\nprint(text)\n\n# The output is deterministic - running this again produces the same result\ntext2 = steadytext.generate(\"Write a Python function to reverse a string\")\nassert text == text2  # Always true\n</code></pre>"},{"location":"examples/basic-usage/#streaming-generation","title":"Streaming Generation","text":"<pre><code># Stream tokens as they're generated\nfor token in steadytext.generate_iter(\"Explain how neural networks work\"):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"examples/basic-usage/#with-custom-seeds","title":"With Custom Seeds","text":"<pre><code># Different seeds produce different (but deterministic) outputs\nresponse1 = steadytext.generate(\"Tell me a fact\", seed=100)\nresponse2 = steadytext.generate(\"Tell me a fact\", seed=200)\nassert response1 != response2  # Different seeds = different outputs\n\n# Same seed always produces same output\nresponse3 = steadytext.generate(\"Tell me a fact\", seed=100)\nassert response1 == response3  # Same seed = same output\n</code></pre>"},{"location":"examples/basic-usage/#embeddings","title":"Embeddings","text":""},{"location":"examples/basic-usage/#single-text-embedding","title":"Single Text Embedding","text":"<pre><code>import numpy as np\n\n# Generate embedding for a single text\nembedding = steadytext.embed(\"machine learning\")\nprint(f\"Shape: {embedding.shape}\")  # (1024,)\nprint(f\"Type: {embedding.dtype}\")    # float32\n\n# Embeddings are L2-normalized\nnorm = np.linalg.norm(embedding)\nprint(f\"L2 norm: {norm:.6f}\")  # ~1.0\n</code></pre>"},{"location":"examples/basic-usage/#batch-embeddings","title":"Batch Embeddings","text":"<pre><code># Embed multiple texts (returns averaged embedding)\ntexts = [\n    \"artificial intelligence\",\n    \"machine learning\",\n    \"deep learning\",\n    \"neural networks\"\n]\n\n# Note: Returns a single averaged embedding, not multiple embeddings\nembedding = steadytext.embed(texts)\nprint(f\"Shape: {embedding.shape}\")  # (1024,) - single vector\n\n# To get individual embeddings, process separately\nembeddings = []\nfor text in texts:\n    embeddings.append(steadytext.embed(text))\n\n# Calculate similarity between two texts using dot product\n# (embeddings are L2-normalized, so dot product = cosine similarity)\nimport numpy as np\nvec1 = steadytext.embed(\"artificial intelligence\")\nvec2 = steadytext.embed(\"machine learning\")\nsimilarity = np.dot(vec1, vec2)\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"examples/basic-usage/#structured-output","title":"Structured Output","text":""},{"location":"examples/basic-usage/#json-generation","title":"JSON Generation","text":"<pre><code>from pydantic import BaseModel\n\nclass Product(BaseModel):\n    name: str\n    price: float\n    in_stock: bool\n\n# Generate structured data\nresult = steadytext.generate(\n    \"Create a product listing for wireless headphones\",\n    schema=Product\n)\nprint(result)\n# Output includes: &lt;json-output&gt;{\"name\": \"...\", \"price\": ..., \"in_stock\": ...}&lt;/json-output&gt;\n\n# Extract just the JSON\njson_data = steadytext.generate_json(\n    \"Create a product listing for wireless headphones\",\n    schema=Product\n)\nprint(json_data)  # {\"name\": \"...\", \"price\": ..., \"in_stock\": ...}\n</code></pre>"},{"location":"examples/basic-usage/#pattern-matching","title":"Pattern Matching","text":"<pre><code># Generate text matching a regex pattern\nphone = steadytext.generate_regex(\n    \"Generate a US phone number\",\n    pattern=r\"\\d{3}-\\d{3}-\\d{4}\"\n)\nprint(phone)  # e.g., \"555-123-4567\"\n\n# Generate from choices\nanswer = steadytext.generate_choice(\n    \"Is Python a compiled or interpreted language?\",\n    choices=[\"compiled\", \"interpreted\", \"both\"]\n)\nprint(answer)  # \"interpreted\"\n</code></pre>"},{"location":"examples/basic-usage/#command-line-usage","title":"Command Line Usage","text":""},{"location":"examples/basic-usage/#basic-cli","title":"Basic CLI","text":"<pre><code># Generate text from command line\necho \"Write a haiku about coding\" | st\n\n# With custom seed\necho \"Tell me a joke\" | st --seed 42\n\n# Wait for complete output (no streaming)\necho \"Explain recursion\" | st --wait\n\n# Output as JSON with metadata\necho \"Hello world\" | st --json\n</code></pre>"},{"location":"examples/basic-usage/#embeddings-via-cli","title":"Embeddings via CLI","text":"<pre><code># Generate embedding\nst embed \"machine learning\"\n\n# Output as numpy array\nst embed \"deep learning\" --format numpy\n\n# Multiple texts\nst embed \"text one\" \"text two\" \"text three\"\n</code></pre>"},{"location":"examples/basic-usage/#error-handling","title":"Error Handling","text":"<pre><code># SteadyText is designed to never fail\n# Even with invalid inputs, it returns deterministic outputs\n\n# Empty input\nresult = steadytext.generate(\"\")  # Returns deterministic output\n\n# Very long input (exceeds context)\nlong_text = \"x\" * 50000\ntry:\n    result = steadytext.generate(long_text)\nexcept steadytext.ContextLengthExceededError as e:\n    print(f\"Input too long: {e.input_tokens} tokens\")\n</code></pre>"},{"location":"examples/basic-usage/#performance-tips","title":"Performance Tips","text":""},{"location":"examples/basic-usage/#model-preloading","title":"Model Preloading","text":"<pre><code># Preload models at startup to avoid first-call latency\nsteadytext.preload_models(verbose=True)\n\n# Now all subsequent calls are fast\ntext = steadytext.generate(\"Hello\")  # No model loading delay\n</code></pre>"},{"location":"examples/basic-usage/#caching","title":"Caching","text":"<pre><code># Results are automatically cached\n# Repeated calls with same input are instant\nfor i in range(100):\n    # First call: ~100ms, subsequent calls: &lt;1ms\n    result = steadytext.generate(\"Same prompt\")\n</code></pre>"},{"location":"examples/basic-usage/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Seeds Guide - Advanced seed usage patterns</li> <li>Testing Guide - Using SteadyText in test suites</li> <li>CLI Tools - Building deterministic CLI tools</li> <li>API Reference - Complete API documentation</li> </ul>"},{"location":"examples/caching/","title":"Caching Guide","text":"<p>Learn how to configure and optimize SteadyText's caching system for maximum performance.</p>"},{"location":"examples/caching/#overview","title":"Overview","text":"<p>SteadyText uses a sophisticated frecency cache (frequency + recency) that combines: - LRU (Least Recently Used): Recent items stay cached - Frequency counting: Popular items are retained longer - Disk persistence: Cache survives restarts - Thread safety: Safe for concurrent access</p>"},{"location":"examples/caching/#cache-architecture","title":"Cache Architecture","text":""},{"location":"examples/caching/#two-tier-cache-system","title":"Two-Tier Cache System","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Application Layer           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     Generation Cache    \u2502 Embedding \u2502\n\u2502    (256 entries, 50MB) \u2502   Cache   \u2502\n\u2502                        \u2502(512, 100MB)\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      SQLite Backend (Thread-Safe)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/caching/#cache-files-location","title":"Cache Files Location","text":"<pre><code>import steadytext\nfrom pathlib import Path\n\n# Get cache directory\ncache_dir = Path.home() / \".cache\" / \"steadytext\" / \"caches\"\nprint(f\"Cache location: {cache_dir}\")\n\n# Cache files\ngeneration_cache = cache_dir / \"generation_cache.db\"\nembedding_cache = cache_dir / \"embedding_cache.db\"\n</code></pre>"},{"location":"examples/caching/#configuration","title":"Configuration","text":""},{"location":"examples/caching/#environment-variables","title":"Environment Variables","text":"<pre><code># Generation cache settings\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256      # Max entries\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50.0  # Max file size\n\n# Embedding cache settings  \nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512       # Max entries\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100.0  # Max file size\n\n# Disable cache entirely (not recommended)\nexport STEADYTEXT_DISABLE_CACHE=1\n</code></pre>"},{"location":"examples/caching/#python-configuration","title":"Python Configuration","text":"<pre><code>import os\nimport steadytext\n\n# Configure before importing/using steadytext\nos.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = '1024'\nos.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = '200.0'\n\n# Verify configuration\nfrom steadytext import get_cache_manager\ncache_manager = get_cache_manager()\nstats = cache_manager.get_cache_stats()\nprint(f\"Generation cache capacity: {stats['generation']['capacity']}\")\n</code></pre>"},{"location":"examples/caching/#cache-management","title":"Cache Management","text":""},{"location":"examples/caching/#monitoring-cache-performance","title":"Monitoring Cache Performance","text":"<pre><code>from steadytext import get_cache_manager\nimport time\n\nclass CacheMonitor:\n    \"\"\"Monitor cache performance and hit rates.\"\"\"\n\n    def __init__(self):\n        self.cache_manager = get_cache_manager()\n        self.initial_stats = self.cache_manager.get_cache_stats()\n\n    def get_hit_rate(self, cache_type='generation'):\n        \"\"\"Calculate cache hit rate.\"\"\"\n        stats = self.cache_manager.get_cache_stats()[cache_type]\n        hits = stats.get('hits', 0)\n        misses = stats.get('misses', 0)\n        total = hits + misses\n\n        if total == 0:\n            return 0.0\n\n        return hits / total * 100\n\n    def monitor_operation(self, operation, *args, **kwargs):\n        \"\"\"Monitor a single operation's cache behavior.\"\"\"\n        stats_before = self.cache_manager.get_cache_stats()\n        start_time = time.time()\n\n        result = operation(*args, **kwargs)\n\n        duration = time.time() - start_time\n        stats_after = self.cache_manager.get_cache_stats()\n\n        # Determine if it was a cache hit\n        gen_hits_diff = stats_after['generation']['hits'] - stats_before['generation']['hits']\n        emb_hits_diff = stats_after['embedding']['hits'] - stats_before['embedding']['hits']\n\n        cache_hit = gen_hits_diff &gt; 0 or emb_hits_diff &gt; 0\n\n        return {\n            'result': result,\n            'duration': duration,\n            'cache_hit': cache_hit,\n            'stats_delta': {\n                'generation_hits': gen_hits_diff,\n                'embedding_hits': emb_hits_diff\n            }\n        }\n\n    def print_summary(self):\n        \"\"\"Print cache performance summary.\"\"\"\n        stats = self.cache_manager.get_cache_stats()\n\n        print(\"=== Cache Performance Summary ===\")\n        for cache_type in ['generation', 'embedding']:\n            cache_stats = stats[cache_type]\n            hit_rate = self.get_hit_rate(cache_type)\n\n            print(f\"\\n{cache_type.title()} Cache:\")\n            print(f\"  Size: {cache_stats['size']} entries\")\n            print(f\"  Hit Rate: {hit_rate:.1f}%\")\n            print(f\"  Hits: {cache_stats.get('hits', 0)}\")\n            print(f\"  Misses: {cache_stats.get('misses', 0)}\")\n\n# Usage example\nmonitor = CacheMonitor()\n\n# Monitor text generation\nresult1 = monitor.monitor_operation(\n    steadytext.generate, \n    \"Write a haiku about caching\"\n)\nprint(f\"First call: {result1['duration']:.3f}s (cache hit: {result1['cache_hit']})\")\n\n# Same prompt - should be cached\nresult2 = monitor.monitor_operation(\n    steadytext.generate, \n    \"Write a haiku about caching\"\n)\nprint(f\"Second call: {result2['duration']:.3f}s (cache hit: {result2['cache_hit']})\")\n\nmonitor.print_summary()\n</code></pre>"},{"location":"examples/caching/#cache-warming","title":"Cache Warming","text":"<pre><code>import steadytext\nfrom typing import List\nimport concurrent.futures\n\ndef warm_cache_sequential(prompts: List[str], seeds: List[int] = None):\n    \"\"\"Warm cache with common prompts sequentially.\"\"\"\n    if seeds is None:\n        seeds = [42]  # Default seed only\n\n    warmed = 0\n    for prompt in prompts:\n        for seed in seeds:\n            _ = steadytext.generate(prompt, seed=seed, max_new_tokens=100)\n            warmed += 1\n\n    return warmed\n\ndef warm_cache_parallel(prompts: List[str], seeds: List[int] = None, max_workers: int = 4):\n    \"\"\"Warm cache with parallel generation.\"\"\"\n    if seeds is None:\n        seeds = [42]\n\n    tasks = [(prompt, seed) for prompt in prompts for seed in seeds]\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(steadytext.generate, prompt, seed=seed, max_new_tokens=100)\n            for prompt, seed in tasks\n        ]\n\n        # Wait for all to complete\n        completed = 0\n        for future in concurrent.futures.as_completed(futures):\n            future.result()  # Get result to ensure completion\n            completed += 1\n\n    return completed\n\n# Common prompts to cache\ncommon_prompts = [\n    \"Write a Python function\",\n    \"Explain this error\",\n    \"Generate test data\",\n    \"Create documentation\",\n    \"Write unit tests\",\n    \"Optimize this code\",\n    \"Review this pull request\",\n    \"Suggest improvements\"\n]\n\n# Common seeds if using multiple\ncommon_seeds = [42, 100, 200]  # Add your common seeds\n\n# Warm cache\nprint(\"Warming cache...\")\nwarmed = warm_cache_parallel(common_prompts, common_seeds)\nprint(f\"Cache warmed with {warmed} entries\")\n\n# Verify cache is warm\nfrom steadytext import get_cache_manager\nstats = get_cache_manager().get_cache_stats()\nprint(f\"Generation cache size: {stats['generation']['size']}\")\n</code></pre>"},{"location":"examples/caching/#cache-optimization-strategies","title":"Cache Optimization Strategies","text":"<pre><code>import steadytext\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\nclass CacheOptimizer:\n    \"\"\"Optimize cache usage patterns.\"\"\"\n\n    def __init__(self):\n        self.usage_patterns = defaultdict(lambda: {\n            'count': 0,\n            'last_used': None,\n            'avg_generation_time': 0\n        })\n\n    def track_usage(self, prompt: str, seed: int, generation_time: float):\n        \"\"\"Track prompt usage patterns.\"\"\"\n        key = f\"{prompt}:{seed}\"\n        pattern = self.usage_patterns[key]\n\n        pattern['count'] += 1\n        pattern['last_used'] = datetime.now()\n\n        # Update average generation time\n        avg = pattern['avg_generation_time']\n        count = pattern['count']\n        pattern['avg_generation_time'] = (avg * (count - 1) + generation_time) / count\n\n    def get_cache_priorities(self, top_n: int = 20):\n        \"\"\"Get prompts that should be prioritized for caching.\"\"\"\n        # Score based on frequency and recency\n        now = datetime.now()\n        scores = []\n\n        for key, pattern in self.usage_patterns.items():\n            # Frequency score\n            freq_score = pattern['count']\n\n            # Recency score (higher for more recent)\n            if pattern['last_used']:\n                age = (now - pattern['last_used']).total_seconds()\n                recency_score = 1 / (1 + age / 3600)  # Decay over hours\n            else:\n                recency_score = 0\n\n            # Generation time score (prioritize slow generations)\n            time_score = pattern['avg_generation_time']\n\n            # Combined score\n            score = freq_score * 0.5 + recency_score * 0.3 + time_score * 0.2\n\n            scores.append((score, key, pattern))\n\n        # Sort by score\n        scores.sort(reverse=True)\n\n        return scores[:top_n]\n\n    def recommend_cache_size(self):\n        \"\"\"Recommend optimal cache size based on usage.\"\"\"\n        total_unique = len(self.usage_patterns)\n        frequently_used = sum(1 for p in self.usage_patterns.values() if p['count'] &gt; 5)\n\n        # Recommend 1.5x frequently used items + buffer\n        recommended = int(frequently_used * 1.5 + 50)\n\n        return {\n            'total_unique_prompts': total_unique,\n            'frequently_used': frequently_used,\n            'recommended_size': recommended,\n            'current_default': 256\n        }\n\n# Example usage\noptimizer = CacheOptimizer()\n\n# Simulate usage tracking\nimport time\ntest_prompts = [\n    (\"Write a function to sort a list\", 42),\n    (\"Explain machine learning\", 42),\n    (\"Write a function to sort a list\", 42),  # Repeated\n    (\"Generate test cases\", 100),\n    (\"Write a function to sort a list\", 42),  # Popular\n]\n\nfor prompt, seed in test_prompts:\n    start = time.time()\n    _ = steadytext.generate(prompt, seed=seed)\n    duration = time.time() - start\n    optimizer.track_usage(prompt, seed, duration)\n\n# Get optimization recommendations\nprint(\"=== Cache Optimization Report ===\")\n\npriorities = optimizer.get_cache_priorities(5)\nprint(\"\\nTop prompts to keep cached:\")\nfor score, key, pattern in priorities:\n    prompt, seed = key.rsplit(':', 1)\n    print(f\"  Score: {score:.2f} - {prompt[:50]}... (seed: {seed})\")\n    print(f\"    Used: {pattern['count']}x, Avg time: {pattern['avg_generation_time']:.3f}s\")\n\nrecommendations = optimizer.recommend_cache_size()\nprint(f\"\\nCache size recommendations:\")\nprint(f\"  Total unique: {recommendations['total_unique_prompts']}\")\nprint(f\"  Frequently used: {recommendations['frequently_used']}\")\nprint(f\"  Recommended size: {recommendations['recommended_size']}\")\n</code></pre>"},{"location":"examples/caching/#advanced-cache-patterns","title":"Advanced Cache Patterns","text":""},{"location":"examples/caching/#hierarchical-caching","title":"Hierarchical Caching","text":"<pre><code>import steadytext\nfrom typing import Dict, Any, Optional\nimport json\nimport hashlib\n\nclass HierarchicalCache:\n    \"\"\"Implement hierarchical caching for complex workflows.\"\"\"\n\n    def __init__(self):\n        self.memory_cache = {}  # Fast in-memory cache\n        self.cache_manager = steadytext.get_cache_manager()\n\n    def _generate_cache_key(self, category: str, subcategory: str, \n                          prompt: str, seed: int) -&gt; str:\n        \"\"\"Generate hierarchical cache key.\"\"\"\n        components = [category, subcategory, prompt, str(seed)]\n        combined = \":\".join(components)\n\n        # Create hash for consistent key length\n        key_hash = hashlib.md5(combined.encode()).hexdigest()\n\n        return f\"{category}:{subcategory}:{key_hash}\"\n\n    def get_or_generate(self, category: str, subcategory: str, \n                       prompt: str, seed: int = 42, **kwargs) -&gt; str:\n        \"\"\"Get from cache or generate with hierarchical key.\"\"\"\n        cache_key = self._generate_cache_key(category, subcategory, prompt, seed)\n\n        # Check memory cache first\n        if cache_key in self.memory_cache:\n            return self.memory_cache[cache_key]\n\n        # Generate and cache\n        result = steadytext.generate(prompt, seed=seed, **kwargs)\n\n        # Store in memory cache\n        self.memory_cache[cache_key] = result\n\n        return result\n\n    def preload_category(self, category: str, items: List[Dict[str, Any]]):\n        \"\"\"Preload entire category into cache.\"\"\"\n        loaded = 0\n\n        for item in items:\n            result = self.get_or_generate(\n                category,\n                item.get('subcategory', 'default'),\n                item['prompt'],\n                item.get('seed', 42),\n                **item.get('kwargs', {})\n            )\n            loaded += 1\n\n        return loaded\n\n    def clear_category(self, category: str):\n        \"\"\"Clear all cache entries for a category.\"\"\"\n        keys_to_remove = [k for k in self.memory_cache if k.startswith(f\"{category}:\")]\n\n        for key in keys_to_remove:\n            del self.memory_cache[key]\n\n        return len(keys_to_remove)\n\n# Usage example\nh_cache = HierarchicalCache()\n\n# Generate with hierarchy\nemail_subject = h_cache.get_or_generate(\n    \"emails\", \n    \"marketing\",\n    \"Write a subject line for Black Friday sale\",\n    seed=100\n)\n\nemail_body = h_cache.get_or_generate(\n    \"emails\",\n    \"marketing\", \n    \"Write email body for Black Friday sale\",\n    seed=100\n)\n\n# Preload documentation category\ndocs_to_cache = [\n    {\n        'subcategory': 'api',\n        'prompt': 'Document a REST API endpoint',\n        'seed': 42,\n        'kwargs': {'max_new_tokens': 200}\n    },\n    {\n        'subcategory': 'functions',\n        'prompt': 'Document a Python function',\n        'seed': 42,\n        'kwargs': {'max_new_tokens': 150}\n    }\n]\n\nloaded = h_cache.preload_category('documentation', docs_to_cache)\nprint(f\"Preloaded {loaded} documentation templates\")\n</code></pre>"},{"location":"examples/caching/#cache-aware-generation","title":"Cache-Aware Generation","text":"<pre><code>import steadytext\nfrom typing import Optional, Tuple\nimport time\n\nclass CacheAwareGenerator:\n    \"\"\"Generator that adapts based on cache state.\"\"\"\n\n    def __init__(self):\n        self.cache_manager = steadytext.get_cache_manager()\n        self.performance_threshold = 0.1  # 100ms\n\n    def is_likely_cached(self, prompt: str, seed: int = 42) -&gt; bool:\n        \"\"\"Check if a prompt is likely cached without generating.\"\"\"\n        # This is a heuristic - actual implementation would need\n        # to check cache internals\n        stats = self.cache_manager.get_cache_stats()\n\n        # Simple heuristic: if we have items in cache and\n        # this is a common prompt pattern\n        if stats['generation']['size'] &gt; 0:\n            common_patterns = ['Write a', 'Explain', 'Create', 'Generate']\n            return any(prompt.startswith(p) for p in common_patterns)\n\n        return False\n\n    def generate_with_fallback(self, primary_prompt: str, \n                             fallback_prompt: Optional[str] = None,\n                             seed: int = 42, **kwargs) -&gt; Tuple[str, bool]:\n        \"\"\"Generate with fallback if primary isn't cached.\"\"\"\n        start_time = time.time()\n\n        # Try primary prompt\n        result = steadytext.generate(primary_prompt, seed=seed, **kwargs)\n        duration = time.time() - start_time\n\n        # If slow (not cached) and we have fallback\n        if duration &gt; self.performance_threshold and fallback_prompt:\n            # Check if fallback might be cached\n            if self.is_likely_cached(fallback_prompt, seed):\n                fallback_result = steadytext.generate(fallback_prompt, seed=seed, **kwargs)\n                return fallback_result, True\n\n        return result, False\n\n    def batch_generate_optimized(self, prompts: List[str], seed: int = 42, **kwargs):\n        \"\"\"Generate batch with cache-aware ordering.\"\"\"\n        results = {}\n        timings = {}\n\n        # First pass: try all prompts and measure timing\n        for prompt in prompts:\n            start = time.time()\n            result = steadytext.generate(prompt, seed=seed, **kwargs)\n            duration = time.time() - start\n\n            results[prompt] = result\n            timings[prompt] = duration\n\n        # Analyze cache performance\n        cached_prompts = [p for p, t in timings.items() if t &lt; self.performance_threshold]\n        uncached_prompts = [p for p, t in timings.items() if t &gt;= self.performance_threshold]\n\n        stats = {\n            'total': len(prompts),\n            'cached': len(cached_prompts),\n            'uncached': len(uncached_prompts),\n            'cache_rate': len(cached_prompts) / len(prompts) * 100,\n            'avg_cached_time': sum(timings[p] for p in cached_prompts) / len(cached_prompts) if cached_prompts else 0,\n            'avg_uncached_time': sum(timings[p] for p in uncached_prompts) / len(uncached_prompts) if uncached_prompts else 0\n        }\n\n        return results, stats\n\n# Usage\ncache_gen = CacheAwareGenerator()\n\n# Single generation with fallback\nprimary = \"Generate a complex analysis of quantum computing applications in cryptography\"\nfallback = \"Explain quantum computing\"  # Likely cached\n\nresult, used_fallback = cache_gen.generate_with_fallback(\n    primary, \n    fallback,\n    max_new_tokens=200\n)\n\nprint(f\"Used fallback: {used_fallback}\")\n\n# Batch generation with analysis\ntest_prompts = [\n    \"Write a Python function\",  # Likely cached\n    \"Explain machine learning\",  # Likely cached\n    \"Analyze the socioeconomic impact of automation on rural communities\",  # Unlikely\n    \"Generate test data\",  # Possibly cached\n    \"Describe the philosophical implications of consciousness in AI systems\"  # Unlikely\n]\n\nresults, stats = cache_gen.batch_generate_optimized(test_prompts, max_new_tokens=100)\n\nprint(\"\\n=== Batch Generation Cache Stats ===\")\nprint(f\"Total prompts: {stats['total']}\")\nprint(f\"Cached: {stats['cached']} ({stats['cache_rate']:.1f}%)\")\nprint(f\"Average cached time: {stats['avg_cached_time']:.3f}s\")\nprint(f\"Average uncached time: {stats['avg_uncached_time']:.3f}s\")\nprint(f\"Speed improvement: {stats['avg_uncached_time'] / stats['avg_cached_time']:.1f}x\")\n</code></pre>"},{"location":"examples/caching/#cache-persistence-patterns","title":"Cache Persistence Patterns","text":"<pre><code>import steadytext\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List\nimport pickle\n\nclass CachePersistenceManager:\n    \"\"\"Manage cache persistence and restoration.\"\"\"\n\n    def __init__(self, backup_dir: str = \"./cache_backups\"):\n        self.backup_dir = Path(backup_dir)\n        self.backup_dir.mkdir(exist_ok=True)\n        self.cache_manager = steadytext.get_cache_manager()\n\n    def export_cache_metadata(self) -&gt; Dict:\n        \"\"\"Export cache metadata for analysis.\"\"\"\n        stats = self.cache_manager.get_cache_stats()\n\n        metadata = {\n            'timestamp': datetime.now().isoformat(),\n            'generation_cache': {\n                'size': stats['generation']['size'],\n                'capacity': stats['generation'].get('capacity', 256),\n                'hit_rate': self._calculate_hit_rate(stats['generation'])\n            },\n            'embedding_cache': {\n                'size': stats['embedding']['size'],\n                'capacity': stats['embedding'].get('capacity', 512),\n                'hit_rate': self._calculate_hit_rate(stats['embedding'])\n            }\n        }\n\n        return metadata\n\n    def _calculate_hit_rate(self, cache_stats: Dict) -&gt; float:\n        \"\"\"Calculate cache hit rate.\"\"\"\n        hits = cache_stats.get('hits', 0)\n        misses = cache_stats.get('misses', 0)\n        total = hits + misses\n\n        return (hits / total * 100) if total &gt; 0 else 0.0\n\n    def save_cache_state(self, name: str):\n        \"\"\"Save current cache state metadata.\"\"\"\n        metadata = self.export_cache_metadata()\n\n        filename = self.backup_dir / f\"cache_state_{name}.json\"\n        with open(filename, 'w') as f:\n            json.dump(metadata, f, indent=2)\n\n        print(f\"Cache state saved to {filename}\")\n        return filename\n\n    def analyze_cache_history(self) -&gt; Dict:\n        \"\"\"Analyze cache performance over time.\"\"\"\n        history_files = list(self.backup_dir.glob(\"cache_state_*.json\"))\n\n        if not history_files:\n            return {\"error\": \"No cache history found\"}\n\n        history = []\n        for file in sorted(history_files):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                data['filename'] = file.name\n                history.append(data)\n\n        # Analyze trends\n        analysis = {\n            'total_snapshots': len(history),\n            'date_range': {\n                'start': history[0]['timestamp'],\n                'end': history[-1]['timestamp']\n            },\n            'generation_cache_trend': {\n                'min_size': min(h['generation_cache']['size'] for h in history),\n                'max_size': max(h['generation_cache']['size'] for h in history),\n                'avg_hit_rate': sum(h['generation_cache']['hit_rate'] for h in history) / len(history)\n            },\n            'embedding_cache_trend': {\n                'min_size': min(h['embedding_cache']['size'] for h in history),\n                'max_size': max(h['embedding_cache']['size'] for h in history),\n                'avg_hit_rate': sum(h['embedding_cache']['hit_rate'] for h in history) / len(history)\n            }\n        }\n\n        return analysis\n\n# Usage\npersistence = CachePersistenceManager()\n\n# Save current state\npersistence.save_cache_state(\"before_optimization\")\n\n# Do some work...\nfor i in range(10):\n    steadytext.generate(f\"Test prompt {i}\", seed=42)\n\n# Save after work\npersistence.save_cache_state(\"after_batch_generation\")\n\n# Analyze history\nanalysis = persistence.analyze_cache_history()\nprint(\"\\n=== Cache History Analysis ===\")\nprint(json.dumps(analysis, indent=2))\n</code></pre>"},{"location":"examples/caching/#cache-performance-tuning","title":"Cache Performance Tuning","text":""},{"location":"examples/caching/#benchmark-cache-impact","title":"Benchmark Cache Impact","text":"<pre><code>import steadytext\nimport time\nimport statistics\nfrom typing import List, Dict\n\nclass CacheBenchmark:\n    \"\"\"Benchmark cache performance impact.\"\"\"\n\n    def __init__(self):\n        self.cache_manager = steadytext.get_cache_manager()\n\n    def benchmark_single_prompt(self, prompt: str, seed: int = 42, \n                              iterations: int = 10) -&gt; Dict:\n        \"\"\"Benchmark a single prompt with cold and warm cache.\"\"\"\n        # Clear cache for cold start\n        self.cache_manager.clear_all_caches()\n\n        timings = {\n            'cold': [],\n            'warm': []\n        }\n\n        # Cold cache timing (first call)\n        start = time.time()\n        _ = steadytext.generate(prompt, seed=seed)\n        timings['cold'].append(time.time() - start)\n\n        # Warm cache timings\n        for _ in range(iterations - 1):\n            start = time.time()\n            _ = steadytext.generate(prompt, seed=seed)\n            timings['warm'].append(time.time() - start)\n\n        return {\n            'prompt': prompt[:50] + '...' if len(prompt) &gt; 50 else prompt,\n            'cold_time': timings['cold'][0],\n            'warm_avg': statistics.mean(timings['warm']),\n            'warm_std': statistics.stdev(timings['warm']) if len(timings['warm']) &gt; 1 else 0,\n            'speedup': timings['cold'][0] / statistics.mean(timings['warm'])\n        }\n\n    def benchmark_cache_sizes(self, test_prompts: List[str], \n                            cache_sizes: List[int]) -&gt; Dict:\n        \"\"\"Benchmark performance with different cache sizes.\"\"\"\n        results = {}\n        original_capacity = os.environ.get('STEADYTEXT_GENERATION_CACHE_CAPACITY', '256')\n\n        try:\n            for size in cache_sizes:\n                # Set cache size\n                os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = str(size)\n\n                # Restart cache with new size\n                # Note: In practice, this would require restarting the process\n                self.cache_manager.clear_all_caches()\n\n                # Benchmark with this cache size\n                hit_count = 0\n                total_time = 0\n\n                for i, prompt in enumerate(test_prompts):\n                    start = time.time()\n                    _ = steadytext.generate(prompt, seed=42)\n                    duration = time.time() - start\n                    total_time += duration\n\n                    # Simple hit detection (fast = hit)\n                    if duration &lt; 0.1:\n                        hit_count += 1\n\n                results[size] = {\n                    'hit_rate': hit_count / len(test_prompts) * 100,\n                    'avg_time': total_time / len(test_prompts),\n                    'total_time': total_time\n                }\n\n        finally:\n            # Restore original capacity\n            os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = original_capacity\n\n        return results\n\n    def find_optimal_cache_size(self, typical_prompts: List[str]) -&gt; int:\n        \"\"\"Find optimal cache size for typical usage.\"\"\"\n        unique_prompts = len(set(typical_prompts))\n        prompt_frequency = {}\n\n        for prompt in typical_prompts:\n            prompt_frequency[prompt] = prompt_frequency.get(prompt, 0) + 1\n\n        # Prompts that appear more than once\n        repeated_prompts = sum(1 for count in prompt_frequency.values() if count &gt; 1)\n\n        # Recommend size based on usage pattern\n        if repeated_prompts / unique_prompts &gt; 0.5:\n            # High repetition - smaller cache OK\n            optimal = int(unique_prompts * 0.7)\n        else:\n            # Low repetition - need larger cache\n            optimal = int(unique_prompts * 1.2)\n\n        # Ensure reasonable bounds\n        optimal = max(64, min(optimal, 1024))\n\n        return optimal\n\n# Run benchmarks\nbenchmark = CacheBenchmark()\n\n# Single prompt benchmark\nprompt = \"Write a comprehensive guide to Python decorators\"\nresult = benchmark.benchmark_single_prompt(prompt, iterations=20)\n\nprint(\"=== Single Prompt Benchmark ===\")\nprint(f\"Prompt: {result['prompt']}\")\nprint(f\"Cold cache: {result['cold_time']:.3f}s\")\nprint(f\"Warm cache: {result['warm_avg']:.3f}s \u00b1 {result['warm_std']:.3f}s\")\nprint(f\"Speedup: {result['speedup']:.1f}x\")\n\n# Typical usage pattern\ntypical_prompts = [\n    \"Write a function\",\n    \"Explain this error\",\n    \"Write a function\",  # Repeated\n    \"Generate test data\",\n    \"Write a function\",  # Popular\n    \"Create documentation\",\n    \"Explain this error\",  # Repeated\n    \"Optimize code\",\n    \"Write unit tests\",\n    \"Write a function\"   # Very popular\n]\n\noptimal = benchmark.find_optimal_cache_size(typical_prompts)\nprint(f\"\\nRecommended cache size for your usage: {optimal}\")\n</code></pre>"},{"location":"examples/caching/#cache-debugging","title":"Cache Debugging","text":""},{"location":"examples/caching/#cache-inspector","title":"Cache Inspector","text":"<pre><code>import steadytext\nfrom typing import Optional\nimport json\n\nclass CacheInspector:\n    \"\"\"Debug and inspect cache behavior.\"\"\"\n\n    def __init__(self):\n        self.cache_manager = steadytext.get_cache_manager()\n        self.generation_log = []\n\n    def trace_generation(self, prompt: str, seed: int = 42, **kwargs):\n        \"\"\"Trace a generation through the cache system.\"\"\"\n        # Get initial stats\n        stats_before = self.cache_manager.get_cache_stats()\n\n        # Time the generation\n        import time\n        start_time = time.time()\n        result = steadytext.generate(prompt, seed=seed, **kwargs)\n        duration = time.time() - start_time\n\n        # Get final stats\n        stats_after = self.cache_manager.get_cache_stats()\n\n        # Analyze what happened\n        gen_cache_before = stats_before['generation']\n        gen_cache_after = stats_after['generation']\n\n        cache_hit = gen_cache_after.get('hits', 0) &gt; gen_cache_before.get('hits', 0)\n\n        trace = {\n            'prompt': prompt,\n            'seed': seed,\n            'duration': duration,\n            'cache_hit': cache_hit,\n            'cache_size_before': gen_cache_before['size'],\n            'cache_size_after': gen_cache_after['size'],\n            'result_preview': result[:100] + '...' if len(result) &gt; 100 else result\n        }\n\n        self.generation_log.append(trace)\n\n        return trace\n\n    def analyze_cache_behavior(self):\n        \"\"\"Analyze patterns in cache behavior.\"\"\"\n        if not self.generation_log:\n            return \"No generation logs to analyze\"\n\n        total = len(self.generation_log)\n        hits = sum(1 for log in self.generation_log if log['cache_hit'])\n\n        hit_timings = [log['duration'] for log in self.generation_log if log['cache_hit']]\n        miss_timings = [log['duration'] for log in self.generation_log if not log['cache_hit']]\n\n        analysis = {\n            'total_generations': total,\n            'cache_hits': hits,\n            'cache_misses': total - hits,\n            'hit_rate': hits / total * 100 if total &gt; 0 else 0,\n            'avg_hit_time': sum(hit_timings) / len(hit_timings) if hit_timings else 0,\n            'avg_miss_time': sum(miss_timings) / len(miss_timings) if miss_timings else 0,\n            'time_saved': sum(miss_timings) - sum(hit_timings) if hit_timings else 0\n        }\n\n        return analysis\n\n    def export_trace_log(self, filename: str):\n        \"\"\"Export trace log for analysis.\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(self.generation_log, f, indent=2)\n\n        print(f\"Trace log exported to {filename}\")\n\n# Debug cache behavior\ninspector = CacheInspector()\n\n# Trace various generations\ntest_cases = [\n    (\"Write a hello world program\", 42),\n    (\"Write a hello world program\", 42),  # Should hit\n    (\"Explain recursion\", 42),\n    (\"Write a hello world program\", 100),  # Different seed\n    (\"Explain recursion\", 42),  # Should hit\n]\n\nprint(\"=== Cache Trace Log ===\")\nfor prompt, seed in test_cases:\n    trace = inspector.trace_generation(prompt, seed)\n    print(f\"Prompt: {prompt[:30]}... | Seed: {seed}\")\n    print(f\"  Hit: {trace['cache_hit']} | Time: {trace['duration']:.3f}s\")\n    print(f\"  Cache size: {trace['cache_size_before']} -&gt; {trace['cache_size_after']}\")\n    print()\n\n# Analyze behavior\nanalysis = inspector.analyze_cache_behavior()\nprint(\"\\n=== Cache Behavior Analysis ===\")\nprint(f\"Hit rate: {analysis['hit_rate']:.1f}%\")\nprint(f\"Average hit time: {analysis['avg_hit_time']:.3f}s\")\nprint(f\"Average miss time: {analysis['avg_miss_time']:.3f}s\")\nprint(f\"Time saved by cache: {analysis['time_saved']:.3f}s\")\n\n# Export for further analysis\ninspector.export_trace_log(\"cache_trace.json\")\n</code></pre>"},{"location":"examples/caching/#best-practices","title":"Best Practices","text":""},{"location":"examples/caching/#1-cache-configuration","title":"1. Cache Configuration","text":"<pre><code># optimal_config.py - Optimal cache configuration\n\nimport os\n\ndef configure_cache_for_production():\n    \"\"\"Configure cache for production use.\"\"\"\n    # Larger cache for production\n    os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = '1024'\n    os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = '200.0'\n\n    # Even larger for embeddings (they're smaller)\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_CAPACITY'] = '2048'\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB'] = '500.0'\n\ndef configure_cache_for_development():\n    \"\"\"Configure cache for development.\"\"\"\n    # Smaller cache for development\n    os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = '128'\n    os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = '25.0'\n\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_CAPACITY'] = '256'\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB'] = '50.0'\n\ndef configure_cache_for_testing():\n    \"\"\"Configure cache for testing.\"\"\"\n    # Minimal cache for testing\n    os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = '32'\n    os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = '10.0'\n\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_CAPACITY'] = '64'\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB'] = '20.0'\n</code></pre>"},{"location":"examples/caching/#2-cache-warming-strategy","title":"2. Cache Warming Strategy","text":"<pre><code># cache_warmer.py - Strategic cache warming\n\nimport steadytext\nfrom typing import List, Dict\n\nclass StrategicCacheWarmer:\n    \"\"\"Warm cache based on usage patterns.\"\"\"\n\n    def __init__(self):\n        self.priority_prompts = {\n            'high': [],      # Always cache\n            'medium': [],    # Cache if space\n            'low': []        # Cache opportunistically\n        }\n\n    def add_prompts(self, prompts: List[str], priority: str = 'medium'):\n        \"\"\"Add prompts to warming queue.\"\"\"\n        self.priority_prompts[priority].extend(prompts)\n\n    def warm_cache(self, available_time: float = 30.0):\n        \"\"\"Warm cache within time budget.\"\"\"\n        import time\n        start_time = time.time()\n        warmed = {'high': 0, 'medium': 0, 'low': 0}\n\n        # Process by priority\n        for priority in ['high', 'medium', 'low']:\n            for prompt in self.priority_prompts[priority]:\n                if time.time() - start_time &gt; available_time:\n                    break\n\n                _ = steadytext.generate(prompt, max_new_tokens=100)\n                warmed[priority] += 1\n\n        return warmed\n\n# Configure warming\nwarmer = StrategicCacheWarmer()\n\n# High priority - critical paths\nwarmer.add_prompts([\n    \"Generate error message\",\n    \"Create validation response\",\n    \"Format API response\"\n], priority='high')\n\n# Medium priority - common operations\nwarmer.add_prompts([\n    \"Write documentation\",\n    \"Generate test data\",\n    \"Create example\"\n], priority='medium')\n\n# Low priority - nice to have\nwarmer.add_prompts([\n    \"Explain concept\",\n    \"Generate tutorial\"\n], priority='low')\n\n# Warm with 10 second budget\nwarmed = warmer.warm_cache(available_time=10.0)\nprint(f\"Cache warmed: {warmed}\")\n</code></pre>"},{"location":"examples/caching/#3-cache-monitoring","title":"3. Cache Monitoring","text":"<pre><code># monitor_cache.py - Production cache monitoring\n\nimport steadytext\nimport time\nimport logging\nfrom datetime import datetime\n\nclass ProductionCacheMonitor:\n    \"\"\"Monitor cache in production.\"\"\"\n\n    def __init__(self, alert_threshold: float = 50.0):\n        self.alert_threshold = alert_threshold\n        self.logger = logging.getLogger(__name__)\n\n    def check_cache_health(self) -&gt; Dict:\n        \"\"\"Check cache health metrics.\"\"\"\n        cache_manager = steadytext.get_cache_manager()\n        stats = cache_manager.get_cache_stats()\n\n        health = {\n            'timestamp': datetime.now().isoformat(),\n            'healthy': True,\n            'warnings': []\n        }\n\n        # Check generation cache\n        gen_stats = stats['generation']\n        gen_hit_rate = self._calculate_hit_rate(gen_stats)\n\n        if gen_hit_rate &lt; self.alert_threshold:\n            health['warnings'].append(\n                f\"Low generation cache hit rate: {gen_hit_rate:.1f}%\"\n            )\n            health['healthy'] = False\n\n        # Check embedding cache\n        emb_stats = stats['embedding']\n        emb_hit_rate = self._calculate_hit_rate(emb_stats)\n\n        if emb_hit_rate &lt; self.alert_threshold:\n            health['warnings'].append(\n                f\"Low embedding cache hit rate: {emb_hit_rate:.1f}%\"\n            )\n            health['healthy'] = False\n\n        # Check cache size\n        if gen_stats['size'] &gt;= gen_stats.get('capacity', 256) * 0.95:\n            health['warnings'].append(\"Generation cache near capacity\")\n\n        if emb_stats['size'] &gt;= emb_stats.get('capacity', 512) * 0.95:\n            health['warnings'].append(\"Embedding cache near capacity\")\n\n        return health\n\n    def _calculate_hit_rate(self, stats: Dict) -&gt; float:\n        \"\"\"Calculate hit rate from stats.\"\"\"\n        hits = stats.get('hits', 0)\n        misses = stats.get('misses', 0)\n        total = hits + misses\n\n        return (hits / total * 100) if total &gt; 0 else 0.0\n\n    def continuous_monitoring(self, interval: int = 300):\n        \"\"\"Monitor cache continuously.\"\"\"\n        while True:\n            health = self.check_cache_health()\n\n            if not health['healthy']:\n                self.logger.warning(f\"Cache health issues: {health['warnings']}\")\n            else:\n                self.logger.info(\"Cache healthy\")\n\n            time.sleep(interval)\n\n# Set up monitoring\nmonitor = ProductionCacheMonitor(alert_threshold=60.0)\nhealth = monitor.check_cache_health()\n\nprint(\"=== Cache Health Check ===\")\nprint(f\"Status: {'Healthy' if health['healthy'] else 'Issues Detected'}\")\nif health['warnings']:\n    print(\"Warnings:\")\n    for warning in health['warnings']:\n        print(f\"  - {warning}\")\n</code></pre>"},{"location":"examples/caching/#summary","title":"Summary","text":"<p>Effective cache management in SteadyText involves:</p> <ol> <li>Configuration: Size caches appropriately for your workload</li> <li>Warming: Pre-populate cache with common prompts</li> <li>Monitoring: Track hit rates and performance</li> <li>Optimization: Adjust based on usage patterns</li> <li>Debugging: Use tools to understand cache behavior</li> </ol> <p>Remember: A well-tuned cache can provide 10-100x speedup for repeated operations!</p>"},{"location":"examples/content-management/","title":"Content Management with AI-Powered Generation","text":"<p>Build a smart content management system that generates, optimizes, and personalizes content using SteadyText's deterministic AI capabilities.</p>"},{"location":"examples/content-management/#overview","title":"Overview","text":"<p>This tutorial demonstrates how to: - Auto-generate product descriptions and SEO metadata - Create content variations for A/B testing - Build a content moderation pipeline - Personalize content based on user segments - Generate multilingual content variants</p>"},{"location":"examples/content-management/#prerequisites","title":"Prerequisites","text":"<pre><code># Start PostgreSQL with SteadyText\ndocker run -d -p 5432:5432 --name steadytext-cms julep/pg-steadytext\n\n# Connect to the database\npsql -h localhost -U postgres\n\n# Enable required extensions\nCREATE EXTENSION IF NOT EXISTS pg_steadytext CASCADE;\nCREATE EXTENSION IF NOT EXISTS pgcrypto;  -- For UUIDs\n</code></pre>"},{"location":"examples/content-management/#database-schema","title":"Database Schema","text":"<p>Let's create a comprehensive content management schema:</p> <pre><code>-- Products table\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    sku VARCHAR(50) UNIQUE NOT NULL,\n    name VARCHAR(200) NOT NULL,\n    category VARCHAR(100),\n    brand VARCHAR(100),\n    price DECIMAL(10, 2),\n    features JSONB,\n    specifications JSONB,\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Product descriptions with versions\nCREATE TABLE product_descriptions (\n    id SERIAL PRIMARY KEY,\n    product_id INTEGER REFERENCES products(id),\n    version INTEGER DEFAULT 1,\n    language VARCHAR(10) DEFAULT 'en',\n    title VARCHAR(200),\n    short_description TEXT,\n    long_description TEXT,\n    seo_title VARCHAR(70),\n    seo_description VARCHAR(160),\n    seo_keywords TEXT[],\n    generated_by VARCHAR(50), -- 'human' or 'ai'\n    is_active BOOLEAN DEFAULT FALSE,\n    performance_score DECIMAL(5, 2), -- A/B test performance\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Content templates\nCREATE TABLE content_templates (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) UNIQUE NOT NULL,\n    template_type VARCHAR(50), -- 'product', 'email', 'landing_page'\n    prompt_template TEXT NOT NULL,\n    variables JSONB, -- Expected variables\n    output_format VARCHAR(20) DEFAULT 'text', -- 'text', 'html', 'json'\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- User segments for personalization\nCREATE TABLE user_segments (\n    id SERIAL PRIMARY KEY,\n    name VARCHAR(100) NOT NULL,\n    criteria JSONB, -- Segment definition\n    preferences JSONB, -- Content preferences\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Content moderation log\nCREATE TABLE moderation_log (\n    id SERIAL PRIMARY KEY,\n    content_type VARCHAR(50),\n    content_id INTEGER,\n    original_content TEXT,\n    moderated_content TEXT,\n    issues_found JSONB,\n    action_taken VARCHAR(50), -- 'approved', 'modified', 'rejected'\n    moderated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Create indexes\nCREATE INDEX idx_descriptions_product ON product_descriptions(product_id);\nCREATE INDEX idx_descriptions_active ON product_descriptions(is_active);\nCREATE INDEX idx_moderation_content ON moderation_log(content_type, content_id);\n</code></pre>"},{"location":"examples/content-management/#auto-generate-product-descriptions","title":"Auto-Generate Product Descriptions","text":"<p>Create a sophisticated product description generator:</p> <pre><code>-- Function to generate product descriptions\nCREATE OR REPLACE FUNCTION generate_product_description(\n    p_product_id INTEGER,\n    p_style VARCHAR DEFAULT 'professional', -- 'professional', 'casual', 'technical'\n    p_length VARCHAR DEFAULT 'medium' -- 'short', 'medium', 'long'\n)\nRETURNS TABLE (\n    title VARCHAR,\n    short_description TEXT,\n    long_description TEXT,\n    seo_title VARCHAR,\n    seo_description VARCHAR,\n    seo_keywords TEXT[]\n) AS $$\nDECLARE\n    v_product products%ROWTYPE;\n    v_features TEXT;\n    v_specs TEXT;\nBEGIN\n    -- Get product details\n    SELECT * INTO v_product FROM products WHERE id = p_product_id;\n\n    -- Format features and specifications\n    v_features := COALESCE(\n        (SELECT string_agg(key || ': ' || value, ', ')\n         FROM jsonb_each_text(v_product.features)),\n        'Standard features'\n    );\n\n    v_specs := COALESCE(\n        (SELECT string_agg(key || ': ' || value, ', ')\n         FROM jsonb_each_text(v_product.specifications)),\n        'Standard specifications'\n    );\n\n    RETURN QUERY\n    SELECT\n        -- Generate compelling title\n        steadytext_generate(\n            format('Create a compelling product title for: %s %s (max 60 chars)',\n                v_product.brand, v_product.name),\n            max_tokens := 20\n        )::VARCHAR AS title,\n\n        -- Short description for product cards\n        steadytext_generate(\n            format('Write a %s style product description for %s %s with features: %s (max 150 chars)',\n                p_style, v_product.brand, v_product.name, v_features),\n            max_tokens := 50\n        ) AS short_description,\n\n        -- Long description for product pages\n        steadytext_generate(\n            format('Write a detailed %s style product description for %s %s. Features: %s. Specs: %s. Price: $%s',\n                p_style, v_product.brand, v_product.name, v_features, v_specs, v_product.price),\n            max_tokens := CASE p_length \n                WHEN 'short' THEN 100\n                WHEN 'long' THEN 300\n                ELSE 200\n            END\n        ) AS long_description,\n\n        -- SEO title (max 70 chars)\n        steadytext_generate(\n            format('Create SEO title for: %s %s %s (max 60 chars)',\n                v_product.brand, v_product.name, v_product.category),\n            max_tokens := 20\n        )::VARCHAR AS seo_title,\n\n        -- SEO meta description (max 160 chars)\n        steadytext_generate(\n            format('Write SEO meta description for %s %s with benefits and call-to-action (max 150 chars)',\n                v_product.brand, v_product.name),\n            max_tokens := 50\n        )::VARCHAR AS seo_description,\n\n        -- SEO keywords\n        string_to_array(\n            steadytext_generate(\n                format('List 5 SEO keywords for: %s %s %s (comma separated)',\n                    v_product.brand, v_product.name, v_product.category),\n                max_tokens := 30\n            ),\n            ', '\n        ) AS seo_keywords;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/content-management/#ab-testing-content-variations","title":"A/B Testing Content Variations","text":"<p>Generate multiple content variations for testing:</p> <pre><code>-- Generate content variations for A/B testing\nCREATE OR REPLACE FUNCTION create_content_variations(\n    p_product_id INTEGER,\n    p_num_variations INTEGER DEFAULT 3\n)\nRETURNS TABLE (\n    variation_id INTEGER,\n    title VARCHAR,\n    description TEXT,\n    style VARCHAR\n) AS $$\nDECLARE\n    v_styles VARCHAR[] := ARRAY['professional', 'casual', 'technical', 'enthusiastic', 'minimalist'];\n    v_style VARCHAR;\n    v_counter INTEGER := 1;\nBEGIN\n    -- Generate variations with different styles\n    WHILE v_counter &lt;= p_num_variations LOOP\n        v_style := v_styles[1 + (v_counter % array_length(v_styles, 1))];\n\n        RETURN QUERY\n        WITH generated AS (\n            SELECT * FROM generate_product_description(\n                p_product_id, \n                v_style, \n                'medium'\n            )\n        )\n        INSERT INTO product_descriptions (\n            product_id, version, title, \n            short_description, long_description,\n            seo_title, seo_description, seo_keywords,\n            generated_by, is_active\n        )\n        SELECT \n            p_product_id,\n            (SELECT COALESCE(MAX(version), 0) + 1 \n             FROM product_descriptions \n             WHERE product_id = p_product_id),\n            g.title,\n            g.short_description,\n            g.long_description,\n            g.seo_title,\n            g.seo_description,\n            g.seo_keywords,\n            'ai',\n            v_counter = 1 -- First variation is active by default\n        FROM generated g\n        RETURNING \n            id AS variation_id,\n            title,\n            short_description AS description,\n            v_style AS style;\n\n        v_counter := v_counter + 1;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/content-management/#content-moderation-pipeline","title":"Content Moderation Pipeline","text":"<p>Automatically moderate user-generated content:</p> <pre><code>-- Content moderation function\nCREATE OR REPLACE FUNCTION moderate_content(\n    p_content TEXT,\n    p_content_type VARCHAR,\n    p_content_id INTEGER\n)\nRETURNS TABLE (\n    moderated_content TEXT,\n    is_safe BOOLEAN,\n    issues JSONB,\n    action VARCHAR\n) AS $$\nDECLARE\n    v_toxicity_check VARCHAR;\n    v_issues JSONB := '[]'::JSONB;\n    v_cleaned_content TEXT;\nBEGIN\n    -- Check for toxicity/inappropriate content\n    v_toxicity_check := steadytext_generate_choice(\n        'Is this content appropriate for all audiences: ' || LEFT(p_content, 500),\n        ARRAY['safe', 'potentially_inappropriate', 'inappropriate', 'toxic']\n    );\n\n    -- Build issues list\n    IF v_toxicity_check != 'safe' THEN\n        v_issues := v_issues || jsonb_build_object(\n            'type', 'toxicity',\n            'severity', v_toxicity_check\n        );\n    END IF;\n\n    -- Check for spam patterns\n    IF p_content ~* '(click here|buy now|limited time|act now){3,}' THEN\n        v_issues := v_issues || jsonb_build_object(\n            'type', 'spam',\n            'severity', 'high'\n        );\n    END IF;\n\n    -- Clean or reject content based on issues\n    IF v_toxicity_check IN ('inappropriate', 'toxic') THEN\n        -- Reject toxic content\n        v_cleaned_content := '[Content removed due to policy violation]';\n\n        INSERT INTO moderation_log (\n            content_type, content_id, original_content,\n            moderated_content, issues_found, action_taken\n        ) VALUES (\n            p_content_type, p_content_id, p_content,\n            v_cleaned_content, v_issues, 'rejected'\n        );\n\n        RETURN QUERY SELECT \n            v_cleaned_content,\n            FALSE,\n            v_issues,\n            'rejected'::VARCHAR;\n    ELSE\n        -- Clean up mild issues\n        v_cleaned_content := steadytext_generate(\n            'Rewrite this content to be more appropriate while keeping the same meaning: ' || p_content,\n            max_tokens := 200\n        );\n\n        INSERT INTO moderation_log (\n            content_type, content_id, original_content,\n            moderated_content, issues_found, action_taken\n        ) VALUES (\n            p_content_type, p_content_id, p_content,\n            v_cleaned_content, v_issues, \n            CASE WHEN jsonb_array_length(v_issues) &gt; 0 THEN 'modified' ELSE 'approved' END\n        );\n\n        RETURN QUERY SELECT \n            v_cleaned_content,\n            jsonb_array_length(v_issues) = 0,\n            v_issues,\n            CASE WHEN jsonb_array_length(v_issues) &gt; 0 THEN 'modified' ELSE 'approved' END::VARCHAR;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/content-management/#dynamic-content-templates","title":"Dynamic Content Templates","text":"<p>Create reusable content templates:</p> <pre><code>-- Insert sample templates\nINSERT INTO content_templates (name, template_type, prompt_template, variables) VALUES\n(\n    'product_email_campaign',\n    'email',\n    'Write a promotional email for {product_name} highlighting {key_feature} with a {tone} tone. Include subject line.',\n    '{\"product_name\": \"string\", \"key_feature\": \"string\", \"tone\": \"string\"}'::JSONB\n),\n(\n    'category_landing_page',\n    'landing_page',\n    'Create landing page copy for {category} products. Include hero text, 3 benefit points, and CTA. Style: {style}',\n    '{\"category\": \"string\", \"style\": \"string\"}'::JSONB\n),\n(\n    'seasonal_promotion',\n    'product',\n    'Write {season} promotional text for {product_name}. Emphasize seasonal benefits and include urgency.',\n    '{\"season\": \"string\", \"product_name\": \"string\"}'::JSONB\n);\n\n-- Function to use templates\nCREATE OR REPLACE FUNCTION generate_from_template(\n    p_template_name VARCHAR,\n    p_variables JSONB\n)\nRETURNS TEXT AS $$\nDECLARE\n    v_template content_templates%ROWTYPE;\n    v_prompt TEXT;\n    v_key TEXT;\n    v_value TEXT;\nBEGIN\n    -- Get template\n    SELECT * INTO v_template FROM content_templates WHERE name = p_template_name;\n\n    -- Replace variables in prompt\n    v_prompt := v_template.prompt_template;\n    FOR v_key, v_value IN SELECT * FROM jsonb_each_text(p_variables) LOOP\n        v_prompt := REPLACE(v_prompt, '{' || v_key || '}', v_value);\n    END LOOP;\n\n    -- Generate content\n    RETURN steadytext_generate(v_prompt, max_tokens := 300);\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/content-management/#personalized-content-generation","title":"Personalized Content Generation","text":"<p>Generate content tailored to user segments:</p> <pre><code>-- Insert sample user segments\nINSERT INTO user_segments (name, criteria, preferences) VALUES\n('budget_conscious', \n '{\"income\": \"low_medium\", \"behavior\": \"price_sensitive\"}'::JSONB,\n '{\"tone\": \"value_focused\", \"highlight\": \"savings\"}'::JSONB),\n('premium_buyers',\n '{\"income\": \"high\", \"behavior\": \"quality_focused\"}'::JSONB,\n '{\"tone\": \"luxurious\", \"highlight\": \"exclusivity\"}'::JSONB),\n('tech_enthusiasts',\n '{\"interests\": [\"technology\", \"gadgets\"], \"age\": \"18-35\"}'::JSONB,\n '{\"tone\": \"technical\", \"highlight\": \"specifications\"}'::JSONB);\n\n-- Personalized content function\nCREATE OR REPLACE FUNCTION generate_personalized_content(\n    p_product_id INTEGER,\n    p_segment_id INTEGER\n)\nRETURNS TEXT AS $$\nDECLARE\n    v_product products%ROWTYPE;\n    v_segment user_segments%ROWTYPE;\n    v_prompt TEXT;\nBEGIN\n    SELECT * INTO v_product FROM products WHERE id = p_product_id;\n    SELECT * INTO v_segment FROM user_segments WHERE id = p_segment_id;\n\n    -- Build personalized prompt\n    v_prompt := format(\n        'Write product description for %s %s targeting %s customers. Tone: %s. Highlight: %s. Price: $%s',\n        v_product.brand,\n        v_product.name,\n        v_segment.name,\n        v_segment.preferences-&gt;&gt;'tone',\n        v_segment.preferences-&gt;&gt;'highlight',\n        v_product.price\n    );\n\n    RETURN steadytext_generate(v_prompt, max_tokens := 150);\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/content-management/#bulk-content-operations","title":"Bulk Content Operations","text":"<p>Process multiple items efficiently:</p> <pre><code>-- Bulk generate descriptions for all products\nCREATE OR REPLACE FUNCTION bulk_generate_descriptions(\n    p_category VARCHAR DEFAULT NULL,\n    p_limit INTEGER DEFAULT 100\n)\nRETURNS TABLE (\n    product_id INTEGER,\n    product_name VARCHAR,\n    status VARCHAR\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH products_to_process AS (\n        SELECT p.id, p.name\n        FROM products p\n        LEFT JOIN product_descriptions pd ON p.id = pd.product_id AND pd.is_active\n        WHERE pd.id IS NULL  -- No active description\n          AND (p_category IS NULL OR p.category = p_category)\n        LIMIT p_limit\n    )\n    SELECT \n        ptp.id,\n        ptp.name,\n        CASE \n            WHEN generate_product_description(ptp.id) IS NOT NULL THEN 'generated'\n            ELSE 'failed'\n        END AS status\n    FROM products_to_process ptp;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/content-management/#content-performance-analytics","title":"Content Performance Analytics","text":"<p>Track and analyze content performance:</p> <pre><code>-- Content performance view\nCREATE OR REPLACE VIEW content_performance AS\nWITH performance_metrics AS (\n    SELECT \n        pd.id,\n        pd.product_id,\n        pd.version,\n        pd.title,\n        pd.generated_by,\n        pd.performance_score,\n        p.name AS product_name,\n        p.category,\n        pd.created_at,\n        RANK() OVER (PARTITION BY pd.product_id ORDER BY pd.performance_score DESC) AS rank\n    FROM product_descriptions pd\n    JOIN products p ON pd.product_id = p.id\n    WHERE pd.performance_score IS NOT NULL\n)\nSELECT \n    *,\n    steadytext_generate(\n        format('Analyze why this content performed %s: Title: %s, Category: %s, Score: %s',\n            CASE \n                WHEN performance_score &gt; 80 THEN 'excellently'\n                WHEN performance_score &gt; 60 THEN 'well'\n                ELSE 'poorly'\n            END,\n            title,\n            category,\n            performance_score\n        ),\n        max_tokens := 100\n    ) AS performance_analysis\nFROM performance_metrics\nWHERE rank &lt;= 3;  -- Top 3 versions per product\n</code></pre>"},{"location":"examples/content-management/#sample-data-and-testing","title":"Sample Data and Testing","text":"<pre><code>-- Insert sample products\nINSERT INTO products (sku, name, category, brand, price, features, specifications) VALUES\n('WH-1000XM5', 'Wireless Noise-Cancelling Headphones', 'Audio', 'Sony', 399.99,\n '{\"noise_cancelling\": \"Industry-leading\", \"battery\": \"30 hours\", \"comfort\": \"Premium\"}'::JSONB,\n '{\"weight\": \"250g\", \"bluetooth\": \"5.2\", \"drivers\": \"30mm\"}'::JSONB),\n('MBA-M2-2023', 'MacBook Air M2', 'Computers', 'Apple', 1199.00,\n '{\"processor\": \"M2 chip\", \"display\": \"13.6-inch Retina\", \"battery\": \"18 hours\"}'::JSONB,\n '{\"ram\": \"8GB\", \"storage\": \"256GB SSD\", \"weight\": \"1.24kg\"}'::JSONB),\n('OLED55C3', '55\" OLED Smart TV', 'Electronics', 'LG', 1299.99,\n '{\"display\": \"OLED\", \"resolution\": \"4K\", \"smart\": \"webOS\"}'::JSONB,\n '{\"size\": \"55 inches\", \"refresh\": \"120Hz\", \"hdr\": \"Dolby Vision\"}'::JSONB);\n\n-- Generate content for products\nSELECT * FROM create_content_variations(1, 3);\nSELECT * FROM bulk_generate_descriptions('Audio', 10);\n\n-- Test content moderation\nSELECT * FROM moderate_content(\n    'This is an amazing product! Buy now for 50% off!!!!! Click here!!!',\n    'review',\n    1\n);\n\n-- Generate from template\nSELECT generate_from_template(\n    'seasonal_promotion',\n    '{\"season\": \"Summer\", \"product_name\": \"Wireless Headphones\"}'::JSONB\n);\n</code></pre>"},{"location":"examples/content-management/#best-practices","title":"Best Practices","text":"<ol> <li>Version Control: Keep all generated content versions for comparison</li> <li>A/B Testing: Always test AI-generated content against human-written</li> <li>Moderation: Review AI outputs before publishing</li> <li>Caching: Leverage SteadyText's built-in caching for repeated generations</li> <li>Templates: Use templates for consistent brand voice</li> </ol>"},{"location":"examples/content-management/#next-steps","title":"Next Steps","text":"<ul> <li>Customer Intelligence Tutorial \u2192</li> <li>Data Pipelines Example \u2192</li> <li>Migration from OpenAI \u2192</li> </ul> <p>Pro Tip</p> <p>Use database triggers to automatically generate content when new products are added. This ensures every product has optimized descriptions from day one.</p>"},{"location":"examples/custom-seeds/","title":"Custom Seeds Guide","text":"<p>Learn how to use custom seeds in SteadyText for reproducible variations in text generation and embeddings.</p>"},{"location":"examples/custom-seeds/#overview","title":"Overview","text":"<p>SteadyText uses seeds to control randomness, allowing you to: - Generate different outputs for the same prompt - Ensure reproducible results across runs - Create variations while maintaining determinism - Control randomness in production systems</p>"},{"location":"examples/custom-seeds/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding Seeds</li> <li>Text Generation with Seeds</li> <li>Streaming with Seeds</li> <li>Embeddings with Seeds</li> <li>Seed Strategies</li> <li>CLI Seed Usage</li> <li>Best Practices</li> <li>Troubleshooting</li> </ul>"},{"location":"examples/custom-seeds/#understanding-seeds","title":"Understanding Seeds","text":""},{"location":"examples/custom-seeds/#what-is-a-seed","title":"What is a Seed?","text":"<p>A seed is an integer that initializes the random number generator. Same seed + same input = same output, always.</p> <pre><code>import steadytext\n\n# Default seed (42) - always same result\ntext1 = steadytext.generate(\"Hello world\")\ntext2 = steadytext.generate(\"Hello world\")\nassert text1 == text2  # Always true\n\n# Custom seeds - different results\ntext3 = steadytext.generate(\"Hello world\", seed=123)\ntext4 = steadytext.generate(\"Hello world\", seed=456)\nassert text3 != text4  # Different seeds, different outputs\n</code></pre>"},{"location":"examples/custom-seeds/#seed-behavior","title":"Seed Behavior","text":"<ul> <li>Deterministic: Same seed always produces same result</li> <li>Independent: Each operation uses its own seed</li> <li>Cascading: Seed affects all random choices in generation</li> <li>Cross-platform: Same seed works identically everywhere</li> </ul>"},{"location":"examples/custom-seeds/#basic-seed-usage","title":"Basic Seed Usage","text":""},{"location":"examples/custom-seeds/#simple-text-generation","title":"Simple Text Generation","text":"<pre><code>import steadytext\n\n# Default seed (42) - consistent across runs\ntext1 = steadytext.generate(\"Write a haiku about AI\")\ntext2 = steadytext.generate(\"Write a haiku about AI\")\nassert text1 == text2  # Always identical\n\n# Custom seed - reproducible but different from default\ntext3 = steadytext.generate(\"Write a haiku about AI\", seed=123)\ntext4 = steadytext.generate(\"Write a haiku about AI\", seed=123)\nassert text3 == text4  # Same seed, same result\nassert text1 != text3  # Different seeds, different results\n\nprint(\"Default seed result:\", text1)\nprint(\"Custom seed result:\", text3)\n</code></pre>"},{"location":"examples/custom-seeds/#embedding-generation","title":"Embedding Generation","text":"<pre><code>import numpy as np\n\n# Default seed embeddings\nemb1 = steadytext.embed(\"artificial intelligence\")\nemb2 = steadytext.embed(\"artificial intelligence\")\nassert np.array_equal(emb1, emb2)  # Identical\n\n# Custom seed embeddings\nemb3 = steadytext.embed(\"artificial intelligence\", seed=456)\nemb4 = steadytext.embed(\"artificial intelligence\", seed=456)\nassert np.array_equal(emb3, emb4)  # Same seed, same result\nassert not np.array_equal(emb1, emb3)  # Different seeds, different embeddings\n\n# Calculate similarity between different seed embeddings\nsimilarity = np.dot(emb1, emb3)  # Cosine similarity (vectors are normalized)\nprint(f\"Similarity between different seeds: {similarity:.3f}\")\n</code></pre>"},{"location":"examples/custom-seeds/#reproducible-research","title":"Reproducible Research","text":""},{"location":"examples/custom-seeds/#research-workflow-example","title":"Research Workflow Example","text":"<p>```python import steadytext import json from datetime import datetime</p> <p>class ReproducibleResearch:     def init(self, base_seed=42):         self.base_seed = base_seed         self.current_seed = base_seed         self.results = []         self.metadata = {             \"start_time\": datetime.now().isoformat(),             \"base_seed\": base_seed,             \"steadytext_version\": \"2.1.0+\",         }</p> <pre><code>def generate_with_logging(self, prompt, **kwargs):\n    \"\"\"Generate text and log the result with seed information.\"\"\"\n    result = steadytext.generate(prompt, seed=self.current_seed, **kwargs)\n\n    self.results.append({\n        \"seed\": self.current_seed,\n        \"prompt\": prompt,\n        \"result\": result,\n        \"kwargs\": kwargs,\n        \"timestamp\": datetime.now().isoformat()\n    })\n\n    self.current_seed += 1  # Increment for next generation\n    return result\n\ndef embed_with_logging(self, text, **kwargs):\n    \"\"\"Generate embedding and log the result with seed information.\"\"\"\n    embedding = steadytext.embed(text, seed=self.current_seed, **kwargs)\n\n    self.results.append({\n        \"seed\": self.current_seed,\n        \"text\": text,\n        \"embedding\": embedding.tolist(),  # Convert numpy array to list\n        \"kwargs\": kwargs,\n        \"timestamp\": datetime.now().isoformat()\n    })\n\n    self.current_seed += 1\n    return embedding\n\ndef save_results(self, filename):\n    \"\"\"Save all results to a JSON file for reproducibility.\"\"\"\n    with open(filename, 'w') as f:\n        json.dump({\n            \"metadata\": self.metadata,\n            \"results\": self.results\n        }, f, indent=2)\n\ndef load_and_verify(self, filename):\n    \"\"\"Load previous results and verify reproducibility.\"\"\"\n    with open(filename, 'r') as f:\n        data = json.load(f)\n\n    print(\"Verifying reproducibility...\")\n    for result in data[\"results\"]:\n        if \"prompt\" in result:  # Text generation\n            regenerated = steadytext.generate(\n                result[\"prompt\"], \n                seed=result[\"seed\"],\n                **result[\"kwargs\"]\n            )\n            if regenerated == result[\"result\"]:\n                print(f\"\u2713 Seed {result['seed']}: Text generation verified\")\n            else:\n                print(f\"\u2717 Seed {result['seed']}: Text generation FAILED\")\n\n        elif \"text\" in result:  # Embedding\n            regenerated = steadytext.embed(\n                result[\"text\"],\n                seed=result[\"seed\"],\n                **result[\"kwargs\"]\n            )\n            if np.allclose(regenerated, result[\"embedding\"], atol=1e-6):\\n                    print(f\"\u2713 Seed {result['seed']}: Embedding verified\")\\n                else:\\n                    print(f\"\u2717 Seed {result['seed']}: Embedding FAILED\")\\n\\n# Usage example\\nresearch = ReproducibleResearch(base_seed=100)\\n\\n# Conduct research with automatic seed management\\nresearch_prompts = [\\n    \"Explain the benefits of renewable energy\",\\n    \"Describe the future of artificial intelligence\",\\n    \"Summarize the importance of biodiversity\"\\n]\\n\\nfor prompt in research_prompts:\\n    result = research.generate_with_logging(prompt, max_new_tokens=200)\\n    print(f\"Generated {len(result)} characters for: {prompt[:50]}...\")\\n\\n# Generate embeddings for analysis\\nembedding_texts = [\"AI\", \"machine learning\", \"deep learning\"]\\nfor text in embedding_texts:\\n    embedding = research.embed_with_logging(text)\\n    print(f\"Generated embedding for: {text}\")\\n\\n# Save results for reproducibility\\nresearch.save_results(\"research_results.json\")\\nprint(\"Results saved to research_results.json\")\\n\\n# Later: verify reproducibility\\nresearch.load_and_verify(\"research_results.json\")\\n```\\n\\n## A/B Testing with Seeds\\n\\n### Content Comparison Framework\\n\\n```python\\nimport steadytext\\nfrom typing import List, Dict, Any\\n\\nclass ABTester:\\n    def __init__(self):\\n        self.variants = {}\\n    \\n    def create_variants(self, prompt: str, variant_seeds: List[int], **kwargs) -&gt; Dict[str, str]:\\n        \"\"\"Create multiple variants of the same prompt using different seeds.\"\"\"\\n        variants = {}\\n        for i, seed in enumerate(variant_seeds):\\n            variant_name = f\"variant_{chr(65 + i)}\"  # A, B, C, etc.\\n            variants[variant_name] = steadytext.generate(\\n                prompt, \\n                seed=seed, \\n                **kwargs\\n            )\\n        return variants\\n    \\n    def compare_variants(self, prompt: str, seeds: List[int], **kwargs) -&gt; Dict[str, Any]:\\n        \"\"\"Generate and compare multiple variants.\"\"\"\\n        variants = self.create_variants(prompt, seeds, **kwargs)\\n        \\n        analysis = {\\n            \"prompt\": prompt,\\n            \"seeds\": seeds,\\n            \"variants\": variants,\\n            \"stats\": {\\n                variant: {\\n                    \"length\": len(text),\\n                    \"word_count\": len(text.split()),\\n                    \"seed\": seeds[i]\\n                }\\n                for i, (variant, text) in enumerate(variants.items())\\n            }\\n        }\\n        \\n        return analysis\\n    \\n    def batch_compare(self, prompts: List[str], seeds: List[int], **kwargs) -&gt; List[Dict[str, Any]]:\\n        \"\"\"Compare variants for multiple prompts.\"\"\"\\n        return [self.compare_variants(prompt, seeds, **kwargs) for prompt in prompts]\\n\\n# Usage example\\ntester = ABTester()\\n\\n# Define test variants with specific seeds\\ntest_seeds = [100, 200, 300, 400, 500]\\n\\n# Single prompt A/B test\\nresult = tester.compare_variants(\\n    \"Write a compelling product description for a smartwatch\",\\n    seeds=test_seeds[:3],  # Test 3 variants\\n    max_new_tokens=150\\n)\\n\\nprint(\"=== A/B Test Results ===\")\\nfor variant, text in result[\"variants\"].items():\\n    stats = result[\"stats\"][variant]\\n    print(f\"\\\\n{variant.upper()} (seed {stats['seed']}):\")\\n    print(f\"Length: {stats['length']} chars, {stats['word_count']} words\")\\n    print(f\"Text: {text[:100]}...\")\\n\\n# Batch testing for multiple prompts\\nmarketing_prompts = [\\n    \"Create an email subject line for a summer sale\",\\n    \"Write a social media post about a new product launch\",\\n    \"Compose a customer testimonial request\"\\n]\\n\\nbatch_results = tester.batch_compare(\\n    marketing_prompts, \\n    seeds=[42, 123, 456],\\n    max_new_tokens=100\\n)\\n\\nprint(\"\\\\n=== Batch A/B Test Results ===\")\\nfor i, result in enumerate(batch_results):\\n    print(f\"\\\\nPrompt {i+1}: {result['prompt'][:50]}...\")\\n    for variant, text in result[\"variants\"].items():\\n        seed = result[\"stats\"][variant][\"seed\"]\\n        print(f\"  {variant} (seed {seed}): {text[:80]}...\")\\n```\\n\\n### Email Campaign Testing\\n\\n```python\\nimport steadytext\\nimport random\\n\\ndef generate_email_variants(subject_base: str, body_base: str, num_variants: int = 5):\\n    \"\"\"Generate email variants for A/B testing.\"\"\"\\n    # Use consistent seed ranges for reproducibility\\n    seeds = [1000 + i * 100 for i in range(num_variants)]\\n    \\n    variants = []\\n    for i, seed in enumerate(seeds):\\n        subject = steadytext.generate(\\n            f\"Create an engaging email subject line based on: {subject_base}\",\\n            seed=seed,\\n            max_new_tokens=20\\n        ).strip()\\n        \\n        body = steadytext.generate(\\n            f\"Write a compelling email body for: {body_base}\",\\n            seed=seed,\\n            max_new_tokens=200\\n        ).strip()\\n        \\n        variants.append({\\n            \"variant_id\": f\"V{i+1}\",\\n            \"seed\": seed,\\n            \"subject\": subject,\\n            \"body\": body\\n        })\\n    \\n    return variants\\n\\n# Generate email campaign variants\\nvariants = generate_email_variants(\\n    subject_base=\"New product launch announcement\",\\n    body_base=\"Introducing our revolutionary AI-powered smartwatch with health monitoring\"\\n)\\n\\nprint(\"=== Email Campaign Variants ===\")\\nfor variant in variants:\\n    print(f\"\\\\n{variant['variant_id']} (seed {variant['seed']}):\\\")\\n    print(f\"Subject: {variant['subject']}\")\\n    print(f\"Body: {variant['body'][:100]}...\")\\n```\\n\\n## Content Variations\\n\\n### Style and Tone Variations\\n\\n```python\\nimport steadytext\\n\\ndef generate_style_variations(base_content: str, styles: Dict[str, int]):\\n    \\\"\\\"\\\"Generate content in different styles using specific seeds.\\\"\\\"\\\"\\n    variations = {}\\n    \\n    for style_name, seed in styles.items():\\n        prompt = f\"Rewrite the following content in a {style_name} style: {base_content}\"\\n        variation = steadytext.generate(\\n            prompt,\\n            seed=seed,\\n            max_new_tokens=250\\n        )\\n        variations[style_name] = {\\n            \"seed\": seed,\\n            \"content\": variation\\n        }\\n    \\n    return variations\\n\\n# Define styles with consistent seeds\\nstyles = {\\n    \"professional\": 2000,\\n    \"casual\": 2100,\\n    \"technical\": 2200,\\n    \"creative\": 2300,\\n    \"humorous\": 2400\\n}\\n\\nbase_content = \"Our new software helps businesses manage their data more efficiently.\"\\n\\nvariations = generate_style_variations(base_content, styles)\\n\\nprint(\"=== Style Variations ===\")\\nfor style, data in variations.items():\\n    print(f\"\\\\n{style.upper()} (seed {data['seed']}):\")\\n    print(data['content'])\\n```\\n\\n### Multi-Language Content\\n\\n```python\\nimport steadytext\\n\\ndef generate_multilingual_content(english_content: str, languages: Dict[str, int]):\\n    \\\"\\\"\\\"Generate content adapted for different languages/cultures using seeds.\\\"\\\"\\\"\\n    adaptations = {}\\n    \\n    for language, seed in languages.items():\\n        prompt = f\\\"Adapt this content for {language} audience, keeping cultural context in mind: {english_content}\\\"\\n        adaptation = steadytext.generate(\\n            prompt,\\n            seed=seed,\\n            max_new_tokens=200\\n        )\\n        adaptations[language] = {\\n            \"seed\": seed,\\n            \"content\": adaptation\\n        }\\n    \\n    return adaptations\\n\\n# Define languages with seeds\\nlanguages = {\\n    \"Spanish\": 3000,\\n    \"French\": 3100,\\n    \"German\": 3200,\\n    \"Japanese\": 3300,\\n    \"Brazilian Portuguese\": 3400\\n}\\n\\nenglish_content = \"Join our community of innovators and discover cutting-edge technology solutions.\"\\n\\nadaptations = generate_multilingual_content(english_content, languages)\\n\\nprint(\"=== Multilingual Adaptations ===\")\\nfor language, data in adaptations.items():\\n    print(f\"\\\\n{language} (seed {data['seed']}):\")\\n    print(data['content'])\\n```\\n\\n## Embedding Experiments\\n\\n### Semantic Similarity Analysis\\n\\n```python\\nimport steadytext\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\nimport matplotlib.pyplot as plt\\n\\ndef analyze_embedding_variations(text: str, seeds: List[int]):\\n    \\\"\\\"\\\"Analyze how different seeds affect embeddings for the same text.\\\"\\\"\\\"\\n    embeddings = []\\n    for seed in seeds:\\n        emb = steadytext.embed(text, seed=seed)\\n        embeddings.append(emb)\\n    \\n    embeddings = np.array(embeddings)\\n    \\n    # Calculate pairwise similarities\\n    similarities = []\\n    for i in range(len(embeddings)):\\n        for j in range(i+1, len(embeddings)):\\n            sim = np.dot(embeddings[i], embeddings[j])\\n            similarities.append(sim)\\n    \\n    analysis = {\\n        \"text\": text,\\n        \"seeds\": seeds,\\n        \"embeddings\": embeddings,\\n        \"pairwise_similarities\": similarities,\\n        \"mean_similarity\": np.mean(similarities),\\n        \"std_similarity\": np.std(similarities),\\n        \"min_similarity\": np.min(similarities),\\n        \"max_similarity\": np.max(similarities)\\n    }\\n    \\n    return analysis\\n\\n# Analyze embedding variations\\ntest_text = \"artificial intelligence and machine learning\"\\ntest_seeds = [4000, 4100, 4200, 4300, 4400]\\n\\nanalysis = analyze_embedding_variations(test_text, test_seeds)\\n\\nprint(f\"=== Embedding Variation Analysis ===\")\\nprint(f\"Text: {analysis['text']}\")\\nprint(f\"Seeds tested: {analysis['seeds']}\")\\nprint(f\"Mean similarity: {analysis['mean_similarity']:.4f}\")\\nprint(f\"Std similarity: {analysis['std_similarity']:.4f}\")\\nprint(f\"Range: {analysis['min_similarity']:.4f} - {analysis['max_similarity']:.4f}\")\\n\\n# Detailed similarity matrix\\nprint(\"\\\\nSimilarity Matrix:\")\\nembeddings = analysis['embeddings']\\nfor i, seed_i in enumerate(test_seeds):\\n    row = []\\n    for j, seed_j in enumerate(test_seeds):\\n        if i == j:\\n            sim = 1.0\\n        else:\\n            sim = np.dot(embeddings[i], embeddings[j])\\n        row.append(f\"{sim:.3f}\")\\n    print(f\"Seed {seed_i}: {' '.join(row)}\")\\n```\\n\\n### Domain-Specific Embedding Clusters\\n\\n```python\\nimport steadytext\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\nfrom collections import defaultdict\\n\\ndef create_domain_embeddings(domains: Dict[str, List[str]], seed_base: int = 5000):\\n    \\\"\\\"\\\"Create embeddings for different domains using consistent seeding.\\\"\\\"\\\"\\n    domain_embeddings = defaultdict(list)\\n    \\n    for domain, texts in domains.items():\\n        domain_seed = seed_base + hash(domain) % 1000  # Consistent seed per domain\\n        \\n        for text in texts:\\n            embedding = steadytext.embed(text, seed=domain_seed)\\n            domain_embeddings[domain].append({\\n                \"text\": text,\\n                \"embedding\": embedding,\\n                \"seed\": domain_seed\\n            })\\n    \\n    return dict(domain_embeddings)\\n\\n# Define domain-specific texts\\ndomains = {\\n    \"technology\": [\\n        \"artificial intelligence\",\\n        \"machine learning\",\\n        \"deep learning\",\\n        \"neural networks\",\\n        \"computer vision\"\\n    ],\\n    \"healthcare\": [\\n        \"medical diagnosis\",\\n        \"patient care\",\\n        \"clinical trials\",\\n        \"pharmaceutical research\",\\n        \"telemedicine\"\\n    ],\\n    \"finance\": [\\n        \"investment strategy\",\\n        \"risk management\",\\n        \"financial planning\",\\n        \"market analysis\",\\n        \"portfolio optimization\"\\n    ]\\n}\\n\\n# Generate embeddings\\ndomain_embeddings = create_domain_embeddings(domains)\\n\\n# Analyze domain clustering\\nall_embeddings = []\\nall_labels = []\\nall_texts = []\\n\\nfor domain, items in domain_embeddings.items():\\n    for item in items:\\n        all_embeddings.append(item['embedding'])\\n        all_labels.append(domain)\\n        all_texts.append(item['text'])\\n\\nall_embeddings = np.array(all_embeddings)\\n\\n# Perform clustering\\nkmeans = KMeans(n_clusters=3, random_state=42)\\ncluster_labels = kmeans.fit_predict(all_embeddings)\\n\\nprint(\"=== Domain Clustering Results ===\")\\nfor i, (text, true_domain, predicted_cluster) in enumerate(zip(all_texts, all_labels, cluster_labels)):\\n    print(f\"{text:25} | True: {true_domain:10} | Cluster: {predicted_cluster}\")\\n\\n# Calculate clustering accuracy\\nfrom sklearn.metrics import adjusted_rand_score\\nlabel_to_int = {label: i for i, label in enumerate(set(all_labels))}\\ntrue_labels_int = [label_to_int[label] for label in all_labels]\\n\\naccuracy = adjusted_rand_score(true_labels_int, cluster_labels)\\nprint(f\"\\\\nClustering accuracy (ARI): {accuracy:.3f}\")\\n```\\n\\n## CLI Workflows\\n\\n### Batch Processing Scripts\\n\\n```bash\\n#!/bin/bash\\n# batch_generation.sh - Generate content variants using CLI\\n\\n# Define seeds for different variants\\nSEEDS=(1000 2000 3000 4000 5000)\\nPROMPT=\"Write a brief product description for a smartwatch\"\\n\\necho \"=== Batch Generation with Different Seeds ===\"\\n\\nfor i in \"${!SEEDS[@]}\"; do\\n    seed=${SEEDS[$i]}\\n    variant_name=\"variant_$(echo $i | tr '0-4' 'A-E')\"  # A, B, C, D, E\\n    \\n    echo \\\"\\\"\\n    echo \\\"$variant_name (seed $seed):\\\"\\n    echo \\\"$PROMPT\\\" | st --seed $seed --max-new-tokens 100\\n    echo \\\"---\\\"\\ndone\\n```\\n\\n```bash\\n#!/bin/bash\\n# embedding_comparison.sh - Compare embeddings with different seeds\\n\\nTEXT=\\\"artificial intelligence\\\"\\nSEEDS=(6000 6100 6200)\\n\\necho \\\"=== Embedding Comparison ===\\\"\\necho \\\"Text: $TEXT\\\"\\n\\nfor seed in \\\"${SEEDS[@]}\\\"; do\\n    echo \\\"\\\"\\n    echo \\\"Seed $seed:\\\"\\n    st embed \\\"$TEXT\\\" --seed $seed --format json | jq '.[:5]'  # Show first 5 dimensions\\ndone\\n```\\n\\n### Reproducible Research Pipeline\\n\\n```bash\\n#!/bin/bash\\n# research_pipeline.sh - Complete research workflow with seeds\\n\\nRESEARCH_DIR=\\\"./research_$(date +%Y%m%d_%H%M%S)\\\"\\nBASE_SEED=7000\\n\\nmkdir -p \\\"$RESEARCH_DIR\\\"\\ncd \\\"$RESEARCH_DIR\\\"\\n\\necho \\\"=== Research Pipeline Started ===\\\" | tee research.log\\necho \\\"Base seed: $BASE_SEED\\\" | tee -a research.log\\necho \\\"Directory: $RESEARCH_DIR\\\" | tee -a research.log\\n\\n# Generate research questions\\necho \\\"Generating research questions...\\\" | tee -a research.log\\necho \\\"Generate 5 research questions about AI ethics\\\" | \\\\\\n    st --seed $BASE_SEED --max-new-tokens 200 &gt; questions.txt\\n\\n# Generate detailed explanations\\necho \\\"Generating detailed explanations...\\\" | tee -a research.log\\ncounter=0\\nwhile IFS= read -r question; do\\n    if [[ -n \\\"$question\\\" &amp;&amp; \\\"$question\\\" != *\\\"Generate\\\"* ]]; then\\n        seed=$((BASE_SEED + 100 + counter * 10))\\n        echo \\\"Processing: $question (seed $seed)\\\" | tee -a research.log\\n        echo \\\"$question\\\" | st --seed $seed --max-new-tokens 300 &gt; \\\"explanation_$counter.txt\\\"\\n        counter=$((counter + 1))\\n    fi\\ndone &lt; questions.txt\\n\\n# Generate embeddings for analysis\\necho \\\"Generating embeddings...\\\" | tee -a research.log\\nfor file in explanation_*.txt; do\\n    if [[ -f \\\"$file\\\" ]]; then\\n        seed=$((BASE_SEED + 500))\\n        echo \\\"Creating embedding for $file (seed $seed)\\\" | tee -a research.log\\n        cat \\\"$file\\\" | st embed --seed $seed --format json &gt; \\\"${file%.txt}_embedding.json\\\"\\n    fi\\ndone\\n\\necho \\\"Research pipeline completed. Results in: $RESEARCH_DIR\\\" | tee -a research.log\\necho \\\"Files generated:\\\" | tee -a research.log\\nls -la | tee -a research.log\\n```\\n\\n## Advanced Patterns\\n\\n### Seed Scheduling and Management\\n\\n```python\\nimport steadytext\\nfrom typing import Iterator, List, Dict, Any\\nimport hashlib\\n\\nclass SeedManager:\\n    \\\"\\\"\\\"Advanced seed management for complex workflows.\\\"\\\"\\\"\\n    \\n    def __init__(self, base_seed: int = 42):\\n        self.base_seed = base_seed\\n        self.used_seeds = set()\\n        self.seed_history = []\\n    \\n    def get_deterministic_seed(self, context: str) -&gt; int:\\n        \\\"\\\"\\\"Generate deterministic seed based on context string.\\\"\\\"\\\"\\n        # Create reproducible seed from context\\n        context_hash = hashlib.md5(context.encode()).hexdigest()\\n        seed = self.base_seed + int(context_hash[:8], 16) % 10000\\n        \\n        self.used_seeds.add(seed)\\n        self.seed_history.append({\\n            \"context\": context,\\n            \"seed\": seed,\\n            \"method\": \"deterministic\"\\n        })\\n        \\n        return seed\\n    \\n    def get_sequential_seed(self, increment: int = 1) -&gt; int:\\n        \\\"\\\"\\\"Get next seed in sequence.\\\"\\\"\\\"\\n        seed = self.base_seed + len(self.seed_history) * increment\\n        \\n        self.used_seeds.add(seed)\\n        self.seed_history.append({\\n            \"context\": f\"sequential_{len(self.seed_history)}\\\",\\n            \"seed\": seed,\\n            \"method\": \"sequential\"\\n        })\\n        \\n        return seed\\n    \\n    def get_category_seed(self, category: str, item_id: int = 0) -&gt; int:\\n        \\\"\\\"\\\"Get seed for specific category and item.\\\"\\\"\\\"\\n        category_base = hash(category) % 1000\\n        seed = self.base_seed + category_base * 100 + item_id\\n        \\n        self.used_seeds.add(seed)\\n        self.seed_history.append({\\n            \"context\": f\"{category}_{item_id}\\\",\\n            \"seed\": seed,\\n            \"method\": \"category\\\",\\n            \\\"category\\\": category,\\n            \\\"item_id\\\": item_id\\n        })\\n        \\n        return seed\\n    \\n    def generate_with_context(self, prompt: str, context: str, **kwargs) -&gt; str:\\n        \\\"\\\"\\\"Generate text with context-based seed.\\\"\\\"\\\"\\n        seed = self.get_deterministic_seed(context)\\n        return steadytext.generate(prompt, seed=seed, **kwargs)\\n    \\n    def embed_with_context(self, text: str, context: str, **kwargs):\\n        \\\"\\\"\\\"Generate embedding with context-based seed.\\\"\\\"\\\"\\n        seed = self.get_deterministic_seed(context)\\n        return steadytext.embed(text, seed=seed, **kwargs)\\n    \\n    def export_seed_history(self) -&gt; List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Export seed usage history for reproducibility.\\\"\\\"\\\"\\n        return self.seed_history.copy()\\n\\n# Usage example\\nmanager = SeedManager(base_seed=10000)\\n\\n# Context-based generation\\ncontents = [\\n    (\\\"Write a technical blog post about AI\\\", \\\"blog_technical_ai\\\"),\\n    (\\\"Create a social media post about innovation\\\", \\\"social_innovation\\\"),\\n    (\\\"Generate a product description\\\", \\\"product_smartwatch\\\")\\n]\\n\\nresults = []\\nfor prompt, context in contents:\\n    result = manager.generate_with_context(\\n        prompt, \\n        context, \\n        max_new_tokens=150\\n    )\\n    results.append({\\n        \\\"context\\\": context,\\n        \\\"prompt\\\": prompt,\\n        \\\"result\\\": result\\n    })\\n\\n# Category-based generation\\ncategories = [\\\"marketing\\\", \\\"technical\\\", \\\"creative\\\"]\\nfor category in categories:\\n    for i in range(3):  # 3 items per category\\n        seed = manager.get_category_seed(category, i)\\n        prompt = f\\\"Write a {category} message about our new product\\\"\\n        result = steadytext.generate(prompt, seed=seed, max_new_tokens=100)\\n        print(f\\\"{category}_{i} (seed {seed}): {result[:50]}...\\\")\\n\\n# Export history for reproducibility\\nhistory = manager.export_seed_history()\\nprint(f\\\"\\\\nGenerated {len(history)} items with managed seeds\\\")\\nfor entry in history[-5:]:  # Show last 5 entries\\n    print(f\\\"Context: {entry['context']}, Seed: {entry['seed']}, Method: {entry['method']}\\\")\\n```\\n\\n### Conditional Seed Strategies\\n\\n```python\\nimport steadytext\\nfrom enum import Enum\\nfrom typing import Optional, Callable\\n\\nclass SeedStrategy(Enum):\\n    DETERMINISTIC = \\\"deterministic\\\"  # Same input always gives same seed\\n    SEQUENTIAL = \\\"sequential\\\"        # Incrementing seed sequence\\n    RANDOM_BOUNDED = \\\"random_bounded\\\" # Random within bounds\\n    CONTENT_BASED = \\\"content_based\\\"   # Seed based on content analysis\\n\\nclass ConditionalSeedGenerator:\\n    \\\"\\\"\\\"Generate seeds based on content and context conditions.\\\"\\\"\\\"\\n    \\n    def __init__(self, base_seed: int = 42):\\n        self.base_seed = base_seed\\n        self.counters = {}\\n    \\n    def analyze_content(self, content: str) -&gt; Dict[str, Any]:\\n        \\\"\\\"\\\"Analyze content to determine appropriate seed strategy.\\\"\\\"\\\"\\n        word_count = len(content.split())\\n        has_technical_terms = any(term in content.lower() for term in \\n                                ['algorithm', 'neural', 'machine', 'ai', 'data'])\\n        has_creative_intent = any(term in content.lower() for term in \\n                             ['story', 'creative', 'imagine', 'artistic'])\\n        \\n        return {\\n            \\\"word_count\\\": word_count,\\n            \\\"is_technical\\\": has_technical_terms,\\n            \\\"is_creative\\\": has_creative_intent,\\n            \\\"is_short\\\": word_count &lt; 10,\\n            \\\"is_long\\\": word_count &gt; 50\\n        }\\n    \\n    def determine_strategy(self, content: str, context: Optional[str] = None) -&gt; SeedStrategy:\\n        \\\"\\\"\\\"Determine best seed strategy based on content analysis.\\\"\\\"\\\"\\n        analysis = self.analyze_content(content)\\n        \\n        if analysis[\\\"is_creative\\\"]:\\n            return SeedStrategy.RANDOM_BOUNDED  # More variation for creative content\\n        elif analysis[\\\"is_technical\\\"]:\\n            return SeedStrategy.DETERMINISTIC   # Consistency for technical content\\n        elif analysis[\\\"is_short\\\"]:\\n            return SeedStrategy.CONTENT_BASED   # Content-based for short prompts\\n        else:\\n            return SeedStrategy.SEQUENTIAL      # Sequential for general content\\n    \\n    def generate_seed(self, content: str, strategy: Optional[SeedStrategy] = None, \\n                     context: Optional[str] = None) -&gt; int:\\n        \\\"\\\"\\\"Generate seed using specified or determined strategy.\\\"\\\"\\\"\\n        if strategy is None:\\n            strategy = self.determine_strategy(content, context)\\n        \\n        if strategy == SeedStrategy.DETERMINISTIC:\\n            # Hash-based deterministic seed\\n            content_hash = hash(content) % 10000\\n            return self.base_seed + content_hash\\n        \\n        elif strategy == SeedStrategy.SEQUENTIAL:\\n            # Sequential counter per context\\n            key = context or \\\"default\\\"\\n            if key not in self.counters:\\n                self.counters[key] = 0\\n            self.counters[key] += 1\\n            return self.base_seed + self.counters[key] * 100\\n        \\n        elif strategy == SeedStrategy.RANDOM_BOUNDED:\\n            # Bounded random based on content\\n            content_hash = abs(hash(content))\\n            random_offset = content_hash % 1000\\n            return self.base_seed + 5000 + random_offset\\n        \\n        elif strategy == SeedStrategy.CONTENT_BASED:\\n            # Seed based on content characteristics\\n            analysis = self.analyze_content(content)\\n            seed_offset = (\\n                analysis[\\\"word_count\\\"] * 10 +\\n                (100 if analysis[\\\"is_technical\\\"] else 0) +\\n                (200 if analysis[\\\"is_creative\\\"] else 0)\\n            )\\n            return self.base_seed + seed_offset\\n        \\n        return self.base_seed\\n    \\n    def smart_generate(self, content: str, context: Optional[str] = None, **kwargs) -&gt; str:\\n        \\\"\\\"\\\"Generate text with automatically chosen seed strategy.\\\"\\\"\\\"\\n        strategy = self.determine_strategy(content, context)\\n        seed = self.generate_seed(content, strategy, context)\\n        \\n        print(f\\\"Strategy: {strategy.value}, Seed: {seed}\\\")\\n        return steadytext.generate(content, seed=seed, **kwargs)\\n\\n# Usage example\\ngenerator = ConditionalSeedGenerator(base_seed=20000)\\n\\n# Test different content types\\ntest_prompts = [\\n    \\\"Write a creative story about a robot\\\",  # Should use RANDOM_BOUNDED\\n    \\\"Explain the neural network algorithm\\\",  # Should use DETERMINISTIC\\n    \\\"Hello\\\",                                 # Should use CONTENT_BASED\\n    \\\"Generate a comprehensive technical report about machine learning applications in healthcare\\\"  # Should use SEQUENTIAL\\n]\\n\\nprint(\\\"=== Conditional Seed Strategy Results ===\\\")\\nfor prompt in test_prompts:\\n    print(f\\\"\\\\nPrompt: {prompt}\\\")\\n    result = generator.smart_generate(prompt, max_new_tokens=50)\\n    print(f\\\"Result: {result[:80]}...\\\")\\n```\\n\\n## Best Practices\\n\\n### 1. Documentation and Reproducibility\\n\\n```python\\n# Always document your seeds\\nCONSTANT_SEEDS = {\\n    \\\"BASELINE_RESEARCH\\\": 42,\\n    \\\"VARIATION_A\\\": 100,\\n    \\\"VARIATION_B\\\": 200,\\n    \\\"CREATIVE_CONTENT\\\": 300,\\n    \\\"TECHNICAL_CONTENT\\\": 400,\\n    \\\"PRODUCTION_DEFAULT\\\": 500\\n}\\n\\n# Use descriptive seed values\\ndef generate_with_purpose(prompt: str, purpose: str):\\n    seed = CONSTANT_SEEDS.get(purpose.upper(), CONSTANT_SEEDS[\\\"BASELINE_RESEARCH\\\"])\\n    return steadytext.generate(prompt, seed=seed)\\n```\\n\\n### 2. Seed Range Management\\n\\n```python\\n# Organize seeds by ranges to avoid conflicts\\nSEED_RANGES = {\\n    \\\"research\\\": (1000, 1999),\\n    \\\"production\\\": (2000, 2999),\\n    \\\"testing\\\": (3000, 3999),\\n    \\\"experiments\\\": (4000, 4999),\\n    \\\"benchmarks\\\": (5000, 5999)\\n}\\n\\ndef get_range_seed(category: str, offset: int = 0) -&gt; int:\\n    if category not in SEED_RANGES:\\n        raise ValueError(f\\\"Unknown category: {category}\\\")\\n    \\n    start, end = SEED_RANGES[category]\\n    seed = start + offset\\n    \\n    if seed &gt; end:\\n        raise ValueError(f\\\"Seed {seed} exceeds range for {category} ({start}-{end})\\\")\\n    \\n    return seed\\n```\\n\\n### 3. Testing and Validation\\n\\n```python\\ndef validate_reproducibility(prompt: str, seed: int, iterations: int = 5):\\n    \\\"\\\"\\\"Validate that a prompt+seed combination is truly reproducible.\\\"\\\"\\\"\\n    results = []\\n    for i in range(iterations):\\n        result = steadytext.generate(prompt, seed=seed)\\n        results.append(result)\\n    \\n    # Check if all results are identical\\n    is_reproducible = all(result == results[0] for result in results)\\n    \\n    print(f\\\"Reproducibility test for seed {seed}: {'PASS' if is_reproducible else 'FAIL'}\\\")\\n    if not is_reproducible:\\n        print(\\\"Different results found:\\\")\\n        for i, result in enumerate(results):\\n            print(f\\\"  Iteration {i+1}: {result[:50]}...\\\")\\n    \\n    return is_reproducible\\n\\n# Test key seeds\\nfor purpose, seed in CONSTANT_SEEDS.items():\\n    validate_reproducibility(\\\"Test prompt for validation\\\", seed)\\n```\\n\\nThis comprehensive guide demonstrates the power and flexibility of custom seeds in SteadyText. By using seeds strategically, you can achieve reproducible research, conduct effective A/B testing, generate controlled variations, and build robust content generation pipelines.\\n\n</code></pre>"},{"location":"examples/customer-intelligence/","title":"Customer Intelligence with AI-Powered Analytics","text":"<p>Transform raw customer data into actionable insights using SteadyText's AI capabilities directly in PostgreSQL.</p>"},{"location":"examples/customer-intelligence/#overview","title":"Overview","text":"<p>This tutorial demonstrates how to build a comprehensive customer intelligence system that: - Analyzes customer feedback at scale - Tracks sentiment trends over time - Identifies churn signals automatically - Creates customer segment profiles - Generates personalized recommendations</p>"},{"location":"examples/customer-intelligence/#prerequisites","title":"Prerequisites","text":"<pre><code># Start PostgreSQL with SteadyText\ndocker run -d -p 5432:5432 --name steadytext-intel julep/pg-steadytext\n\n# Connect and enable extensions\npsql -h localhost -U postgres -c \"CREATE EXTENSION IF NOT EXISTS pg_steadytext CASCADE;\"\npsql -h localhost -U postgres -c \"CREATE EXTENSION IF NOT EXISTS pgcrypto;\"\n</code></pre>"},{"location":"examples/customer-intelligence/#database-schema","title":"Database Schema","text":"<p>Create a comprehensive customer intelligence schema:</p> <pre><code>-- Customers table\nCREATE TABLE customers (\n    id SERIAL PRIMARY KEY,\n    customer_id UUID DEFAULT gen_random_uuid(),\n    email VARCHAR(255) UNIQUE NOT NULL,\n    first_name VARCHAR(100),\n    last_name VARCHAR(100),\n    segment VARCHAR(50),\n    lifetime_value DECIMAL(10, 2) DEFAULT 0,\n    acquisition_date DATE,\n    last_active_date DATE,\n    churn_risk_score DECIMAL(3, 2),\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    updated_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Customer interactions\nCREATE TABLE customer_interactions (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    interaction_type VARCHAR(50), -- 'support', 'purchase', 'review', 'email', 'chat'\n    channel VARCHAR(50), -- 'web', 'mobile', 'email', 'phone'\n    content TEXT,\n    metadata JSONB,\n    sentiment_score DECIMAL(3, 2), -- -1 to 1\n    timestamp TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Product reviews\nCREATE TABLE product_reviews (\n    id SERIAL PRIMARY KEY,\n    customer_id INTEGER REFERENCES customers(id),\n    product_id INTEGER,\n    rating INTEGER CHECK (rating &gt;= 1 AND rating &lt;= 5),\n    title VARCHAR(200),\n    review_text TEXT,\n    verified_purchase BOOLEAN DEFAULT FALSE,\n    helpful_count INTEGER DEFAULT 0,\n    ai_summary TEXT,\n    sentiment VARCHAR(20),\n    key_themes TEXT[],\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Support tickets\nCREATE TABLE support_tickets (\n    id SERIAL PRIMARY KEY,\n    ticket_number VARCHAR(20) UNIQUE,\n    customer_id INTEGER REFERENCES customers(id),\n    category VARCHAR(50),\n    priority VARCHAR(20),\n    subject VARCHAR(200),\n    description TEXT,\n    resolution TEXT,\n    satisfaction_score INTEGER,\n    ai_analysis JSONB,\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    resolved_at TIMESTAMPTZ\n);\n\n-- Customer segments with AI insights\nCREATE TABLE customer_segments_analysis (\n    id SERIAL PRIMARY KEY,\n    segment_name VARCHAR(50) UNIQUE,\n    customer_count INTEGER,\n    avg_lifetime_value DECIMAL(10, 2),\n    common_behaviors TEXT[],\n    ai_profile TEXT,\n    recommendations JSONB,\n    last_updated TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Indexes for performance\nCREATE INDEX idx_interactions_customer ON customer_interactions(customer_id, timestamp);\nCREATE INDEX idx_reviews_customer ON product_reviews(customer_id);\nCREATE INDEX idx_reviews_sentiment ON product_reviews(sentiment);\nCREATE INDEX idx_tickets_customer ON support_tickets(customer_id);\nCREATE INDEX idx_customers_segment ON customers(segment);\nCREATE INDEX idx_customers_churn ON customers(churn_risk_score);\n</code></pre>"},{"location":"examples/customer-intelligence/#real-time-review-analysis","title":"Real-Time Review Analysis","text":"<p>Automatically analyze customer reviews as they come in:</p> <pre><code>-- Trigger function to analyze reviews\nCREATE OR REPLACE FUNCTION analyze_review_on_insert()\nRETURNS TRIGGER AS $$\nDECLARE\n    v_sentiment VARCHAR;\n    v_summary TEXT;\n    v_themes TEXT[];\n    v_sentiment_score DECIMAL(3, 2);\nBEGIN\n    -- Determine sentiment\n    v_sentiment := steadytext_generate_choice(\n        format('Classify sentiment of this review (rating %s/5): %s',\n            NEW.rating, NEW.review_text),\n        ARRAY['very_positive', 'positive', 'neutral', 'negative', 'very_negative']\n    );\n\n    -- Generate summary\n    v_summary := steadytext_generate(\n        format('Summarize this customer review in one sentence: %s',\n            NEW.review_text),\n        max_tokens := 50\n    );\n\n    -- Extract key themes\n    v_themes := string_to_array(\n        steadytext_generate(\n            format('List 3 key themes from this review (comma-separated): %s',\n                NEW.review_text),\n            max_tokens := 30\n        ),\n        ', '\n    );\n\n    -- Calculate numeric sentiment score\n    v_sentiment_score := CASE v_sentiment\n        WHEN 'very_positive' THEN 1.0\n        WHEN 'positive' THEN 0.5\n        WHEN 'neutral' THEN 0.0\n        WHEN 'negative' THEN -0.5\n        WHEN 'very_negative' THEN -1.0\n    END;\n\n    -- Update the review record\n    NEW.sentiment := v_sentiment;\n    NEW.ai_summary := v_summary;\n    NEW.key_themes := v_themes;\n\n    -- Also log this as an interaction\n    INSERT INTO customer_interactions (\n        customer_id, interaction_type, channel, \n        content, sentiment_score, metadata\n    ) VALUES (\n        NEW.customer_id, 'review', 'web',\n        NEW.review_text, v_sentiment_score,\n        jsonb_build_object(\n            'rating', NEW.rating,\n            'product_id', NEW.product_id,\n            'themes', v_themes\n        )\n    );\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create the trigger\nCREATE TRIGGER analyze_review_before_insert\n    BEFORE INSERT ON product_reviews\n    FOR EACH ROW\n    EXECUTE FUNCTION analyze_review_on_insert();\n</code></pre>"},{"location":"examples/customer-intelligence/#customer-sentiment-tracking","title":"Customer Sentiment Tracking","text":"<p>Track sentiment trends over time:</p> <pre><code>-- Customer sentiment dashboard view\nCREATE OR REPLACE VIEW customer_sentiment_dashboard AS\nWITH sentiment_data AS (\n    SELECT \n        c.id,\n        c.email,\n        c.segment,\n        AVG(ci.sentiment_score) AS avg_sentiment,\n        COUNT(ci.id) AS interaction_count,\n        MAX(ci.timestamp) AS last_interaction,\n        array_agg(DISTINCT ci.interaction_type) AS interaction_types\n    FROM customers c\n    LEFT JOIN customer_interactions ci ON c.id = ci.customer_id\n    WHERE ci.timestamp &gt; NOW() - INTERVAL '90 days'\n    GROUP BY c.id, c.email, c.segment\n),\nrecent_issues AS (\n    SELECT \n        customer_id,\n        COUNT(*) AS issue_count,\n        AVG(CASE WHEN satisfaction_score IS NOT NULL \n            THEN satisfaction_score ELSE 3 END) AS avg_satisfaction\n    FROM support_tickets\n    WHERE created_at &gt; NOW() - INTERVAL '30 days'\n    GROUP BY customer_id\n)\nSELECT \n    sd.*,\n    ri.issue_count,\n    ri.avg_satisfaction,\n    CASE \n        WHEN sd.avg_sentiment &lt; -0.3 AND ri.issue_count &gt; 2 THEN 'high_risk'\n        WHEN sd.avg_sentiment &lt; 0 OR ri.issue_count &gt; 3 THEN 'medium_risk'\n        WHEN sd.avg_sentiment &gt; 0.5 AND ri.issue_count = 0 THEN 'loyal'\n        ELSE 'normal'\n    END AS customer_status,\n    steadytext_generate(\n        format('Analyze customer behavior: Sentiment: %s, Interactions: %s, Issues: %s',\n            ROUND(sd.avg_sentiment, 2),\n            sd.interaction_count,\n            COALESCE(ri.issue_count, 0)\n        ),\n        max_tokens := 100\n    ) AS ai_insights\nFROM sentiment_data sd\nLEFT JOIN recent_issues ri ON sd.id = ri.customer_id;\n</code></pre>"},{"location":"examples/customer-intelligence/#churn-prediction-system","title":"Churn Prediction System","text":"<p>Identify customers at risk of churning:</p> <pre><code>-- Churn risk calculation function\nCREATE OR REPLACE FUNCTION calculate_churn_risk()\nRETURNS TABLE (\n    customer_id INTEGER,\n    risk_score DECIMAL(3, 2),\n    risk_factors JSONB,\n    retention_strategy TEXT\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH customer_metrics AS (\n        SELECT \n            c.id,\n            c.last_active_date,\n            c.lifetime_value,\n            COUNT(DISTINCT ci.id) AS interaction_count,\n            AVG(ci.sentiment_score) AS avg_sentiment,\n            MAX(ci.timestamp) AS last_interaction,\n            COUNT(DISTINCT st.id) AS support_tickets,\n            AVG(st.satisfaction_score) AS avg_satisfaction\n        FROM customers c\n        LEFT JOIN customer_interactions ci ON c.id = ci.customer_id\n            AND ci.timestamp &gt; NOW() - INTERVAL '90 days'\n        LEFT JOIN support_tickets st ON c.id = st.customer_id\n            AND st.created_at &gt; NOW() - INTERVAL '90 days'\n        GROUP BY c.id, c.last_active_date, c.lifetime_value\n    ),\n    risk_scoring AS (\n        SELECT \n            id,\n            -- Calculate risk score based on multiple factors\n            LEAST(1.0, GREATEST(0.0,\n                0.3 * (EXTRACT(EPOCH FROM (NOW() - last_active_date)) / 86400.0 / 30.0) + -- Days inactive\n                0.2 * (1.0 - COALESCE(avg_sentiment + 1, 1.0) / 2.0) + -- Sentiment\n                0.2 * (support_tickets::FLOAT / GREATEST(interaction_count, 1)) + -- Support ratio\n                0.3 * (CASE WHEN avg_satisfaction &lt; 3 THEN 1.0 ELSE 0.0 END) -- Low satisfaction\n            )) AS risk_score,\n            jsonb_build_object(\n                'days_inactive', EXTRACT(EPOCH FROM (NOW() - last_active_date)) / 86400.0,\n                'sentiment_score', COALESCE(avg_sentiment, 0),\n                'support_tickets', support_tickets,\n                'satisfaction', COALESCE(avg_satisfaction, 3),\n                'lifetime_value', lifetime_value\n            ) AS risk_factors\n        FROM customer_metrics\n    )\n    SELECT \n        rs.id,\n        rs.risk_score,\n        rs.risk_factors,\n        steadytext_generate(\n            format('Create retention strategy for customer with risk score %s and factors: %s',\n                ROUND(rs.risk_score, 2),\n                rs.risk_factors::TEXT\n            ),\n            max_tokens := 150\n        ) AS retention_strategy\n    FROM risk_scoring rs\n    WHERE rs.risk_score &gt; 0.3;\n\n    -- Update customer records\n    UPDATE customers c\n    SET churn_risk_score = rs.risk_score,\n        updated_at = NOW()\n    FROM risk_scoring rs\n    WHERE c.id = rs.id;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/customer-intelligence/#customer-segment-analysis","title":"Customer Segment Analysis","text":"<p>Generate AI-powered insights for customer segments:</p> <pre><code>-- Analyze and profile customer segments\nCREATE OR REPLACE FUNCTION analyze_customer_segments()\nRETURNS VOID AS $$\nDECLARE\n    v_segment RECORD;\nBEGIN\n    -- Clear previous analysis\n    TRUNCATE customer_segments_analysis;\n\n    -- Analyze each segment\n    FOR v_segment IN \n        SELECT DISTINCT segment FROM customers WHERE segment IS NOT NULL\n    LOOP\n        INSERT INTO customer_segments_analysis (\n            segment_name,\n            customer_count,\n            avg_lifetime_value,\n            common_behaviors,\n            ai_profile,\n            recommendations\n        )\n        WITH segment_data AS (\n            SELECT \n                COUNT(DISTINCT c.id) AS customer_count,\n                AVG(c.lifetime_value) AS avg_ltv,\n                array_agg(DISTINCT ci.interaction_type) AS interaction_types,\n                AVG(ci.sentiment_score) AS avg_sentiment,\n                COUNT(DISTINCT pr.id) AS review_count,\n                AVG(pr.rating) AS avg_rating\n            FROM customers c\n            LEFT JOIN customer_interactions ci ON c.id = ci.customer_id\n            LEFT JOIN product_reviews pr ON c.id = pr.customer_id\n            WHERE c.segment = v_segment.segment\n        ),\n        behavior_analysis AS (\n            SELECT \n                array_agg(DISTINCT theme) AS common_themes\n            FROM (\n                SELECT unnest(key_themes) AS theme\n                FROM product_reviews pr\n                JOIN customers c ON pr.customer_id = c.id\n                WHERE c.segment = v_segment.segment\n            ) t\n        )\n        SELECT \n            v_segment.segment,\n            sd.customer_count,\n            sd.avg_ltv,\n            sd.interaction_types,\n            steadytext_generate(\n                format('Create detailed profile for %s customer segment with %s customers, $%s avg LTV, %s sentiment',\n                    v_segment.segment,\n                    sd.customer_count,\n                    ROUND(sd.avg_ltv, 2),\n                    CASE \n                        WHEN sd.avg_sentiment &gt; 0.5 THEN 'very positive'\n                        WHEN sd.avg_sentiment &gt; 0 THEN 'positive'\n                        WHEN sd.avg_sentiment &gt; -0.5 THEN 'neutral'\n                        ELSE 'negative'\n                    END\n                ),\n                max_tokens := 200\n            ) AS ai_profile,\n            steadytext_generate_json(\n                format('Suggest 3 marketing strategies for %s segment', v_segment.segment),\n                '{\n                    \"strategies\": {\n                        \"type\": \"array\",\n                        \"items\": {\n                            \"type\": \"object\",\n                            \"properties\": {\n                                \"name\": {\"type\": \"string\"},\n                                \"description\": {\"type\": \"string\"},\n                                \"expected_impact\": {\"type\": \"string\"}\n                            }\n                        }\n                    }\n                }'::json\n            )::jsonb AS recommendations\n        FROM segment_data sd, behavior_analysis ba;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/customer-intelligence/#support-ticket-intelligence","title":"Support Ticket Intelligence","text":"<p>Extract insights from support interactions:</p> <pre><code>-- Analyze support tickets with AI\nCREATE OR REPLACE FUNCTION analyze_support_ticket(\n    p_ticket_id INTEGER\n)\nRETURNS VOID AS $$\nDECLARE\n    v_ticket support_tickets%ROWTYPE;\n    v_analysis JSONB;\nBEGIN\n    SELECT * INTO v_ticket FROM support_tickets WHERE id = p_ticket_id;\n\n    -- Generate comprehensive analysis\n    v_analysis := steadytext_generate_json(\n        format('Analyze support ticket: Subject: %s, Description: %s, Category: %s',\n            v_ticket.subject,\n            v_ticket.description,\n            v_ticket.category\n        ),\n        '{\n            \"urgency\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\", \"critical\"]},\n            \"sentiment\": {\"type\": \"string\", \"enum\": [\"angry\", \"frustrated\", \"neutral\", \"satisfied\", \"happy\"]},\n            \"root_cause\": {\"type\": \"string\"},\n            \"suggested_resolution\": {\"type\": \"string\"},\n            \"follow_up_needed\": {\"type\": \"boolean\"},\n            \"tags\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n        }'::json\n    )::jsonb;\n\n    -- Update ticket with analysis\n    UPDATE support_tickets\n    SET ai_analysis = v_analysis,\n        priority = COALESCE(priority, v_analysis-&gt;&gt;'urgency')\n    WHERE id = p_ticket_id;\n\n    -- Log as interaction\n    INSERT INTO customer_interactions (\n        customer_id, interaction_type, channel,\n        content, sentiment_score, metadata\n    ) VALUES (\n        v_ticket.customer_id,\n        'support',\n        'ticket',\n        v_ticket.description,\n        CASE v_analysis-&gt;&gt;'sentiment'\n            WHEN 'angry' THEN -1.0\n            WHEN 'frustrated' THEN -0.5\n            WHEN 'neutral' THEN 0.0\n            WHEN 'satisfied' THEN 0.5\n            WHEN 'happy' THEN 1.0\n        END,\n        v_analysis\n    );\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/customer-intelligence/#personalized-recommendations","title":"Personalized Recommendations","text":"<p>Generate personalized product recommendations:</p> <pre><code>-- Generate personalized recommendations\nCREATE OR REPLACE FUNCTION generate_customer_recommendations(\n    p_customer_id INTEGER,\n    p_num_recommendations INTEGER DEFAULT 5\n)\nRETURNS TABLE (\n    recommendation_type VARCHAR,\n    title VARCHAR,\n    description TEXT,\n    priority INTEGER\n) AS $$\nDECLARE\n    v_customer RECORD;\n    v_profile TEXT;\nBEGIN\n    -- Get customer profile\n    SELECT \n        c.*,\n        cs.ai_profile,\n        array_agg(DISTINCT pr.key_themes) AS interests,\n        AVG(pr.rating) AS avg_rating_given\n    INTO v_customer\n    FROM customers c\n    LEFT JOIN customer_segments_analysis cs ON c.segment = cs.segment_name\n    LEFT JOIN product_reviews pr ON c.id = pr.customer_id\n    WHERE c.id = p_customer_id\n    GROUP BY c.id, c.customer_id, c.email, c.first_name, c.last_name, \n             c.segment, c.lifetime_value, c.acquisition_date, c.last_active_date,\n             c.churn_risk_score, c.created_at, c.updated_at, cs.ai_profile;\n\n    -- Build customer profile for AI\n    v_profile := format('Customer: %s %s, Segment: %s, LTV: $%s, Risk: %s, Interests: %s',\n        v_customer.first_name,\n        v_customer.last_name,\n        v_customer.segment,\n        v_customer.lifetime_value,\n        COALESCE(v_customer.churn_risk_score, 0),\n        array_to_string(v_customer.interests, ', ')\n    );\n\n    -- Generate recommendations\n    RETURN QUERY\n    WITH recommendations AS (\n        SELECT \n            'product' AS rec_type,\n            steadytext_generate(\n                format('Suggest product for: %s', v_profile),\n                max_tokens := 50\n            ) AS title,\n            steadytext_generate(\n                format('Why this product is perfect for: %s', v_profile),\n                max_tokens := 100\n            ) AS description,\n            1 AS priority\n        UNION ALL\n        SELECT \n            'retention' AS rec_type,\n            steadytext_generate(\n                format('Create retention offer for: %s', v_profile),\n                max_tokens := 50\n            ) AS title,\n            steadytext_generate(\n                format('Explain retention offer benefits for: %s', v_profile),\n                max_tokens := 100\n            ) AS description,\n            CASE WHEN v_customer.churn_risk_score &gt; 0.5 THEN 1 ELSE 2 END AS priority\n        UNION ALL\n        SELECT \n            'upsell' AS rec_type,\n            steadytext_generate(\n                format('Suggest upsell opportunity for: %s', v_profile),\n                max_tokens := 50\n            ) AS title,\n            steadytext_generate(\n                format('Upsell pitch for: %s', v_profile),\n                max_tokens := 100\n            ) AS description,\n            3 AS priority\n    )\n    SELECT * FROM recommendations\n    ORDER BY priority\n    LIMIT p_num_recommendations;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/customer-intelligence/#sample-data-and-dashboards","title":"Sample Data and Dashboards","text":"<pre><code>-- Insert sample customers\nINSERT INTO customers (email, first_name, last_name, segment, lifetime_value, acquisition_date)\nVALUES \n    ('john.doe@email.com', 'John', 'Doe', 'premium', 2500.00, '2023-01-15'),\n    ('jane.smith@email.com', 'Jane', 'Smith', 'regular', 450.00, '2023-06-20'),\n    ('bob.wilson@email.com', 'Bob', 'Wilson', 'budget', 150.00, '2024-01-10');\n\n-- Insert sample reviews\nINSERT INTO product_reviews (customer_id, product_id, rating, title, review_text)\nVALUES\n    (1, 101, 5, 'Excellent product!', 'This product exceeded my expectations. The quality is outstanding and the customer service was impeccable. Would definitely recommend to friends and family.'),\n    (2, 102, 3, 'Decent but could be better', 'The product works as advertised but the shipping took forever and the packaging was damaged. The product itself is okay for the price.'),\n    (3, 103, 1, 'Very disappointed', 'Product broke after just one week of use. Customer support was unhelpful and refused to honor the warranty. Will not buy from this company again.');\n\n-- Insert sample support tickets\nINSERT INTO support_tickets (ticket_number, customer_id, category, subject, description)\nVALUES\n    ('TICK-001', 2, 'shipping', 'Late delivery', 'My order was supposed to arrive last week but still hasnt been delivered. Tracking shows no updates.'),\n    ('TICK-002', 3, 'product_issue', 'Product defect', 'The product stopped working after one week. It wont turn on anymore despite following all troubleshooting steps.');\n\n-- Run analysis\nSELECT calculate_churn_risk();\nSELECT analyze_customer_segments();\n\n-- View insights\nSELECT * FROM customer_sentiment_dashboard;\nSELECT * FROM generate_customer_recommendations(1);\n</code></pre>"},{"location":"examples/customer-intelligence/#executive-dashboard-query","title":"Executive Dashboard Query","text":"<pre><code>-- Executive customer intelligence summary\nCREATE OR REPLACE VIEW executive_customer_summary AS\nWITH summary_stats AS (\n    SELECT \n        COUNT(DISTINCT c.id) AS total_customers,\n        COUNT(DISTINCT c.id) FILTER (WHERE c.churn_risk_score &gt; 0.7) AS high_risk_customers,\n        AVG(c.lifetime_value) AS avg_ltv,\n        COUNT(DISTINCT pr.id) AS total_reviews,\n        AVG(pr.rating) AS avg_rating,\n        COUNT(DISTINCT st.id) AS open_tickets\n    FROM customers c\n    LEFT JOIN product_reviews pr ON c.id = pr.customer_id\n    LEFT JOIN support_tickets st ON c.id = st.customer_id \n        AND st.resolved_at IS NULL\n)\nSELECT \n    *,\n    steadytext_generate(\n        format('Executive summary: %s customers, %s at high risk, $%s avg LTV, %s rating, %s open tickets',\n            total_customers,\n            high_risk_customers,\n            ROUND(avg_ltv, 2),\n            ROUND(avg_rating, 1),\n            open_tickets\n        ),\n        max_tokens := 200\n    ) AS executive_insights\nFROM summary_stats;\n</code></pre>"},{"location":"examples/customer-intelligence/#best-practices","title":"Best Practices","text":"<ol> <li>Privacy First: Always anonymize data in AI prompts</li> <li>Batch Processing: Use background jobs for large-scale analysis</li> <li>Caching Strategy: Leverage SteadyText's caching for repeated analyses</li> <li>Feedback Loop: Use AI insights to improve models over time</li> <li>Human Review: Always have humans validate critical decisions</li> </ol>"},{"location":"examples/customer-intelligence/#next-steps","title":"Next Steps","text":"<ul> <li>Data Pipelines Example \u2192</li> <li>TimescaleDB Integration \u2192</li> <li>Production Deployment \u2192</li> </ul> <p>Pro Tip</p> <p>Combine customer intelligence with TimescaleDB continuous aggregates for real-time dashboards that update automatically as new data arrives.</p>"},{"location":"examples/daemon-usage/","title":"Daemon Usage Guide","text":"<p>Learn how to use SteadyText's daemon mode for persistent model serving and 160x faster response times.</p>"},{"location":"examples/daemon-usage/#overview","title":"Overview","text":"<p>The SteadyText daemon is a background service that keeps models loaded in memory, eliminating the 2-3 second startup overhead for each operation. It provides:</p> <ul> <li>160x faster first response - No model loading delay</li> <li>Shared cache - All clients benefit from cached results</li> <li>Automatic fallback - Operations work without daemon</li> <li>Zero configuration - Used by default when available</li> <li>Thread-safe - Handles concurrent requests efficiently</li> </ul>"},{"location":"examples/daemon-usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding the Daemon</li> <li>Starting and Stopping</li> <li>Configuration</li> <li>Python SDK Usage</li> <li>CLI Integration</li> <li>Production Deployment</li> <li>Monitoring and Debugging</li> <li>Performance Optimization</li> <li>Troubleshooting</li> <li>Best Practices</li> </ul>"},{"location":"examples/daemon-usage/#understanding-the-daemon","title":"Understanding the Daemon","text":""},{"location":"examples/daemon-usage/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Client Applications            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Python SDK  \u2502  CLI Tools  \u2502  Custom Apps   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           ZeroMQ Client Layer               \u2502\n\u2502         (Automatic Fallback)                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              ZeroMQ REP Server              \u2502\n\u2502            (TCP Port 5557)                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           Daemon Server Process             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Loaded Models  \u2502  Shared Cache     \u2502   \u2502\n\u2502  \u2502  - Gemma-3n     \u2502  - Generation     \u2502   \u2502\n\u2502  \u2502  - Qwen3        \u2502  - Embeddings     \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/daemon-usage/#how-it-works","title":"How It Works","text":"<ol> <li>First Request: Client checks if daemon is running</li> <li>Daemon Available: Sends request via ZeroMQ</li> <li>Daemon Unavailable: Falls back to direct model loading</li> <li>Response: Client receives result (cached or generated)</li> </ol>"},{"location":"examples/daemon-usage/#starting-and-stopping","title":"Starting and Stopping","text":""},{"location":"examples/daemon-usage/#basic-commands","title":"Basic Commands","text":"<pre><code># Start daemon in background (default)\nst daemon start\n\n# Start with custom settings\nst daemon start --host 0.0.0.0 --port 5557 --seed 42\n\n# Check status\nst daemon status\n\n# Stop daemon\nst daemon stop\n\n# Restart daemon\nst daemon restart\n</code></pre>"},{"location":"examples/daemon-usage/#foreground-mode-debugging","title":"Foreground Mode (Debugging)","text":"<pre><code># Run in foreground to see logs\nst daemon start --foreground\n\n# Output:\n# SteadyText daemon starting...\n# Loading generation model...\n# Loading embedding model...\n# Daemon ready on tcp://127.0.0.1:5557\n# [2024-01-15 10:23:45] Request: generate (seed=42)\n# [2024-01-15 10:23:45] Cache hit for generation\n</code></pre>"},{"location":"examples/daemon-usage/#systemd-service-production","title":"Systemd Service (Production)","text":"<pre><code># /etc/systemd/system/steadytext.service\n[Unit]\nDescription=SteadyText Daemon\nAfter=network.target\n\n[Service]\nType=simple\nUser=steadytext\nGroup=steadytext\nWorkingDirectory=/var/lib/steadytext\nExecStart=/usr/local/bin/st daemon start --foreground\nExecStop=/usr/local/bin/st daemon stop\nRestart=always\nRestartSec=10\nStandardOutput=append:/var/log/steadytext/daemon.log\nStandardError=append:/var/log/steadytext/daemon.error.log\n\n# Environment\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_CAPACITY=1024\"\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=200\"\nEnvironment=\"PYTHONUNBUFFERED=1\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start: <pre><code>sudo systemctl enable steadytext\nsudo systemctl start steadytext\nsudo systemctl status steadytext\n</code></pre></p>"},{"location":"examples/daemon-usage/#configuration","title":"Configuration","text":""},{"location":"examples/daemon-usage/#environment-variables","title":"Environment Variables","text":"<pre><code># Daemon settings\nexport STEADYTEXT_DAEMON_HOST=0.0.0.0      # Bind address\nexport STEADYTEXT_DAEMON_PORT=5557         # Port number\nexport STEADYTEXT_DISABLE_DAEMON=1         # Disable daemon usage\n\n# Cache settings (shared by daemon)\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=1024\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=200\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=2048\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=400\n\n# Model settings\nexport STEADYTEXT_DEFAULT_SEED=42\nexport STEADYTEXT_MODEL_DIR=/path/to/models\n</code></pre>"},{"location":"examples/daemon-usage/#configuration-file","title":"Configuration File","text":"<pre><code># steadytext_config.py\nimport os\n\n# Daemon configuration\nDAEMON_CONFIG = {\n    \"host\": os.getenv(\"STEADYTEXT_DAEMON_HOST\", \"127.0.0.1\"),\n    \"port\": int(os.getenv(\"STEADYTEXT_DAEMON_PORT\", 5557)),\n    \"timeout\": 5000,  # milliseconds\n    \"max_retries\": 3,\n    \"retry_delay\": 0.1  # seconds\n}\n\n# Cache configuration\nCACHE_CONFIG = {\n    \"generation\": {\n        \"capacity\": int(os.getenv(\"STEADYTEXT_GENERATION_CACHE_CAPACITY\", 256)),\n        \"max_size_mb\": float(os.getenv(\"STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB\", 50.0))\n    },\n    \"embedding\": {\n        \"capacity\": int(os.getenv(\"STEADYTEXT_EMBEDDING_CACHE_CAPACITY\", 512)),\n        \"max_size_mb\": float(os.getenv(\"STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB\", 100.0))\n    }\n}\n\n# Apply configuration\nos.environ.update({\n    \"STEADYTEXT_DAEMON_HOST\": DAEMON_CONFIG[\"host\"],\n    \"STEADYTEXT_DAEMON_PORT\": str(DAEMON_CONFIG[\"port\"]),\n    \"STEADYTEXT_GENERATION_CACHE_CAPACITY\": str(CACHE_CONFIG[\"generation\"][\"capacity\"]),\n    \"STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB\": str(CACHE_CONFIG[\"generation\"][\"max_size_mb\"]),\n    \"STEADYTEXT_EMBEDDING_CACHE_CAPACITY\": str(CACHE_CONFIG[\"embedding\"][\"capacity\"]),\n    \"STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB\": str(CACHE_CONFIG[\"embedding\"][\"max_size_mb\"])\n})\n</code></pre>"},{"location":"examples/daemon-usage/#python-sdk-usage","title":"Python SDK Usage","text":""},{"location":"examples/daemon-usage/#automatic-daemon-usage","title":"Automatic Daemon Usage","text":"<pre><code>import steadytext\n\n# Daemon is used automatically if available\ntext = steadytext.generate(\"Hello world\", seed=42)  # Fast if daemon running\nembedding = steadytext.embed(\"test text\", seed=123)  # Uses daemon\n\n# Check if daemon was used\nfrom steadytext.daemon.client import is_daemon_running\nif is_daemon_running():\n    print(\"Using daemon for fast responses\")\nelse:\n    print(\"Daemon not available, using direct mode\")\n</code></pre>"},{"location":"examples/daemon-usage/#explicit-daemon-context","title":"Explicit Daemon Context","text":"<pre><code>from steadytext.daemon import use_daemon\nimport steadytext\n\n# Force daemon usage (raises error if not available)\nwith use_daemon():\n    text = steadytext.generate(\"Hello world\", seed=42)\n    embedding = steadytext.embed(\"test\", seed=123)\n\n    # All operations in this context use daemon\n    for i in range(100):\n        result = steadytext.generate(f\"Item {i}\", seed=i)\n</code></pre>"},{"location":"examples/daemon-usage/#connection-management","title":"Connection Management","text":"<pre><code>from steadytext.daemon.client import DaemonClient\nimport time\n\nclass ManagedDaemonClient:\n    \"\"\"Daemon client with connection pooling and retries.\"\"\"\n\n    def __init__(self, max_retries=3, timeout=5000):\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self._client = None\n\n    def _get_client(self):\n        \"\"\"Get or create daemon client.\"\"\"\n        if self._client is None:\n            self._client = DaemonClient(timeout=self.timeout)\n        return self._client\n\n    def generate_with_retry(self, prompt, **kwargs):\n        \"\"\"Generate with automatic retry on failure.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                client = self._get_client()\n                return client.generate(prompt, **kwargs)\n            except Exception as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n                time.sleep(0.1 * (attempt + 1))\n                self._client = None  # Reset connection\n\n    def close(self):\n        \"\"\"Close daemon connection.\"\"\"\n        if self._client:\n            self._client.close()\n            self._client = None\n\n# Usage\nclient = ManagedDaemonClient()\ntry:\n    text = client.generate_with_retry(\"Hello world\", seed=42)\n    print(text)\nfinally:\n    client.close()\n</code></pre>"},{"location":"examples/daemon-usage/#streaming-with-daemon","title":"Streaming with Daemon","text":"<pre><code>import steadytext\nfrom steadytext.daemon import use_daemon\n\n# Streaming works identically with daemon\nwith use_daemon():\n    print(\"Streaming with daemon:\")\n    for token in steadytext.generate_iter(\"Tell me a story\", seed=42):\n        print(token, end=\"\", flush=True)\n    print()\n\n# The daemon handles streaming efficiently:\n# 1. Client sends streaming request\n# 2. Daemon generates tokens\n# 3. Tokens sent with acknowledgment protocol\n# 4. Client controls flow with ACK messages\n</code></pre>"},{"location":"examples/daemon-usage/#batch-operations","title":"Batch Operations","text":"<pre><code>import concurrent.futures\nimport steadytext\nfrom steadytext.daemon import use_daemon\nimport time\n\ndef benchmark_daemon_performance():\n    \"\"\"Compare daemon vs direct performance.\"\"\"\n    prompts = [f\"Generate text for item {i}\" for i in range(20)]\n\n    # Test without daemon\n    start = time.time()\n    results_direct = []\n    for prompt in prompts:\n        # Force direct mode\n        import os\n        os.environ[\"STEADYTEXT_DISABLE_DAEMON\"] = \"1\"\n        result = steadytext.generate(prompt, seed=42)\n        results_direct.append(result)\n        del os.environ[\"STEADYTEXT_DISABLE_DAEMON\"]\n    direct_time = time.time() - start\n\n    # Test with daemon\n    start = time.time()\n    results_daemon = []\n    with use_daemon():\n        for prompt in prompts:\n            result = steadytext.generate(prompt, seed=42)\n            results_daemon.append(result)\n    daemon_time = time.time() - start\n\n    print(f\"Direct mode: {direct_time:.2f}s\")\n    print(f\"Daemon mode: {daemon_time:.2f}s\")\n    print(f\"Speedup: {direct_time/daemon_time:.1f}x\")\n\n# Parallel batch processing\ndef process_batch_parallel(prompts, max_workers=4):\n    \"\"\"Process prompts in parallel using daemon.\"\"\"\n    with use_daemon():\n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all tasks\n            futures = {\n                executor.submit(steadytext.generate, prompt, seed=idx): (prompt, idx)\n                for idx, prompt in enumerate(prompts)\n            }\n\n            # Collect results\n            results = {}\n            for future in concurrent.futures.as_completed(futures):\n                prompt, idx = futures[future]\n                try:\n                    result = future.result()\n                    results[idx] = result\n                except Exception as e:\n                    print(f\"Error processing {prompt}: {e}\")\n                    results[idx] = None\n\n            # Return in order\n            return [results[i] for i in range(len(prompts))]\n\n# Usage\nprompts = [\"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\"]\nresults = process_batch_parallel(prompts)\n</code></pre>"},{"location":"examples/daemon-usage/#cli-integration","title":"CLI Integration","text":""},{"location":"examples/daemon-usage/#automatic-daemon-usage_1","title":"Automatic Daemon Usage","text":"<pre><code># CLI automatically uses daemon if available\nst generate \"Hello world\" --seed 42\n\n# Check if daemon is being used\nst daemon status &amp;&amp; echo \"Daemon active\" || echo \"No daemon\"\n\n# Force direct mode (bypass daemon)\nSTEADYTEXT_DISABLE_DAEMON=1 st generate \"Hello world\"\n</code></pre>"},{"location":"examples/daemon-usage/#shell-script-integration","title":"Shell Script Integration","text":"<pre><code>#!/bin/bash\n# daemon_batch.sh - Batch processing with daemon\n\n# Ensure daemon is running\nensure_daemon() {\n    if ! st daemon status &gt;/dev/null 2&gt;&amp;1; then\n        echo \"Starting daemon...\"\n        st daemon start\n        sleep 2  # Wait for startup\n    fi\n}\n\n# Process files with daemon\nprocess_files() {\n    local files=(\"$@\")\n\n    ensure_daemon\n\n    for file in \"${files[@]}\"; do\n        echo \"Processing: $file\"\n\n        # Generate summary using daemon\n        summary=$(cat \"$file\" | st generate \"Summarize this text\" --wait --seed 42)\n\n        # Generate embedding using daemon  \n        embedding=$(cat \"$file\" | st embed --format json --seed 42)\n\n        # Save results\n        echo \"$summary\" &gt; \"${file%.txt}_summary.txt\"\n        echo \"$embedding\" &gt; \"${file%.txt}_embedding.json\"\n    done\n}\n\n# Main\nif [ $# -eq 0 ]; then\n    echo \"Usage: $0 file1.txt file2.txt ...\"\n    exit 1\nfi\n\nprocess_files \"$@\"\n\necho \"Processing complete!\"\n</code></pre>"},{"location":"examples/daemon-usage/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>#!/bin/bash\n# monitor_daemon.sh - Monitor daemon performance\n\n# Function to time operations\ntime_operation() {\n    local operation=\"$1\"\n    local start=$(date +%s.%N)\n    eval \"$operation\" &gt;/dev/null 2&gt;&amp;1\n    local end=$(date +%s.%N)\n    echo \"$(echo \"$end - $start\" | bc)\"\n}\n\n# Monitor daemon performance\nmonitor_daemon() {\n    echo \"Daemon Performance Monitor\"\n    echo \"=========================\"\n\n    # Check daemon status\n    if st daemon status --json | jq -e '.running' &gt;/dev/null; then\n        echo \"\u2713 Daemon is running\"\n    else\n        echo \"\u2717 Daemon is not running\"\n        return 1\n    fi\n\n    # Test generation speed\n    echo -e \"\\nGeneration Performance:\"\n    for i in {1..5}; do\n        time=$(time_operation \"echo 'test' | st --seed $i\")\n        echo \"  Request $i: ${time}s\"\n    done\n\n    # Test embedding speed\n    echo -e \"\\nEmbedding Performance:\"\n    for i in {1..5}; do\n        time=$(time_operation \"st embed 'test text' --seed $i\")\n        echo \"  Request $i: ${time}s\"\n    done\n\n    # Cache statistics\n    echo -e \"\\nCache Statistics:\"\n    st cache --status\n}\n\n# Run monitoring\nmonitor_daemon\n</code></pre>"},{"location":"examples/daemon-usage/#production-deployment","title":"Production Deployment","text":""},{"location":"examples/daemon-usage/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Dockerfile\nFROM python:3.11-slim\n\n# Install dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create app user\nRUN useradd -m -s /bin/bash steadytext\n\n# Install SteadyText\nRUN pip install steadytext\n\n# Create directories\nRUN mkdir -p /var/log/steadytext /var/lib/steadytext &amp;&amp; \\\n    chown -R steadytext:steadytext /var/log/steadytext /var/lib/steadytext\n\n# Switch to app user\nUSER steadytext\nWORKDIR /home/steadytext\n\n# Download models during build\nRUN st models download --all\n\n# Expose daemon port\nEXPOSE 5557\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3 \\\n    CMD st daemon status || exit 1\n\n# Start daemon\nCMD [\"st\", \"daemon\", \"start\", \"--foreground\", \"--host\", \"0.0.0.0\"]\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  steadytext:\n    build: .\n    ports:\n      - \"5557:5557\"\n    volumes:\n      - steadytext-cache:/home/steadytext/.cache/steadytext\n      - ./logs:/var/log/steadytext\n    environment:\n      - STEADYTEXT_GENERATION_CACHE_CAPACITY=1024\n      - STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=200\n      - STEADYTEXT_EMBEDDING_CACHE_CAPACITY=2048\n      - STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=400\n      - STEADYTEXT_DEFAULT_SEED=42\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"st\", \"daemon\", \"status\"]\n      interval: 30s\n      timeout: 3s\n      retries: 3\n\nvolumes:\n  steadytext-cache:\n</code></pre>"},{"location":"examples/daemon-usage/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># steadytext-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: steadytext-daemon\n  labels:\n    app: steadytext\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: steadytext\n  template:\n    metadata:\n      labels:\n        app: steadytext\n    spec:\n      containers:\n      - name: steadytext\n        image: steadytext:latest\n        ports:\n        - containerPort: 5557\n        env:\n        - name: STEADYTEXT_GENERATION_CACHE_CAPACITY\n          value: \"2048\"\n        - name: STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB\n          value: \"500\"\n        resources:\n          requests:\n            memory: \"4Gi\"\n            cpu: \"2\"\n          limits:\n            memory: \"8Gi\"\n            cpu: \"4\"\n        livenessProbe:\n          exec:\n            command:\n            - st\n            - daemon\n            - status\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 5557\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        volumeMounts:\n        - name: cache\n          mountPath: /home/steadytext/.cache/steadytext\n      volumes:\n      - name: cache\n        persistentVolumeClaim:\n          claimName: steadytext-cache-pvc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: steadytext-service\nspec:\n  selector:\n    app: steadytext\n  ports:\n    - protocol: TCP\n      port: 5557\n      targetPort: 5557\n  type: LoadBalancer\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: steadytext-cache-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 50Gi\n</code></pre>"},{"location":"examples/daemon-usage/#high-availability-setup","title":"High Availability Setup","text":"<pre><code># ha_daemon_client.py - High availability daemon client\nimport random\nimport time\nfrom typing import List, Optional\nimport steadytext\nfrom steadytext.daemon.client import DaemonClient\n\nclass HADaemonClient:\n    \"\"\"High availability client with multiple daemon endpoints.\"\"\"\n\n    def __init__(self, endpoints: List[tuple]):\n        \"\"\"\n        Initialize with multiple endpoints.\n\n        Args:\n            endpoints: List of (host, port) tuples\n        \"\"\"\n        self.endpoints = endpoints\n        self.clients = {}\n        self.failed_endpoints = set()\n        self.last_health_check = 0\n        self.health_check_interval = 60  # seconds\n\n    def _get_client(self, endpoint: tuple) -&gt; Optional[DaemonClient]:\n        \"\"\"Get or create client for endpoint.\"\"\"\n        if endpoint not in self.clients:\n            try:\n                host, port = endpoint\n                client = DaemonClient(host=host, port=port, timeout=2000)\n                # Test connection\n                client._send_request({\"type\": \"ping\"})\n                self.clients[endpoint] = client\n            except Exception:\n                return None\n        return self.clients.get(endpoint)\n\n    def _health_check(self):\n        \"\"\"Periodic health check of failed endpoints.\"\"\"\n        if time.time() - self.last_health_check &gt; self.health_check_interval:\n            recovered = set()\n            for endpoint in self.failed_endpoints:\n                if self._get_client(endpoint):\n                    recovered.add(endpoint)\n            self.failed_endpoints -= recovered\n            self.last_health_check = time.time()\n\n    def _get_available_endpoint(self) -&gt; Optional[tuple]:\n        \"\"\"Get random available endpoint.\"\"\"\n        self._health_check()\n        available = [ep for ep in self.endpoints if ep not in self.failed_endpoints]\n        return random.choice(available) if available else None\n\n    def generate(self, prompt: str, **kwargs):\n        \"\"\"Generate with automatic failover.\"\"\"\n        attempts = 0\n        endpoints_tried = set()\n\n        while attempts &lt; len(self.endpoints):\n            endpoint = self._get_available_endpoint()\n            if not endpoint or endpoint in endpoints_tried:\n                break\n\n            endpoints_tried.add(endpoint)\n            client = self._get_client(endpoint)\n\n            if client:\n                try:\n                    return client.generate(prompt, **kwargs)\n                except Exception as e:\n                    print(f\"Failed on {endpoint}: {e}\")\n                    self.failed_endpoints.add(endpoint)\n                    if endpoint in self.clients:\n                        del self.clients[endpoint]\n\n            attempts += 1\n\n        # All endpoints failed, fall back to direct mode\n        print(\"All daemon endpoints failed, using direct mode\")\n        return steadytext.generate(prompt, **kwargs)\n\n    def embed(self, text: str, **kwargs):\n        \"\"\"Embed with automatic failover.\"\"\"\n        # Similar implementation to generate\n        pass\n\n# Usage\nha_client = HADaemonClient([\n    (\"daemon1.example.com\", 5557),\n    (\"daemon2.example.com\", 5557),\n    (\"daemon3.example.com\", 5557)\n])\n\n# Automatic failover\nresult = ha_client.generate(\"Hello world\", seed=42)\n</code></pre>"},{"location":"examples/daemon-usage/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"examples/daemon-usage/#logging-configuration","title":"Logging Configuration","text":"<pre><code># logging_config.py\nimport logging\nimport sys\nfrom pathlib import Path\n\ndef setup_daemon_logging(log_dir=\"/var/log/steadytext\"):\n    \"\"\"Configure comprehensive daemon logging.\"\"\"\n    log_dir = Path(log_dir)\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Configure formatters\n    detailed_formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n    )\n    simple_formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(message)s'\n    )\n\n    # File handler for all logs\n    all_handler = logging.FileHandler(log_dir / \"daemon.log\")\n    all_handler.setLevel(logging.DEBUG)\n    all_handler.setFormatter(detailed_formatter)\n\n    # File handler for errors only\n    error_handler = logging.FileHandler(log_dir / \"daemon.error.log\")\n    error_handler.setLevel(logging.ERROR)\n    error_handler.setFormatter(detailed_formatter)\n\n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.INFO)\n    console_handler.setFormatter(simple_formatter)\n\n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n    root_logger.addHandler(all_handler)\n    root_logger.addHandler(error_handler)\n    root_logger.addHandler(console_handler)\n\n    # Configure specific loggers\n    logging.getLogger(\"steadytext.daemon\").setLevel(logging.DEBUG)\n    logging.getLogger(\"zmq\").setLevel(logging.WARNING)\n\n    return root_logger\n\n# Request logging middleware\nclass RequestLogger:\n    \"\"\"Log all daemon requests for debugging.\"\"\"\n\n    def __init__(self, daemon_server):\n        self.daemon_server = daemon_server\n        self.logger = logging.getLogger(\"steadytext.daemon.requests\")\n\n    def log_request(self, request_id, request_type, request_data):\n        \"\"\"Log incoming request.\"\"\"\n        self.logger.info(f\"Request {request_id}: {request_type}\", extra={\n            \"request_id\": request_id,\n            \"request_type\": request_type,\n            \"seed\": request_data.get(\"seed\"),\n            \"prompt_length\": len(request_data.get(\"prompt\", \"\")),\n            \"timestamp\": time.time()\n        })\n\n    def log_response(self, request_id, response_data, duration):\n        \"\"\"Log outgoing response.\"\"\"\n        self.logger.info(f\"Response {request_id}: {duration:.3f}s\", extra={\n            \"request_id\": request_id,\n            \"success\": response_data.get(\"success\"),\n            \"cached\": response_data.get(\"cached\", False),\n            \"duration\": duration,\n            \"timestamp\": time.time()\n        })\n</code></pre>"},{"location":"examples/daemon-usage/#performance-metrics","title":"Performance Metrics","text":"<pre><code># metrics.py - Daemon performance metrics\nimport time\nimport psutil\nimport json\nfrom collections import deque\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nclass DaemonMetrics:\n    \"\"\"Collect and report daemon performance metrics.\"\"\"\n\n    def __init__(self, window_size=1000):\n        self.request_times = deque(maxlen=window_size)\n        self.cache_hits = 0\n        self.cache_misses = 0\n        self.total_requests = 0\n        self.errors = 0\n        self.start_time = time.time()\n        self.process = psutil.Process()\n\n    def record_request(self, duration: float, cached: bool, success: bool):\n        \"\"\"Record request metrics.\"\"\"\n        self.total_requests += 1\n        self.request_times.append(duration)\n\n        if cached:\n            self.cache_hits += 1\n        else:\n            self.cache_misses += 1\n\n        if not success:\n            self.errors += 1\n\n    def get_metrics(self) -&gt; Dict[str, Any]:\n        \"\"\"Get current metrics snapshot.\"\"\"\n        uptime = time.time() - self.start_time\n\n        # Calculate percentiles\n        if self.request_times:\n            sorted_times = sorted(self.request_times)\n            p50 = sorted_times[len(sorted_times) // 2]\n            p95 = sorted_times[int(len(sorted_times) * 0.95)]\n            p99 = sorted_times[int(len(sorted_times) * 0.99)]\n            avg_time = sum(sorted_times) / len(sorted_times)\n        else:\n            p50 = p95 = p99 = avg_time = 0\n\n        # System metrics\n        cpu_percent = self.process.cpu_percent()\n        memory_info = self.process.memory_info()\n\n        return {\n            \"timestamp\": datetime.now().isoformat(),\n            \"uptime_seconds\": uptime,\n            \"total_requests\": self.total_requests,\n            \"requests_per_second\": self.total_requests / uptime if uptime &gt; 0 else 0,\n            \"cache_hit_rate\": self.cache_hits / self.total_requests if self.total_requests &gt; 0 else 0,\n            \"error_rate\": self.errors / self.total_requests if self.total_requests &gt; 0 else 0,\n            \"response_times\": {\n                \"average\": avg_time,\n                \"p50\": p50,\n                \"p95\": p95,\n                \"p99\": p99\n            },\n            \"system\": {\n                \"cpu_percent\": cpu_percent,\n                \"memory_mb\": memory_info.rss / 1024 / 1024,\n                \"threads\": self.process.num_threads()\n            }\n        }\n\n    def export_prometheus(self) -&gt; str:\n        \"\"\"Export metrics in Prometheus format.\"\"\"\n        metrics = self.get_metrics()\n        lines = []\n\n        # Request metrics\n        lines.append(f'steadytext_requests_total {metrics[\"total_requests\"]}')\n        lines.append(f'steadytext_requests_per_second {metrics[\"requests_per_second\"]:.2f}')\n        lines.append(f'steadytext_cache_hit_rate {metrics[\"cache_hit_rate\"]:.4f}')\n        lines.append(f'steadytext_error_rate {metrics[\"error_rate\"]:.4f}')\n\n        # Response time metrics\n        lines.append(f'steadytext_response_time_seconds{{quantile=\"0.5\"}} {metrics[\"response_times\"][\"p50\"]:.4f}')\n        lines.append(f'steadytext_response_time_seconds{{quantile=\"0.95\"}} {metrics[\"response_times\"][\"p95\"]:.4f}')\n        lines.append(f'steadytext_response_time_seconds{{quantile=\"0.99\"}} {metrics[\"response_times\"][\"p99\"]:.4f}')\n\n        # System metrics\n        lines.append(f'steadytext_cpu_percent {metrics[\"system\"][\"cpu_percent\"]:.2f}')\n        lines.append(f'steadytext_memory_megabytes {metrics[\"system\"][\"memory_mb\"]:.2f}')\n        lines.append(f'steadytext_threads {metrics[\"system\"][\"threads\"]}')\n\n        return '\\n'.join(lines)\n\n# HTTP metrics endpoint\nfrom flask import Flask, Response\n\napp = Flask(__name__)\nmetrics = DaemonMetrics()\n\n@app.route('/metrics')\ndef prometheus_metrics():\n    return Response(metrics.export_prometheus(), mimetype='text/plain')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=9090)\n</code></pre>"},{"location":"examples/daemon-usage/#debug-tools","title":"Debug Tools","text":"<pre><code>#!/bin/bash\n# debug_daemon.sh - Comprehensive daemon debugging\n\n# Function to trace daemon requests\ntrace_requests() {\n    echo \"Tracing daemon requests...\"\n\n    # Start tcpdump on daemon port\n    sudo tcpdump -i lo -w daemon_trace.pcap port 5557 &amp;\n    TCPDUMP_PID=$!\n\n    # Run test requests\n    for i in {1..10}; do\n        st generate \"Test $i\" --seed $i &amp;\n    done\n    wait\n\n    # Stop tcpdump\n    sudo kill $TCPDUMP_PID\n\n    echo \"Trace saved to daemon_trace.pcap\"\n}\n\n# Function to profile daemon\nprofile_daemon() {\n    echo \"Profiling daemon performance...\"\n\n    # Get daemon PID\n    DAEMON_PID=$(st daemon status --json | jq -r '.pid')\n\n    if [ -z \"$DAEMON_PID\" ]; then\n        echo \"Daemon not running\"\n        return 1\n    fi\n\n    # CPU profiling\n    echo \"CPU profiling for 30 seconds...\"\n    sudo perf record -F 99 -p $DAEMON_PID -g -- sleep 30\n    sudo perf report &gt; daemon_cpu_profile.txt\n\n    # Memory profiling\n    echo \"Memory snapshot...\"\n    sudo gcore -o daemon_memory $DAEMON_PID\n\n    # Strace\n    echo \"System call trace for 10 seconds...\"\n    sudo strace -p $DAEMON_PID -o daemon_strace.log -f -T &amp;\n    STRACE_PID=$!\n    sleep 10\n    sudo kill $STRACE_PID\n\n    echo \"Profiling complete\"\n}\n\n# Function to stress test daemon\nstress_test() {\n    local concurrent=${1:-10}\n    local requests=${2:-100}\n\n    echo \"Stress testing with $concurrent concurrent clients, $requests requests each\"\n\n    # Start monitoring\n    st daemon status --json &gt; stress_test_before.json\n\n    # Run concurrent requests\n    for i in $(seq 1 $concurrent); do\n        (\n            for j in $(seq 1 $requests); do\n                st generate \"Stress test $i-$j\" --seed $((i*1000+j)) &gt;/dev/null 2&gt;&amp;1\n            done\n            echo \"Client $i completed\"\n        ) &amp;\n    done\n\n    # Wait for completion\n    wait\n\n    # Get final status\n    st daemon status --json &gt; stress_test_after.json\n\n    echo \"Stress test complete\"\n}\n\n# Main menu\necho \"SteadyText Daemon Debug Tools\"\necho \"1. Trace requests\"\necho \"2. Profile daemon\"\necho \"3. Stress test\"\necho \"4. View logs\"\necho \"5. Export metrics\"\n\nread -p \"Select option: \" choice\n\ncase $choice in\n    1) trace_requests ;;\n    2) profile_daemon ;;\n    3) \n        read -p \"Concurrent clients (default 10): \" concurrent\n        read -p \"Requests per client (default 100): \" requests\n        stress_test ${concurrent:-10} ${requests:-100}\n        ;;\n    4) \n        tail -f /var/log/steadytext/daemon.log\n        ;;\n    5)\n        curl -s http://localhost:9090/metrics\n        ;;\n    *) echo \"Invalid option\" ;;\nesac\n</code></pre>"},{"location":"examples/daemon-usage/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/daemon-usage/#cache-warming","title":"Cache Warming","text":"<pre><code># cache_warmer.py - Pre-populate daemon cache\nimport steadytext\nfrom steadytext.daemon import use_daemon\nimport json\nfrom pathlib import Path\n\nclass DaemonCacheWarmer:\n    \"\"\"Warm up daemon cache with common requests.\"\"\"\n\n    def __init__(self, warmup_file=\"warmup_prompts.json\"):\n        self.warmup_file = Path(warmup_file)\n        self.load_prompts()\n\n    def load_prompts(self):\n        \"\"\"Load warmup prompts from file.\"\"\"\n        if self.warmup_file.exists():\n            with open(self.warmup_file) as f:\n                self.warmup_data = json.load(f)\n        else:\n            # Default warmup prompts\n            self.warmup_data = {\n                \"generation\": [\n                    {\"prompt\": \"Hello\", \"seed\": 42},\n                    {\"prompt\": \"Write a summary\", \"seed\": 42},\n                    {\"prompt\": \"Explain this concept\", \"seed\": 42},\n                    {\"prompt\": \"Generate code\", \"seed\": 42},\n                    {\"prompt\": \"Create documentation\", \"seed\": 42}\n                ],\n                \"embedding\": [\n                    {\"text\": \"search query\", \"seed\": 42},\n                    {\"text\": \"document text\", \"seed\": 42},\n                    {\"text\": \"user input\", \"seed\": 42}\n                ]\n            }\n\n    def warm_generation_cache(self):\n        \"\"\"Warm up generation cache.\"\"\"\n        print(\"Warming generation cache...\")\n\n        with use_daemon():\n            for item in self.warmup_data[\"generation\"]:\n                try:\n                    result = steadytext.generate(\n                        item[\"prompt\"],\n                        seed=item.get(\"seed\", 42),\n                        max_new_tokens=item.get(\"max_tokens\", 512)\n                    )\n                    print(f\"\u2713 Cached: {item['prompt'][:30]}...\")\n                except Exception as e:\n                    print(f\"\u2717 Failed: {item['prompt'][:30]}... - {e}\")\n\n    def warm_embedding_cache(self):\n        \"\"\"Warm up embedding cache.\"\"\"\n        print(\"\\nWarming embedding cache...\")\n\n        with use_daemon():\n            for item in self.warmup_data[\"embedding\"]:\n                try:\n                    result = steadytext.embed(\n                        item[\"text\"],\n                        seed=item.get(\"seed\", 42)\n                    )\n                    print(f\"\u2713 Cached: {item['text'][:30]}...\")\n                except Exception as e:\n                    print(f\"\u2717 Failed: {item['text'][:30]}... - {e}\")\n\n    def run(self):\n        \"\"\"Run complete cache warming.\"\"\"\n        print(\"Starting daemon cache warming...\")\n        self.warm_generation_cache()\n        self.warm_embedding_cache()\n        print(\"\\nCache warming complete!\")\n\n    def save_common_prompts(self, prompts_file=\"access.log\"):\n        \"\"\"Extract common prompts from access logs.\"\"\"\n        # Parse access logs to find common prompts\n        prompt_counts = {}\n\n        with open(prompts_file) as f:\n            for line in f:\n                # Extract prompt from log line\n                # Adjust parsing based on your log format\n                if \"prompt:\" in line:\n                    prompt = line.split(\"prompt:\")[1].strip()\n                    prompt_counts[prompt] = prompt_counts.get(prompt, 0) + 1\n\n        # Get top prompts\n        top_prompts = sorted(\n            prompt_counts.items(),\n            key=lambda x: x[1],\n            reverse=True\n        )[:50]\n\n        # Update warmup data\n        self.warmup_data[\"generation\"] = [\n            {\"prompt\": prompt, \"seed\": 42}\n            for prompt, _ in top_prompts\n        ]\n\n        # Save to file\n        with open(self.warmup_file, 'w') as f:\n            json.dump(self.warmup_data, f, indent=2)\n\n# Usage\nif __name__ == \"__main__\":\n    warmer = DaemonCacheWarmer()\n    warmer.run()\n</code></pre>"},{"location":"examples/daemon-usage/#connection-pooling","title":"Connection Pooling","text":"<pre><code># connection_pool.py - Daemon connection pooling\nimport queue\nimport threading\nfrom contextlib import contextmanager\nfrom steadytext.daemon.client import DaemonClient\n\nclass DaemonConnectionPool:\n    \"\"\"Thread-safe connection pool for daemon clients.\"\"\"\n\n    def __init__(self, host=\"127.0.0.1\", port=5557, pool_size=10, timeout=5000):\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self.pool_size = pool_size\n        self._pool = queue.Queue(maxsize=pool_size)\n        self._all_connections = []\n        self._lock = threading.Lock()\n        self._initialize_pool()\n\n    def _initialize_pool(self):\n        \"\"\"Create initial connections.\"\"\"\n        for _ in range(self.pool_size):\n            conn = self._create_connection()\n            if conn:\n                self._pool.put(conn)\n                self._all_connections.append(conn)\n\n    def _create_connection(self):\n        \"\"\"Create new daemon connection.\"\"\"\n        try:\n            return DaemonClient(\n                host=self.host,\n                port=self.port,\n                timeout=self.timeout\n            )\n        except Exception as e:\n            print(f\"Failed to create connection: {e}\")\n            return None\n\n    @contextmanager\n    def get_connection(self, timeout=None):\n        \"\"\"Get connection from pool.\"\"\"\n        connection = None\n        try:\n            connection = self._pool.get(timeout=timeout)\n            yield connection\n        finally:\n            if connection:\n                self._pool.put(connection)\n\n    def close_all(self):\n        \"\"\"Close all connections.\"\"\"\n        with self._lock:\n            while not self._pool.empty():\n                try:\n                    conn = self._pool.get_nowait()\n                    conn.close()\n                except:\n                    pass\n            self._all_connections.clear()\n\n# Global connection pool\n_connection_pool = None\n\ndef get_connection_pool():\n    \"\"\"Get or create global connection pool.\"\"\"\n    global _connection_pool\n    if _connection_pool is None:\n        _connection_pool = DaemonConnectionPool()\n    return _connection_pool\n\n# Usage example\ndef parallel_generate(prompts):\n    \"\"\"Generate text in parallel using connection pool.\"\"\"\n    pool = get_connection_pool()\n    results = {}\n\n    def process_prompt(idx, prompt):\n        with pool.get_connection() as conn:\n            if conn:\n                try:\n                    result = conn.generate(prompt, seed=idx)\n                    results[idx] = result\n                except Exception as e:\n                    results[idx] = f\"Error: {e}\"\n\n    threads = []\n    for idx, prompt in enumerate(prompts):\n        t = threading.Thread(target=process_prompt, args=(idx, prompt))\n        t.start()\n        threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    return [results[i] for i in range(len(prompts))]\n</code></pre>"},{"location":"examples/daemon-usage/#memory-optimization","title":"Memory Optimization","text":"<pre><code># memory_optimization.py - Optimize daemon memory usage\nimport gc\nimport resource\nimport psutil\nfrom steadytext import get_cache_manager\n\nclass DaemonMemoryOptimizer:\n    \"\"\"Optimize memory usage for long-running daemons.\"\"\"\n\n    def __init__(self, max_memory_mb=4096):\n        self.max_memory_mb = max_memory_mb\n        self.process = psutil.Process()\n        self.cache_manager = get_cache_manager()\n\n    def set_memory_limits(self):\n        \"\"\"Set process memory limits.\"\"\"\n        # Convert MB to bytes\n        max_memory_bytes = self.max_memory_mb * 1024 * 1024\n\n        # Set soft and hard limits\n        resource.setrlimit(\n            resource.RLIMIT_AS,\n            (max_memory_bytes, max_memory_bytes)\n        )\n\n        print(f\"Memory limit set to {self.max_memory_mb}MB\")\n\n    def get_memory_usage(self):\n        \"\"\"Get current memory usage.\"\"\"\n        memory_info = self.process.memory_info()\n        return {\n            \"rss_mb\": memory_info.rss / 1024 / 1024,\n            \"vms_mb\": memory_info.vms / 1024 / 1024,\n            \"percent\": self.process.memory_percent()\n        }\n\n    def optimize_caches(self):\n        \"\"\"Optimize cache sizes based on memory usage.\"\"\"\n        usage = self.get_memory_usage()\n\n        if usage[\"percent\"] &gt; 80:\n            # Reduce cache sizes\n            print(\"High memory usage, reducing cache sizes...\")\n\n            # Get current stats\n            stats = self.cache_manager.get_cache_stats()\n\n            # Clear least recently used entries\n            self.cache_manager.clear_old_entries(keep_ratio=0.5)\n\n            # Force garbage collection\n            gc.collect()\n\n    def periodic_optimization(self, interval=300):\n        \"\"\"Run periodic memory optimization.\"\"\"\n        import time\n        import threading\n\n        def optimize():\n            while True:\n                try:\n                    self.optimize_caches()\n                    usage = self.get_memory_usage()\n                    print(f\"Memory: {usage['rss_mb']:.1f}MB ({usage['percent']:.1f}%)\")\n                except Exception as e:\n                    print(f\"Optimization error: {e}\")\n\n                time.sleep(interval)\n\n        thread = threading.Thread(target=optimize, daemon=True)\n        thread.start()\n\n# Apply optimizations at daemon startup\noptimizer = DaemonMemoryOptimizer(max_memory_mb=4096)\noptimizer.set_memory_limits()\noptimizer.periodic_optimization()\n</code></pre>"},{"location":"examples/daemon-usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/daemon-usage/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"examples/daemon-usage/#daemon-wont-start","title":"Daemon Won't Start","text":"<pre><code># Problem: Address already in use\n$ st daemon start\nError: Address already in use (127.0.0.1:5557)\n\n# Solution 1: Check for existing process\n$ lsof -i :5557\n$ kill -9 &lt;PID&gt;\n\n# Solution 2: Use different port\n$ st daemon start --port 5558\n</code></pre>"},{"location":"examples/daemon-usage/#connection-timeouts","title":"Connection Timeouts","text":"<pre><code># Problem: Timeout errors with daemon\n\n# Solution 1: Increase timeout\nfrom steadytext.daemon.client import DaemonClient\nclient = DaemonClient(timeout=10000)  # 10 seconds\n\n# Solution 2: Check daemon health\nimport requests\ntry:\n    response = requests.get(\"http://localhost:9090/metrics\", timeout=1)\n    print(\"Daemon healthy\")\nexcept:\n    print(\"Daemon unhealthy\")\n\n# Solution 3: Restart daemon\nimport subprocess\nsubprocess.run([\"st\", \"daemon\", \"restart\"])\n</code></pre>"},{"location":"examples/daemon-usage/#memory-issues","title":"Memory Issues","text":"<pre><code># Problem: Daemon using too much memory\n\n# Solution 1: Clear caches\n$ st cache --clear\n\n# Solution 2: Reduce cache sizes\n$ export STEADYTEXT_GENERATION_CACHE_CAPACITY=128\n$ export STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=25\n$ st daemon restart\n\n# Solution 3: Monitor memory usage\n$ watch -n 1 'ps aux | grep \"st daemon\" | grep -v grep'\n</code></pre>"},{"location":"examples/daemon-usage/#performance-degradation","title":"Performance Degradation","text":"<pre><code># diagnose_performance.py\nimport time\nimport statistics\nimport steadytext\nfrom steadytext.daemon import use_daemon\n\ndef diagnose_daemon_performance():\n    \"\"\"Diagnose daemon performance issues.\"\"\"\n\n    # Test direct mode\n    direct_times = []\n    for i in range(10):\n        start = time.time()\n        steadytext.generate(\"test\", seed=i)\n        direct_times.append(time.time() - start)\n\n    # Test daemon mode\n    daemon_times = []\n    with use_daemon():\n        for i in range(10):\n            start = time.time()\n            steadytext.generate(\"test\", seed=i+100)\n            daemon_times.append(time.time() - start)\n\n    print(\"Direct mode:\")\n    print(f\"  Mean: {statistics.mean(direct_times):.3f}s\")\n    print(f\"  Stdev: {statistics.stdev(direct_times):.3f}s\")\n\n    print(\"\\nDaemon mode:\")\n    print(f\"  Mean: {statistics.mean(daemon_times):.3f}s\")\n    print(f\"  Stdev: {statistics.stdev(daemon_times):.3f}s\")\n\n    if statistics.mean(daemon_times) &gt; statistics.mean(direct_times):\n        print(\"\\nWARNING: Daemon is slower than direct mode!\")\n        print(\"Possible causes:\")\n        print(\"- Network latency\")\n        print(\"- Daemon overloaded\")\n        print(\"- Cache thrashing\")\n\ndiagnose_daemon_performance()\n</code></pre>"},{"location":"examples/daemon-usage/#debug-checklist","title":"Debug Checklist","text":"<ol> <li> <p>Check daemon status <pre><code>st daemon status --json | jq .\n</code></pre></p> </li> <li> <p>Verify connectivity <pre><code>nc -zv 127.0.0.1 5557\n</code></pre></p> </li> <li> <p>Check logs <pre><code>tail -f /var/log/steadytext/daemon.log\ngrep ERROR /var/log/steadytext/daemon.error.log\n</code></pre></p> </li> <li> <p>Monitor resources <pre><code>htop -p $(pgrep -f \"st daemon\")\n</code></pre></p> </li> <li> <p>Test basic operations <pre><code>from steadytext.daemon.client import DaemonClient\nclient = DaemonClient()\nprint(client._send_request({\"type\": \"ping\"}))\n</code></pre></p> </li> </ol>"},{"location":"examples/daemon-usage/#best-practices","title":"Best Practices","text":""},{"location":"examples/daemon-usage/#1-production-configuration","title":"1. Production Configuration","text":"<pre><code># production.env\nSTEADYTEXT_DAEMON_HOST=0.0.0.0\nSTEADYTEXT_DAEMON_PORT=5557\nSTEADYTEXT_GENERATION_CACHE_CAPACITY=2048\nSTEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=500\nSTEADYTEXT_EMBEDDING_CACHE_CAPACITY=4096\nSTEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=1000\nSTEADYTEXT_DEFAULT_SEED=42\nPYTHONUNBUFFERED=1\n</code></pre>"},{"location":"examples/daemon-usage/#2-health-monitoring","title":"2. Health Monitoring","text":"<pre><code># health_check.py\ndef health_check():\n    \"\"\"Comprehensive daemon health check.\"\"\"\n    checks = {\n        \"daemon_running\": False,\n        \"response_time\": None,\n        \"cache_available\": False,\n        \"memory_ok\": False\n    }\n\n    # Check if daemon is running\n    try:\n        result = subprocess.run(\n            [\"st\", \"daemon\", \"status\", \"--json\"],\n            capture_output=True,\n            text=True\n        )\n        if result.returncode == 0:\n            status = json.loads(result.stdout)\n            checks[\"daemon_running\"] = status.get(\"running\", False)\n    except:\n        pass\n\n    # Check response time\n    if checks[\"daemon_running\"]:\n        start = time.time()\n        try:\n            steadytext.generate(\"health check\", seed=42)\n            checks[\"response_time\"] = time.time() - start\n        except:\n            pass\n\n    # Check cache\n    try:\n        cache_manager = get_cache_manager()\n        stats = cache_manager.get_cache_stats()\n        checks[\"cache_available\"] = True\n    except:\n        pass\n\n    # Check memory\n    if checks[\"daemon_running\"]:\n        memory = psutil.Process().memory_info().rss / 1024 / 1024\n        checks[\"memory_ok\"] = memory &lt; 4096  # 4GB limit\n\n    return checks\n</code></pre>"},{"location":"examples/daemon-usage/#3-graceful-degradation","title":"3. Graceful Degradation","text":"<pre><code># graceful_degradation.py\nclass ResilientClient:\n    \"\"\"Client with graceful degradation.\"\"\"\n\n    def __init__(self):\n        self.use_daemon = True\n        self.fallback_count = 0\n        self.max_fallbacks = 3\n\n    def generate(self, prompt, **kwargs):\n        \"\"\"Generate with automatic fallback.\"\"\"\n        if self.use_daemon and self.fallback_count &lt; self.max_fallbacks:\n            try:\n                with use_daemon():\n                    return steadytext.generate(prompt, **kwargs)\n            except Exception as e:\n                self.fallback_count += 1\n                print(f\"Daemon failed ({self.fallback_count}/{self.max_fallbacks}): {e}\")\n\n                if self.fallback_count &gt;= self.max_fallbacks:\n                    self.use_daemon = False\n                    print(\"Disabling daemon due to repeated failures\")\n\n        # Direct mode fallback\n        return steadytext.generate(prompt, **kwargs)\n</code></pre>"},{"location":"examples/daemon-usage/#4-security-considerations","title":"4. Security Considerations","text":"<pre><code># secure_daemon.py\nimport ssl\nimport secrets\n\nclass SecureDaemonConfig:\n    \"\"\"Secure daemon configuration.\"\"\"\n\n    @staticmethod\n    def generate_auth_token():\n        \"\"\"Generate secure auth token.\"\"\"\n        return secrets.token_urlsafe(32)\n\n    @staticmethod\n    def configure_tls(cert_path, key_path):\n        \"\"\"Configure TLS for daemon.\"\"\"\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(cert_path, key_path)\n        return context\n\n    @staticmethod\n    def restrict_bind_address():\n        \"\"\"Restrict daemon to localhost only.\"\"\"\n        return {\n            \"host\": \"127.0.0.1\",  # Never use 0.0.0.0 in production\n            \"port\": 5557\n        }\n</code></pre> <p>This comprehensive guide covers all aspects of using SteadyText's daemon mode, from basic usage to advanced production deployments. The daemon provides significant performance benefits while maintaining the simplicity and reliability that SteadyText is known for.</p>"},{"location":"examples/data-pipelines/","title":"Data Pipelines with AI Enrichment","text":"<p>Build intelligent ETL pipelines that enrich, transform, and analyze data using SteadyText's AI capabilities directly in PostgreSQL.</p>"},{"location":"examples/data-pipelines/#overview","title":"Overview","text":"<p>This tutorial shows how to create data pipelines that: - Enrich raw data with AI-generated insights - Transform unstructured data into structured formats - Monitor data quality with AI validation - Generate automated reports and summaries - Create real-time data enrichment streams</p>"},{"location":"examples/data-pipelines/#prerequisites","title":"Prerequisites","text":"<pre><code># Start PostgreSQL with SteadyText\ndocker run -d -p 5432:5432 --name steadytext-etl julep/pg-steadytext\n\n# Connect and setup\npsql -h localhost -U postgres -c \"CREATE EXTENSION IF NOT EXISTS pg_steadytext CASCADE;\"\npsql -h localhost -U postgres -c \"CREATE EXTENSION IF NOT EXISTS pg_cron;\"  # For scheduling\n</code></pre>"},{"location":"examples/data-pipelines/#pipeline-architecture","title":"Pipeline Architecture","text":"<p>Create a flexible pipeline schema:</p> <pre><code>-- Pipeline definitions\nCREATE TABLE data_pipelines (\n    id SERIAL PRIMARY KEY,\n    pipeline_name VARCHAR(100) UNIQUE NOT NULL,\n    description TEXT,\n    source_table VARCHAR(100),\n    target_table VARCHAR(100),\n    transform_function VARCHAR(100),\n    schedule_cron VARCHAR(50),\n    is_active BOOLEAN DEFAULT TRUE,\n    last_run TIMESTAMPTZ,\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Pipeline execution log\nCREATE TABLE pipeline_runs (\n    id SERIAL PRIMARY KEY,\n    pipeline_id INTEGER REFERENCES data_pipelines(id),\n    start_time TIMESTAMPTZ NOT NULL,\n    end_time TIMESTAMPTZ,\n    status VARCHAR(20), -- 'running', 'completed', 'failed'\n    records_processed INTEGER,\n    records_enriched INTEGER,\n    error_message TEXT,\n    execution_stats JSONB\n);\n\n-- Raw data staging table\nCREATE TABLE raw_data_staging (\n    id SERIAL PRIMARY KEY,\n    source_system VARCHAR(50),\n    raw_content TEXT,\n    metadata JSONB,\n    processed BOOLEAN DEFAULT FALSE,\n    ingested_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Enriched data warehouse\nCREATE TABLE enriched_data (\n    id SERIAL PRIMARY KEY,\n    source_id INTEGER,\n    source_system VARCHAR(50),\n    original_content TEXT,\n    ai_summary TEXT,\n    extracted_entities JSONB,\n    sentiment_analysis JSONB,\n    categories TEXT[],\n    quality_score DECIMAL(3, 2),\n    enriched_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Data quality monitoring\nCREATE TABLE data_quality_issues (\n    id SERIAL PRIMARY KEY,\n    pipeline_id INTEGER REFERENCES data_pipelines(id),\n    record_id INTEGER,\n    issue_type VARCHAR(50),\n    severity VARCHAR(20),\n    description TEXT,\n    ai_recommendation TEXT,\n    detected_at TIMESTAMPTZ DEFAULT NOW()\n);\n</code></pre>"},{"location":"examples/data-pipelines/#real-time-data-enrichment-pipeline","title":"Real-Time Data Enrichment Pipeline","text":"<p>Create a pipeline that enriches incoming data in real-time:</p> <pre><code>-- Main enrichment function\nCREATE OR REPLACE FUNCTION enrich_raw_data()\nRETURNS TRIGGER AS $$\nDECLARE\n    v_summary TEXT;\n    v_entities JSONB;\n    v_sentiment JSONB;\n    v_categories TEXT[];\n    v_quality_score DECIMAL(3, 2);\nBEGIN\n    -- Skip if already processed\n    IF NEW.processed THEN\n        RETURN NEW;\n    END IF;\n\n    -- Generate AI summary\n    v_summary := steadytext_generate(\n        format('Summarize this data in 2 sentences: %s',\n            LEFT(NEW.raw_content, 1000)),\n        max_tokens := 100\n    );\n\n    -- Extract entities\n    v_entities := steadytext_generate_json(\n        format('Extract entities from: %s', LEFT(NEW.raw_content, 500)),\n        '{\n            \"people\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n            \"organizations\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n            \"locations\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n            \"dates\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}},\n            \"amounts\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}\n        }'::json\n    )::jsonb;\n\n    -- Sentiment analysis\n    v_sentiment := jsonb_build_object(\n        'overall', steadytext_generate_choice(\n            'Overall sentiment: ' || LEFT(NEW.raw_content, 500),\n            ARRAY['positive', 'neutral', 'negative']\n        ),\n        'confidence', 0.85 + random() * 0.15  -- Simulated confidence\n    );\n\n    -- Categorization\n    v_categories := string_to_array(\n        steadytext_generate(\n            format('List up to 3 categories for this content (comma-separated): %s',\n                LEFT(NEW.raw_content, 500)),\n            max_tokens := 30\n        ),\n        ', '\n    );\n\n    -- Calculate quality score\n    v_quality_score := CASE\n        WHEN length(NEW.raw_content) &lt; 50 THEN 0.3\n        WHEN v_entities IS NULL OR jsonb_typeof(v_entities) != 'object' THEN 0.5\n        ELSE 0.7 + random() * 0.3\n    END;\n\n    -- Insert enriched data\n    INSERT INTO enriched_data (\n        source_id, source_system, original_content,\n        ai_summary, extracted_entities, sentiment_analysis,\n        categories, quality_score\n    ) VALUES (\n        NEW.id, NEW.source_system, NEW.raw_content,\n        v_summary, v_entities, v_sentiment,\n        v_categories, v_quality_score\n    );\n\n    -- Mark as processed\n    NEW.processed := TRUE;\n\n    -- Check for quality issues\n    IF v_quality_score &lt; 0.5 THEN\n        INSERT INTO data_quality_issues (\n            record_id, issue_type, severity, description, ai_recommendation\n        ) VALUES (\n            NEW.id,\n            'low_quality_content',\n            'medium',\n            format('Quality score %s is below threshold', v_quality_score),\n            steadytext_generate(\n                'Suggest how to improve data quality for: ' || LEFT(NEW.raw_content, 200),\n                max_tokens := 100\n            )\n        );\n    END IF;\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create trigger for real-time enrichment\nCREATE TRIGGER enrich_on_insert\n    BEFORE INSERT OR UPDATE ON raw_data_staging\n    FOR EACH ROW\n    EXECUTE FUNCTION enrich_raw_data();\n</code></pre>"},{"location":"examples/data-pipelines/#batch-processing-pipeline","title":"Batch Processing Pipeline","text":"<p>Create a batch pipeline for large-scale data processing:</p> <pre><code>-- Batch enrichment function\nCREATE OR REPLACE FUNCTION batch_enrich_pipeline(\n    p_pipeline_name VARCHAR,\n    p_batch_size INTEGER DEFAULT 100\n)\nRETURNS TABLE (\n    processed_count INTEGER,\n    enriched_count INTEGER,\n    error_count INTEGER,\n    execution_time INTERVAL\n) AS $$\nDECLARE\n    v_pipeline data_pipelines%ROWTYPE;\n    v_run_id INTEGER;\n    v_start_time TIMESTAMPTZ;\n    v_processed INTEGER := 0;\n    v_enriched INTEGER := 0;\n    v_errors INTEGER := 0;\nBEGIN\n    v_start_time := NOW();\n\n    -- Get pipeline configuration\n    SELECT * INTO v_pipeline FROM data_pipelines \n    WHERE pipeline_name = p_pipeline_name AND is_active;\n\n    IF NOT FOUND THEN\n        RAISE EXCEPTION 'Pipeline % not found or inactive', p_pipeline_name;\n    END IF;\n\n    -- Create pipeline run record\n    INSERT INTO pipeline_runs (pipeline_id, start_time, status)\n    VALUES (v_pipeline.id, v_start_time, 'running')\n    RETURNING id INTO v_run_id;\n\n    -- Process in batches\n    FOR r IN \n        SELECT * FROM raw_data_staging \n        WHERE NOT processed \n        ORDER BY ingested_at \n        LIMIT p_batch_size\n    LOOP\n        BEGIN\n            -- Process individual record\n            UPDATE raw_data_staging SET processed = TRUE WHERE id = r.id;\n            v_processed := v_processed + 1;\n\n            -- The trigger will handle enrichment\n            v_enriched := v_enriched + 1;\n\n        EXCEPTION WHEN OTHERS THEN\n            v_errors := v_errors + 1;\n\n            INSERT INTO data_quality_issues (\n                pipeline_id, record_id, issue_type, severity, description\n            ) VALUES (\n                v_pipeline.id, r.id, 'processing_error', 'high', SQLERRM\n            );\n        END;\n    END LOOP;\n\n    -- Update pipeline run status\n    UPDATE pipeline_runs \n    SET end_time = NOW(),\n        status = 'completed',\n        records_processed = v_processed,\n        records_enriched = v_enriched,\n        execution_stats = jsonb_build_object(\n            'errors', v_errors,\n            'avg_processing_time_ms', \n            EXTRACT(MILLISECONDS FROM (NOW() - v_start_time)) / NULLIF(v_processed, 0)\n        )\n    WHERE id = v_run_id;\n\n    -- Update pipeline last run\n    UPDATE data_pipelines \n    SET last_run = NOW() \n    WHERE id = v_pipeline.id;\n\n    RETURN QUERY SELECT \n        v_processed,\n        v_enriched,\n        v_errors,\n        NOW() - v_start_time;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/data-pipelines/#data-quality-monitoring","title":"Data Quality Monitoring","text":"<p>Implement AI-powered data quality checks:</p> <pre><code>-- Data quality monitoring function\nCREATE OR REPLACE FUNCTION monitor_data_quality(\n    p_hours_back INTEGER DEFAULT 24\n)\nRETURNS TABLE (\n    quality_metric VARCHAR,\n    score DECIMAL,\n    issues_found INTEGER,\n    ai_insights TEXT\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH quality_metrics AS (\n        SELECT \n            'completeness' AS metric,\n            AVG(CASE \n                WHEN ai_summary IS NOT NULL \n                 AND extracted_entities IS NOT NULL \n                 AND categories IS NOT NULL \n                THEN 1.0 ELSE 0.0 \n            END) AS score,\n            COUNT(*) FILTER (WHERE quality_score &lt; 0.5) AS issues\n        FROM enriched_data\n        WHERE enriched_at &gt; NOW() - (p_hours_back || ' hours')::INTERVAL\n\n        UNION ALL\n\n        SELECT \n            'accuracy' AS metric,\n            AVG(quality_score) AS score,\n            COUNT(DISTINCT dqi.id) AS issues\n        FROM enriched_data ed\n        LEFT JOIN data_quality_issues dqi ON dqi.record_id = ed.source_id\n        WHERE ed.enriched_at &gt; NOW() - (p_hours_back || ' hours')::INTERVAL\n\n        UNION ALL\n\n        SELECT \n            'timeliness' AS metric,\n            CASE \n                WHEN AVG(EXTRACT(EPOCH FROM (enriched_at - ed.enriched_at))) &lt; 300 \n                THEN 1.0 \n                ELSE 0.5 \n            END AS score,\n            COUNT(*) FILTER (\n                WHERE EXTRACT(EPOCH FROM (enriched_at - ed.enriched_at)) &gt; 600\n            ) AS issues\n        FROM enriched_data ed\n        JOIN raw_data_staging rs ON ed.source_id = rs.id\n        WHERE ed.enriched_at &gt; NOW() - (p_hours_back || ' hours')::INTERVAL\n    )\n    SELECT \n        metric,\n        ROUND(score, 2),\n        issues,\n        steadytext_generate(\n            format('Analyze data quality: %s score is %s with %s issues',\n                metric, ROUND(score, 2), issues),\n            max_tokens := 100\n        ) AS ai_insights\n    FROM quality_metrics;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/data-pipelines/#automated-report-generation","title":"Automated Report Generation","text":"<p>Generate intelligent reports from pipeline data:</p> <pre><code>-- Automated report generation\nCREATE OR REPLACE FUNCTION generate_pipeline_report(\n    p_pipeline_id INTEGER,\n    p_period INTERVAL DEFAULT INTERVAL '1 day'\n)\nRETURNS TABLE (\n    report_section VARCHAR,\n    content TEXT,\n    metrics JSONB\n) AS $$\nDECLARE\n    v_pipeline data_pipelines%ROWTYPE;\nBEGIN\n    SELECT * INTO v_pipeline FROM data_pipelines WHERE id = p_pipeline_id;\n\n    RETURN QUERY\n    -- Executive Summary\n    WITH pipeline_stats AS (\n        SELECT \n            COUNT(*) AS total_runs,\n            SUM(records_processed) AS total_processed,\n            SUM(records_enriched) AS total_enriched,\n            AVG(EXTRACT(EPOCH FROM (end_time - start_time))) AS avg_duration_seconds,\n            COUNT(*) FILTER (WHERE status = 'failed') AS failed_runs\n        FROM pipeline_runs\n        WHERE pipeline_id = p_pipeline_id\n          AND start_time &gt; NOW() - p_period\n    )\n    SELECT \n        'executive_summary' AS report_section,\n        steadytext_generate(\n            format('Pipeline %s processed %s records in %s runs over the past %s. Average duration: %s seconds. Failed runs: %s',\n                v_pipeline.pipeline_name,\n                total_processed,\n                total_runs,\n                p_period,\n                ROUND(avg_duration_seconds, 2),\n                failed_runs\n            ),\n            max_tokens := 200\n        ) AS content,\n        to_jsonb(pipeline_stats.*) AS metrics\n    FROM pipeline_stats\n\n    UNION ALL\n\n    -- Data Quality Analysis\n    WITH quality_analysis AS (\n        SELECT \n            AVG(quality_score) AS avg_quality,\n            COUNT(*) FILTER (WHERE quality_score &lt; 0.5) AS low_quality_count,\n            array_agg(DISTINCT unnest(categories)) AS all_categories\n        FROM enriched_data ed\n        JOIN raw_data_staging rs ON ed.source_id = rs.id\n        JOIN pipeline_runs pr ON pr.pipeline_id = p_pipeline_id\n        WHERE ed.enriched_at BETWEEN pr.start_time AND COALESCE(pr.end_time, NOW())\n          AND pr.start_time &gt; NOW() - p_period\n    )\n    SELECT \n        'quality_analysis',\n        steadytext_generate(\n            format('Data quality analysis: Average score %s. Low quality records: %s. Categories covered: %s',\n                ROUND(avg_quality, 2),\n                low_quality_count,\n                array_to_string(all_categories[1:5], ', ')\n            ),\n            max_tokens := 150\n        ),\n        jsonb_build_object(\n            'avg_quality_score', avg_quality,\n            'low_quality_count', low_quality_count,\n            'category_count', array_length(all_categories, 1)\n        )\n    FROM quality_analysis\n\n    UNION ALL\n\n    -- Trend Analysis\n    SELECT \n        'trend_analysis',\n        steadytext_generate(\n            format('Analyze trends for pipeline %s based on: %s',\n                v_pipeline.pipeline_name,\n                jsonb_pretty(\n                    jsonb_build_object(\n                        'processing_volume', \n                        (SELECT array_agg(records_processed ORDER BY start_time)\n                         FROM pipeline_runs \n                         WHERE pipeline_id = p_pipeline_id \n                           AND start_time &gt; NOW() - p_period\n                         LIMIT 10)\n                    )\n                )\n            ),\n            max_tokens := 200\n        ),\n        NULL;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/data-pipelines/#stream-processing-integration","title":"Stream Processing Integration","text":"<p>Handle real-time data streams:</p> <pre><code>-- Streaming data handler\nCREATE OR REPLACE FUNCTION process_data_stream(\n    p_stream_data JSONB\n)\nRETURNS VOID AS $$\nDECLARE\n    v_record JSONB;\n    v_source_system VARCHAR;\nBEGIN\n    -- Extract source system\n    v_source_system := p_stream_data-&gt;&gt;'source_system';\n\n    -- Process each record in the stream\n    FOR v_record IN SELECT * FROM jsonb_array_elements(p_stream_data-&gt;'records')\n    LOOP\n        INSERT INTO raw_data_staging (\n            source_system,\n            raw_content,\n            metadata\n        ) VALUES (\n            v_source_system,\n            v_record-&gt;&gt;'content',\n            v_record-&gt;'metadata'\n        );\n    END LOOP;\n\n    -- Trigger batch processing if needed\n    IF (SELECT COUNT(*) FROM raw_data_staging WHERE NOT processed) &gt; 1000 THEN\n        PERFORM batch_enrich_pipeline('main_pipeline', 1000);\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- API endpoint for streaming\nCREATE OR REPLACE FUNCTION api_ingest_stream(\n    p_api_key VARCHAR,\n    p_data JSONB\n)\nRETURNS JSONB AS $$\nDECLARE\n    v_result JSONB;\nBEGIN\n    -- Validate API key (simplified)\n    IF p_api_key != 'your-secret-key' THEN\n        RETURN jsonb_build_object('error', 'Invalid API key');\n    END IF;\n\n    -- Process the stream\n    PERFORM process_data_stream(p_data);\n\n    -- Return success response\n    RETURN jsonb_build_object(\n        'status', 'success',\n        'records_received', jsonb_array_length(p_data-&gt;'records'),\n        'timestamp', NOW()\n    );\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/data-pipelines/#pipeline-orchestration","title":"Pipeline Orchestration","text":"<p>Schedule and orchestrate complex pipelines:</p> <pre><code>-- Create sample pipelines\nINSERT INTO data_pipelines (pipeline_name, description, schedule_cron) VALUES\n('hourly_enrichment', 'Process and enrich data every hour', '0 * * * *'),\n('daily_quality_check', 'Daily data quality monitoring', '0 9 * * *'),\n('weekly_report', 'Generate weekly executive reports', '0 10 * * 1');\n\n-- Schedule with pg_cron\nSELECT cron.schedule(\n    'hourly_enrichment_job',\n    '0 * * * *',\n    $$SELECT batch_enrich_pipeline('hourly_enrichment', 500);$$\n);\n\nSELECT cron.schedule(\n    'daily_quality_job',\n    '0 9 * * *',\n    $$INSERT INTO data_quality_reports \n      SELECT NOW(), * FROM monitor_data_quality(24);$$\n);\n\n-- Complex pipeline with dependencies\nCREATE OR REPLACE FUNCTION orchestrate_complex_pipeline()\nRETURNS VOID AS $$\nBEGIN\n    -- Step 1: Ingest raw data\n    PERFORM process_data_stream(\n        jsonb_build_object(\n            'source_system', 'automated_import',\n            'records', (\n                SELECT jsonb_agg(\n                    jsonb_build_object(\n                        'content', external_content,\n                        'metadata', metadata\n                    )\n                )\n                FROM external_data_source\n                WHERE import_date = CURRENT_DATE\n            )\n        )\n    );\n\n    -- Step 2: Enrich data\n    PERFORM batch_enrich_pipeline('hourly_enrichment', 1000);\n\n    -- Step 3: Quality check\n    INSERT INTO data_quality_reports\n    SELECT NOW(), * FROM monitor_data_quality(1);\n\n    -- Step 4: Generate insights\n    INSERT INTO executive_insights\n    SELECT * FROM generate_pipeline_report(\n        (SELECT id FROM data_pipelines WHERE pipeline_name = 'hourly_enrichment'),\n        INTERVAL '1 hour'\n    );\n\n    -- Step 5: Alert on issues\n    PERFORM pg_notify('pipeline_complete', \n        json_build_object(\n            'pipeline', 'complex_orchestration',\n            'status', 'completed',\n            'timestamp', NOW()\n        )::text\n    );\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/data-pipelines/#sample-data-and-testing","title":"Sample Data and Testing","text":"<pre><code>-- Insert test data\nINSERT INTO raw_data_staging (source_system, raw_content, metadata) VALUES\n('crm', 'Customer John Smith called about product issue. He was frustrated with the delayed shipping and wants a refund. Order #12345.', \n '{\"customer_id\": \"C123\", \"call_duration\": \"15:32\"}'::jsonb),\n('social_media', 'Just received my new headphones from @YourCompany! Amazing sound quality and super comfortable. Best purchase this year! #Happy', \n '{\"platform\": \"twitter\", \"engagement\": {\"likes\": 45, \"retweets\": 12}}'::jsonb),\n('support_email', 'Subject: Technical Issue\\n\\nDear Support,\\n\\nI am experiencing connectivity issues with model XZ-500. The device keeps disconnecting every few minutes. I have tried resetting but the problem persists.\\n\\nPlease help.\\n\\nRegards,\\nJane Doe', \n '{\"ticket_id\": \"T789\", \"priority\": \"high\"}'::jsonb);\n\n-- Run enrichment pipeline\nSELECT * FROM batch_enrich_pipeline('hourly_enrichment', 10);\n\n-- Check enriched data\nSELECT \n    source_system,\n    ai_summary,\n    sentiment_analysis-&gt;&gt;'overall' AS sentiment,\n    categories,\n    quality_score\nFROM enriched_data\nORDER BY enriched_at DESC\nLIMIT 5;\n\n-- Monitor quality\nSELECT * FROM monitor_data_quality(24);\n\n-- Generate report\nSELECT * FROM generate_pipeline_report(1, INTERVAL '1 day');\n</code></pre>"},{"location":"examples/data-pipelines/#performance-optimization","title":"Performance Optimization","text":"<pre><code>-- Parallel processing function\nCREATE OR REPLACE FUNCTION parallel_enrich_pipeline(\n    p_pipeline_name VARCHAR,\n    p_parallel_workers INTEGER DEFAULT 4\n)\nRETURNS VOID AS $$\nBEGIN\n    -- Use PostgreSQL parallel queries\n    SET max_parallel_workers_per_gather = p_parallel_workers;\n\n    -- Process in parallel\n    UPDATE raw_data_staging rs\n    SET processed = TRUE\n    FROM (\n        SELECT id, \n               steadytext_generate('Summarize: ' || raw_content, 100) AS summary\n        FROM raw_data_staging\n        WHERE NOT processed\n        LIMIT 1000\n    ) enriched\n    WHERE rs.id = enriched.id;\n\n    RESET max_parallel_workers_per_gather;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/data-pipelines/#best-practices","title":"Best Practices","text":"<ol> <li>Batch Size: Tune batch sizes based on your hardware</li> <li>Error Handling: Always implement comprehensive error handling</li> <li>Monitoring: Set up alerts for pipeline failures</li> <li>Caching: Use SteadyText's caching for repeated AI operations</li> <li>Scheduling: Use pg_cron for reliable pipeline scheduling</li> </ol>"},{"location":"examples/data-pipelines/#next-steps","title":"Next Steps","text":"<ul> <li>TimescaleDB Integration \u2192</li> <li>Production Deployment \u2192</li> <li>Migration Guides \u2192</li> </ul> <p>Pro Tip</p> <p>For high-volume pipelines, consider partitioning your staging tables by date and using parallel workers to maximize throughput.</p>"},{"location":"examples/error-handling/","title":"Error Handling Guide","text":"<p>Learn how to handle errors gracefully in SteadyText, implement robust fallback strategies, and build resilient applications.</p>"},{"location":"examples/error-handling/#overview","title":"Overview","text":"<p>SteadyText follows a \"never fail\" philosophy (with v2.1.0+ updates):</p> <ul> <li>Functions return <code>None</code> when models are unavailable (v2.1.0+)</li> <li>No exceptions are raised during normal operations</li> <li>Graceful degradation with predictable behavior</li> <li>Clear error indicators for proper handling</li> <li>Deterministic fallbacks respect seed values</li> </ul> <p>Breaking Change in v2.1.0</p> <p>The deterministic fallback behavior has been disabled. Functions now return <code>None</code> instead of generating fallback text/embeddings when models are unavailable.</p>"},{"location":"examples/error-handling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Error Types and Handling</li> <li>Generation Error Handling</li> <li>Embedding Error Handling</li> <li>Streaming Error Handling</li> <li>Daemon Error Handling</li> <li>CLI Error Handling</li> <li>Production Patterns</li> <li>Monitoring and Alerting</li> <li>Recovery Strategies</li> <li>Best Practices</li> </ul>"},{"location":"examples/error-handling/#error-types-and-handling","title":"Error Types and Handling","text":""},{"location":"examples/error-handling/#common-error-scenarios","title":"Common Error Scenarios","text":"<pre><code>import steadytext\nimport logging\nfrom typing import Optional, Union\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef handle_generation_result(result: Optional[str], prompt: str) -&gt; str:\n    \"\"\"Handle generation result with proper error checking.\"\"\"\n    if result is None:\n        logger.error(f\"Generation failed for prompt: {prompt}\")\n        # Implement your fallback strategy\n        return f\"[Error: Unable to generate response for: {prompt}]\"\n\n    if not result.strip():\n        logger.warning(f\"Empty generation for prompt: {prompt}\")\n        return \"[Error: Empty response generated]\"\n\n    return result\n\n# Usage example\nprompt = \"Write a summary\"\nresult = steadytext.generate(prompt, seed=42)\nhandled_result = handle_generation_result(result, prompt)\nprint(handled_result)\n</code></pre>"},{"location":"examples/error-handling/#error-categories","title":"Error Categories","text":"<pre><code>from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Optional, Any\n\nclass ErrorType(Enum):\n    \"\"\"Types of errors in SteadyText operations.\"\"\"\n    MODEL_NOT_LOADED = \"model_not_loaded\"\n    INVALID_INPUT = \"invalid_input\"\n    DAEMON_UNAVAILABLE = \"daemon_unavailable\"\n    CACHE_ERROR = \"cache_error\"\n    TIMEOUT = \"timeout\"\n    MEMORY_ERROR = \"memory_error\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass SteadyTextError:\n    \"\"\"Structured error information.\"\"\"\n    error_type: ErrorType\n    message: str\n    context: dict\n    recoverable: bool\n    suggested_action: Optional[str] = None\n\nclass ErrorHandler:\n    \"\"\"Centralized error handling for SteadyText operations.\"\"\"\n\n    def __init__(self):\n        self.error_log = []\n        self.error_counts = {error_type: 0 for error_type in ErrorType}\n\n    def handle_error(self, error_type: ErrorType, message: str, \n                     context: dict = None, recoverable: bool = True) -&gt; SteadyTextError:\n        \"\"\"Handle and log an error.\"\"\"\n        error = SteadyTextError(\n            error_type=error_type,\n            message=message,\n            context=context or {},\n            recoverable=recoverable,\n            suggested_action=self._get_suggested_action(error_type)\n        )\n\n        self.error_log.append(error)\n        self.error_counts[error_type] += 1\n\n        logger.error(f\"{error_type.value}: {message}\", extra=context)\n\n        return error\n\n    def _get_suggested_action(self, error_type: ErrorType) -&gt; str:\n        \"\"\"Get suggested action for error type.\"\"\"\n        actions = {\n            ErrorType.MODEL_NOT_LOADED: \"Run 'st models download' to download models\",\n            ErrorType.INVALID_INPUT: \"Check input format and constraints\",\n            ErrorType.DAEMON_UNAVAILABLE: \"Start daemon with 'st daemon start'\",\n            ErrorType.CACHE_ERROR: \"Clear cache with 'st cache --clear'\",\n            ErrorType.TIMEOUT: \"Increase timeout or retry operation\",\n            ErrorType.MEMORY_ERROR: \"Reduce batch size or restart daemon\",\n            ErrorType.UNKNOWN: \"Check logs for more information\"\n        }\n        return actions.get(error_type, \"\")\n\n    def get_error_summary(self) -&gt; dict:\n        \"\"\"Get summary of all errors.\"\"\"\n        return {\n            \"total_errors\": len(self.error_log),\n            \"error_counts\": dict(self.error_counts),\n            \"recent_errors\": self.error_log[-10:],\n            \"most_common\": max(self.error_counts.items(), key=lambda x: x[1])\n        }\n\n# Global error handler\nerror_handler = ErrorHandler()\n</code></pre>"},{"location":"examples/error-handling/#generation-error-handling","title":"Generation Error Handling","text":""},{"location":"examples/error-handling/#basic-error-handling","title":"Basic Error Handling","text":"<pre><code>import steadytext\nfrom typing import Optional\n\ndef safe_generate(prompt: str, seed: int = 42, max_retries: int = 3) -&gt; Optional[str]:\n    \"\"\"Generate text with retry logic and error handling.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            result = steadytext.generate(prompt, seed=seed)\n\n            if result is None:\n                logger.warning(f\"Generation returned None (attempt {attempt + 1}/{max_retries})\")\n                if attempt &lt; max_retries - 1:\n                    time.sleep(0.5 * (attempt + 1))  # Exponential backoff\n                    continue\n                else:\n                    error_handler.handle_error(\n                        ErrorType.MODEL_NOT_LOADED,\n                        \"Generation failed after all retries\",\n                        {\"prompt\": prompt, \"seed\": seed, \"attempts\": max_retries}\n                    )\n                    return None\n\n            return result\n\n        except Exception as e:\n            logger.exception(f\"Unexpected error in generation: {e}\")\n            error_handler.handle_error(\n                ErrorType.UNKNOWN,\n                str(e),\n                {\"prompt\": prompt, \"seed\": seed, \"attempt\": attempt + 1}\n            )\n\n            if attempt &lt; max_retries - 1:\n                time.sleep(0.5 * (attempt + 1))\n            else:\n                return None\n\n    return None\n\n# Usage\nresult = safe_generate(\"Write a poem\", seed=123)\nif result:\n    print(result)\nelse:\n    print(\"Failed to generate text. Please check the error log.\")\n</code></pre>"},{"location":"examples/error-handling/#advanced-generation-error-handling","title":"Advanced Generation Error Handling","text":"<pre><code>import steadytext\nfrom typing import Optional, Dict, Any, Callable\nimport time\nimport hashlib\n\nclass RobustGenerator:\n    \"\"\"Robust text generation with comprehensive error handling.\"\"\"\n\n    def __init__(self, \n                 fallback_strategy: str = \"template\",\n                 cache_fallbacks: bool = True,\n                 alert_threshold: int = 5):\n        self.fallback_strategy = fallback_strategy\n        self.cache_fallbacks = cache_fallbacks\n        self.alert_threshold = alert_threshold\n        self.fallback_cache = {}\n        self.consecutive_failures = 0\n        self.success_callbacks = []\n        self.failure_callbacks = []\n\n    def on_success(self, callback: Callable):\n        \"\"\"Register success callback.\"\"\"\n        self.success_callbacks.append(callback)\n\n    def on_failure(self, callback: Callable):\n        \"\"\"Register failure callback.\"\"\"\n        self.failure_callbacks.append(callback)\n\n    def generate(self, prompt: str, seed: int = 42, **kwargs) -&gt; str:\n        \"\"\"Generate text with comprehensive error handling.\"\"\"\n        start_time = time.time()\n\n        try:\n            # Attempt generation\n            result = steadytext.generate(prompt, seed=seed, **kwargs)\n\n            if result is not None:\n                # Success\n                self.consecutive_failures = 0\n                self._notify_success(prompt, result, time.time() - start_time)\n                return result\n\n            # Generation failed\n            self.consecutive_failures += 1\n            self._notify_failure(prompt, \"Generation returned None\")\n\n            # Check alert threshold\n            if self.consecutive_failures &gt;= self.alert_threshold:\n                self._trigger_alert(f\"Generation failures exceeded threshold: {self.consecutive_failures}\")\n\n            # Apply fallback strategy\n            return self._apply_fallback(prompt, seed)\n\n        except Exception as e:\n            self.consecutive_failures += 1\n            self._notify_failure(prompt, str(e))\n            return self._apply_fallback(prompt, seed)\n\n    def _apply_fallback(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Apply fallback strategy based on configuration.\"\"\"\n        # Check cache first\n        cache_key = self._get_cache_key(prompt, seed)\n        if self.cache_fallbacks and cache_key in self.fallback_cache:\n            return self.fallback_cache[cache_key]\n\n        # Generate fallback\n        if self.fallback_strategy == \"template\":\n            fallback = self._template_fallback(prompt)\n        elif self.fallback_strategy == \"hash\":\n            fallback = self._hash_fallback(prompt, seed)\n        elif self.fallback_strategy == \"empty\":\n            fallback = \"\"\n        elif self.fallback_strategy == \"error\":\n            fallback = f\"[Error: Unable to generate response for: {prompt[:50]}...]\"\n        else:\n            fallback = \"[Generation failed]\"\n\n        # Cache fallback\n        if self.cache_fallbacks:\n            self.fallback_cache[cache_key] = fallback\n\n        return fallback\n\n    def _template_fallback(self, prompt: str) -&gt; str:\n        \"\"\"Generate template-based fallback.\"\"\"\n        templates = {\n            \"summary\": \"This is a summary of the requested content.\",\n            \"explanation\": \"This explains the requested concept.\",\n            \"code\": \"# Code implementation would go here\",\n            \"story\": \"Once upon a time, there was a story to be told.\",\n            \"default\": \"Response generated for: {}\"\n        }\n\n        # Detect prompt type\n        prompt_lower = prompt.lower()\n        for key in templates:\n            if key in prompt_lower:\n                return templates[key].format(prompt[:30] + \"...\")\n\n        return templates[\"default\"].format(prompt[:30] + \"...\")\n\n    def _hash_fallback(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Generate deterministic hash-based fallback.\"\"\"\n        # Create deterministic hash\n        hash_input = f\"{prompt}:{seed}\"\n        hash_value = hashlib.sha256(hash_input.encode()).hexdigest()[:8]\n\n        return f\"[Fallback response {hash_value} for: {prompt[:30]}...]\"\n\n    def _get_cache_key(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Generate cache key for fallback.\"\"\"\n        return f\"{prompt}:{seed}\"\n\n    def _notify_success(self, prompt: str, result: str, duration: float):\n        \"\"\"Notify success callbacks.\"\"\"\n        for callback in self.success_callbacks:\n            try:\n                callback(prompt, result, duration)\n            except Exception as e:\n                logger.error(f\"Error in success callback: {e}\")\n\n    def _notify_failure(self, prompt: str, error: str):\n        \"\"\"Notify failure callbacks.\"\"\"\n        for callback in self.failure_callbacks:\n            try:\n                callback(prompt, error)\n            except Exception as e:\n                logger.error(f\"Error in failure callback: {e}\")\n\n    def _trigger_alert(self, message: str):\n        \"\"\"Trigger alert for critical errors.\"\"\"\n        logger.critical(f\"ALERT: {message}\")\n        # Implement your alerting mechanism here\n        # e.g., send email, Slack message, PagerDuty alert\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get generator statistics.\"\"\"\n        return {\n            \"consecutive_failures\": self.consecutive_failures,\n            \"fallback_cache_size\": len(self.fallback_cache),\n            \"fallback_strategy\": self.fallback_strategy\n        }\n\n# Usage example\ngenerator = RobustGenerator(fallback_strategy=\"template\")\n\n# Register callbacks\ngenerator.on_success(lambda p, r, d: logger.info(f\"Generated in {d:.2f}s\"))\ngenerator.on_failure(lambda p, e: logger.error(f\"Failed: {e}\"))\n\n# Generate with robust error handling\nresult = generator.generate(\"Write a technical blog post\", seed=42)\nprint(result)\n\n# Check stats\nstats = generator.get_stats()\nprint(f\"Generator stats: {stats}\")\n</code></pre>"},{"location":"examples/error-handling/#embedding-error-handling","title":"Embedding Error Handling","text":""},{"location":"examples/error-handling/#basic-embedding-error-handling","title":"Basic Embedding Error Handling","text":"<pre><code>import steadytext\nimport numpy as np\nfrom typing import Optional\n\ndef safe_embed(text: str, seed: int = 42) -&gt; Optional[np.ndarray]:\n    \"\"\"Safely generate embeddings with error handling.\"\"\"\n    try:\n        embedding = steadytext.embed(text, seed=seed)\n\n        if embedding is None:\n            logger.error(f\"Embedding returned None for text: {text[:50]}...\")\n            return None\n\n        # Validate embedding\n        if not isinstance(embedding, np.ndarray):\n            logger.error(f\"Invalid embedding type: {type(embedding)}\")\n            return None\n\n        if embedding.shape != (1024,):\n            logger.error(f\"Invalid embedding shape: {embedding.shape}\")\n            return None\n\n        # Check for NaN or Inf values\n        if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):\n            logger.error(\"Embedding contains NaN or Inf values\")\n            return None\n\n        return embedding\n\n    except Exception as e:\n        logger.exception(f\"Error generating embedding: {e}\")\n        return None\n\n# Usage with fallback\ndef get_embedding_with_fallback(text: str, seed: int = 42) -&gt; np.ndarray:\n    \"\"\"Get embedding with zero-vector fallback.\"\"\"\n    embedding = safe_embed(text, seed=seed)\n\n    if embedding is None:\n        logger.warning(\"Using zero-vector fallback for embedding\")\n        # Return zero vector with correct shape\n        return np.zeros(1024, dtype=np.float32)\n\n    return embedding\n</code></pre>"},{"location":"examples/error-handling/#advanced-embedding-error-handling","title":"Advanced Embedding Error Handling","text":"<pre><code>import steadytext\nimport numpy as np\nfrom typing import List, Optional, Dict, Tuple\nimport hashlib\n\nclass RobustEmbedder:\n    \"\"\"Robust embedding generation with comprehensive error handling.\"\"\"\n\n    def __init__(self, \n                 fallback_method: str = \"zero\",\n                 cache_embeddings: bool = True,\n                 similarity_threshold: float = 0.95):\n        self.fallback_method = fallback_method\n        self.cache_embeddings = cache_embeddings\n        self.similarity_threshold = similarity_threshold\n        self.embedding_cache = {}\n        self.error_count = 0\n        self.success_count = 0\n\n    def embed(self, text: str, seed: int = 42) -&gt; np.ndarray:\n        \"\"\"Generate embedding with error handling.\"\"\"\n        # Check cache\n        cache_key = self._get_cache_key(text, seed)\n        if self.cache_embeddings and cache_key in self.embedding_cache:\n            return self.embedding_cache[cache_key]\n\n        try:\n            # Attempt embedding\n            embedding = steadytext.embed(text, seed=seed)\n\n            if embedding is not None and self._validate_embedding(embedding):\n                self.success_count += 1\n\n                # Cache successful embedding\n                if self.cache_embeddings:\n                    self.embedding_cache[cache_key] = embedding\n\n                return embedding\n\n            # Embedding failed\n            self.error_count += 1\n            return self._generate_fallback(text, seed)\n\n        except Exception as e:\n            logger.exception(f\"Embedding error: {e}\")\n            self.error_count += 1\n            return self._generate_fallback(text, seed)\n\n    def embed_batch(self, texts: List[str], seed: int = 42) -&gt; List[np.ndarray]:\n        \"\"\"Generate embeddings for multiple texts with error handling.\"\"\"\n        embeddings = []\n        failed_indices = []\n\n        for i, text in enumerate(texts):\n            try:\n                # Use different seed for each text in batch\n                text_seed = seed + i\n                embedding = self.embed(text, seed=text_seed)\n                embeddings.append(embedding)\n            except Exception as e:\n                logger.error(f\"Failed to embed text {i}: {e}\")\n                failed_indices.append(i)\n                embeddings.append(self._generate_fallback(text, seed + i))\n\n        if failed_indices:\n            logger.warning(f\"Failed to embed {len(failed_indices)} texts: {failed_indices}\")\n\n        return embeddings\n\n    def find_similar_cached(self, embedding: np.ndarray, top_k: int = 5) -&gt; List[Tuple[str, float]]:\n        \"\"\"Find similar embeddings from cache.\"\"\"\n        if not self.embedding_cache:\n            return []\n\n        similarities = []\n        for cache_key, cached_embedding in self.embedding_cache.items():\n            similarity = np.dot(embedding, cached_embedding)\n            if similarity &gt;= self.similarity_threshold:\n                text = cache_key.split(\":\")[0]  # Extract text from cache key\n                similarities.append((text, similarity))\n\n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n\n    def _validate_embedding(self, embedding: np.ndarray) -&gt; bool:\n        \"\"\"Validate embedding array.\"\"\"\n        if not isinstance(embedding, np.ndarray):\n            return False\n\n        if embedding.shape != (1024,):\n            return False\n\n        if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):\n            return False\n\n        # Check if embedding is normalized\n        norm = np.linalg.norm(embedding)\n        if not np.isclose(norm, 1.0, atol=1e-6):\n            logger.warning(f\"Embedding not normalized: norm={norm}\")\n\n        return True\n\n    def _generate_fallback(self, text: str, seed: int) -&gt; np.ndarray:\n        \"\"\"Generate fallback embedding based on method.\"\"\"\n        if self.fallback_method == \"zero\":\n            return np.zeros(1024, dtype=np.float32)\n\n        elif self.fallback_method == \"random\":\n            # Deterministic random based on text and seed\n            np.random.seed(hash(f\"{text}:{seed}\") % (2**32))\n            embedding = np.random.randn(1024).astype(np.float32)\n            # Normalize\n            embedding = embedding / np.linalg.norm(embedding)\n            return embedding\n\n        elif self.fallback_method == \"hash\":\n            # Hash-based deterministic embedding\n            hash_input = f\"{text}:{seed}\".encode()\n            hash_bytes = hashlib.sha256(hash_input).digest()\n\n            # Convert hash to embedding\n            embedding = np.frombuffer(hash_bytes * 32, dtype=np.float32)[:1024]\n            # Normalize to [-1, 1] range\n            embedding = 2 * (embedding / 255.0) - 1\n            # L2 normalize\n            embedding = embedding / np.linalg.norm(embedding)\n            return embedding\n\n        else:\n            return np.zeros(1024, dtype=np.float32)\n\n    def _get_cache_key(self, text: str, seed: int) -&gt; str:\n        \"\"\"Generate cache key.\"\"\"\n        return f\"{text}:{seed}\"\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get embedder statistics.\"\"\"\n        total = self.success_count + self.error_count\n        return {\n            \"success_count\": self.success_count,\n            \"error_count\": self.error_count,\n            \"success_rate\": self.success_count / total if total &gt; 0 else 0,\n            \"cache_size\": len(self.embedding_cache),\n            \"fallback_method\": self.fallback_method\n        }\n\n# Usage example\nembedder = RobustEmbedder(fallback_method=\"hash\")\n\n# Single embedding\nembedding = embedder.embed(\"test text\", seed=42)\nprint(f\"Embedding shape: {embedding.shape}, norm: {np.linalg.norm(embedding):.4f}\")\n\n# Batch embedding\ntexts = [\"text 1\", \"text 2\", \"text 3\"]\nembeddings = embedder.embed_batch(texts, seed=100)\nprint(f\"Generated {len(embeddings)} embeddings\")\n\n# Find similar\nsimilar = embedder.find_similar_cached(embedding, top_k=3)\nprint(f\"Similar embeddings: {similar}\")\n\n# Stats\nprint(f\"Embedder stats: {embedder.get_stats()}\")\n</code></pre>"},{"location":"examples/error-handling/#streaming-error-handling","title":"Streaming Error Handling","text":""},{"location":"examples/error-handling/#basic-streaming-error-handling","title":"Basic Streaming Error Handling","text":"<pre><code>import steadytext\nfrom typing import Iterator, Optional\n\ndef safe_generate_iter(prompt: str, seed: int = 42) -&gt; Iterator[str]:\n    \"\"\"Safely generate streaming text with error handling.\"\"\"\n    try:\n        stream = steadytext.generate_iter(prompt, seed=seed)\n\n        # Check if stream is empty (indicates error)\n        first_token = None\n        try:\n            first_token = next(stream)\n        except StopIteration:\n            logger.error(\"Empty stream returned\")\n            yield \"[Error: No content generated]\"\n            return\n\n        # Yield first token\n        if first_token:\n            yield first_token\n\n        # Yield remaining tokens\n        for token in stream:\n            yield token\n\n    except Exception as e:\n        logger.exception(f\"Streaming error: {e}\")\n        yield f\"[Error: {str(e)}]\"\n\n# Usage with timeout\ndef generate_with_timeout(prompt: str, seed: int = 42, timeout: float = 30.0) -&gt; str:\n    \"\"\"Generate with streaming and timeout.\"\"\"\n    import signal\n\n    def timeout_handler(signum, frame):\n        raise TimeoutError(\"Generation timed out\")\n\n    # Set timeout\n    signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(int(timeout))\n\n    try:\n        result = []\n        for token in safe_generate_iter(prompt, seed=seed):\n            result.append(token)\n\n        # Cancel timeout\n        signal.alarm(0)\n        return \"\".join(result)\n\n    except TimeoutError:\n        logger.error(f\"Generation timed out after {timeout}s\")\n        return \"[Error: Generation timed out]\"\n    finally:\n        # Ensure timeout is cancelled\n        signal.alarm(0)\n</code></pre>"},{"location":"examples/error-handling/#advanced-streaming-error-handling","title":"Advanced Streaming Error Handling","text":"<pre><code>import steadytext\nfrom typing import Iterator, Optional, Callable\nimport time\nimport threading\nfrom queue import Queue, Empty\n\nclass RobustStreamer:\n    \"\"\"Robust streaming generation with comprehensive error handling.\"\"\"\n\n    def __init__(self,\n                 timeout: float = 30.0,\n                 max_tokens: int = 512,\n                 heartbeat_interval: float = 1.0):\n        self.timeout = timeout\n        self.max_tokens = max_tokens\n        self.heartbeat_interval = heartbeat_interval\n        self.error_handlers = []\n        self.token_validators = []\n\n    def on_error(self, handler: Callable):\n        \"\"\"Register error handler.\"\"\"\n        self.error_handlers.append(handler)\n\n    def add_token_validator(self, validator: Callable[[str], bool]):\n        \"\"\"Add token validator.\"\"\"\n        self.token_validators.append(validator)\n\n    def generate_stream(self, prompt: str, seed: int = 42) -&gt; Iterator[str]:\n        \"\"\"Generate streaming text with comprehensive error handling.\"\"\"\n        start_time = time.time()\n        tokens_generated = 0\n        last_token_time = start_time\n\n        try:\n            stream = steadytext.generate_iter(prompt, seed=seed)\n\n            for token in stream:\n                current_time = time.time()\n\n                # Check timeout\n                if current_time - start_time &gt; self.timeout:\n                    self._handle_error(\"Timeout\", prompt, tokens_generated)\n                    yield \"[Timeout]\"\n                    break\n\n                # Check token count\n                if tokens_generated &gt;= self.max_tokens:\n                    logger.warning(f\"Max tokens ({self.max_tokens}) reached\")\n                    break\n\n                # Validate token\n                if not self._validate_token(token):\n                    logger.warning(f\"Invalid token detected: {repr(token)}\")\n                    continue\n\n                # Check for stalled generation\n                if current_time - last_token_time &gt; self.heartbeat_interval * 10:\n                    self._handle_error(\"Stalled generation\", prompt, tokens_generated)\n                    yield \"[Stalled]\"\n                    break\n\n                # Yield valid token\n                yield token\n                tokens_generated += 1\n                last_token_time = current_time\n\n            # Check if generation completed successfully\n            if tokens_generated == 0:\n                self._handle_error(\"No tokens generated\", prompt, 0)\n                yield \"[No content]\"\n\n        except Exception as e:\n            self._handle_error(str(e), prompt, tokens_generated)\n            yield f\"[Error: {str(e)}]\"\n\n    def generate_async(self, prompt: str, seed: int = 42, \n                      callback: Optional[Callable] = None) -&gt; threading.Thread:\n        \"\"\"Generate asynchronously with error handling.\"\"\"\n        result_queue = Queue()\n\n        def worker():\n            try:\n                tokens = []\n                for token in self.generate_stream(prompt, seed):\n                    tokens.append(token)\n                    if callback:\n                        callback(token)\n\n                result_queue.put((\"success\", \"\".join(tokens)))\n\n            except Exception as e:\n                result_queue.put((\"error\", str(e)))\n\n        thread = threading.Thread(target=worker)\n        thread.start()\n\n        # Return thread and queue for monitoring\n        thread.result_queue = result_queue\n        return thread\n\n    def generate_with_fallback(self, prompt: str, seed: int = 42,\n                              fallback_prompts: Optional[List[str]] = None) -&gt; Iterator[str]:\n        \"\"\"Generate with fallback prompts on error.\"\"\"\n        fallback_prompts = fallback_prompts or [\n            f\"Please provide a response about: {prompt[:50]}...\",\n            \"Generate a helpful response.\",\n            \"Provide relevant information.\"\n        ]\n\n        # Try main prompt\n        tokens = list(self.generate_stream(prompt, seed))\n        if self._is_valid_generation(tokens):\n            for token in tokens:\n                yield token\n            return\n\n        # Try fallback prompts\n        for i, fallback in enumerate(fallback_prompts):\n            logger.info(f\"Trying fallback prompt {i+1}\")\n            tokens = list(self.generate_stream(fallback, seed + i + 1))\n            if self._is_valid_generation(tokens):\n                for token in tokens:\n                    yield token\n                return\n\n        # All attempts failed\n        yield \"[All generation attempts failed]\"\n\n    def _validate_token(self, token: str) -&gt; bool:\n        \"\"\"Validate a token.\"\"\"\n        # Basic validation\n        if not isinstance(token, str):\n            return False\n\n        # Custom validators\n        for validator in self.token_validators:\n            if not validator(token):\n                return False\n\n        return True\n\n    def _is_valid_generation(self, tokens: List[str]) -&gt; bool:\n        \"\"\"Check if generation is valid.\"\"\"\n        if not tokens:\n            return False\n\n        content = \"\".join(tokens)\n\n        # Check for error markers\n        if any(marker in content for marker in [\"[Error\", \"[Timeout\", \"[Stalled\", \"[No content\"]):\n            return False\n\n        # Check minimum length\n        if len(content.strip()) &lt; 10:\n            return False\n\n        return True\n\n    def _handle_error(self, error: str, prompt: str, tokens_generated: int):\n        \"\"\"Handle streaming error.\"\"\"\n        error_info = {\n            \"error\": error,\n            \"prompt\": prompt,\n            \"tokens_generated\": tokens_generated,\n            \"timestamp\": time.time()\n        }\n\n        logger.error(f\"Streaming error: {error_info}\")\n\n        for handler in self.error_handlers:\n            try:\n                handler(error_info)\n            except Exception as e:\n                logger.error(f\"Error in error handler: {e}\")\n\n# Usage example\nstreamer = RobustStreamer(timeout=20.0, max_tokens=300)\n\n# Add custom token validator\nstreamer.add_token_validator(lambda token: len(token) &lt; 100)\n\n# Add error handler\nstreamer.on_error(lambda info: print(f\"Error handled: {info['error']}\"))\n\n# Stream with error handling\nprint(\"Streaming with error handling:\")\nfor token in streamer.generate_stream(\"Write a story\", seed=42):\n    print(token, end=\"\", flush=True)\nprint()\n\n# Async generation\nprint(\"\\nAsync generation:\")\nthread = streamer.generate_async(\n    \"Explain quantum computing\",\n    seed=123,\n    callback=lambda token: print(token, end=\"\", flush=True)\n)\n\n# Wait for completion\nthread.join()\ntry:\n    status, result = thread.result_queue.get(timeout=1)\n    print(f\"\\nAsync result: {status}\")\nexcept Empty:\n    print(\"\\nAsync generation did not complete\")\n\n# Generation with fallbacks\nprint(\"\\nGeneration with fallbacks:\")\nfor token in streamer.generate_with_fallback(\"Complex prompt that might fail\", seed=456):\n    print(token, end=\"\", flush=True)\nprint()\n</code></pre>"},{"location":"examples/error-handling/#daemon-error-handling","title":"Daemon Error Handling","text":""},{"location":"examples/error-handling/#basic-daemon-error-handling","title":"Basic Daemon Error Handling","text":"<pre><code>import steadytext\nfrom steadytext.daemon import use_daemon\nfrom steadytext.daemon.client import is_daemon_running\nimport subprocess\nimport time\n\ndef ensure_daemon_running(max_retries: int = 3) -&gt; bool:\n    \"\"\"Ensure daemon is running with retries.\"\"\"\n    for attempt in range(max_retries):\n        if is_daemon_running():\n            return True\n\n        logger.info(f\"Daemon not running, attempting to start (attempt {attempt + 1}/{max_retries})\")\n\n        try:\n            # Start daemon\n            result = subprocess.run(\n                [\"st\", \"daemon\", \"start\"],\n                capture_output=True,\n                text=True,\n                timeout=10\n            )\n\n            if result.returncode == 0:\n                # Wait for daemon to be ready\n                time.sleep(2)\n                if is_daemon_running():\n                    logger.info(\"Daemon started successfully\")\n                    return True\n            else:\n                logger.error(f\"Failed to start daemon: {result.stderr}\")\n\n        except subprocess.TimeoutExpired:\n            logger.error(\"Daemon start timed out\")\n        except Exception as e:\n            logger.error(f\"Error starting daemon: {e}\")\n\n        time.sleep(1)\n\n    return False\n\ndef generate_with_daemon_fallback(prompt: str, seed: int = 42) -&gt; Optional[str]:\n    \"\"\"Generate with automatic daemon fallback.\"\"\"\n    try:\n        # Try with daemon first\n        with use_daemon():\n            return steadytext.generate(prompt, seed=seed)\n    except Exception as e:\n        logger.warning(f\"Daemon generation failed: {e}, falling back to direct mode\")\n\n        # Fall back to direct generation\n        try:\n            return steadytext.generate(prompt, seed=seed)\n        except Exception as e2:\n            logger.error(f\"Direct generation also failed: {e2}\")\n            return None\n\n# Usage\nif ensure_daemon_running():\n    result = generate_with_daemon_fallback(\"Hello world\", seed=42)\n    if result:\n        print(result)\n    else:\n        print(\"Generation failed\")\nelse:\n    print(\"Could not start daemon, using direct mode\")\n    result = steadytext.generate(\"Hello world\", seed=42)\n</code></pre>"},{"location":"examples/error-handling/#advanced-daemon-error-handling","title":"Advanced Daemon Error Handling","text":"<pre><code>import steadytext\nfrom steadytext.daemon import use_daemon\nfrom steadytext.daemon.client import DaemonClient, is_daemon_running\nimport time\nimport threading\nfrom typing import Optional, Dict, Any, Callable\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\n@dataclass\nclass DaemonHealth:\n    \"\"\"Daemon health status.\"\"\"\n    is_running: bool\n    response_time: Optional[float]\n    last_check: datetime\n    consecutive_failures: int\n    error_message: Optional[str] = None\n\nclass ResilientDaemonClient:\n    \"\"\"Resilient daemon client with comprehensive error handling.\"\"\"\n\n    def __init__(self,\n                 health_check_interval: int = 60,\n                 max_consecutive_failures: int = 5,\n                 auto_restart: bool = True):\n        self.health_check_interval = health_check_interval\n        self.max_consecutive_failures = max_consecutive_failures\n        self.auto_restart = auto_restart\n        self.health = DaemonHealth(\n            is_running=False,\n            response_time=None,\n            last_check=datetime.now(),\n            consecutive_failures=0\n        )\n        self._health_check_thread = None\n        self._stop_health_check = threading.Event()\n        self._fallback_mode = False\n        self._callbacks = {\n            \"daemon_down\": [],\n            \"daemon_up\": [],\n            \"daemon_slow\": [],\n            \"fallback_activated\": []\n        }\n\n    def on(self, event: str, callback: Callable):\n        \"\"\"Register event callback.\"\"\"\n        if event in self._callbacks:\n            self._callbacks[event].append(callback)\n\n    def start_monitoring(self):\n        \"\"\"Start health monitoring thread.\"\"\"\n        if self._health_check_thread is None or not self._health_check_thread.is_alive():\n            self._stop_health_check.clear()\n            self._health_check_thread = threading.Thread(\n                target=self._health_monitor_loop,\n                daemon=True\n            )\n            self._health_check_thread.start()\n\n    def stop_monitoring(self):\n        \"\"\"Stop health monitoring.\"\"\"\n        self._stop_health_check.set()\n        if self._health_check_thread:\n            self._health_check_thread.join(timeout=5)\n\n    def generate(self, prompt: str, seed: int = 42, **kwargs) -&gt; Optional[str]:\n        \"\"\"Generate with comprehensive error handling.\"\"\"\n        # Check if we should use fallback mode\n        if self._fallback_mode or not self.health.is_running:\n            return self._fallback_generate(prompt, seed, **kwargs)\n\n        try:\n            # Attempt daemon generation\n            start_time = time.time()\n\n            with use_daemon():\n                result = steadytext.generate(prompt, seed=seed, **kwargs)\n\n            # Update response time\n            response_time = time.time() - start_time\n            self._update_health(True, response_time)\n\n            # Check for slow responses\n            if response_time &gt; 5.0:\n                self._trigger_event(\"daemon_slow\", {\"response_time\": response_time})\n\n            return result\n\n        except Exception as e:\n            logger.error(f\"Daemon generation error: {e}\")\n            self._update_health(False, error=str(e))\n\n            # Check if we should switch to fallback mode\n            if self.health.consecutive_failures &gt;= self.max_consecutive_failures:\n                self._activate_fallback_mode()\n\n            # Try direct generation\n            return self._fallback_generate(prompt, seed, **kwargs)\n\n    def embed(self, text: str, seed: int = 42) -&gt; Optional[np.ndarray]:\n        \"\"\"Embed with comprehensive error handling.\"\"\"\n        # Similar implementation to generate\n        pass\n\n    def _health_monitor_loop(self):\n        \"\"\"Background health monitoring loop.\"\"\"\n        while not self._stop_health_check.is_set():\n            try:\n                self._perform_health_check()\n            except Exception as e:\n                logger.error(f\"Health check error: {e}\")\n\n            # Wait for next check\n            self._stop_health_check.wait(self.health_check_interval)\n\n    def _perform_health_check(self):\n        \"\"\"Perform daemon health check.\"\"\"\n        try:\n            start_time = time.time()\n\n            # Check if daemon is running\n            if not is_daemon_running():\n                self._update_health(False, error=\"Daemon not running\")\n\n                if self.auto_restart and self.health.consecutive_failures &gt; 0:\n                    self._attempt_restart()\n                return\n\n            # Test daemon responsiveness\n            client = DaemonClient()\n            response = client._send_request({\"type\": \"ping\"})\n\n            if response and response.get(\"success\"):\n                response_time = time.time() - start_time\n                self._update_health(True, response_time)\n\n                # Check if we can deactivate fallback mode\n                if self._fallback_mode and self.health.consecutive_failures == 0:\n                    self._deactivate_fallback_mode()\n            else:\n                self._update_health(False, error=\"Ping failed\")\n\n        except Exception as e:\n            self._update_health(False, error=str(e))\n\n    def _update_health(self, success: bool, response_time: Optional[float] = None,\n                      error: Optional[str] = None):\n        \"\"\"Update health status.\"\"\"\n        self.health.last_check = datetime.now()\n\n        if success:\n            self.health.is_running = True\n            self.health.response_time = response_time\n            self.health.error_message = None\n\n            if self.health.consecutive_failures &gt; 0:\n                # Daemon recovered\n                self.health.consecutive_failures = 0\n                self._trigger_event(\"daemon_up\", {\"response_time\": response_time})\n        else:\n            self.health.consecutive_failures += 1\n            self.health.error_message = error\n\n            if self.health.consecutive_failures == 1:\n                # Daemon just went down\n                self._trigger_event(\"daemon_down\", {\"error\": error})\n\n            if self.health.consecutive_failures &gt;= self.max_consecutive_failures:\n                self.health.is_running = False\n\n    def _activate_fallback_mode(self):\n        \"\"\"Activate fallback mode.\"\"\"\n        if not self._fallback_mode:\n            self._fallback_mode = True\n            logger.warning(\"Activating fallback mode due to daemon failures\")\n            self._trigger_event(\"fallback_activated\", {\n                \"consecutive_failures\": self.health.consecutive_failures\n            })\n\n    def _deactivate_fallback_mode(self):\n        \"\"\"Deactivate fallback mode.\"\"\"\n        if self._fallback_mode:\n            self._fallback_mode = False\n            logger.info(\"Deactivating fallback mode - daemon recovered\")\n\n    def _fallback_generate(self, prompt: str, seed: int = 42, **kwargs) -&gt; Optional[str]:\n        \"\"\"Fallback generation without daemon.\"\"\"\n        try:\n            logger.info(\"Using direct generation (fallback mode)\")\n            return steadytext.generate(prompt, seed=seed, **kwargs)\n        except Exception as e:\n            logger.error(f\"Fallback generation error: {e}\")\n            return None\n\n    def _attempt_restart(self):\n        \"\"\"Attempt to restart daemon.\"\"\"\n        logger.info(\"Attempting to restart daemon\")\n\n        try:\n            # Stop daemon if running\n            subprocess.run([\"st\", \"daemon\", \"stop\"], timeout=5)\n            time.sleep(1)\n\n            # Start daemon\n            result = subprocess.run(\n                [\"st\", \"daemon\", \"start\"],\n                capture_output=True,\n                text=True,\n                timeout=10\n            )\n\n            if result.returncode == 0:\n                logger.info(\"Daemon restart successful\")\n                time.sleep(2)  # Wait for startup\n            else:\n                logger.error(f\"Daemon restart failed: {result.stderr}\")\n\n        except Exception as e:\n            logger.error(f\"Error restarting daemon: {e}\")\n\n    def _trigger_event(self, event: str, data: Dict[str, Any]):\n        \"\"\"Trigger event callbacks.\"\"\"\n        for callback in self._callbacks.get(event, []):\n            try:\n                callback(data)\n            except Exception as e:\n                logger.error(f\"Error in {event} callback: {e}\")\n\n    def get_status(self) -&gt; Dict[str, Any]:\n        \"\"\"Get current status.\"\"\"\n        return {\n            \"health\": {\n                \"is_running\": self.health.is_running,\n                \"response_time\": self.health.response_time,\n                \"last_check\": self.health.last_check.isoformat(),\n                \"consecutive_failures\": self.health.consecutive_failures,\n                \"error_message\": self.health.error_message\n            },\n            \"fallback_mode\": self._fallback_mode,\n            \"monitoring\": self._health_check_thread.is_alive() if self._health_check_thread else False\n        }\n\n# Usage example\nclient = ResilientDaemonClient(auto_restart=True)\n\n# Register event handlers\nclient.on(\"daemon_down\", lambda data: print(f\"Daemon down: {data}\"))\nclient.on(\"daemon_up\", lambda data: print(f\"Daemon recovered: {data}\"))\nclient.on(\"daemon_slow\", lambda data: print(f\"Slow response: {data}\"))\nclient.on(\"fallback_activated\", lambda data: print(f\"Fallback mode: {data}\"))\n\n# Start monitoring\nclient.start_monitoring()\n\n# Use with automatic error handling\nresult = client.generate(\"Write a poem\", seed=42)\nif result:\n    print(result)\nelse:\n    print(\"Generation failed\")\n\n# Check status\nstatus = client.get_status()\nprint(f\"Client status: {status}\")\n\n# Stop monitoring when done\nclient.stop_monitoring()\n</code></pre>"},{"location":"examples/error-handling/#cli-error-handling","title":"CLI Error Handling","text":""},{"location":"examples/error-handling/#shell-script-error-handling","title":"Shell Script Error Handling","text":"<pre><code>#!/bin/bash\n# robust_cli.sh - Robust CLI usage with error handling\n\nset -euo pipefail  # Exit on error, undefined variable, pipe failure\n\n# Error handling function\nhandle_error() {\n    local exit_code=$?\n    local line_number=$1\n    echo \"Error on line $line_number: Command exited with status $exit_code\" &gt;&amp;2\n\n    # Log error\n    echo \"[$(date)] Error on line $line_number, exit code $exit_code\" &gt;&gt; steadytext_errors.log\n\n    # Cleanup if needed\n    cleanup\n\n    exit $exit_code\n}\n\n# Set error trap\ntrap 'handle_error ${LINENO}' ERR\n\n# Cleanup function\ncleanup() {\n    # Remove temporary files\n    rm -f /tmp/steadytext_temp_*\n}\n\n# Function to safely generate text\nsafe_generate() {\n    local prompt=\"$1\"\n    local seed=\"${2:-42}\"\n    local max_retries=3\n    local retry_count=0\n\n    while [ $retry_count -lt $max_retries ]; do\n        if result=$(st generate \"$prompt\" --seed \"$seed\" --json 2&gt;/dev/null); then\n            # Extract text from JSON\n            if text=$(echo \"$result\" | jq -r '.text' 2&gt;/dev/null); then\n                echo \"$text\"\n                return 0\n            else\n                echo \"Error: Invalid JSON response\" &gt;&amp;2\n            fi\n        else\n            echo \"Error: Generation failed (attempt $((retry_count + 1))/$max_retries)\" &gt;&amp;2\n        fi\n\n        retry_count=$((retry_count + 1))\n        sleep 1\n    done\n\n    return 1\n}\n\n# Function to check daemon status\ncheck_daemon() {\n    if st daemon status &gt;/dev/null 2&gt;&amp;1; then\n        echo \"Daemon is running\"\n        return 0\n    else\n        echo \"Daemon is not running\"\n        return 1\n    fi\n}\n\n# Function to ensure daemon is running\nensure_daemon() {\n    if ! check_daemon; then\n        echo \"Starting daemon...\"\n        if st daemon start; then\n            sleep 2\n            if check_daemon; then\n                echo \"Daemon started successfully\"\n                return 0\n            fi\n        fi\n        echo \"Failed to start daemon\" &gt;&amp;2\n        return 1\n    fi\n    return 0\n}\n\n# Main script\nmain() {\n    echo \"SteadyText Robust CLI Example\"\n    echo \"=============================\"\n\n    # Ensure daemon is running (optional)\n    if ensure_daemon; then\n        echo \"Using daemon mode\"\n    else\n        echo \"Using direct mode\"\n    fi\n\n    # Generate with error handling\n    echo -e \"\\nGenerating text...\"\n    if text=$(safe_generate \"Write a haiku about error handling\" 123); then\n        echo \"Generated text:\"\n        echo \"$text\"\n    else\n        echo \"Failed to generate text\"\n        exit 1\n    fi\n\n    # Batch processing with error handling\n    echo -e \"\\nBatch processing...\"\n    prompts=(\"Task 1\" \"Task 2\" \"Task 3\")\n\n    for i in \"${!prompts[@]}\"; do\n        prompt=\"${prompts[$i]}\"\n        echo -n \"Processing '$prompt': \"\n\n        if result=$(safe_generate \"$prompt\" $((100 + i))); then\n            echo \"Success\"\n            echo \"$result\" &gt; \"output_$i.txt\"\n        else\n            echo \"Failed\"\n            # Continue with next prompt instead of exiting\n        fi\n    done\n\n    echo -e \"\\nCompleted successfully\"\n}\n\n# Run main function\nmain \"$@\"\n</code></pre>"},{"location":"examples/error-handling/#python-cli-wrapper","title":"Python CLI Wrapper","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nrobust_cli_wrapper.py - Robust wrapper for SteadyText CLI\n\"\"\"\n\nimport subprocess\nimport json\nimport time\nimport sys\nimport logging\nfrom typing import Optional, Dict, Any, List\nfrom pathlib import Path\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass SteadyTextCLI:\n    \"\"\"Robust wrapper for SteadyText CLI with error handling.\"\"\"\n\n    def __init__(self, \n                 timeout: int = 30,\n                 max_retries: int = 3,\n                 retry_delay: float = 1.0):\n        self.timeout = timeout\n        self.max_retries = max_retries\n        self.retry_delay = retry_delay\n        self.daemon_checked = False\n        self.daemon_available = False\n\n    def _run_command(self, cmd: List[str], input_text: Optional[str] = None) -&gt; subprocess.CompletedProcess:\n        \"\"\"Run CLI command with timeout and error handling.\"\"\"\n        try:\n            result = subprocess.run(\n                cmd,\n                input=input_text,\n                capture_output=True,\n                text=True,\n                timeout=self.timeout\n            )\n            return result\n\n        except subprocess.TimeoutExpired as e:\n            logger.error(f\"Command timed out: {' '.join(cmd)}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Command error: {e}\")\n            raise\n\n    def check_daemon(self) -&gt; bool:\n        \"\"\"Check if daemon is running.\"\"\"\n        if not self.daemon_checked:\n            try:\n                result = self._run_command([\"st\", \"daemon\", \"status\", \"--json\"])\n                if result.returncode == 0:\n                    status = json.loads(result.stdout)\n                    self.daemon_available = status.get(\"running\", False)\n            except:\n                self.daemon_available = False\n\n            self.daemon_checked = True\n            logger.info(f\"Daemon available: {self.daemon_available}\")\n\n        return self.daemon_available\n\n    def generate(self, prompt: str, seed: int = 42, **kwargs) -&gt; Optional[str]:\n        \"\"\"Generate text with error handling and retries.\"\"\"\n        cmd = [\"st\", \"generate\", prompt, \"--seed\", str(seed), \"--json\", \"--wait\"]\n\n        # Add additional options\n        if \"max_new_tokens\" in kwargs:\n            cmd.extend([\"--max-new-tokens\", str(kwargs[\"max_new_tokens\"])])\n\n        for attempt in range(self.max_retries):\n            try:\n                result = self._run_command(cmd)\n\n                if result.returncode == 0:\n                    # Parse JSON response\n                    try:\n                        data = json.loads(result.stdout)\n                        return data.get(\"text\")\n                    except json.JSONDecodeError:\n                        logger.error(f\"Invalid JSON response: {result.stdout}\")\n                else:\n                    logger.error(f\"Generation failed: {result.stderr}\")\n\n            except subprocess.TimeoutExpired:\n                logger.error(f\"Generation timed out (attempt {attempt + 1}/{self.max_retries})\")\n            except Exception as e:\n                logger.error(f\"Generation error: {e}\")\n\n            if attempt &lt; self.max_retries - 1:\n                time.sleep(self.retry_delay * (attempt + 1))\n\n        return None\n\n    def generate_stream(self, prompt: str, seed: int = 42, callback=None) -&gt; bool:\n        \"\"\"Stream generation with error handling.\"\"\"\n        cmd = [\"st\", \"generate\", prompt, \"--seed\", str(seed)]\n\n        try:\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                bufsize=1\n            )\n\n            # Read output character by character\n            while True:\n                char = process.stdout.read(1)\n                if not char:\n                    break\n\n                if callback:\n                    callback(char)\n                else:\n                    print(char, end=\"\", flush=True)\n\n            # Wait for process to complete\n            process.wait(timeout=self.timeout)\n\n            return process.returncode == 0\n\n        except subprocess.TimeoutExpired:\n            logger.error(\"Streaming generation timed out\")\n            process.kill()\n            return False\n        except Exception as e:\n            logger.error(f\"Streaming error: {e}\")\n            return False\n\n    def embed(self, text: str, seed: int = 42) -&gt; Optional[List[float]]:\n        \"\"\"Generate embedding with error handling.\"\"\"\n        cmd = [\"st\", \"embed\", text, \"--seed\", str(seed), \"--format\", \"json\"]\n\n        for attempt in range(self.max_retries):\n            try:\n                result = self._run_command(cmd)\n\n                if result.returncode == 0:\n                    # Parse JSON array\n                    try:\n                        embedding = json.loads(result.stdout)\n                        if isinstance(embedding, list) and len(embedding) == 1024:\n                            return embedding\n                        else:\n                            logger.error(\"Invalid embedding format\")\n                    except json.JSONDecodeError:\n                        logger.error(\"Invalid JSON embedding\")\n                else:\n                    logger.error(f\"Embedding failed: {result.stderr}\")\n\n            except Exception as e:\n                logger.error(f\"Embedding error: {e}\")\n\n            if attempt &lt; self.max_retries - 1:\n                time.sleep(self.retry_delay)\n\n        return None\n\n    def batch_generate(self, prompts: List[str], seeds: Optional[List[int]] = None) -&gt; List[Optional[str]]:\n        \"\"\"Batch generate with parallel processing.\"\"\"\n        import concurrent.futures\n\n        if seeds is None:\n            seeds = [42 + i for i in range(len(prompts))]\n\n        results = []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n            futures = [\n                executor.submit(self.generate, prompt, seed)\n                for prompt, seed in zip(prompts, seeds)\n            ]\n\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    result = future.result()\n                    results.append(result)\n                except Exception as e:\n                    logger.error(f\"Batch generation error: {e}\")\n                    results.append(None)\n\n        return results\n\ndef main():\n    \"\"\"Example usage of robust CLI wrapper.\"\"\"\n    cli = SteadyTextCLI()\n\n    # Check daemon\n    if cli.check_daemon():\n        print(\"\u2713 Daemon is running\")\n    else:\n        print(\"\u2717 Daemon not available, using direct mode\")\n\n    # Single generation\n    print(\"\\nGenerating text...\")\n    text = cli.generate(\"Write a short poem\", seed=123)\n    if text:\n        print(f\"Generated: {text}\")\n    else:\n        print(\"Generation failed\")\n\n    # Streaming generation\n    print(\"\\nStreaming generation...\")\n    success = cli.generate_stream(\"Tell me a story\", seed=456)\n    print(f\"\\nStreaming {'succeeded' if success else 'failed'}\")\n\n    # Batch generation\n    print(\"\\nBatch generation...\")\n    prompts = [\"Task 1\", \"Task 2\", \"Task 3\"]\n    results = cli.batch_generate(prompts)\n    for i, (prompt, result) in enumerate(zip(prompts, results)):\n        status = \"\u2713\" if result else \"\u2717\"\n        print(f\"{status} {prompt}: {result[:50] if result else 'Failed'}...\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/error-handling/#production-patterns","title":"Production Patterns","text":""},{"location":"examples/error-handling/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>from enum import Enum\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Callable, Any\nimport threading\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"\n    OPEN = \"open\"\n    HALF_OPEN = \"half_open\"\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker for SteadyText operations.\"\"\"\n\n    def __init__(self,\n                 failure_threshold: int = 5,\n                 recovery_timeout: int = 60,\n                 expected_exception: type = Exception):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.expected_exception = expected_exception\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = CircuitState.CLOSED\n        self._lock = threading.Lock()\n\n    def call(self, func: Callable, *args, **kwargs) -&gt; Any:\n        \"\"\"Execute function with circuit breaker protection.\"\"\"\n        with self._lock:\n            if self.state == CircuitState.OPEN:\n                if self._should_attempt_reset():\n                    self.state = CircuitState.HALF_OPEN\n                else:\n                    raise Exception(\"Circuit breaker is OPEN\")\n\n        try:\n            result = func(*args, **kwargs)\n            self._on_success()\n            return result\n        except self.expected_exception as e:\n            self._on_failure()\n            raise\n\n    def _should_attempt_reset(self) -&gt; bool:\n        \"\"\"Check if we should attempt to reset circuit.\"\"\"\n        return (self.last_failure_time and\n                datetime.now() - self.last_failure_time &gt; timedelta(seconds=self.recovery_timeout))\n\n    def _on_success(self):\n        \"\"\"Handle successful call.\"\"\"\n        with self._lock:\n            self.failure_count = 0\n            if self.state == CircuitState.HALF_OPEN:\n                self.state = CircuitState.CLOSED\n\n    def _on_failure(self):\n        \"\"\"Handle failed call.\"\"\"\n        with self._lock:\n            self.failure_count += 1\n            self.last_failure_time = datetime.now()\n\n            if self.failure_count &gt;= self.failure_threshold:\n                self.state = CircuitState.OPEN\n            elif self.state == CircuitState.HALF_OPEN:\n                self.state = CircuitState.OPEN\n\n    def get_state(self) -&gt; Dict[str, Any]:\n        \"\"\"Get circuit breaker state.\"\"\"\n        with self._lock:\n            return {\n                \"state\": self.state.value,\n                \"failure_count\": self.failure_count,\n                \"last_failure\": self.last_failure_time.isoformat() if self.last_failure_time else None\n            }\n\n# Usage with SteadyText\ncircuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n\ndef protected_generate(prompt: str, seed: int = 42) -&gt; Optional[str]:\n    \"\"\"Generate with circuit breaker protection.\"\"\"\n    try:\n        return circuit_breaker.call(steadytext.generate, prompt, seed=seed)\n    except Exception as e:\n        logger.error(f\"Circuit breaker triggered: {e}\")\n        return None\n</code></pre>"},{"location":"examples/error-handling/#retry-with-exponential-backoff","title":"Retry with Exponential Backoff","text":"<pre><code>import time\nimport random\nfrom typing import TypeVar, Callable, Optional, Any\n\nT = TypeVar('T')\n\ndef retry_with_backoff(\n    func: Callable[..., T],\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True\n) -&gt; Callable[..., Optional[T]]:\n    \"\"\"Decorator for retry with exponential backoff.\"\"\"\n\n    def wrapper(*args, **kwargs) -&gt; Optional[T]:\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                last_exception = e\n\n                if attempt == max_retries - 1:\n                    logger.error(f\"All {max_retries} attempts failed: {e}\")\n                    break\n\n                # Calculate delay with exponential backoff\n                delay = min(base_delay * (exponential_base ** attempt), max_delay)\n\n                # Add jitter\n                if jitter:\n                    delay = delay * (0.5 + random.random())\n\n                logger.warning(f\"Attempt {attempt + 1} failed, retrying in {delay:.2f}s: {e}\")\n                time.sleep(delay)\n\n        return None\n\n    return wrapper\n\n# Apply to SteadyText functions\n@retry_with_backoff\ndef robust_generate(prompt: str, seed: int = 42) -&gt; Optional[str]:\n    \"\"\"Generate with automatic retry.\"\"\"\n    result = steadytext.generate(prompt, seed=seed)\n    if result is None:\n        raise Exception(\"Generation returned None\")\n    return result\n\n@retry_with_backoff\ndef robust_embed(text: str, seed: int = 42) -&gt; Optional[np.ndarray]:\n    \"\"\"Embed with automatic retry.\"\"\"\n    result = steadytext.embed(text, seed=seed)\n    if result is None:\n        raise Exception(\"Embedding returned None\")\n    return result\n</code></pre>"},{"location":"examples/error-handling/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"examples/error-handling/#error-monitoring-system","title":"Error Monitoring System","text":"<pre><code>import time\nimport json\nfrom datetime import datetime, timedelta\nfrom collections import deque, defaultdict\nfrom typing import Dict, List, Any, Optional\nimport smtplib\nfrom email.mime.text import MIMEText\n\nclass ErrorMonitor:\n    \"\"\"Comprehensive error monitoring for SteadyText.\"\"\"\n\n    def __init__(self,\n                 window_size: int = 1000,\n                 alert_threshold: int = 10,\n                 alert_window: int = 300):  # 5 minutes\n        self.window_size = window_size\n        self.alert_threshold = alert_threshold\n        self.alert_window = alert_window\n        self.errors = deque(maxlen=window_size)\n        self.error_counts = defaultdict(int)\n        self.alert_sent = {}\n        self.metrics = {\n            \"total_errors\": 0,\n            \"errors_by_type\": defaultdict(int),\n            \"errors_by_hour\": defaultdict(int),\n            \"recent_error_rate\": 0.0\n        }\n\n    def record_error(self, error_type: str, error_message: str,\n                    context: Optional[Dict[str, Any]] = None):\n        \"\"\"Record an error occurrence.\"\"\"\n        error = {\n            \"timestamp\": datetime.now(),\n            \"type\": error_type,\n            \"message\": error_message,\n            \"context\": context or {}\n        }\n\n        self.errors.append(error)\n        self.error_counts[error_type] += 1\n        self.metrics[\"total_errors\"] += 1\n        self.metrics[\"errors_by_type\"][error_type] += 1\n\n        # Update hourly metrics\n        hour = datetime.now().strftime(\"%Y-%m-%d %H:00\")\n        self.metrics[\"errors_by_hour\"][hour] += 1\n\n        # Check if alert needed\n        self._check_alert_condition(error_type)\n\n    def _check_alert_condition(self, error_type: str):\n        \"\"\"Check if we should send an alert.\"\"\"\n        # Count recent errors of this type\n        cutoff_time = datetime.now() - timedelta(seconds=self.alert_window)\n        recent_errors = sum(\n            1 for error in self.errors\n            if error[\"type\"] == error_type and error[\"timestamp\"] &gt; cutoff_time\n        )\n\n        # Check threshold\n        if recent_errors &gt;= self.alert_threshold:\n            last_alert = self.alert_sent.get(error_type)\n            if not last_alert or datetime.now() - last_alert &gt; timedelta(seconds=self.alert_window):\n                self._send_alert(error_type, recent_errors)\n                self.alert_sent[error_type] = datetime.now()\n\n    def _send_alert(self, error_type: str, error_count: int):\n        \"\"\"Send alert notification.\"\"\"\n        message = f\"\"\"\n        SteadyText Error Alert\n\n        Error Type: {error_type}\n        Count: {error_count} errors in last {self.alert_window} seconds\n        Threshold: {self.alert_threshold}\n        Time: {datetime.now().isoformat()}\n\n        Recent errors:\n        \"\"\"\n\n        # Add recent errors\n        recent = [e for e in self.errors if e[\"type\"] == error_type][-5:]\n        for error in recent:\n            message += f\"\\n- {error['timestamp']}: {error['message']}\"\n\n        logger.critical(f\"ALERT: {message}\")\n\n        # Implement your alert mechanism here\n        # e.g., send email, Slack, PagerDuty, etc.\n\n    def get_error_rate(self, window_seconds: int = 60) -&gt; float:\n        \"\"\"Calculate error rate in errors per second.\"\"\"\n        cutoff_time = datetime.now() - timedelta(seconds=window_seconds)\n        recent_errors = sum(\n            1 for error in self.errors\n            if error[\"timestamp\"] &gt; cutoff_time\n        )\n        return recent_errors / window_seconds\n\n    def get_report(self) -&gt; Dict[str, Any]:\n        \"\"\"Generate error report.\"\"\"\n        return {\n            \"summary\": {\n                \"total_errors\": self.metrics[\"total_errors\"],\n                \"unique_error_types\": len(self.error_counts),\n                \"error_rate_per_minute\": self.get_error_rate(60) * 60,\n                \"most_common_error\": max(self.error_counts.items(), key=lambda x: x[1]) if self.error_counts else None\n            },\n            \"errors_by_type\": dict(self.metrics[\"errors_by_type\"]),\n            \"recent_errors\": [\n                {\n                    \"timestamp\": e[\"timestamp\"].isoformat(),\n                    \"type\": e[\"type\"],\n                    \"message\": e[\"message\"]\n                }\n                for e in list(self.errors)[-10:]\n            ],\n            \"alerts_sent\": {\n                error_type: timestamp.isoformat()\n                for error_type, timestamp in self.alert_sent.items()\n            }\n        }\n\n    def export_metrics(self, filepath: str):\n        \"\"\"Export metrics to file.\"\"\"\n        with open(filepath, 'w') as f:\n            json.dump(self.get_report(), f, indent=2)\n\n# Global error monitor\nerror_monitor = ErrorMonitor(alert_threshold=5, alert_window=300)\n\n# Integration with SteadyText operations\ndef monitored_generate(prompt: str, seed: int = 42) -&gt; Optional[str]:\n    \"\"\"Generate with error monitoring.\"\"\"\n    try:\n        result = steadytext.generate(prompt, seed=seed)\n\n        if result is None:\n            error_monitor.record_error(\n                \"generation_failed\",\n                \"Generation returned None\",\n                {\"prompt\": prompt[:50], \"seed\": seed}\n            )\n\n        return result\n\n    except Exception as e:\n        error_monitor.record_error(\n            \"generation_exception\",\n            str(e),\n            {\"prompt\": prompt[:50], \"seed\": seed}\n        )\n        return None\n</code></pre>"},{"location":"examples/error-handling/#recovery-strategies","title":"Recovery Strategies","text":""},{"location":"examples/error-handling/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>class GracefulDegradationManager:\n    \"\"\"Manage graceful degradation strategies.\"\"\"\n\n    def __init__(self):\n        self.degradation_levels = {\n            0: \"full_service\",\n            1: \"reduced_quality\",\n            2: \"cached_only\",\n            3: \"static_responses\",\n            4: \"maintenance_mode\"\n        }\n        self.current_level = 0\n        self.level_thresholds = {\n            \"error_rate\": [0.1, 0.3, 0.5, 0.7, 0.9],\n            \"response_time\": [2.0, 5.0, 10.0, 20.0, 30.0]\n        }\n\n    def evaluate_service_health(self, metrics: Dict[str, float]) -&gt; int:\n        \"\"\"Evaluate service health and determine degradation level.\"\"\"\n        error_rate = metrics.get(\"error_rate\", 0.0)\n        response_time = metrics.get(\"response_time\", 0.0)\n\n        # Determine level based on metrics\n        level = 0\n        for i, (error_threshold, time_threshold) in enumerate(\n            zip(self.level_thresholds[\"error_rate\"], \n                self.level_thresholds[\"response_time\"])\n        ):\n            if error_rate &gt; error_threshold or response_time &gt; time_threshold:\n                level = i + 1\n\n        return min(level, 4)\n\n    def apply_degradation_strategy(self, level: int, operation: str, **kwargs) -&gt; Any:\n        \"\"\"Apply appropriate degradation strategy.\"\"\"\n        self.current_level = level\n        strategy = self.degradation_levels[level]\n\n        logger.info(f\"Applying degradation strategy: {strategy}\")\n\n        if strategy == \"full_service\":\n            return self._full_service(operation, **kwargs)\n        elif strategy == \"reduced_quality\":\n            return self._reduced_quality(operation, **kwargs)\n        elif strategy == \"cached_only\":\n            return self._cached_only(operation, **kwargs)\n        elif strategy == \"static_responses\":\n            return self._static_responses(operation, **kwargs)\n        else:  # maintenance_mode\n            return self._maintenance_mode(operation, **kwargs)\n\n    def _full_service(self, operation: str, **kwargs):\n        \"\"\"Normal operation.\"\"\"\n        if operation == \"generate\":\n            return steadytext.generate(**kwargs)\n        elif operation == \"embed\":\n            return steadytext.embed(**kwargs)\n\n    def _reduced_quality(self, operation: str, **kwargs):\n        \"\"\"Reduced quality but faster.\"\"\"\n        if operation == \"generate\":\n            # Reduce token count\n            kwargs[\"max_new_tokens\"] = min(kwargs.get(\"max_new_tokens\", 512), 100)\n            return steadytext.generate(**kwargs)\n\n    def _cached_only(self, operation: str, **kwargs):\n        \"\"\"Return only cached responses.\"\"\"\n        # Check cache directly\n        cache_manager = get_cache_manager()\n        # Implement cache-only logic\n        return None\n\n    def _static_responses(self, operation: str, **kwargs):\n        \"\"\"Return static pre-defined responses.\"\"\"\n        static_responses = {\n            \"generate\": \"Service is currently limited. Please try again later.\",\n            \"embed\": np.zeros(1024, dtype=np.float32)\n        }\n        return static_responses.get(operation)\n\n    def _maintenance_mode(self, operation: str, **kwargs):\n        \"\"\"System in maintenance mode.\"\"\"\n        return None\n</code></pre>"},{"location":"examples/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"examples/error-handling/#1-comprehensive-error-handler","title":"1. Comprehensive Error Handler","text":"<pre><code>class SteadyTextErrorHandler:\n    \"\"\"Comprehensive error handler for all SteadyText operations.\"\"\"\n\n    def __init__(self):\n        self.handlers = {\n            \"generation\": self._handle_generation_error,\n            \"embedding\": self._handle_embedding_error,\n            \"streaming\": self._handle_streaming_error,\n            \"daemon\": self._handle_daemon_error\n        }\n        self.fallback_strategies = {\n            \"generation\": self._generation_fallback,\n            \"embedding\": self._embedding_fallback\n        }\n\n    def handle(self, operation: str, error: Any, context: Dict[str, Any]) -&gt; Any:\n        \"\"\"Central error handling.\"\"\"\n        handler = self.handlers.get(operation, self._default_handler)\n        return handler(error, context)\n\n    def _handle_generation_error(self, error: Any, context: Dict[str, Any]):\n        \"\"\"Handle generation errors.\"\"\"\n        logger.error(f\"Generation error: {error}\", extra=context)\n\n        # Try fallback\n        fallback = self.fallback_strategies[\"generation\"]\n        return fallback(context)\n\n    def _generation_fallback(self, context: Dict[str, Any]) -&gt; str:\n        \"\"\"Generation fallback strategy.\"\"\"\n        prompt = context.get(\"prompt\", \"\")\n        seed = context.get(\"seed\", 42)\n\n        # Try different approaches\n        approaches = [\n            lambda: f\"[Unable to generate response for: {prompt[:50]}...]\",\n            lambda: \"[Service temporarily unavailable]\",\n            lambda: \"\"\n        ]\n\n        for approach in approaches:\n            try:\n                return approach()\n            except:\n                continue\n\n        return \"[Critical error]\"\n</code></pre>"},{"location":"examples/error-handling/#2-error-context-manager","title":"2. Error Context Manager","text":"<pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef error_handling(operation: str, **context):\n    \"\"\"Context manager for consistent error handling.\"\"\"\n    start_time = time.time()\n    try:\n        yield\n    except Exception as e:\n        duration = time.time() - start_time\n\n        # Log error with context\n        logger.error(f\"{operation} failed after {duration:.2f}s\", extra={\n            \"operation\": operation,\n            \"error\": str(e),\n            \"error_type\": type(e).__name__,\n            \"duration\": duration,\n            **context\n        })\n\n        # Record in monitoring\n        error_monitor.record_error(\n            f\"{operation}_error\",\n            str(e),\n            context\n        )\n\n        # Re-raise or handle based on configuration\n        if should_reraise(e):\n            raise\n        else:\n            return handle_gracefully(operation, e, context)\n\n# Usage\nwith error_handling(\"generation\", prompt=\"test\", seed=42):\n    result = steadytext.generate(\"test\", seed=42)\n</code></pre>"},{"location":"examples/error-handling/#3-testing-error-scenarios","title":"3. Testing Error Scenarios","text":"<pre><code>import unittest\nfrom unittest.mock import patch, MagicMock\n\nclass TestErrorHandling(unittest.TestCase):\n    \"\"\"Test error handling scenarios.\"\"\"\n\n    def test_generation_returns_none(self):\n        \"\"\"Test handling when generation returns None.\"\"\"\n        handler = SteadyTextErrorHandler()\n\n        with patch('steadytext.generate', return_value=None):\n            result = monitored_generate(\"test prompt\", seed=42)\n            self.assertIsNone(result)\n\n            # Check error was recorded\n            report = error_monitor.get_report()\n            self.assertGreater(report[\"summary\"][\"total_errors\"], 0)\n\n    def test_daemon_failure_fallback(self):\n        \"\"\"Test daemon failure with fallback.\"\"\"\n        with patch('steadytext.daemon.client.is_daemon_running', return_value=False):\n            result = generate_with_daemon_fallback(\"test\", seed=42)\n            self.assertIsNotNone(result)  # Should use direct mode\n\n    def test_circuit_breaker_opens(self):\n        \"\"\"Test circuit breaker opening after failures.\"\"\"\n        breaker = CircuitBreaker(failure_threshold=2)\n\n        def failing_func():\n            raise Exception(\"Test failure\")\n\n        # First failures\n        for _ in range(2):\n            with self.assertRaises(Exception):\n                breaker.call(failing_func)\n\n        # Circuit should be open\n        self.assertEqual(breaker.state, CircuitState.OPEN)\n\n        # Further calls should fail immediately\n        with self.assertRaises(Exception) as ctx:\n            breaker.call(failing_func)\n        self.assertIn(\"Circuit breaker is OPEN\", str(ctx.exception))\n</code></pre> <p>This comprehensive guide covers all aspects of error handling in SteadyText, from basic None checks to advanced production patterns like circuit breakers and graceful degradation. The key principle is that SteadyText's \"never fail\" philosophy requires careful handling of None returns and proper fallback strategies.</p>"},{"location":"examples/log-analysis/","title":"Log Analysis with AI-Powered Summarization","text":"<p>Transform your logs from noise into insights using SteadyText's AI capabilities directly in PostgreSQL.</p>"},{"location":"examples/log-analysis/#overview","title":"Overview","text":"<p>This tutorial shows how to build an intelligent log analysis system that: - Automatically summarizes error patterns - Identifies security threats in real-time - Creates hourly/daily AI-powered reports - Integrates seamlessly with TimescaleDB for time-series analysis</p>"},{"location":"examples/log-analysis/#prerequisites","title":"Prerequisites","text":"<pre><code># Install PostgreSQL with SteadyText\ndocker run -d -p 5432:5432 --name steadytext-logs julep/pg-steadytext\n\n# Connect to the database\npsql -h localhost -U postgres\n</code></pre>"},{"location":"examples/log-analysis/#setting-up-the-schema","title":"Setting Up the Schema","text":"<p>First, let's create a table for our application logs:</p> <pre><code>-- Create the logs table\nCREATE TABLE application_logs (\n    id SERIAL PRIMARY KEY,\n    timestamp TIMESTAMPTZ NOT NULL DEFAULT NOW(),\n    level VARCHAR(10) NOT NULL,\n    service VARCHAR(50) NOT NULL,\n    message TEXT NOT NULL,\n    metadata JSONB,\n    request_id UUID,\n    user_id INTEGER,\n    ip_address INET,\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Create indexes for performance\nCREATE INDEX idx_logs_timestamp ON application_logs(timestamp DESC);\nCREATE INDEX idx_logs_level ON application_logs(level);\nCREATE INDEX idx_logs_service ON application_logs(service);\nCREATE INDEX idx_logs_metadata ON application_logs USING GIN(metadata);\n\n-- Enable the SteadyText extension\nCREATE EXTENSION IF NOT EXISTS pg_steadytext CASCADE;\n</code></pre>"},{"location":"examples/log-analysis/#real-time-error-summarization","title":"Real-Time Error Summarization","text":"<p>Create a function that summarizes errors in real-time:</p> <pre><code>-- Function to analyze error patterns\nCREATE OR REPLACE FUNCTION analyze_error_patterns(\n    time_window INTERVAL DEFAULT '1 hour'\n)\nRETURNS TABLE (\n    error_summary TEXT,\n    affected_services TEXT[],\n    error_count INTEGER,\n    severity_score INTEGER,\n    recommended_actions TEXT\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH recent_errors AS (\n        SELECT \n            level,\n            service,\n            message,\n            COUNT(*) as count\n        FROM application_logs\n        WHERE timestamp &gt; NOW() - time_window\n          AND level IN ('ERROR', 'CRITICAL')\n        GROUP BY level, service, message\n    ),\n    aggregated AS (\n        SELECT \n            string_agg(\n                format('%s (%s): %s [%s times]', \n                    level, service, \n                    LEFT(message, 100), count::text\n                ), \n                '; '\n            ) AS error_details,\n            array_agg(DISTINCT service) AS services,\n            SUM(count)::INTEGER AS total_errors\n        FROM recent_errors\n    )\n    SELECT \n        steadytext_generate(\n            'Analyze these application errors and provide a concise summary: ' || \n            error_details\n        ) AS error_summary,\n        services AS affected_services,\n        total_errors AS error_count,\n        CASE \n            WHEN total_errors &gt; 100 THEN 5\n            WHEN total_errors &gt; 50 THEN 4\n            WHEN total_errors &gt; 20 THEN 3\n            WHEN total_errors &gt; 5 THEN 2\n            ELSE 1\n        END AS severity_score,\n        steadytext_generate(\n            'Based on these errors, suggest 3 immediate actions: ' || \n            error_details\n        ) AS recommended_actions\n    FROM aggregated\n    WHERE total_errors &gt; 0;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/log-analysis/#timescaledb-integration-for-historical-analysis","title":"TimescaleDB Integration for Historical Analysis","text":"<p>If you're using TimescaleDB, create continuous aggregates for automatic summarization:</p> <pre><code>-- Convert to hypertable (TimescaleDB)\nSELECT create_hypertable('application_logs', 'timestamp', \n    chunk_time_interval =&gt; INTERVAL '1 day',\n    if_not_exists =&gt; TRUE\n);\n\n-- Create hourly error summaries\nCREATE MATERIALIZED VIEW hourly_error_insights\nWITH (timescaledb.continuous) AS\nSELECT \n    time_bucket('1 hour'::interval, timestamp) AS hour,\n    level,\n    service,\n    COUNT(*) AS error_count,\n    steadytext_generate(\n        format('Summarize these %s errors from %s service: %s',\n            level,\n            service,\n            string_agg(LEFT(message, 200), '; ' ORDER BY timestamp)\n        )\n    ) AS ai_summary,\n    array_agg(DISTINCT user_id) FILTER (WHERE user_id IS NOT NULL) AS affected_users,\n    array_agg(DISTINCT ip_address) FILTER (WHERE ip_address IS NOT NULL) AS source_ips\nFROM application_logs\nWHERE level IN ('ERROR', 'CRITICAL', 'WARNING')\n  AND timestamp &gt; NOW() - INTERVAL '7 days'\nGROUP BY hour, level, service;\n\n-- Add refresh policy\nSELECT add_continuous_aggregate_policy('hourly_error_insights',\n    start_offset =&gt; INTERVAL '2 hours',\n    end_offset =&gt; INTERVAL '10 minutes',\n    schedule_interval =&gt; INTERVAL '10 minutes'\n);\n</code></pre>"},{"location":"examples/log-analysis/#security-threat-detection","title":"Security Threat Detection","text":"<p>Use AI to identify potential security threats in your logs:</p> <pre><code>-- Security analysis view\nCREATE OR REPLACE VIEW security_alerts AS\nWITH suspicious_activity AS (\n    SELECT \n        timestamp,\n        ip_address,\n        user_id,\n        service,\n        message,\n        metadata,\n        steadytext_generate_choice(\n            'Classify security risk: ' || message,\n            ARRAY['safe', 'low_risk', 'medium_risk', 'high_risk', 'critical']\n        ) AS risk_level\n    FROM application_logs\n    WHERE timestamp &gt; NOW() - INTERVAL '1 hour'\n      AND (\n        message ILIKE '%failed login%'\n        OR message ILIKE '%unauthorized%'\n        OR message ILIKE '%injection%'\n        OR message ILIKE '%suspicious%'\n        OR metadata-&gt;&gt;'status_code' IN ('401', '403')\n      )\n)\nSELECT \n    timestamp,\n    ip_address,\n    risk_level,\n    COUNT(*) OVER (PARTITION BY ip_address) AS attempts_from_ip,\n    steadytext_generate(\n        format('Analyze security threat: IP %s attempted: %s',\n            ip_address,\n            string_agg(message, '; ')\n        )\n    ) AS threat_analysis\nFROM suspicious_activity\nWHERE risk_level NOT IN ('safe', 'low_risk')\nGROUP BY timestamp, ip_address, risk_level, user_id, service, message;\n</code></pre>"},{"location":"examples/log-analysis/#daily-executive-summary","title":"Daily Executive Summary","text":"<p>Create automated daily reports for stakeholders:</p> <pre><code>-- Daily summary function\nCREATE OR REPLACE FUNCTION generate_daily_log_report(\n    report_date DATE DEFAULT CURRENT_DATE - 1\n)\nRETURNS TABLE (\n    report_date DATE,\n    executive_summary TEXT,\n    key_metrics JSONB,\n    top_issues TEXT[],\n    recommendations TEXT\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH daily_stats AS (\n        SELECT \n            COUNT(*) AS total_logs,\n            COUNT(*) FILTER (WHERE level = 'ERROR') AS error_count,\n            COUNT(*) FILTER (WHERE level = 'CRITICAL') AS critical_count,\n            COUNT(DISTINCT service) AS active_services,\n            COUNT(DISTINCT user_id) AS active_users,\n            array_agg(DISTINCT service) FILTER (\n                WHERE level IN ('ERROR', 'CRITICAL')\n            ) AS problematic_services\n        FROM application_logs\n        WHERE DATE(timestamp) = report_date\n    ),\n    error_details AS (\n        SELECT \n            string_agg(\n                format('%s: %s (%s times)', \n                    service, \n                    LEFT(message, 100), \n                    COUNT(*)::text\n                ),\n                '; '\n            ) AS error_summary\n        FROM application_logs\n        WHERE DATE(timestamp) = report_date\n          AND level IN ('ERROR', 'CRITICAL')\n        GROUP BY service, message\n        ORDER BY COUNT(*) DESC\n        LIMIT 10\n    )\n    SELECT \n        report_date,\n        steadytext_generate(\n            format('Executive summary for %s: Total logs: %s, Errors: %s, Critical: %s. Top errors: %s',\n                report_date,\n                total_logs,\n                error_count,\n                critical_count,\n                error_summary\n            )\n        ) AS executive_summary,\n        jsonb_build_object(\n            'total_logs', total_logs,\n            'error_count', error_count,\n            'critical_count', critical_count,\n            'error_rate', ROUND((error_count::NUMERIC / NULLIF(total_logs, 0) * 100), 2),\n            'active_services', active_services,\n            'active_users', active_users,\n            'problematic_services', problematic_services\n        ) AS key_metrics,\n        ARRAY(\n            SELECT DISTINCT \n                service || ': ' || LEFT(message, 100)\n            FROM application_logs\n            WHERE DATE(timestamp) = report_date\n              AND level IN ('ERROR', 'CRITICAL')\n            ORDER BY 1\n            LIMIT 5\n        ) AS top_issues,\n        steadytext_generate(\n            'Based on these metrics, provide 3 actionable recommendations: ' || \n            format('Error rate: %s%%, Critical issues: %s, Problematic services: %s',\n                ROUND((error_count::NUMERIC / NULLIF(total_logs, 0) * 100), 2),\n                critical_count,\n                array_to_string(problematic_services, ', ')\n            )\n        ) AS recommendations\n    FROM daily_stats, error_details;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/log-analysis/#pattern-recognition-and-anomaly-detection","title":"Pattern Recognition and Anomaly Detection","text":"<p>Identify unusual patterns in your logs:</p> <pre><code>-- Anomaly detection view\nCREATE OR REPLACE VIEW log_anomalies AS\nWITH baseline AS (\n    -- Calculate baseline metrics for the past week\n    SELECT \n        service,\n        level,\n        EXTRACT(HOUR FROM timestamp) AS hour_of_day,\n        AVG(COUNT(*)) OVER (\n            PARTITION BY service, level, EXTRACT(HOUR FROM timestamp)\n        ) AS avg_count,\n        STDDEV(COUNT(*)) OVER (\n            PARTITION BY service, level, EXTRACT(HOUR FROM timestamp)\n        ) AS stddev_count\n    FROM application_logs\n    WHERE timestamp &gt; NOW() - INTERVAL '7 days'\n      AND timestamp &lt; NOW() - INTERVAL '1 hour'\n    GROUP BY service, level, EXTRACT(HOUR FROM timestamp), DATE(timestamp)\n),\ncurrent_hour AS (\n    -- Get current hour's metrics\n    SELECT \n        service,\n        level,\n        COUNT(*) AS current_count\n    FROM application_logs\n    WHERE timestamp &gt; NOW() - INTERVAL '1 hour'\n    GROUP BY service, level\n)\nSELECT \n    c.service,\n    c.level,\n    c.current_count,\n    ROUND(b.avg_count, 2) AS expected_count,\n    CASE \n        WHEN b.stddev_count &gt; 0 AND \n             ABS(c.current_count - b.avg_count) &gt; 2 * b.stddev_count \n        THEN 'ANOMALY'\n        ELSE 'NORMAL'\n    END AS status,\n    steadytext_generate(\n        format('Analyze anomaly: %s service %s level - Current: %s, Expected: %s (\u00b1%s)',\n            c.service,\n            c.level,\n            c.current_count,\n            ROUND(b.avg_count, 2),\n            ROUND(b.stddev_count, 2)\n        )\n    ) AS anomaly_analysis\nFROM current_hour c\nJOIN baseline b ON \n    c.service = b.service \n    AND c.level = b.level \n    AND EXTRACT(HOUR FROM NOW()) = b.hour_of_day\nWHERE ABS(c.current_count - b.avg_count) &gt; b.stddev_count;\n</code></pre>"},{"location":"examples/log-analysis/#sample-data-and-testing","title":"Sample Data and Testing","text":"<p>Let's insert some sample data to test our log analysis:</p> <pre><code>-- Insert sample log data\nINSERT INTO application_logs (timestamp, level, service, message, metadata, user_id, ip_address)\nVALUES \n    (NOW() - INTERVAL '2 hours', 'ERROR', 'auth', 'Failed login attempt', '{\"attempts\": 3}'::jsonb, 123, '192.168.1.100'::inet),\n    (NOW() - INTERVAL '90 minutes', 'ERROR', 'auth', 'Failed login attempt', '{\"attempts\": 5}'::jsonb, 123, '192.168.1.100'::inet),\n    (NOW() - INTERVAL '1 hour', 'CRITICAL', 'auth', 'Potential brute force attack detected', '{\"attempts\": 10}'::jsonb, NULL, '192.168.1.100'::inet),\n    (NOW() - INTERVAL '45 minutes', 'ERROR', 'api', 'Database connection timeout', '{\"duration\": 5000}'::jsonb, 456, '10.0.0.50'::inet),\n    (NOW() - INTERVAL '30 minutes', 'WARNING', 'api', 'Slow query detected', '{\"query_time\": 3.5}'::jsonb, 789, '10.0.0.51'::inet),\n    (NOW() - INTERVAL '15 minutes', 'ERROR', 'payment', 'Payment processing failed', '{\"error\": \"Gateway timeout\"}'::jsonb, 321, '172.16.0.10'::inet),\n    (NOW() - INTERVAL '5 minutes', 'INFO', 'api', 'User logged in successfully', '{\"method\": \"OAuth\"}'::jsonb, 654, '192.168.1.50'::inet);\n\n-- Test our analysis functions\nSELECT * FROM analyze_error_patterns('2 hours');\nSELECT * FROM security_alerts;\nSELECT * FROM generate_daily_log_report();\n</code></pre>"},{"location":"examples/log-analysis/#automation-with-pg_cron","title":"Automation with pg_cron","text":"<p>Schedule automatic reports using pg_cron:</p> <pre><code>-- Enable pg_cron\nCREATE EXTENSION IF NOT EXISTS pg_cron;\n\n-- Schedule hourly error analysis\nSELECT cron.schedule(\n    'hourly-error-analysis',\n    '0 * * * *',\n    $$INSERT INTO error_analysis_history \n      SELECT NOW(), * FROM analyze_error_patterns('1 hour')$$\n);\n\n-- Schedule daily reports\nSELECT cron.schedule(\n    'daily-log-report',\n    '0 8 * * *',\n    $$INSERT INTO daily_reports \n      SELECT * FROM generate_daily_log_report()$$\n);\n</code></pre>"},{"location":"examples/log-analysis/#best-practices","title":"Best Practices","text":"<ol> <li>Index Strategy: Always index timestamp and frequently queried fields</li> <li>Partitioning: Use TimescaleDB or native partitioning for large datasets</li> <li>Caching: SteadyText caches AI results automatically</li> <li>Batch Processing: Process logs in batches for better performance</li> <li>Retention: Set up automatic data retention policies</li> </ol>"},{"location":"examples/log-analysis/#performance-optimization","title":"Performance Optimization","text":"<pre><code>-- Create a summary table for faster queries\nCREATE TABLE log_summaries (\n    id SERIAL PRIMARY KEY,\n    period_start TIMESTAMPTZ NOT NULL,\n    period_end TIMESTAMPTZ NOT NULL,\n    service VARCHAR(50),\n    level VARCHAR(10),\n    count INTEGER,\n    ai_summary TEXT,\n    created_at TIMESTAMPTZ DEFAULT NOW()\n);\n\n-- Create index for fast lookups\nCREATE INDEX idx_log_summaries_period \nON log_summaries(period_start, period_end);\n\n-- Batch process historical data\nINSERT INTO log_summaries (period_start, period_end, service, level, count, ai_summary)\nSELECT \n    date_trunc('hour', timestamp) AS period_start,\n    date_trunc('hour', timestamp) + INTERVAL '1 hour' AS period_end,\n    service,\n    level,\n    COUNT(*) as count,\n    steadytext_generate(\n        'Summarize: ' || string_agg(LEFT(message, 100), '; ')\n    ) AS ai_summary\nFROM application_logs\nWHERE timestamp &lt; NOW() - INTERVAL '1 day'\nGROUP BY date_trunc('hour', timestamp), service, level;\n</code></pre>"},{"location":"examples/log-analysis/#next-steps","title":"Next Steps","text":"<ul> <li>Content Management Examples \u2192</li> <li>Customer Intelligence Tutorial \u2192</li> <li>TimescaleDB Integration Guide \u2192</li> </ul> <p>Pro Tip</p> <p>Use materialized views with SteadyText for pre-computed AI summaries. This gives you instant query performance while keeping the AI insights fresh.</p>"},{"location":"examples/performance-tuning/","title":"Performance Tuning Guide","text":"<p>Optimize SteadyText for maximum performance, reduced latency, and efficient resource usage.</p>"},{"location":"examples/performance-tuning/#overview","title":"Overview","text":"<p>SteadyText performance optimization focuses on:</p> <ul> <li>Daemon mode: 160x faster first response</li> <li>Cache optimization: Hit rates up to 95%+</li> <li>Batch processing: Amortize model loading costs</li> <li>Resource management: Memory and CPU optimization</li> <li>Concurrent operations: Thread-safe parallel processing</li> </ul>"},{"location":"examples/performance-tuning/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Performance Metrics</li> <li>Daemon Optimization</li> <li>Cache Tuning</li> <li>Model Performance</li> <li>Batch Processing</li> <li>Memory Management</li> <li>Concurrent Operations</li> <li>Monitoring and Profiling</li> <li>Production Optimization</li> <li>Benchmarking</li> </ul>"},{"location":"examples/performance-tuning/#performance-metrics","title":"Performance Metrics","text":""},{"location":"examples/performance-tuning/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<pre><code>import time\nimport psutil\nimport steadytext\nfrom steadytext import get_cache_manager\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nimport statistics\n\n@dataclass\nclass PerformanceMetrics:\n    \"\"\"Performance measurement results.\"\"\"\n    operation: str\n    latency_ms: float\n    throughput_tps: float\n    memory_mb: float\n    cache_hit: bool\n    cpu_percent: float\n\nclass PerformanceMonitor:\n    \"\"\"Monitor SteadyText performance metrics.\"\"\"\n\n    def __init__(self):\n        self.metrics: List[PerformanceMetrics] = []\n        self.cache_manager = get_cache_manager()\n        self.process = psutil.Process()\n\n    def measure_operation(self, func, *args, **kwargs):\n        \"\"\"Measure performance of a single operation.\"\"\"\n        # Get initial state\n        initial_mem = self.process.memory_info().rss / 1024 / 1024\n        initial_cache_stats = self.cache_manager.get_cache_stats()\n\n        # Measure CPU\n        self.process.cpu_percent()  # Initialize\n\n        # Time the operation\n        start_time = time.perf_counter()\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter()\n\n        # Calculate metrics\n        latency_ms = (end_time - start_time) * 1000\n        throughput_tps = 1000 / latency_ms\n        final_mem = self.process.memory_info().rss / 1024 / 1024\n        memory_delta = final_mem - initial_mem\n        cpu_percent = self.process.cpu_percent()\n\n        # Check cache hit\n        final_cache_stats = self.cache_manager.get_cache_stats()\n        cache_hit = self._detect_cache_hit(initial_cache_stats, final_cache_stats)\n\n        metric = PerformanceMetrics(\n            operation=func.__name__,\n            latency_ms=latency_ms,\n            throughput_tps=throughput_tps,\n            memory_mb=memory_delta,\n            cache_hit=cache_hit,\n            cpu_percent=cpu_percent\n        )\n\n        self.metrics.append(metric)\n        return result, metric\n\n    def _detect_cache_hit(self, initial: dict, final: dict) -&gt; bool:\n        \"\"\"Detect if a cache hit occurred.\"\"\"\n        for cache_type in ['generation', 'embedding']:\n            initial_hits = initial.get(cache_type, {}).get('hits', 0)\n            final_hits = final.get(cache_type, {}).get('hits', 0)\n            if final_hits &gt; initial_hits:\n                return True\n        return False\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get performance summary statistics.\"\"\"\n        if not self.metrics:\n            return {}\n\n        latencies = [m.latency_ms for m in self.metrics]\n        throughputs = [m.throughput_tps for m in self.metrics]\n        memory_deltas = [m.memory_mb for m in self.metrics]\n        cpu_percents = [m.cpu_percent for m in self.metrics]\n        cache_hits = sum(1 for m in self.metrics if m.cache_hit)\n\n        return {\n            'operations': len(self.metrics),\n            'cache_hit_rate': cache_hits / len(self.metrics),\n            'latency': {\n                'mean': statistics.mean(latencies),\n                'median': statistics.median(latencies),\n                'p95': sorted(latencies)[int(len(latencies) * 0.95)],\n                'p99': sorted(latencies)[int(len(latencies) * 0.99)],\n            },\n            'throughput': {\n                'mean': statistics.mean(throughputs),\n                'total': sum(throughputs),\n            },\n            'memory': {\n                'total_mb': sum(memory_deltas),\n                'mean_mb': statistics.mean(memory_deltas),\n            },\n            'cpu': {\n                'mean_percent': statistics.mean(cpu_percents),\n                'max_percent': max(cpu_percents),\n            }\n        }\n\n# Usage example\nmonitor = PerformanceMonitor()\n\n# Measure generation performance\nfor i in range(100):\n    prompt = f\"Test prompt {i}\"\n    result, metric = monitor.measure_operation(\n        steadytext.generate, \n        prompt, \n        seed=42\n    )\n    print(f\"Latency: {metric.latency_ms:.2f}ms, Cache: {metric.cache_hit}\")\n\n# Get summary\nsummary = monitor.get_summary()\nprint(f\"\\nPerformance Summary:\")\nprint(f\"Cache hit rate: {summary['cache_hit_rate']:.2%}\")\nprint(f\"Mean latency: {summary['latency']['mean']:.2f}ms\")\nprint(f\"P95 latency: {summary['latency']['p95']:.2f}ms\")\n</code></pre>"},{"location":"examples/performance-tuning/#daemon-optimization","title":"Daemon Optimization","text":""},{"location":"examples/performance-tuning/#startup-performance","title":"Startup Performance","text":"<pre><code>import subprocess\nimport time\nfrom typing import Optional\n\nclass DaemonOptimizer:\n    \"\"\"Optimize daemon startup and performance.\"\"\"\n\n    @staticmethod\n    def start_daemon_with_profiling():\n        \"\"\"Start daemon with performance profiling.\"\"\"\n        start_time = time.time()\n\n        # Start daemon\n        result = subprocess.run([\n            'st', 'daemon', 'start',\n            '--seed', '42'\n        ], capture_output=True, text=True)\n\n        # Wait for daemon to be ready\n        ready = False\n        for _ in range(30):  # 30 second timeout\n            status = subprocess.run([\n                'st', 'daemon', 'status', '--json'\n            ], capture_output=True, text=True)\n\n            if status.returncode == 0:\n                import json\n                data = json.loads(status.stdout)\n                if data.get('running'):\n                    ready = True\n                    break\n\n            time.sleep(0.1)\n\n        startup_time = time.time() - start_time\n        print(f\"Daemon startup time: {startup_time:.2f}s\")\n\n        return ready\n\n    @staticmethod\n    def benchmark_daemon_vs_direct():\n        \"\"\"Compare daemon vs direct performance.\"\"\"\n        import steadytext\n        from steadytext.daemon import use_daemon\n\n        prompt = \"Benchmark test prompt\"\n        iterations = 50\n\n        # Benchmark direct access\n        print(\"Benchmarking direct access...\")\n        direct_times = []\n        for _ in range(iterations):\n            start = time.perf_counter()\n            _ = steadytext.generate(prompt, seed=42)\n            direct_times.append(time.perf_counter() - start)\n\n        # Benchmark daemon access\n        print(\"Benchmarking daemon access...\")\n        daemon_times = []\n        with use_daemon():\n            for _ in range(iterations):\n                start = time.perf_counter()\n                _ = steadytext.generate(prompt, seed=42)\n                daemon_times.append(time.perf_counter() - start)\n\n        # Calculate statistics\n        direct_avg = sum(direct_times) / len(direct_times) * 1000\n        daemon_avg = sum(daemon_times) / len(daemon_times) * 1000\n        speedup = direct_avg / daemon_avg\n\n        print(f\"\\nResults:\")\n        print(f\"Direct access: {direct_avg:.2f}ms average\")\n        print(f\"Daemon access: {daemon_avg:.2f}ms average\")\n        print(f\"Speedup: {speedup:.1f}x\")\n\n        # First response comparison\n        print(f\"\\nFirst response:\")\n        print(f\"Direct: {direct_times[0]*1000:.2f}ms\")\n        print(f\"Daemon: {daemon_times[0]*1000:.2f}ms\")\n        print(f\"First response speedup: {direct_times[0]/daemon_times[0]:.1f}x\")\n</code></pre>"},{"location":"examples/performance-tuning/#connection-pooling","title":"Connection Pooling","text":"<pre><code>import zmq\nfrom contextlib import contextmanager\nfrom threading import Lock\nfrom typing import Dict, Any\n\nclass DaemonConnectionPool:\n    \"\"\"Connection pool for daemon clients.\"\"\"\n\n    def __init__(self, host='127.0.0.1', port=5557, pool_size=10):\n        self.host = host\n        self.port = port\n        self.pool_size = pool_size\n        self.connections = []\n        self.available = []\n        self.lock = Lock()\n        self._initialize_pool()\n\n    def _initialize_pool(self):\n        \"\"\"Initialize connection pool.\"\"\"\n        context = zmq.Context()\n        for _ in range(self.pool_size):\n            socket = context.socket(zmq.REQ)\n            socket.connect(f\"tcp://{self.host}:{self.port}\")\n            socket.setsockopt(zmq.LINGER, 0)\n            socket.setsockopt(zmq.RCVTIMEO, 5000)  # 5 second timeout\n            self.connections.append(socket)\n            self.available.append(socket)\n\n    @contextmanager\n    def get_connection(self):\n        \"\"\"Get a connection from the pool.\"\"\"\n        socket = None\n        try:\n            with self.lock:\n                if self.available:\n                    socket = self.available.pop()\n\n            if socket is None:\n                raise RuntimeError(\"No connections available\")\n\n            yield socket\n\n        finally:\n            if socket:\n                with self.lock:\n                    self.available.append(socket)\n\n    def execute_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute request using pooled connection.\"\"\"\n        import json\n\n        with self.get_connection() as socket:\n            socket.send_json(request)\n            response = socket.recv_json()\n            return response\n\n# Usage\npool = DaemonConnectionPool(pool_size=20)\n\n# Concurrent requests\nimport concurrent.futures\n\ndef make_request(i):\n    request = {\n        'type': 'generate',\n        'prompt': f'Test {i}',\n        'seed': 42\n    }\n    return pool.execute_request(request)\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    futures = [executor.submit(make_request, i) for i in range(100)]\n    results = [f.result() for f in futures]\n</code></pre>"},{"location":"examples/performance-tuning/#cache-tuning","title":"Cache Tuning","text":""},{"location":"examples/performance-tuning/#optimal-cache-configuration","title":"Optimal Cache Configuration","text":"<pre><code>import os\nfrom typing import Dict, Tuple\n\nclass CacheTuner:\n    \"\"\"Tune cache settings for optimal performance.\"\"\"\n\n    @staticmethod\n    def calculate_optimal_settings(\n        available_memory_mb: float,\n        expected_qps: float,\n        avg_prompt_length: int,\n        cache_for_hours: float = 24\n    ) -&gt; Dict[str, Dict[str, float]]:\n        \"\"\"Calculate optimal cache settings based on workload.\"\"\"\n\n        # Estimate cache entry sizes\n        gen_entry_size_kb = 2 + (avg_prompt_length * 0.001)  # Rough estimate\n        embed_entry_size_kb = 4.2  # 1024 floats + metadata\n\n        # Calculate expected entries\n        expected_requests = expected_qps * 3600 * cache_for_hours\n        unique_ratio = 0.3  # Assume 30% unique requests\n        expected_unique = expected_requests * unique_ratio\n\n        # Allocate memory (70% for generation, 30% for embedding)\n        gen_memory_mb = available_memory_mb * 0.7\n        embed_memory_mb = available_memory_mb * 0.3\n\n        # Calculate capacities\n        gen_capacity = min(\n            int(gen_memory_mb * 1024 / gen_entry_size_kb),\n            int(expected_unique * 0.8)  # 80% of expected unique\n        )\n\n        embed_capacity = min(\n            int(embed_memory_mb * 1024 / embed_entry_size_kb),\n            int(expected_unique * 0.5)  # 50% of expected unique\n        )\n\n        return {\n            'generation': {\n                'capacity': gen_capacity,\n                'max_size_mb': gen_memory_mb\n            },\n            'embedding': {\n                'capacity': embed_capacity,\n                'max_size_mb': embed_memory_mb\n            }\n        }\n\n    @staticmethod\n    def apply_settings(settings: Dict[str, Dict[str, float]]):\n        \"\"\"Apply cache settings via environment variables.\"\"\"\n        os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = str(\n            int(settings['generation']['capacity'])\n        )\n        os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = str(\n            settings['generation']['max_size_mb']\n        )\n        os.environ['STEADYTEXT_EMBEDDING_CACHE_CAPACITY'] = str(\n            int(settings['embedding']['capacity'])\n        )\n        os.environ['STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB'] = str(\n            settings['embedding']['max_size_mb']\n        )\n\n        print(\"Applied cache settings:\")\n        print(f\"Generation: {settings['generation']['capacity']} entries, \"\n              f\"{settings['generation']['max_size_mb']:.1f}MB\")\n        print(f\"Embedding: {settings['embedding']['capacity']} entries, \"\n              f\"{settings['embedding']['max_size_mb']:.1f}MB\")\n\n# Example usage\ntuner = CacheTuner()\n\n# For a server with 1GB available for caching, expecting 10 QPS\nsettings = tuner.calculate_optimal_settings(\n    available_memory_mb=1024,\n    expected_qps=10,\n    avg_prompt_length=100,\n    cache_for_hours=24\n)\n\ntuner.apply_settings(settings)\n</code></pre>"},{"location":"examples/performance-tuning/#cache-warming","title":"Cache Warming","text":"<pre><code>import asyncio\nfrom typing import List, Tuple\nimport steadytext\n\nclass CacheWarmer:\n    \"\"\"Warm up caches with common queries.\"\"\"\n\n    def __init__(self, prompts: List[str], seeds: List[int] = None):\n        self.prompts = prompts\n        self.seeds = seeds or [42]\n\n    async def warm_generation_cache(self):\n        \"\"\"Warm generation cache asynchronously.\"\"\"\n        tasks = []\n\n        for prompt in self.prompts:\n            for seed in self.seeds:\n                task = asyncio.create_task(\n                    self._generate_async(prompt, seed)\n                )\n                tasks.append(task)\n\n        results = await asyncio.gather(*tasks)\n        successful = sum(1 for r in results if r is not None)\n        print(f\"Warmed generation cache: {successful}/{len(tasks)} entries\")\n\n    async def _generate_async(self, prompt: str, seed: int):\n        \"\"\"Generate text asynchronously.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None, \n            steadytext.generate, \n            prompt, \n            seed\n        )\n\n    def warm_embedding_cache(self):\n        \"\"\"Warm embedding cache.\"\"\"\n        successful = 0\n\n        for text in self.prompts:\n            for seed in self.seeds:\n                try:\n                    _ = steadytext.embed(text, seed=seed)\n                    successful += 1\n                except Exception as e:\n                    print(f\"Failed to warm embed cache: {e}\")\n\n        print(f\"Warmed embedding cache: {successful}/{len(self.prompts) * len(self.seeds)} entries\")\n\n# Common prompts for warming\nCOMMON_PROMPTS = [\n    \"Explain machine learning\",\n    \"Write a Python function\",\n    \"What is artificial intelligence?\",\n    \"How does deep learning work?\",\n    \"Summarize this text:\",\n    \"Translate to Spanish:\",\n    \"Generate documentation for\",\n    \"Create a test case\",\n    \"Explain the error:\",\n    \"Optimize this code:\"\n]\n\n# Warm caches on startup\nasync def warm_caches():\n    warmer = CacheWarmer(COMMON_PROMPTS, seeds=[42, 123, 456])\n    await warmer.warm_generation_cache()\n    warmer.warm_embedding_cache()\n\n# Run warming\nasyncio.run(warm_caches())\n</code></pre>"},{"location":"examples/performance-tuning/#model-performance","title":"Model Performance","text":""},{"location":"examples/performance-tuning/#model-size-selection","title":"Model Size Selection","text":"<pre><code>from typing import Dict, Any\nimport time\n\nclass ModelBenchmark:\n    \"\"\"Benchmark different model configurations.\"\"\"\n\n    @staticmethod\n    def compare_model_sizes():\n        \"\"\"Compare performance of different model sizes.\"\"\"\n        import subprocess\n        import json\n\n        test_prompts = [\n            \"Write a short function\",\n            \"Explain quantum computing in simple terms\",\n            \"Generate a creative story about AI\"\n        ]\n\n        results = {}\n\n        for size in ['small', 'large']:\n            print(f\"\\nBenchmarking {size} model...\")\n            size_results = {\n                'latencies': [],\n                'quality_scores': [],\n                'memory_usage': []\n            }\n\n            for prompt in test_prompts:\n                # Measure latency\n                start = time.perf_counter()\n                result = subprocess.run([\n                    'st', 'generate', prompt,\n                    '--size', size,\n                    '--json',\n                    '--wait'\n                ], capture_output=True, text=True)\n                latency = time.perf_counter() - start\n\n                if result.returncode == 0:\n                    data = json.loads(result.stdout)\n                    size_results['latencies'].append(latency)\n\n                    # Simple quality metric (length and vocabulary)\n                    text = data['text']\n                    quality = len(set(text.split())) / len(text.split())\n                    size_results['quality_scores'].append(quality)\n\n            results[size] = size_results\n\n        # Print comparison\n        print(\"\\nModel Size Comparison:\")\n        print(\"-\" * 50)\n        for size, data in results.items():\n            avg_latency = sum(data['latencies']) / len(data['latencies'])\n            avg_quality = sum(data['quality_scores']) / len(data['quality_scores'])\n\n            print(f\"{size.capitalize()} Model:\")\n            print(f\"  Average latency: {avg_latency:.2f}s\")\n            print(f\"  Quality score: {avg_quality:.3f}\")\n            print(f\"  Latency range: {min(data['latencies']):.2f}s - {max(data['latencies']):.2f}s\")\n\n        return results\n</code></pre>"},{"location":"examples/performance-tuning/#custom-model-configuration","title":"Custom Model Configuration","text":"<pre><code>class ModelOptimizer:\n    \"\"\"Optimize model loading and configuration.\"\"\"\n\n    @staticmethod\n    def get_optimal_config(use_case: str) -&gt; Dict[str, Any]:\n        \"\"\"Get optimal model configuration for use case.\"\"\"\n\n        configs = {\n            'realtime': {\n                'model': 'small',\n                'n_threads': 4,\n                'n_batch': 8,\n                'context_length': 512,\n                'use_mlock': True,\n                'use_mmap': True\n            },\n            'quality': {\n                'model': 'large',\n                'n_threads': 8,\n                'n_batch': 16,\n                'context_length': 2048,\n                'use_mlock': True,\n                'use_mmap': True\n            },\n            'batch': {\n                'model': 'large',\n                'n_threads': 16,\n                'n_batch': 32,\n                'context_length': 1024,\n                'use_mlock': False,\n                'use_mmap': True\n            }\n        }\n\n        return configs.get(use_case, configs['realtime'])\n\n    @staticmethod\n    def optimize_for_hardware():\n        \"\"\"Detect hardware and optimize configuration.\"\"\"\n        import psutil\n\n        # Get system info\n        cpu_count = psutil.cpu_count(logical=True)\n        memory_gb = psutil.virtual_memory().total / (1024**3)\n\n        # Determine optimal settings\n        if memory_gb &gt;= 32 and cpu_count &gt;= 16:\n            config = {\n                'profile': 'high-performance',\n                'model': 'large',\n                'n_threads': min(cpu_count - 2, 24),\n                'cache_size_mb': 2048\n            }\n        elif memory_gb &gt;= 16 and cpu_count &gt;= 8:\n            config = {\n                'profile': 'balanced',\n                'model': 'large',\n                'n_threads': min(cpu_count - 1, 12),\n                'cache_size_mb': 1024\n            }\n        else:\n            config = {\n                'profile': 'low-resource',\n                'model': 'small',\n                'n_threads': min(cpu_count, 4),\n                'cache_size_mb': 256\n            }\n\n        print(f\"Hardware profile: {config['profile']}\")\n        print(f\"Detected: {cpu_count} CPUs, {memory_gb:.1f}GB RAM\")\n        print(f\"Recommended: {config['model']} model, {config['n_threads']} threads\")\n\n        return config\n</code></pre>"},{"location":"examples/performance-tuning/#batch-processing","title":"Batch Processing","text":""},{"location":"examples/performance-tuning/#efficient-batch-operations","title":"Efficient Batch Operations","text":"<pre><code>from typing import List, Dict, Any\nimport concurrent.futures\nimport asyncio\n\nclass BatchProcessor:\n    \"\"\"Process multiple requests efficiently.\"\"\"\n\n    def __init__(self, max_workers: int = 4):\n        self.max_workers = max_workers\n\n    def process_batch_sync(\n        self, \n        prompts: List[str], \n        seeds: List[int] = None\n    ) -&gt; List[str]:\n        \"\"\"Process batch synchronously with thread pool.\"\"\"\n        if seeds is None:\n            seeds = [42] * len(prompts)\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            futures = []\n            for prompt, seed in zip(prompts, seeds):\n                future = executor.submit(steadytext.generate, prompt, seed)\n                futures.append(future)\n\n            results = []\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    result = future.result()\n                    results.append(result)\n                except Exception as e:\n                    results.append(f\"Error: {e}\")\n\n            return results\n\n    async def process_batch_async(\n        self, \n        prompts: List[str],\n        seeds: List[int] = None\n    ) -&gt; List[str]:\n        \"\"\"Process batch asynchronously.\"\"\"\n        if seeds is None:\n            seeds = [42] * len(prompts)\n\n        tasks = []\n        for prompt, seed in zip(prompts, seeds):\n            task = asyncio.create_task(\n                self._generate_async(prompt, seed)\n            )\n            tasks.append(task)\n\n        return await asyncio.gather(*tasks)\n\n    async def _generate_async(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Generate text asynchronously.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None,\n            steadytext.generate,\n            prompt,\n            seed\n        )\n\n    def process_streaming_batch(\n        self,\n        prompts: List[str],\n        callback: callable\n    ):\n        \"\"\"Process batch with streaming results.\"\"\"\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Submit all tasks\n            future_to_prompt = {\n                executor.submit(steadytext.generate, prompt, 42): prompt\n                for prompt in prompts\n            }\n\n            # Process results as they complete\n            for future in concurrent.futures.as_completed(future_to_prompt):\n                prompt = future_to_prompt[future]\n                try:\n                    result = future.result()\n                    callback(prompt, result, None)\n                except Exception as e:\n                    callback(prompt, None, e)\n\n# Usage example\nprocessor = BatchProcessor(max_workers=8)\n\n# Sync batch processing\nprompts = [\"Explain \" + topic for topic in [\"AI\", \"ML\", \"DL\", \"NLP\"]]\nresults = processor.process_batch_sync(prompts)\n\n# Async batch processing\nasync def async_example():\n    results = await processor.process_batch_async(prompts)\n    for prompt, result in zip(prompts, results):\n        print(f\"{prompt}: {len(result)} chars\")\n\n# Streaming results\ndef handle_result(prompt, result, error):\n    if error:\n        print(f\"Error for '{prompt}': {error}\")\n    else:\n        print(f\"Completed '{prompt}': {len(result)} chars\")\n\nprocessor.process_streaming_batch(prompts, handle_result)\n</code></pre>"},{"location":"examples/performance-tuning/#pipeline-optimization","title":"Pipeline Optimization","text":"<pre><code>class Pipeline:\n    \"\"\"Optimized processing pipeline.\"\"\"\n\n    def __init__(self):\n        self.stages = []\n\n    def add_stage(self, func, name=None):\n        \"\"\"Add processing stage.\"\"\"\n        self.stages.append({\n            'func': func,\n            'name': name or func.__name__\n        })\n        return self\n\n    async def process(self, items: List[Any]) -&gt; List[Any]:\n        \"\"\"Process items through pipeline.\"\"\"\n        current = items\n\n        for stage in self.stages:\n            print(f\"Processing stage: {stage['name']}\")\n\n            # Process stage in parallel\n            tasks = []\n            for item in current:\n                task = asyncio.create_task(\n                    self._process_item(stage['func'], item)\n                )\n                tasks.append(task)\n\n            current = await asyncio.gather(*tasks)\n\n        return current\n\n    async def _process_item(self, func, item):\n        \"\"\"Process single item.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, func, item)\n\n# Example: Text processing pipeline\nasync def text_pipeline_example():\n    # Define stages\n    def clean_text(text):\n        return text.strip().lower()\n\n    def generate_summary(text):\n        prompt = f\"Summarize in one sentence: {text}\"\n        return steadytext.generate(prompt, seed=42)\n\n    def extract_keywords(summary):\n        prompt = f\"Extract 3 keywords from: {summary}\"\n        return steadytext.generate(prompt, seed=123)\n\n    # Build pipeline\n    pipeline = Pipeline()\n    pipeline.add_stage(clean_text, \"Clean\")\n    pipeline.add_stage(generate_summary, \"Summarize\")\n    pipeline.add_stage(extract_keywords, \"Keywords\")\n\n    # Process texts\n    texts = [\n        \"Machine learning is transforming industries...\",\n        \"Artificial intelligence enables computers...\",\n        \"Deep learning uses neural networks...\"\n    ]\n\n    results = await pipeline.process(texts)\n    return results\n</code></pre>"},{"location":"examples/performance-tuning/#memory-management","title":"Memory Management","text":""},{"location":"examples/performance-tuning/#memory-optimization-strategies","title":"Memory Optimization Strategies","text":"<pre><code>import gc\nimport tracemalloc\nfrom typing import List, Dict, Any\n\nclass MemoryOptimizer:\n    \"\"\"Optimize memory usage for SteadyText operations.\"\"\"\n\n    def __init__(self):\n        self.snapshots = []\n\n    def start_profiling(self):\n        \"\"\"Start memory profiling.\"\"\"\n        tracemalloc.start()\n        self.snapshots = []\n\n    def take_snapshot(self, label: str):\n        \"\"\"Take memory snapshot.\"\"\"\n        snapshot = tracemalloc.take_snapshot()\n        self.snapshots.append((label, snapshot))\n\n    def get_memory_report(self) -&gt; str:\n        \"\"\"Generate memory usage report.\"\"\"\n        if len(self.snapshots) &lt; 2:\n            return \"Not enough snapshots for comparison\"\n\n        report = []\n\n        for i in range(1, len(self.snapshots)):\n            label1, snap1 = self.snapshots[i-1]\n            label2, snap2 = self.snapshots[i]\n\n            diff = snap2.compare_to(snap1, 'lineno')\n            report.append(f\"\\n{label1} -&gt; {label2}:\")\n\n            for stat in diff[:10]:  # Top 10 differences\n                report.append(f\"  {stat}\")\n\n        return \"\\n\".join(report)\n\n    @staticmethod\n    def optimize_batch_memory(items: List[Any], batch_size: int = 100):\n        \"\"\"Process items in batches to control memory.\"\"\"\n        results = []\n\n        for i in range(0, len(items), batch_size):\n            batch = items[i:i + batch_size]\n\n            # Process batch\n            batch_results = [\n                steadytext.generate(item, seed=42)\n                for item in batch\n            ]\n\n            results.extend(batch_results)\n\n            # Force garbage collection after each batch\n            gc.collect()\n\n        return results\n\n    @staticmethod\n    def memory_efficient_streaming(prompts: List[str]):\n        \"\"\"Memory-efficient streaming generation.\"\"\"\n        for prompt in prompts:\n            # Generate and yield immediately\n            result = steadytext.generate(prompt, seed=42)\n            yield result\n\n            # Clear any references\n            del result\n\n            # Periodic garbage collection\n            if prompts.index(prompt) % 100 == 0:\n                gc.collect()\n\n# Example usage\noptimizer = MemoryOptimizer()\noptimizer.start_profiling()\n\n# Take initial snapshot\noptimizer.take_snapshot(\"Initial\")\n\n# Generate some text\ntexts = []\nfor i in range(1000):\n    text = steadytext.generate(f\"Test {i}\", seed=42)\n    texts.append(text)\n\noptimizer.take_snapshot(\"After 1000 generations\")\n\n# Clear and collect\ntexts.clear()\ngc.collect()\n\noptimizer.take_snapshot(\"After cleanup\")\n\n# Get report\nprint(optimizer.get_memory_report())\n</code></pre>"},{"location":"examples/performance-tuning/#resource-limits","title":"Resource Limits","text":"<pre><code>import resource\nimport signal\nfrom contextlib import contextmanager\n\nclass ResourceLimiter:\n    \"\"\"Set resource limits for operations.\"\"\"\n\n    @staticmethod\n    @contextmanager\n    def limit_memory(max_memory_mb: int):\n        \"\"\"Limit memory usage.\"\"\"\n        # Convert MB to bytes\n        max_memory = max_memory_mb * 1024 * 1024\n\n        # Set soft and hard limits\n        resource.setrlimit(\n            resource.RLIMIT_AS,\n            (max_memory, max_memory)\n        )\n\n        try:\n            yield\n        finally:\n            # Reset to unlimited\n            resource.setrlimit(\n                resource.RLIMIT_AS,\n                (resource.RLIM_INFINITY, resource.RLIM_INFINITY)\n            )\n\n    @staticmethod\n    @contextmanager\n    def timeout(seconds: int):\n        \"\"\"Set operation timeout.\"\"\"\n        def timeout_handler(signum, frame):\n            raise TimeoutError(f\"Operation timed out after {seconds} seconds\")\n\n        # Set handler\n        old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n        signal.alarm(seconds)\n\n        try:\n            yield\n        finally:\n            signal.alarm(0)\n            signal.signal(signal.SIGALRM, old_handler)\n\n# Usage\nlimiter = ResourceLimiter()\n\n# Limit memory usage\ntry:\n    with limiter.limit_memory(1024):  # 1GB limit\n        # Memory-intensive operation\n        results = [\n            steadytext.generate(f\"Prompt {i}\", seed=42)\n            for i in range(10000)\n        ]\nexcept MemoryError:\n    print(\"Memory limit exceeded\")\n\n# Set timeout\ntry:\n    with limiter.timeout(5):  # 5 second timeout\n        result = steadytext.generate(\"Complex prompt\", seed=42)\nexcept TimeoutError:\n    print(\"Operation timed out\")\n</code></pre>"},{"location":"examples/performance-tuning/#concurrent-operations","title":"Concurrent Operations","text":""},{"location":"examples/performance-tuning/#thread-safe-operations","title":"Thread-Safe Operations","text":"<pre><code>import threading\nfrom queue import Queue\nfrom typing import List, Tuple, Any\n\nclass ConcurrentProcessor:\n    \"\"\"Thread-safe concurrent processing.\"\"\"\n\n    def __init__(self, num_workers: int = 4):\n        self.num_workers = num_workers\n        self.input_queue = Queue()\n        self.output_queue = Queue()\n        self.workers = []\n        self.running = False\n\n    def start(self):\n        \"\"\"Start worker threads.\"\"\"\n        self.running = True\n\n        for i in range(self.num_workers):\n            worker = threading.Thread(\n                target=self._worker,\n                name=f\"Worker-{i}\"\n            )\n            worker.daemon = True\n            worker.start()\n            self.workers.append(worker)\n\n    def stop(self):\n        \"\"\"Stop all workers.\"\"\"\n        self.running = False\n\n        # Add stop signals\n        for _ in range(self.num_workers):\n            self.input_queue.put(None)\n\n        # Wait for workers\n        for worker in self.workers:\n            worker.join()\n\n    def _worker(self):\n        \"\"\"Worker thread function.\"\"\"\n        while self.running:\n            item = self.input_queue.get()\n\n            if item is None:\n                break\n\n            prompt, seed, request_id = item\n\n            try:\n                result = steadytext.generate(prompt, seed=seed)\n                self.output_queue.put((request_id, result, None))\n            except Exception as e:\n                self.output_queue.put((request_id, None, e))\n\n            self.input_queue.task_done()\n\n    def process_concurrent(\n        self, \n        prompts: List[str], \n        seeds: List[int] = None\n    ) -&gt; List[Tuple[int, Any, Any]]:\n        \"\"\"Process prompts concurrently.\"\"\"\n        if seeds is None:\n            seeds = [42] * len(prompts)\n\n        # Add all items to queue\n        for i, (prompt, seed) in enumerate(zip(prompts, seeds)):\n            self.input_queue.put((prompt, seed, i))\n\n        # Collect results\n        results = []\n        for _ in range(len(prompts)):\n            result = self.output_queue.get()\n            results.append(result)\n\n        # Sort by request ID\n        results.sort(key=lambda x: x[0])\n\n        return results\n\n# Usage\nprocessor = ConcurrentProcessor(num_workers=8)\nprocessor.start()\n\n# Process requests\nprompts = [f\"Generate text about topic {i}\" for i in range(100)]\nresults = processor.process_concurrent(prompts)\n\n# Check results\nsuccessful = sum(1 for _, result, error in results if error is None)\nprint(f\"Processed {successful}/{len(prompts)} successfully\")\n\nprocessor.stop()\n</code></pre>"},{"location":"examples/performance-tuning/#async-concurrency","title":"Async Concurrency","text":"<pre><code>import asyncio\nfrom typing import List, Dict, Any\n\nclass AsyncConcurrentProcessor:\n    \"\"\"Asynchronous concurrent processing.\"\"\"\n\n    def __init__(self, max_concurrent: int = 10):\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n        self.results = {}\n\n    async def process_with_limit(\n        self,\n        prompt: str,\n        seed: int,\n        request_id: int\n    ):\n        \"\"\"Process with concurrency limit.\"\"\"\n        async with self.semaphore:\n            result = await self._generate_async(prompt, seed)\n            self.results[request_id] = result\n            return result\n\n    async def _generate_async(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Async generation wrapper.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None,\n            steadytext.generate,\n            prompt,\n            seed\n        )\n\n    async def process_batch_limited(\n        self,\n        prompts: List[str],\n        seeds: List[int] = None\n    ) -&gt; List[str]:\n        \"\"\"Process batch with concurrency limit.\"\"\"\n        if seeds is None:\n            seeds = [42] * len(prompts)\n\n        tasks = []\n        for i, (prompt, seed) in enumerate(zip(prompts, seeds)):\n            task = self.process_with_limit(prompt, seed, i)\n            tasks.append(task)\n\n        await asyncio.gather(*tasks)\n\n        # Return results in order\n        return [self.results[i] for i in range(len(prompts))]\n\n# Usage\nasync def concurrent_example():\n    processor = AsyncConcurrentProcessor(max_concurrent=20)\n\n    # Generate 100 prompts\n    prompts = [f\"Explain concept {i}\" for i in range(100)]\n\n    start_time = asyncio.get_event_loop().time()\n    results = await processor.process_batch_limited(prompts)\n    end_time = asyncio.get_event_loop().time()\n\n    print(f\"Processed {len(results)} prompts in {end_time - start_time:.2f}s\")\n    print(f\"Average: {(end_time - start_time) / len(results):.3f}s per prompt\")\n\n# Run\nasyncio.run(concurrent_example())\n</code></pre>"},{"location":"examples/performance-tuning/#monitoring-and-profiling","title":"Monitoring and Profiling","text":""},{"location":"examples/performance-tuning/#performance-dashboard","title":"Performance Dashboard","text":"<pre><code>import time\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import Deque, Dict, Any\nimport threading\n\n@dataclass\nclass MetricPoint:\n    \"\"\"Single metric data point.\"\"\"\n    timestamp: float\n    value: float\n    labels: Dict[str, str]\n\nclass PerformanceDashboard:\n    \"\"\"Real-time performance monitoring dashboard.\"\"\"\n\n    def __init__(self, window_size: int = 1000):\n        self.window_size = window_size\n        self.metrics: Dict[str, Deque[MetricPoint]] = {}\n        self.lock = threading.Lock()\n\n    def record_metric(self, name: str, value: float, labels: Dict[str, str] = None):\n        \"\"\"Record a metric value.\"\"\"\n        with self.lock:\n            if name not in self.metrics:\n                self.metrics[name] = deque(maxlen=self.window_size)\n\n            point = MetricPoint(\n                timestamp=time.time(),\n                value=value,\n                labels=labels or {}\n            )\n\n            self.metrics[name].append(point)\n\n    def get_stats(self, name: str, window_seconds: float = 60) -&gt; Dict[str, float]:\n        \"\"\"Get statistics for a metric.\"\"\"\n        with self.lock:\n            if name not in self.metrics:\n                return {}\n\n            current_time = time.time()\n            cutoff_time = current_time - window_seconds\n\n            # Filter points within window\n            points = [\n                p.value for p in self.metrics[name]\n                if p.timestamp &gt;= cutoff_time\n            ]\n\n            if not points:\n                return {}\n\n            return {\n                'count': len(points),\n                'mean': sum(points) / len(points),\n                'min': min(points),\n                'max': max(points),\n                'rate': len(points) / window_seconds\n            }\n\n    def print_dashboard(self):\n        \"\"\"Print performance dashboard.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"SteadyText Performance Dashboard\")\n        print(\"=\"*60)\n\n        for metric_name in sorted(self.metrics.keys()):\n            stats = self.get_stats(metric_name)\n            if stats:\n                print(f\"\\n{metric_name}:\")\n                print(f\"  Rate: {stats['rate']:.2f}/s\")\n                print(f\"  Mean: {stats['mean']:.2f}\")\n                print(f\"  Range: {stats['min']:.2f} - {stats['max']:.2f}\")\n\n# Global dashboard instance\ndashboard = PerformanceDashboard()\n\n# Instrumented generation function\ndef monitored_generate(prompt: str, seed: int = 42) -&gt; str:\n    \"\"\"Generate with monitoring.\"\"\"\n    start_time = time.perf_counter()\n\n    try:\n        result = steadytext.generate(prompt, seed=seed)\n        latency = (time.perf_counter() - start_time) * 1000\n\n        dashboard.record_metric('generation_latency_ms', latency)\n        dashboard.record_metric('generation_success', 1)\n\n        return result\n    except Exception as e:\n        dashboard.record_metric('generation_error', 1)\n        raise\n\n# Background monitoring thread\ndef monitor_system():\n    \"\"\"Monitor system metrics.\"\"\"\n    import psutil\n\n    while True:\n        # CPU usage\n        cpu_percent = psutil.cpu_percent(interval=1)\n        dashboard.record_metric('cpu_percent', cpu_percent)\n\n        # Memory usage\n        memory = psutil.virtual_memory()\n        dashboard.record_metric('memory_percent', memory.percent)\n        dashboard.record_metric('memory_mb', memory.used / 1024 / 1024)\n\n        # Print dashboard every 10 seconds\n        time.sleep(10)\n        dashboard.print_dashboard()\n\n# Start monitoring\nmonitor_thread = threading.Thread(target=monitor_system, daemon=True)\nmonitor_thread.start()\n</code></pre>"},{"location":"examples/performance-tuning/#production-optimization","title":"Production Optimization","text":""},{"location":"examples/performance-tuning/#production-configuration","title":"Production Configuration","text":"<pre><code>from typing import Dict, Any\nimport yaml\n\nclass ProductionConfig:\n    \"\"\"Production-optimized configuration.\"\"\"\n\n    @staticmethod\n    def generate_config(environment: str = 'production') -&gt; Dict[str, Any]:\n        \"\"\"Generate environment-specific configuration.\"\"\"\n\n        configs = {\n            'development': {\n                'daemon': {\n                    'enabled': False,\n                    'host': '127.0.0.1',\n                    'port': 5557\n                },\n                'cache': {\n                    'generation_capacity': 256,\n                    'generation_max_size_mb': 50,\n                    'embedding_capacity': 512,\n                    'embedding_max_size_mb': 100\n                },\n                'models': {\n                    'default_size': 'small',\n                    'preload': False\n                },\n                'monitoring': {\n                    'enabled': True,\n                    'verbose': True\n                }\n            },\n            'staging': {\n                'daemon': {\n                    'enabled': True,\n                    'host': '0.0.0.0',\n                    'port': 5557,\n                    'workers': 4\n                },\n                'cache': {\n                    'generation_capacity': 1024,\n                    'generation_max_size_mb': 200,\n                    'embedding_capacity': 2048,\n                    'embedding_max_size_mb': 400\n                },\n                'models': {\n                    'default_size': 'large',\n                    'preload': True\n                },\n                'monitoring': {\n                    'enabled': True,\n                    'verbose': False\n                }\n            },\n            'production': {\n                'daemon': {\n                    'enabled': True,\n                    'host': '0.0.0.0',\n                    'port': 5557,\n                    'workers': 16,\n                    'max_connections': 1000\n                },\n                'cache': {\n                    'generation_capacity': 4096,\n                    'generation_max_size_mb': 1024,\n                    'embedding_capacity': 8192,\n                    'embedding_max_size_mb': 2048\n                },\n                'models': {\n                    'default_size': 'large',\n                    'preload': True,\n                    'mlock': True\n                },\n                'monitoring': {\n                    'enabled': True,\n                    'verbose': False,\n                    'metrics_endpoint': '/metrics'\n                },\n                'security': {\n                    'rate_limiting': True,\n                    'max_requests_per_minute': 600,\n                    'require_auth': True\n                }\n            }\n        }\n\n        return configs.get(environment, configs['production'])\n\n    @staticmethod\n    def save_config(config: Dict[str, Any], filename: str):\n        \"\"\"Save configuration to file.\"\"\"\n        with open(filename, 'w') as f:\n            yaml.dump(config, f, default_flow_style=False)\n\n    @staticmethod\n    def apply_config(config: Dict[str, Any]):\n        \"\"\"Apply configuration to environment.\"\"\"\n        import os\n\n        # Apply cache settings\n        cache = config.get('cache', {})\n        os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = str(\n            cache.get('generation_capacity', 256)\n        )\n        os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = str(\n            cache.get('generation_max_size_mb', 50)\n        )\n\n        # Apply model settings\n        models = config.get('models', {})\n        if models.get('preload'):\n            import subprocess\n            subprocess.run(['st', 'models', 'preload'], check=True)\n\n        print(f\"Applied configuration for environment\")\n\n# Generate and apply production config\nconfig = ProductionConfig.generate_config('production')\nProductionConfig.save_config(config, 'steadytext-prod.yaml')\nProductionConfig.apply_config(config)\n</code></pre>"},{"location":"examples/performance-tuning/#health-checks","title":"Health Checks","text":"<pre><code>import asyncio\nfrom enum import Enum\nfrom typing import Dict, Any, List\n\nclass HealthStatus(Enum):\n    \"\"\"Health check status.\"\"\"\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n\nclass HealthChecker:\n    \"\"\"Production health checking.\"\"\"\n\n    def __init__(self):\n        self.checks = {}\n\n    async def check_daemon_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check daemon health.\"\"\"\n        import subprocess\n        import json\n\n        try:\n            result = subprocess.run([\n                'st', 'daemon', 'status', '--json'\n            ], capture_output=True, text=True, timeout=5)\n\n            if result.returncode == 0:\n                data = json.loads(result.stdout)\n                return {\n                    'status': HealthStatus.HEALTHY if data.get('running') else HealthStatus.UNHEALTHY,\n                    'details': data\n                }\n            else:\n                return {\n                    'status': HealthStatus.UNHEALTHY,\n                    'error': 'Daemon not responding'\n                }\n        except Exception as e:\n            return {\n                'status': HealthStatus.UNHEALTHY,\n                'error': str(e)\n            }\n\n    async def check_model_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check model availability.\"\"\"\n        try:\n            # Quick generation test\n            start = time.time()\n            result = steadytext.generate(\"health check\", seed=42)\n            latency = time.time() - start\n\n            if result and latency &lt; 5.0:\n                status = HealthStatus.HEALTHY\n            elif result and latency &lt; 10.0:\n                status = HealthStatus.DEGRADED\n            else:\n                status = HealthStatus.UNHEALTHY\n\n            return {\n                'status': status,\n                'latency': latency,\n                'model_loaded': result is not None\n            }\n        except Exception as e:\n            return {\n                'status': HealthStatus.UNHEALTHY,\n                'error': str(e)\n            }\n\n    async def check_cache_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check cache health.\"\"\"\n        try:\n            cache_manager = get_cache_manager()\n            stats = cache_manager.get_cache_stats()\n\n            # Check if caches are responsive\n            gen_size = stats.get('generation', {}).get('size', 0)\n            embed_size = stats.get('embedding', {}).get('size', 0)\n\n            return {\n                'status': HealthStatus.HEALTHY,\n                'generation_cache_size': gen_size,\n                'embedding_cache_size': embed_size\n            }\n        except Exception as e:\n            return {\n                'status': HealthStatus.UNHEALTHY,\n                'error': str(e)\n            }\n\n    async def run_all_checks(self) -&gt; Dict[str, Any]:\n        \"\"\"Run all health checks.\"\"\"\n        checks = {\n            'daemon': self.check_daemon_health(),\n            'model': self.check_model_health(),\n            'cache': self.check_cache_health()\n        }\n\n        results = {}\n        for name, check in checks.items():\n            results[name] = await check\n\n        # Overall status\n        statuses = [r['status'] for r in results.values()]\n        if all(s == HealthStatus.HEALTHY for s in statuses):\n            overall = HealthStatus.HEALTHY\n        elif any(s == HealthStatus.UNHEALTHY for s in statuses):\n            overall = HealthStatus.UNHEALTHY\n        else:\n            overall = HealthStatus.DEGRADED\n\n        return {\n            'status': overall.value,\n            'checks': results,\n            'timestamp': time.time()\n        }\n\n# Health check endpoint\nasync def health_endpoint():\n    \"\"\"Health check endpoint for monitoring.\"\"\"\n    checker = HealthChecker()\n    result = await checker.run_all_checks()\n\n    # Return appropriate HTTP status\n    if result['status'] == 'healthy':\n        return result, 200\n    elif result['status'] == 'degraded':\n        return result, 200\n    else:\n        return result, 503\n</code></pre>"},{"location":"examples/performance-tuning/#benchmarking","title":"Benchmarking","text":""},{"location":"examples/performance-tuning/#comprehensive-benchmark-suite","title":"Comprehensive Benchmark Suite","text":"<pre><code>import json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nclass BenchmarkSuite:\n    \"\"\"Comprehensive performance benchmarking.\"\"\"\n\n    def __init__(self, output_dir: str = \"./benchmarks\"):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True)\n\n    def run_latency_benchmark(self) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark operation latencies.\"\"\"\n        results = {\n            'generation': [],\n            'embedding': []\n        }\n\n        # Test different prompt lengths\n        prompt_lengths = [10, 50, 100, 500, 1000]\n\n        for length in prompt_lengths:\n            prompt = \" \".join([\"word\"] * length)\n\n            # Generation latency\n            start = time.perf_counter()\n            _ = steadytext.generate(prompt, seed=42)\n            gen_latency = (time.perf_counter() - start) * 1000\n\n            # Embedding latency\n            start = time.perf_counter()\n            _ = steadytext.embed(prompt, seed=42)\n            embed_latency = (time.perf_counter() - start) * 1000\n\n            results['generation'].append({\n                'prompt_length': length,\n                'latency_ms': gen_latency\n            })\n\n            results['embedding'].append({\n                'text_length': length,\n                'latency_ms': embed_latency\n            })\n\n        return results\n\n    def run_throughput_benchmark(self, duration_seconds: int = 60) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark throughput over time.\"\"\"\n        results = {\n            'generation': {'requests': 0, 'duration': duration_seconds},\n            'embedding': {'requests': 0, 'duration': duration_seconds}\n        }\n\n        # Generation throughput\n        start_time = time.time()\n        gen_count = 0\n        while time.time() - start_time &lt; duration_seconds / 2:\n            _ = steadytext.generate(f\"Test {gen_count}\", seed=42)\n            gen_count += 1\n        results['generation']['requests'] = gen_count\n        results['generation']['rps'] = gen_count / (duration_seconds / 2)\n\n        # Embedding throughput\n        start_time = time.time()\n        embed_count = 0\n        while time.time() - start_time &lt; duration_seconds / 2:\n            _ = steadytext.embed(f\"Test {embed_count}\", seed=42)\n            embed_count += 1\n        results['embedding']['requests'] = embed_count\n        results['embedding']['rps'] = embed_count / (duration_seconds / 2)\n\n        return results\n\n    def run_cache_benchmark(self) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark cache performance.\"\"\"\n        from steadytext import get_cache_manager\n\n        cache_manager = get_cache_manager()\n        results = {'before': {}, 'after': {}}\n\n        # Clear caches\n        cache_manager.clear_all_caches()\n\n        # Get initial stats\n        results['before'] = cache_manager.get_cache_stats()\n\n        # Generate cache misses\n        miss_times = []\n        for i in range(100):\n            start = time.perf_counter()\n            _ = steadytext.generate(f\"Unique prompt {i}\", seed=42)\n            miss_times.append((time.perf_counter() - start) * 1000)\n\n        # Generate cache hits\n        hit_times = []\n        for i in range(100):\n            start = time.perf_counter()\n            _ = steadytext.generate(f\"Unique prompt {i}\", seed=42)\n            hit_times.append((time.perf_counter() - start) * 1000)\n\n        # Get final stats\n        results['after'] = cache_manager.get_cache_stats()\n\n        results['performance'] = {\n            'miss_latency_avg': sum(miss_times) / len(miss_times),\n            'hit_latency_avg': sum(hit_times) / len(hit_times),\n            'speedup': sum(miss_times) / sum(hit_times)\n        }\n\n        return results\n\n    def run_full_benchmark(self) -&gt; Dict[str, Any]:\n        \"\"\"Run complete benchmark suite.\"\"\"\n        print(\"Running SteadyText Performance Benchmark Suite...\")\n\n        results = {\n            'timestamp': time.time(),\n            'latency': self.run_latency_benchmark(),\n            'throughput': self.run_throughput_benchmark(30),\n            'cache': self.run_cache_benchmark()\n        }\n\n        # Save results\n        output_file = self.output_dir / f\"benchmark_{int(time.time())}.json\"\n        with open(output_file, 'w') as f:\n            json.dump(results, f, indent=2)\n\n        print(f\"Benchmark complete. Results saved to {output_file}\")\n\n        # Print summary\n        self.print_summary(results)\n\n        return results\n\n    def print_summary(self, results: Dict[str, Any]):\n        \"\"\"Print benchmark summary.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"Benchmark Summary\")\n        print(\"=\"*60)\n\n        # Latency summary\n        gen_latencies = [r['latency_ms'] for r in results['latency']['generation']]\n        print(f\"\\nGeneration Latency:\")\n        print(f\"  Min: {min(gen_latencies):.2f}ms\")\n        print(f\"  Max: {max(gen_latencies):.2f}ms\")\n        print(f\"  Avg: {sum(gen_latencies)/len(gen_latencies):.2f}ms\")\n\n        # Throughput summary\n        print(f\"\\nThroughput:\")\n        print(f\"  Generation: {results['throughput']['generation']['rps']:.2f} req/s\")\n        print(f\"  Embedding: {results['throughput']['embedding']['rps']:.2f} req/s\")\n\n        # Cache summary\n        cache_perf = results['cache']['performance']\n        print(f\"\\nCache Performance:\")\n        print(f\"  Miss latency: {cache_perf['miss_latency_avg']:.2f}ms\")\n        print(f\"  Hit latency: {cache_perf['hit_latency_avg']:.2f}ms\")\n        print(f\"  Speedup: {cache_perf['speedup']:.1f}x\")\n\n# Run benchmarks\nif __name__ == \"__main__\":\n    suite = BenchmarkSuite()\n    suite.run_full_benchmark()\n</code></pre>"},{"location":"examples/performance-tuning/#best-practices","title":"Best Practices","text":""},{"location":"examples/performance-tuning/#performance-checklist","title":"Performance Checklist","text":"<ol> <li>Always use daemon mode for production deployments</li> <li>Configure caches based on workload and available memory</li> <li>Use appropriate model sizes - small for real-time, large for quality</li> <li>Batch operations when processing multiple items</li> <li>Monitor performance continuously in production</li> <li>Set resource limits to prevent runaway processes</li> <li>Use connection pooling for high-concurrency scenarios</li> <li>Implement health checks for production monitoring</li> <li>Profile regularly to identify bottlenecks</li> <li>Optimize for your hardware - use all available cores</li> </ol>"},{"location":"examples/performance-tuning/#quick-optimization-guide","title":"Quick Optimization Guide","text":"<pre><code># 1. Start daemon for 160x faster responses\nst daemon start\n\n# 2. Preload models to avoid first-request delay\nst models preload\n\n# 3. Configure optimal cache sizes\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=500\n\n# 4. Use batch processing in your code\n# 5. Monitor with built-in tools\nst cache --status\n\n# 6. Run benchmarks to validate\npython benchmarks/run_all_benchmarks.py --quick\n</code></pre>"},{"location":"examples/performance-tuning/#common-pitfalls","title":"Common Pitfalls","text":"<p>Performance Pitfalls to Avoid</p> <ul> <li>Not using daemon mode - 160x slower first requests</li> <li>Cache thrashing - Set appropriate capacity limits</li> <li>Memory leaks - Use batch processing with cleanup</li> <li>Thread contention - Limit concurrent operations</li> <li>Inefficient prompts - Keep prompts concise</li> <li>Ignoring monitoring - Always track performance metrics</li> </ul>"},{"location":"examples/postgresql-integration/","title":"PostgreSQL Integration Examples","text":"<p>This guide provides comprehensive examples for integrating SteadyText with PostgreSQL using the <code>pg_steadytext</code> extension. Learn how to build powerful applications that combine structured data with AI-generated content and embeddings.</p>"},{"location":"examples/postgresql-integration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Setup</li> <li>Content Management</li> <li>Semantic Search</li> <li>Real-time Applications</li> <li>Advanced Workflows</li> <li>Performance Optimization</li> </ul>"},{"location":"examples/postgresql-integration/#basic-setup","title":"Basic Setup","text":""},{"location":"examples/postgresql-integration/#installation-and-configuration","title":"Installation and Configuration","text":"<pre><code>-- Enable required extensions\nCREATE EXTENSION IF NOT EXISTS plpython3u CASCADE;\nCREATE EXTENSION IF NOT EXISTS omni_python CASCADE;\nCREATE EXTENSION IF NOT EXISTS pgvector CASCADE;\nCREATE EXTENSION IF NOT EXISTS pg_steadytext CASCADE;\n\n-- Verify installation\nSELECT steadytext_version();\nSELECT * FROM steadytext_config;\n\n-- Configure for your environment\nSELECT steadytext_config_set('default_max_tokens', '256');\nSELECT steadytext_config_set('default_seed', '42');\nSELECT steadytext_config_set('cache_enabled', 'true');\n\n-- Start the daemon for better performance\nSELECT steadytext_daemon_start();\nSELECT * FROM steadytext_daemon_status();\n</code></pre>"},{"location":"examples/postgresql-integration/#basic-text-generation","title":"Basic Text Generation","text":"<pre><code>-- Simple text generation (uses default seed 42)\nSELECT steadytext_generate('Write a product description for a smartphone');\n\n-- With custom parameters and error handling\nSELECT \n    CASE \n        WHEN steadytext_generate(\n            'Explain machine learning',\n            max_tokens := 200,\n            seed := 123\n        ) IS NOT NULL \n        THEN steadytext_generate(\n            'Explain machine learning',\n            max_tokens := 200,\n            seed := 123\n        )\n        ELSE 'Error: Failed to generate content. Please check daemon status.'\n    END AS result;\n\n-- Batch generation with NULL handling\nWITH prompts AS (\n    SELECT unnest(ARRAY[\n        'Describe artificial intelligence',\n        'Explain quantum computing',\n        'What is blockchain technology'\n    ]) AS prompt\n)\nSELECT \n    prompt,\n    COALESCE(\n        steadytext_generate(prompt, max_tokens := 150, seed := 42),\n        '[Generation failed for this prompt]'\n    ) AS response,\n    CASE \n        WHEN steadytext_generate(prompt, max_tokens := 150, seed := 42) IS NOT NULL \n        THEN 'Success'\n        ELSE 'Failed'\n    END AS status\nFROM prompts;\n</code></pre>"},{"location":"examples/postgresql-integration/#basic-embeddings","title":"Basic Embeddings","text":"<pre><code>-- Generate embeddings (uses default seed 42)\nSELECT steadytext_embed('artificial intelligence');\n\n-- Generate with error checking\nSELECT \n    'artificial intelligence' AS text,\n    CASE \n        WHEN steadytext_embed('artificial intelligence') IS NOT NULL \n        THEN 'Embedding generated successfully'\n        ELSE 'Embedding generation failed'\n    END AS status;\n\n-- Create a table with embeddings\nCREATE TABLE concepts (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT,\n    embedding vector(1024),\n    embedding_status TEXT DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Populate with embeddings (with error handling)\nWITH embedding_data AS (\n    SELECT \n        name,\n        description,\n        text,\n        steadytext_embed(text, seed := 42) AS embedding\n    FROM (VALUES \n        ('AI', 'Artificial Intelligence', 'artificial intelligence'),\n        ('ML', 'Machine Learning', 'machine learning'),\n        ('DL', 'Deep Learning', 'deep learning')\n    ) AS concepts(name, description, text)\n)\nINSERT INTO concepts (name, description, embedding, embedding_status)\nSELECT \n    name,\n    description,\n    embedding,\n    CASE \n        WHEN embedding IS NOT NULL THEN 'success'\n        ELSE 'failed'\n    END AS embedding_status\nFROM embedding_data;\n\n-- Find similar concepts (with NULL handling)\nWITH query_embedding AS (\n    SELECT steadytext_embed('neural networks', seed := 42) AS embedding\n)\nSELECT \n    c.name,\n    c.description,\n    CASE \n        WHEN c.embedding IS NOT NULL AND qe.embedding IS NOT NULL \n        THEN 1 - (c.embedding &lt;=&gt; qe.embedding)\n        ELSE NULL\n    END AS similarity,\n    CASE \n        WHEN c.embedding IS NULL THEN 'Missing concept embedding'\n        WHEN qe.embedding IS NULL THEN 'Query embedding failed'\n        ELSE 'OK'\n    END AS status\nFROM concepts c\nCROSS JOIN query_embedding qe\nWHERE c.embedding_status = 'success'\nORDER BY similarity DESC NULLS LAST;\n</code></pre>"},{"location":"examples/postgresql-integration/#content-management","title":"Content Management","text":""},{"location":"examples/postgresql-integration/#blog-platform","title":"Blog Platform","text":"<p>Build a complete blog platform with AI-generated content and semantic search.</p> <pre><code>-- Create blog schema\nCREATE TABLE authors (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    bio TEXT,\n    bio_embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE blog_posts (\n    id SERIAL PRIMARY KEY,\n    author_id INTEGER REFERENCES authors(id),\n    title TEXT NOT NULL,\n    content TEXT,\n    summary TEXT,\n    tags TEXT[],\n    title_embedding vector(1024),\n    content_embedding vector(1024),\n    published_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE comments (\n    id SERIAL PRIMARY KEY,\n    post_id INTEGER REFERENCES blog_posts(id),\n    author_name TEXT NOT NULL,\n    content TEXT NOT NULL,\n    sentiment_score REAL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Create indexes for vector similarity search\nCREATE INDEX ON blog_posts USING ivfflat (title_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON blog_posts USING ivfflat (content_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON authors USING ivfflat (bio_embedding vector_cosine_ops) WITH (lists = 100);\n\n-- Function to auto-generate blog content with error handling\nCREATE OR REPLACE FUNCTION generate_blog_post(\n    topic TEXT,\n    target_length INTEGER DEFAULT 500,\n    writing_style TEXT DEFAULT 'informative',\n    post_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(title TEXT, content TEXT, summary TEXT, tags TEXT[], generation_status TEXT) AS $$\nDECLARE\n    generated_title TEXT;\n    generated_content TEXT;\n    generated_summary TEXT;\n    generated_tags_text TEXT;\n    status TEXT := 'success';\nBEGIN\n    -- Generate title with error handling\n    generated_title := steadytext_generate(\n        format('Create an engaging blog post title about: %s', topic),\n        max_tokens := 20,\n        seed := post_seed\n    );\n\n    IF generated_title IS NULL THEN\n        generated_title := format('[Generated title for: %s]', topic);\n        status := 'partial_failure';\n    END IF;\n\n    -- Generate content with error handling\n    generated_content := steadytext_generate(\n        format('Write a %s word blog post about %s in a %s style', \n               target_length, topic, writing_style),\n        max_tokens := target_length,\n        seed := post_seed + 1\n    );\n\n    IF generated_content IS NULL THEN\n        generated_content := format('[Content about %s could not be generated]', topic);\n        status := 'partial_failure';\n    END IF;\n\n    -- Generate summary with error handling\n    IF generated_content IS NOT NULL AND generated_content NOT LIKE '[Content%' THEN\n        generated_summary := steadytext_generate(\n            format('Write a brief summary of this blog post: %s', generated_content),\n            max_tokens := 100,\n            seed := post_seed + 2\n        );\n    END IF;\n\n    IF generated_summary IS NULL THEN\n        generated_summary := format('Summary of %s blog post', topic);\n        status := 'partial_failure';\n    END IF;\n\n    -- Generate tags with error handling\n    generated_tags_text := steadytext_generate(\n        format('List 5 relevant tags for a blog post about: %s (comma-separated)', topic),\n        max_tokens := 30,\n        seed := post_seed + 3\n    );\n\n    IF generated_tags_text IS NOT NULL THEN\n        tags := string_to_array(generated_tags_text, ',');\n    ELSE\n        tags := ARRAY[topic, 'technology', 'blog'];\n        status := 'partial_failure';\n    END IF;\n\n    -- Return results\n    title := generated_title;\n    content := generated_content;\n    summary := generated_summary;\n    generation_status := status;\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Auto-populate blog with generated content (with error handling)\nWITH author_embeddings AS (\n    SELECT \n        name,\n        bio,\n        steadytext_embed(bio, seed := 100 + ROW_NUMBER() OVER()) AS bio_embedding\n    FROM (VALUES \n        ('AI Writer', 'An AI-powered content creator specializing in technology topics.'),\n        ('Tech Analyst', 'Expert in emerging technologies and digital transformation.')\n    ) AS authors(name, bio)\n)\nINSERT INTO authors (name, bio, bio_embedding)\nSELECT \n    name,\n    bio,\n    CASE \n        WHEN bio_embedding IS NOT NULL THEN bio_embedding\n        ELSE vector(array_fill(0::real, ARRAY[1024]))  -- Zero vector fallback\n    END\nFROM author_embeddings;\n\n-- Generate blog posts with comprehensive error handling\nWITH generated_posts AS (\n    SELECT \n        1 as author_id,\n        'Machine Learning' as topic,\n        generate_blog_post('Machine Learning', 400, 'technical', 200) as post_data\n    UNION ALL\n    SELECT \n        2 as author_id,\n        'Quantum Computing' as topic,\n        generate_blog_post('Quantum Computing', 350, 'educational', 300) as post_data\n    UNION ALL\n    SELECT \n        1 as author_id,\n        'Blockchain Technology' as topic,\n        generate_blog_post('Blockchain Technology', 450, 'analytical', 400) as post_data\n),\nembedded_posts AS (\n    SELECT \n        gp.author_id,\n        (gp.post_data).title,\n        (gp.post_data).content,\n        (gp.post_data).summary,\n        (gp.post_data).tags,\n        (gp.post_data).generation_status,\n        steadytext_embed((gp.post_data).title, seed := 500) AS title_embedding,\n        steadytext_embed((gp.post_data).content, seed := 600) AS content_embedding\n    FROM generated_posts gp\n)\nINSERT INTO blog_posts (author_id, title, content, summary, tags, title_embedding, content_embedding)\nSELECT \n    author_id,\n    title,\n    content,\n    summary,\n    tags,\n    COALESCE(title_embedding, vector(array_fill(0::real, ARRAY[1024]))),\n    COALESCE(content_embedding, vector(array_fill(0::real, ARRAY[1024])))\nFROM embedded_posts\nWHERE generation_status IS NOT NULL;  -- Only insert posts that were generated\n\n-- Semantic blog search function with NULL handling\nCREATE OR REPLACE FUNCTION search_blog_posts(\n    search_query TEXT,\n    max_results INTEGER DEFAULT 10,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    id INTEGER,\n    title TEXT,\n    summary TEXT,\n    author_name TEXT,\n    similarity_score REAL,\n    published_at TIMESTAMP,\n    search_status TEXT\n) AS $$\nDECLARE\n    query_embedding vector(1024);\nBEGIN\n    -- Generate query embedding with error handling\n    query_embedding := steadytext_embed(search_query, seed := search_seed);\n\n    -- Return empty result if embedding generation failed\n    IF query_embedding IS NULL THEN\n        RAISE WARNING 'Failed to generate embedding for search query: %', search_query;\n        RETURN;\n    END IF;\n\n    RETURN QUERY\n    SELECT \n        bp.id,\n        bp.title,\n        bp.summary,\n        a.name as author_name,\n        1 - (bp.content_embedding &lt;=&gt; query_embedding) AS similarity_score,\n        bp.published_at,\n        'success' AS search_status\n    FROM blog_posts bp\n    JOIN authors a ON bp.author_id = a.id\n    WHERE bp.content_embedding IS NOT NULL\n    ORDER BY bp.content_embedding &lt;=&gt; query_embedding\n    LIMIT max_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Search for posts with status checking\nSELECT \n    id,\n    title,\n    summary,\n    author_name,\n    similarity_score,\n    published_at,\n    search_status\nFROM search_blog_posts('artificial intelligence applications', 5);\n\n-- Handle failed searches\nWITH search_results AS (\n    SELECT * FROM search_blog_posts('future technology trends', 3)\n)\nSELECT \n    CASE \n        WHEN COUNT(*) &gt; 0 THEN 'Search completed successfully'\n        ELSE 'Search failed - no results returned'\n    END AS search_summary,\n    COUNT(*) AS result_count\nFROM search_results;\n\n-- Auto-generate related posts\nCREATE OR REPLACE FUNCTION suggest_related_posts(\n    post_id_input INTEGER,\n    max_suggestions INTEGER DEFAULT 3\n)\nRETURNS TABLE(\n    suggested_id INTEGER,\n    suggested_title TEXT,\n    similarity_score REAL\n) AS $$\nDECLARE\n    source_embedding vector(1024);\nBEGIN\n    -- Get the embedding of the source post\n    SELECT content_embedding INTO source_embedding\n    FROM blog_posts\n    WHERE id = post_id_input;\n\n    IF source_embedding IS NULL THEN\n        RAISE EXCEPTION 'Post not found or has no embedding';\n    END IF;\n\n    RETURN QUERY\n    SELECT \n        bp.id as suggested_id,\n        bp.title as suggested_title,\n        1 - (bp.content_embedding &lt;=&gt; source_embedding) AS similarity_score\n    FROM blog_posts bp\n    WHERE bp.id != post_id_input \n      AND bp.content_embedding IS NOT NULL\n    ORDER BY bp.content_embedding &lt;=&gt; source_embedding\n    LIMIT max_suggestions;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Find related posts\nSELECT * FROM suggest_related_posts(1, 3);\n</code></pre>"},{"location":"examples/postgresql-integration/#e-commerce-product-catalog","title":"E-commerce Product Catalog","text":"<p>Create an intelligent product catalog with AI-generated descriptions and semantic search.</p> <pre><code>-- E-commerce schema\nCREATE TABLE categories (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT,\n    description_embedding vector(1024)\n);\n\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    category_id INTEGER REFERENCES categories(id),\n    name TEXT NOT NULL,\n    base_features JSONB,\n    ai_description TEXT,\n    ai_marketing_copy TEXT,\n    technical_specs TEXT,\n    price DECIMAL(10,2),\n    embedding vector(1024),\n    marketing_embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE product_reviews (\n    id SERIAL PRIMARY KEY,\n    product_id INTEGER REFERENCES products(id),\n    customer_name TEXT,\n    rating INTEGER CHECK (rating BETWEEN 1 AND 5),\n    review_text TEXT,\n    sentiment_analysis TEXT,\n    embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Indexes for vector search\nCREATE INDEX ON products USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON products USING ivfflat (marketing_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON product_reviews USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n\n-- Product content generation function\nCREATE OR REPLACE FUNCTION generate_product_content(\n    product_name TEXT,\n    features JSONB,\n    category_name TEXT,\n    price_value DECIMAL,\n    content_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    description TEXT,\n    marketing_copy TEXT,\n    technical_specs TEXT\n) AS $$\nDECLARE\n    features_text TEXT;\nBEGIN\n    -- Convert JSONB features to readable text\n    features_text := (\n        SELECT string_agg(key || ': ' || value, ', ')\n        FROM jsonb_each_text(features)\n    );\n\n    -- Generate product description\n    description := steadytext_generate(\n        format('Write a detailed product description for %s in the %s category. Features: %s. Price: $%s',\n               product_name, category_name, features_text, price_value),\n        max_tokens := 200,\n        seed := content_seed\n    );\n\n    -- Generate marketing copy\n    marketing_copy := steadytext_generate(\n        format('Create compelling marketing copy for %s. Highlight benefits and unique selling points. Features: %s',\n               product_name, features_text),\n        max_tokens := 150,\n        seed := content_seed + 100\n    );\n\n    -- Generate technical specifications\n    technical_specs := steadytext_generate(\n        format('Create detailed technical specifications for %s based on these features: %s',\n               product_name, features_text),\n        max_tokens := 250,\n        seed := content_seed + 200\n    );\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Populate categories\nINSERT INTO categories (name, description, description_embedding)\nVALUES \n    ('Smartphones', 'Mobile devices with advanced computing capabilities',\n     steadytext_embed('Mobile devices with advanced computing capabilities', seed := 1000)),\n    ('Laptops', 'Portable computers for professional and personal use',\n     steadytext_embed('Portable computers for professional and personal use', seed := 1001)),\n    ('Smartwatches', 'Wearable devices with health and connectivity features',\n     steadytext_embed('Wearable devices with health and connectivity features', seed := 1002));\n\n-- Generate products with AI content\nWITH product_data AS (\n    SELECT \n        'iPhone 15 Pro' as name,\n        1 as category_id,\n        '{\"screen\": \"6.1 inch OLED\", \"storage\": \"256GB\", \"camera\": \"48MP\", \"battery\": \"3274mAh\"}'::jsonb as features,\n        'Smartphones' as category_name,\n        999.99 as price,\n        2000 as seed\n    UNION ALL\n    SELECT \n        'MacBook Air M3' as name,\n        2 as category_id,\n        '{\"processor\": \"Apple M3\", \"memory\": \"16GB\", \"storage\": \"512GB SSD\", \"display\": \"13.6 inch Retina\"}'::jsonb as features,\n        'Laptops' as category_name,\n        1299.99 as price,\n        2100 as seed\n    UNION ALL\n    SELECT \n        'Apple Watch Series 9' as name,\n        3 as category_id,\n        '{\"display\": \"45mm Always-On Retina\", \"health\": \"Blood Oxygen, ECG\", \"battery\": \"18 hours\", \"connectivity\": \"GPS + Cellular\"}'::jsonb as features,\n        'Smartwatches' as category_name,\n        429.99 as price,\n        2200 as seed\n)\nINSERT INTO products (category_id, name, base_features, ai_description, ai_marketing_copy, technical_specs, price, embedding, marketing_embedding)\nSELECT \n    pd.category_id,\n    pd.name,\n    pd.features,\n    (generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).description,\n    (generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).marketing_copy,\n    (generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).technical_specs,\n    pd.price,\n    steadytext_embed(pd.name || ' ' || (generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).description, seed := 3000),\n    steadytext_embed((generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).marketing_copy, seed := 3100)\nFROM product_data pd;\n\n-- Product search functions\nCREATE OR REPLACE FUNCTION search_products(\n    search_query TEXT,\n    search_type TEXT DEFAULT 'general', -- 'general', 'marketing', 'technical'\n    max_results INTEGER DEFAULT 10,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    id INTEGER,\n    name TEXT,\n    description TEXT,\n    price DECIMAL,\n    category_name TEXT,\n    similarity_score REAL\n) AS $$\nDECLARE\n    query_embedding vector(1024);\n    embedding_column TEXT;\nBEGIN\n    -- Generate embedding for search query\n    query_embedding := steadytext_embed(search_query, seed := search_seed);\n\n    -- Choose embedding column based on search type\n    embedding_column := CASE search_type\n        WHEN 'marketing' THEN 'marketing_embedding'\n        ELSE 'embedding'\n    END;\n\n    RETURN QUERY EXECUTE format('\n        SELECT \n            p.id,\n            p.name,\n            p.ai_description as description,\n            p.price,\n            c.name as category_name,\n            1 - (p.%I &lt;=&gt; $1) AS similarity_score\n        FROM products p\n        JOIN categories c ON p.category_id = c.id\n        WHERE p.%I IS NOT NULL\n        ORDER BY p.%I &lt;=&gt; $1\n        LIMIT $2\n    ', embedding_column, embedding_column, embedding_column)\n    USING query_embedding, max_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Search examples\nSELECT * FROM search_products('high-performance laptop for programming', 'general', 5);\nSELECT * FROM search_products('best smartphone camera', 'marketing', 3);\n\n-- Auto-generate product comparisons\nCREATE OR REPLACE FUNCTION compare_products(\n    product_ids INTEGER[],\n    comparison_seed INTEGER DEFAULT 42\n)\nRETURNS TEXT AS $$\nDECLARE\n    product_info TEXT;\n    comparison_result TEXT;\nBEGIN\n    -- Gather product information\n    SELECT string_agg(\n        format('%s: %s (Price: $%s)', name, ai_description, price),\n        E'\\n'\n    ) INTO product_info\n    FROM products\n    WHERE id = ANY(product_ids);\n\n    -- Generate comparison\n    comparison_result := steadytext_generate(\n        format('Compare these products and highlight their key differences and advantages:\\n%s', product_info),\n        max_tokens := 400,\n        seed := comparison_seed\n    );\n\n    RETURN comparison_result;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Compare products\nSELECT compare_products(ARRAY[1, 2, 3], 4000);\n</code></pre>"},{"location":"examples/postgresql-integration/#semantic-search","title":"Semantic Search","text":""},{"location":"examples/postgresql-integration/#document-management-system","title":"Document Management System","text":"<p>Build a comprehensive document search and analysis system.</p> <pre><code>-- Document management schema\nCREATE TABLE document_types (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT,\n    processing_instructions TEXT,\n    embedding vector(1024)\n);\n\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    type_id INTEGER REFERENCES document_types(id),\n    title TEXT NOT NULL,\n    content TEXT NOT NULL,\n    summary TEXT,\n    key_points TEXT[],\n    metadata JSONB,\n    content_embedding vector(1024),\n    summary_embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW(),\n    processed_at TIMESTAMP\n);\n\nCREATE TABLE document_sections (\n    id SERIAL PRIMARY KEY,\n    document_id INTEGER REFERENCES documents(id),\n    section_title TEXT,\n    section_content TEXT NOT NULL,\n    section_order INTEGER,\n    embedding vector(1024)\n);\n\nCREATE TABLE document_relationships (\n    id SERIAL PRIMARY KEY,\n    source_doc_id INTEGER REFERENCES documents(id),\n    target_doc_id INTEGER REFERENCES documents(id),\n    relationship_type TEXT, -- 'similar', 'references', 'updated_version'\n    confidence_score REAL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Indexes for document search\nCREATE INDEX ON documents USING ivfflat (content_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON documents USING ivfflat (summary_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON document_sections USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n\n-- Document processing function\nCREATE OR REPLACE FUNCTION process_document(\n    doc_id INTEGER,\n    processing_seed INTEGER DEFAULT 42\n)\nRETURNS BOOLEAN AS $$\nDECLARE\n    doc_record documents%ROWTYPE;\n    doc_summary TEXT;\n    doc_key_points TEXT[];\n    section_texts TEXT[];\n    section_text TEXT;\n    i INTEGER;\nBEGIN\n    -- Get document\n    SELECT * INTO doc_record FROM documents WHERE id = doc_id;\n\n    IF NOT FOUND THEN\n        RAISE EXCEPTION 'Document not found';\n    END IF;\n\n    -- Generate summary\n    doc_summary := steadytext_generate(\n        format('Summarize the following document in 2-3 sentences:\\n%s', \n               left(doc_record.content, 2000)),\n        max_tokens := 150,\n        seed := processing_seed\n    );\n\n    -- Extract key points\n    SELECT string_to_array(\n        steadytext_generate(\n            format('Extract 5 key points from this document (one per line):\\n%s',\n                   left(doc_record.content, 1500)),\n            max_tokens := 200,\n            seed := processing_seed + 100\n        ),\n        E'\\n'\n    ) INTO doc_key_points;\n\n    -- Update document with processed information\n    UPDATE documents SET\n        summary = doc_summary,\n        key_points = doc_key_points,\n        content_embedding = steadytext_embed(doc_record.content, seed := processing_seed + 200),\n        summary_embedding = steadytext_embed(doc_summary, seed := processing_seed + 300),\n        processed_at = NOW()\n    WHERE id = doc_id;\n\n    -- Create document sections (split content into chunks)\n    section_texts := string_to_array(doc_record.content, E'\\n\\n');\n\n    FOR i IN 1..array_length(section_texts, 1) LOOP\n        section_text := section_texts[i];\n        IF length(section_text) &gt; 50 THEN  -- Only process substantial sections\n            INSERT INTO document_sections (document_id, section_content, section_order, embedding)\n            VALUES (\n                doc_id,\n                section_text,\n                i,\n                steadytext_embed(section_text, seed := processing_seed + 400 + i)\n            );\n        END IF;\n    END LOOP;\n\n    RETURN TRUE;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Advanced search function\nCREATE OR REPLACE FUNCTION search_documents(\n    search_query TEXT,\n    search_mode TEXT DEFAULT 'content', -- 'content', 'summary', 'sections'\n    doc_type_filter INTEGER DEFAULT NULL,\n    max_results INTEGER DEFAULT 10,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    document_id INTEGER,\n    title TEXT,\n    summary TEXT,\n    similarity_score REAL,\n    doc_type TEXT,\n    section_match TEXT\n) AS $$\nDECLARE\n    query_embedding vector(1024);\nBEGIN\n    query_embedding := steadytext_embed(search_query, seed := search_seed);\n\n    IF search_mode = 'sections' THEN\n        -- Search within document sections\n        RETURN QUERY\n        SELECT \n            d.id as document_id,\n            d.title,\n            d.summary,\n            1 - (ds.embedding &lt;=&gt; query_embedding) AS similarity_score,\n            dt.name as doc_type,\n            left(ds.section_content, 200) as section_match\n        FROM document_sections ds\n        JOIN documents d ON ds.document_id = d.id\n        JOIN document_types dt ON d.type_id = dt.id\n        WHERE (doc_type_filter IS NULL OR d.type_id = doc_type_filter)\n          AND ds.embedding IS NOT NULL\n        ORDER BY ds.embedding &lt;=&gt; query_embedding\n        LIMIT max_results;\n\n    ELSIF search_mode = 'summary' THEN\n        -- Search within summaries\n        RETURN QUERY\n        SELECT \n            d.id as document_id,\n            d.title,\n            d.summary,\n            1 - (d.summary_embedding &lt;=&gt; query_embedding) AS similarity_score,\n            dt.name as doc_type,\n            NULL::TEXT as section_match\n        FROM documents d\n        JOIN document_types dt ON d.type_id = dt.id\n        WHERE (doc_type_filter IS NULL OR d.type_id = doc_type_filter)\n          AND d.summary_embedding IS NOT NULL\n        ORDER BY d.summary_embedding &lt;=&gt; query_embedding\n        LIMIT max_results;\n\n    ELSE\n        -- Default: search within full content\n        RETURN QUERY\n        SELECT \n            d.id as document_id,\n            d.title,\n            d.summary,\n            1 - (d.content_embedding &lt;=&gt; query_embedding) AS similarity_score,\n            dt.name as doc_type,\n            NULL::TEXT as section_match\n        FROM documents d\n        JOIN document_types dt ON d.type_id = dt.id\n        WHERE (doc_type_filter IS NULL OR d.type_id = doc_type_filter)\n          AND d.content_embedding IS NOT NULL\n        ORDER BY d.content_embedding &lt;=&gt; query_embedding\n        LIMIT max_results;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Setup document types and sample data\nINSERT INTO document_types (name, description, processing_instructions, embedding)\nVALUES \n    ('Technical Manual', 'Technical documentation and user guides',\n     'Focus on technical details and step-by-step instructions',\n     steadytext_embed('Technical documentation and user guides', seed := 5000)),\n    ('Research Paper', 'Academic and research publications',\n     'Extract methodology, results, and conclusions',\n     steadytext_embed('Academic and research publications', seed := 5001)),\n    ('Legal Document', 'Contracts, agreements, and legal texts',\n     'Identify key terms, obligations, and legal implications',\n     steadytext_embed('Contracts, agreements, and legal texts', seed := 5002));\n\n-- Add sample documents\nINSERT INTO documents (type_id, title, content)\nVALUES \n    (1, 'API Integration Guide', \n     'This guide explains how to integrate our REST API into your application. The API provides endpoints for user authentication, data retrieval, and real-time updates. Authentication is handled via JWT tokens that must be included in request headers. Rate limiting is enforced at 1000 requests per hour per API key.'),\n    (2, 'Machine Learning in Healthcare',\n     'Recent advances in machine learning have shown significant promise in healthcare applications. This study examines the effectiveness of neural networks in medical diagnosis, analyzing data from 10,000 patient records. Results indicate 95% accuracy in early disease detection when compared to traditional diagnostic methods.'),\n    (3, 'Software License Agreement',\n     'This agreement governs the use of the software product. The licensee agrees to use the software solely for internal business purposes. Redistribution is prohibited without written consent. The license term is perpetual but may be terminated for breach of terms. Limitation of liability applies to all claims.');\n\n-- Process all documents\nSELECT process_document(id, 6000 + id) FROM documents;\n\n-- Search examples\nSELECT * FROM search_documents('API authentication methods', 'content', NULL, 5);\nSELECT * FROM search_documents('machine learning accuracy', 'summary', 2, 3);\nSELECT * FROM search_documents('license terms', 'sections', 3, 5);\n\n-- Find document relationships\nCREATE OR REPLACE FUNCTION find_related_documents(\n    source_doc_id INTEGER,\n    similarity_threshold REAL DEFAULT 0.7,\n    max_related INTEGER DEFAULT 5\n)\nRETURNS TABLE(\n    related_doc_id INTEGER,\n    related_title TEXT,\n    similarity_score REAL,\n    relationship_type TEXT\n) AS $$\nDECLARE\n    source_embedding vector(1024);\n    source_type_id INTEGER;\nBEGIN\n    -- Get source document embedding and type\n    SELECT content_embedding, type_id \n    INTO source_embedding, source_type_id\n    FROM documents \n    WHERE id = source_doc_id;\n\n    IF source_embedding IS NULL THEN\n        RAISE EXCEPTION 'Source document not found or not processed';\n    END IF;\n\n    RETURN QUERY\n    SELECT \n        d.id as related_doc_id,\n        d.title as related_title,\n        1 - (d.content_embedding &lt;=&gt; source_embedding) AS similarity_score,\n        CASE \n            WHEN d.type_id = source_type_id THEN 'same_type'\n            ELSE 'different_type'\n        END as relationship_type\n    FROM documents d\n    WHERE d.id != source_doc_id\n      AND d.content_embedding IS NOT NULL\n      AND 1 - (d.content_embedding &lt;=&gt; source_embedding) &gt;= similarity_threshold\n    ORDER BY d.content_embedding &lt;=&gt; source_embedding\n    LIMIT max_related;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Find related documents\nSELECT * FROM find_related_documents(1, 0.5, 3);\n</code></pre>"},{"location":"examples/postgresql-integration/#real-time-applications","title":"Real-time Applications","text":""},{"location":"examples/postgresql-integration/#chat-system-with-ai-assistance","title":"Chat System with AI Assistance","text":"<p>Build a real-time chat system with AI-powered features.</p> <pre><code>-- Chat system schema\nCREATE TABLE chat_rooms (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT,\n    ai_assistant_enabled BOOLEAN DEFAULT false,\n    ai_personality TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE chat_participants (\n    id SERIAL PRIMARY KEY,\n    room_id INTEGER REFERENCES chat_rooms(id),\n    user_name TEXT NOT NULL,\n    joined_at TIMESTAMP DEFAULT NOW(),\n    is_ai_user BOOLEAN DEFAULT false\n);\n\nCREATE TABLE chat_messages (\n    id SERIAL PRIMARY KEY,\n    room_id INTEGER REFERENCES chat_rooms(id),\n    participant_id INTEGER REFERENCES chat_participants(id),\n    message_text TEXT NOT NULL,\n    ai_generated BOOLEAN DEFAULT false,\n    ai_confidence REAL,\n    message_embedding vector(1024),\n    reply_to_message_id INTEGER REFERENCES chat_messages(id),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE chat_context (\n    id SERIAL PRIMARY KEY,\n    room_id INTEGER REFERENCES chat_rooms(id),\n    context_summary TEXT,\n    key_topics TEXT[],\n    context_embedding vector(1024),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Indexes for chat search and recommendations\nCREATE INDEX ON chat_messages USING ivfflat (message_embedding vector_cosine_ops) WITH (lists = 50);\nCREATE INDEX ON chat_context USING ivfflat (context_embedding vector_cosine_ops) WITH (lists = 20);\nCREATE INDEX ON chat_messages (room_id, created_at DESC);\n\n-- AI assistant function\nCREATE OR REPLACE FUNCTION generate_ai_response(\n    room_id_input INTEGER,\n    context_messages TEXT,\n    ai_personality_input TEXT DEFAULT 'helpful assistant',\n    response_seed INTEGER DEFAULT 42\n)\nRETURNS TEXT AS $$\nDECLARE\n    ai_response TEXT;\n    prompt_text TEXT;\nBEGIN\n    -- Build context-aware prompt\n    prompt_text := format(\n        'You are a %s in a chat room. Based on this recent conversation context, provide a helpful and relevant response:\\n\\n%s\\n\\nResponse:',\n        ai_personality_input,\n        context_messages\n    );\n\n    -- Generate AI response\n    ai_response := steadytext_generate(\n        prompt_text,\n        max_tokens := 200,\n        seed := response_seed\n    );\n\n    RETURN ai_response;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Context summarization function\nCREATE OR REPLACE FUNCTION update_chat_context(\n    room_id_input INTEGER,\n    context_seed INTEGER DEFAULT 42\n)\nRETURNS BOOLEAN AS $$\nDECLARE\n    recent_messages TEXT;\n    context_summary_new TEXT;\n    key_topics_new TEXT[];\nBEGIN\n    -- Get recent messages (last 20)\n    SELECT string_agg(\n        format('[%s] %s: %s', \n               to_char(cm.created_at, 'HH24:MI'), \n               cp.user_name, \n               cm.message_text),\n        E'\\n' ORDER BY cm.created_at DESC\n    )\n    INTO recent_messages\n    FROM chat_messages cm\n    JOIN chat_participants cp ON cm.participant_id = cp.id\n    WHERE cm.room_id = room_id_input\n      AND cm.created_at &gt; NOW() - INTERVAL '1 hour'\n    LIMIT 20;\n\n    IF recent_messages IS NULL THEN\n        RETURN FALSE;\n    END IF;\n\n    -- Generate context summary\n    context_summary_new := steadytext_generate(\n        format('Summarize the key points and current discussion topics from this chat conversation:\\n%s', recent_messages),\n        max_tokens := 150,\n        seed := context_seed\n    );\n\n    -- Extract key topics\n    SELECT string_to_array(\n        steadytext_generate(\n            format('Extract 5 main topics being discussed in this chat (comma-separated):\\n%s', recent_messages),\n            max_tokens := 50,\n            seed := context_seed + 100\n        ),\n        ','\n    ) INTO key_topics_new;\n\n    -- Update or insert context\n    INSERT INTO chat_context (room_id, context_summary, key_topics, context_embedding, updated_at)\n    VALUES (\n        room_id_input,\n        context_summary_new,\n        key_topics_new,\n        steadytext_embed(context_summary_new, seed := context_seed + 200),\n        NOW()\n    )\n    ON CONFLICT (room_id) DO UPDATE SET\n        context_summary = EXCLUDED.context_summary,\n        key_topics = EXCLUDED.key_topics,\n        context_embedding = EXCLUDED.context_embedding,\n        updated_at = EXCLUDED.updated_at;\n\n    RETURN TRUE;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Message processing trigger function\nCREATE OR REPLACE FUNCTION process_chat_message()\nRETURNS TRIGGER AS $$\nDECLARE\n    room_has_ai BOOLEAN;\n    ai_personality_val TEXT;\n    recent_context TEXT;\n    ai_participant_id INTEGER;\n    ai_response TEXT;\nBEGIN\n    -- Generate embedding for the new message\n    NEW.message_embedding := steadytext_embed(NEW.message_text, seed := 42);\n\n    -- Check if room has AI assistant enabled\n    SELECT ai_assistant_enabled, ai_personality \n    INTO room_has_ai, ai_personality_val\n    FROM chat_rooms \n    WHERE id = NEW.room_id;\n\n    -- If AI is enabled and this isn't an AI message, potentially generate response\n    IF room_has_ai AND NOT NEW.ai_generated THEN\n        -- Get recent conversation context\n        SELECT string_agg(\n            format('%s: %s', cp.user_name, cm.message_text),\n            E'\\n' ORDER BY cm.created_at DESC\n        )\n        INTO recent_context\n        FROM chat_messages cm\n        JOIN chat_participants cp ON cm.participant_id = cp.id\n        WHERE cm.room_id = NEW.room_id\n          AND cm.created_at &gt; NOW() - INTERVAL '10 minutes'\n        LIMIT 5;\n\n        -- Decide if AI should respond (simple logic - respond to questions or mentions)\n        IF NEW.message_text ILIKE '%?%' OR NEW.message_text ILIKE '%ai%' THEN\n            -- Get AI participant\n            SELECT id INTO ai_participant_id\n            FROM chat_participants\n            WHERE room_id = NEW.room_id AND is_ai_user = true\n            LIMIT 1;\n\n            IF ai_participant_id IS NOT NULL THEN\n                -- Generate AI response\n                ai_response := generate_ai_response(\n                    NEW.room_id,\n                    recent_context,\n                    ai_personality_val,\n                    extract(epoch from NOW())::integer % 10000\n                );\n\n                -- Insert AI response\n                INSERT INTO chat_messages (\n                    room_id, \n                    participant_id, \n                    message_text, \n                    ai_generated, \n                    ai_confidence,\n                    reply_to_message_id\n                )\n                VALUES (\n                    NEW.room_id,\n                    ai_participant_id,\n                    ai_response,\n                    true,\n                    0.8,\n                    NEW.id\n                );\n            END IF;\n        END IF;\n\n        -- Update chat context periodically\n        PERFORM update_chat_context(NEW.room_id);\n    END IF;\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create trigger for message processing\nCREATE TRIGGER chat_message_processing\n    BEFORE INSERT ON chat_messages\n    FOR EACH ROW\n    EXECUTE FUNCTION process_chat_message();\n\n-- Message search function\nCREATE OR REPLACE FUNCTION search_chat_history(\n    room_id_input INTEGER,\n    search_query TEXT,\n    max_results INTEGER DEFAULT 10,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    message_id INTEGER,\n    user_name TEXT,\n    message_text TEXT,\n    similarity_score REAL,\n    created_at TIMESTAMP\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT \n        cm.id as message_id,\n        cp.user_name,\n        cm.message_text,\n        1 - (cm.message_embedding &lt;=&gt; steadytext_embed(search_query, seed := search_seed)) AS similarity_score,\n        cm.created_at\n    FROM chat_messages cm\n    JOIN chat_participants cp ON cm.participant_id = cp.id\n    WHERE cm.room_id = room_id_input\n      AND cm.message_embedding IS NOT NULL\n    ORDER BY cm.message_embedding &lt;=&gt; steadytext_embed(search_query, seed := search_seed)\n    LIMIT max_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Setup sample chat room\nINSERT INTO chat_rooms (name, description, ai_assistant_enabled, ai_personality)\nVALUES ('Tech Discussion', 'General technology discussion room', true, 'knowledgeable tech expert');\n\n-- Add participants\nINSERT INTO chat_participants (room_id, user_name, is_ai_user)\nVALUES \n    (1, 'Alice', false),\n    (1, 'Bob', false),\n    (1, 'TechBot', true);\n\n-- Simulate conversation\nINSERT INTO chat_messages (room_id, participant_id, message_text, ai_generated)\nVALUES \n    (1, 1, 'Hi everyone! What do you think about the latest AI developments?', false),\n    (1, 2, 'I think machine learning is advancing really fast. What are your thoughts on GPT models?', false);\n\n-- Search chat history\nSELECT * FROM search_chat_history(1, 'artificial intelligence developments', 5);\n</code></pre>"},{"location":"examples/postgresql-integration/#advanced-workflows","title":"Advanced Workflows","text":""},{"location":"examples/postgresql-integration/#content-pipeline-with-quality-control","title":"Content Pipeline with Quality Control","text":"<pre><code>-- Content pipeline schema\nCREATE TABLE content_templates (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    template_text TEXT NOT NULL,\n    variables JSONB,\n    quality_criteria JSONB,\n    embedding vector(1024)\n);\n\nCREATE TABLE content_jobs (\n    id SERIAL PRIMARY KEY,\n    template_id INTEGER REFERENCES content_templates(id),\n    input_parameters JSONB,\n    status TEXT DEFAULT 'pending', -- pending, processing, completed, failed, review_needed\n    priority INTEGER DEFAULT 5,\n    created_at TIMESTAMP DEFAULT NOW(),\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP\n);\n\nCREATE TABLE generated_content (\n    id SERIAL PRIMARY KEY,\n    job_id INTEGER REFERENCES content_jobs(id),\n    content_text TEXT NOT NULL,\n    quality_score REAL,\n    quality_issues TEXT[],\n    embedding vector(1024),\n    approved BOOLEAN DEFAULT false,\n    human_review_notes TEXT,\n    generation_seed INTEGER,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Quality assessment function\nCREATE OR REPLACE FUNCTION assess_content_quality(\n    content_text TEXT,\n    quality_criteria JSONB,\n    assessment_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    quality_score REAL,\n    quality_issues TEXT[]\n) AS $$\nDECLARE\n    criteria_text TEXT;\n    assessment_prompt TEXT;\n    assessment_result TEXT;\n    score_match TEXT;\n    issues_text TEXT;\nBEGIN\n    -- Convert criteria to readable format\n    SELECT string_agg(key || ': ' || value, ', ')\n    INTO criteria_text\n    FROM jsonb_each_text(quality_criteria);\n\n    -- Generate quality assessment\n    assessment_prompt := format(\n        'Assess the quality of this content against these criteria: %s\\n\\nContent to assess:\\n%s\\n\\nProvide a score from 0.0 to 1.0 and list any issues (format: Score: X.X, Issues: issue1, issue2)',\n        criteria_text,\n        content_text\n    );\n\n    assessment_result := steadytext_generate(\n        assessment_prompt,\n        max_tokens := 200,\n        seed := assessment_seed\n    );\n\n    -- Extract score (simple pattern matching)\n    score_match := substring(assessment_result from 'Score: ([0-9.]+)');\n    quality_score := COALESCE(score_match::real, 0.5);\n\n    -- Extract issues\n    issues_text := substring(assessment_result from 'Issues: (.+)');\n    IF issues_text IS NOT NULL THEN\n        quality_issues := string_to_array(trim(issues_text), ', ');\n    ELSE\n        quality_issues := ARRAY[]::TEXT[];\n    END IF;\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Content generation worker function\nCREATE OR REPLACE FUNCTION process_content_job(\n    job_id_input INTEGER\n)\nRETURNS BOOLEAN AS $$\nDECLARE\n    job_record content_jobs%ROWTYPE;\n    template_record content_templates%ROWTYPE;\n    generation_prompt TEXT;\n    generated_text TEXT;\n    quality_result RECORD;\n    generation_seed INTEGER;\nBEGIN\n    -- Get job and template\n    SELECT * INTO job_record FROM content_jobs WHERE id = job_id_input;\n    SELECT * INTO template_record FROM content_templates WHERE id = job_record.template_id;\n\n    IF NOT FOUND THEN\n        RAISE EXCEPTION 'Job or template not found';\n    END IF;\n\n    -- Update job status\n    UPDATE content_jobs SET \n        status = 'processing',\n        started_at = NOW()\n    WHERE id = job_id_input;\n\n    -- Build generation prompt by substituting variables\n    generation_prompt := template_record.template_text;\n\n    -- Simple variable substitution (in practice, you'd want more sophisticated templating)\n    FOR variable_key, variable_value IN \n        SELECT key, value FROM jsonb_each_text(job_record.input_parameters)\n    LOOP\n        generation_prompt := replace(generation_prompt, '{{' || variable_key || '}}', variable_value);\n    END LOOP;\n\n    -- Generate unique seed for this job\n    generation_seed := 50000 + job_id_input;\n\n    -- Generate content\n    generated_text := steadytext_generate(\n        generation_prompt,\n        max_tokens := 500,\n        seed := generation_seed\n    );\n\n    -- Assess quality\n    SELECT * INTO quality_result FROM assess_content_quality(\n        generated_text,\n        template_record.quality_criteria,\n        generation_seed + 1000\n    );\n\n    -- Store generated content\n    INSERT INTO generated_content (\n        job_id,\n        content_text,\n        quality_score,\n        quality_issues,\n        embedding,\n        approved,\n        generation_seed\n    ) VALUES (\n        job_id_input,\n        generated_text,\n        quality_result.quality_score,\n        quality_result.quality_issues,\n        steadytext_embed(generated_text, seed := generation_seed + 2000),\n        quality_result.quality_score &gt;= 0.8, -- Auto-approve high quality content\n        generation_seed\n    );\n\n    -- Update job status\n    UPDATE content_jobs SET\n        status = CASE \n            WHEN quality_result.quality_score &gt;= 0.8 THEN 'completed'\n            ELSE 'review_needed'\n        END,\n        completed_at = NOW()\n    WHERE id = job_id_input;\n\n    RETURN TRUE;\nEXCEPTION\n    WHEN OTHERS THEN\n        UPDATE content_jobs SET status = 'failed' WHERE id = job_id_input;\n        RETURN FALSE;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Setup content templates\nINSERT INTO content_templates (name, template_text, variables, quality_criteria, embedding)\nVALUES \n    ('Product Description',\n     'Write a compelling product description for {{product_name}} in the {{category}} category. Key features: {{features}}. Target audience: {{audience}}.',\n     '{\"product_name\": \"string\", \"category\": \"string\", \"features\": \"string\", \"audience\": \"string\"}',\n     '{\"clarity\": \"high\", \"engagement\": \"high\", \"accuracy\": \"high\", \"length\": \"150-300 words\"}',\n     steadytext_embed('Product description template for e-commerce', seed := 60000)),\n\n    ('Blog Post Introduction',\n     'Write an engaging introduction for a blog post about {{topic}}. The post should appeal to {{audience}} and have a {{tone}} tone.',\n     '{\"topic\": \"string\", \"audience\": \"string\", \"tone\": \"string\"}',\n     '{\"hook\": \"strong\", \"clarity\": \"high\", \"engagement\": \"high\", \"length\": \"100-200 words\"}',\n     steadytext_embed('Blog post introduction template', seed := 60001));\n\n-- Create content jobs\nINSERT INTO content_jobs (template_id, input_parameters, priority)\nVALUES \n    (1, '{\"product_name\": \"Smart Fitness Tracker\", \"category\": \"wearables\", \"features\": \"heart rate monitoring, sleep tracking, waterproof\", \"audience\": \"fitness enthusiasts\"}', 1),\n    (2, '{\"topic\": \"sustainable technology\", \"audience\": \"environmentally conscious consumers\", \"tone\": \"informative yet inspiring\"}', 2);\n\n-- Process content jobs\nSELECT process_content_job(1);\nSELECT process_content_job(2);\n\n-- Review generated content\nSELECT \n    cj.id as job_id,\n    ct.name as template_name,\n    gc.content_text,\n    gc.quality_score,\n    gc.quality_issues,\n    gc.approved,\n    cj.status\nFROM content_jobs cj\nJOIN content_templates ct ON cj.template_id = ct.id\nJOIN generated_content gc ON cj.id = gc.job_id\nORDER BY cj.created_at DESC;\n</code></pre>"},{"location":"examples/postgresql-integration/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/postgresql-integration/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<pre><code>-- Performance monitoring schema\nCREATE TABLE api_usage_logs (\n    id SERIAL PRIMARY KEY,\n    function_name TEXT NOT NULL,\n    input_text TEXT,\n    input_length INTEGER,\n    output_text TEXT,\n    output_length INTEGER,\n    processing_time_ms INTEGER,\n    cache_hit BOOLEAN,\n    seed_used INTEGER,\n    embedding_similarity REAL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE cache_performance (\n    id SERIAL PRIMARY KEY,\n    cache_type TEXT NOT NULL, -- 'generation', 'embedding'\n    hit_rate REAL,\n    total_requests INTEGER,\n    cache_hits INTEGER,\n    cache_misses INTEGER,\n    avg_response_time_ms REAL,\n    measured_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Performance monitoring functions\nCREATE OR REPLACE FUNCTION log_steadytext_usage(\n    func_name TEXT,\n    input_text TEXT,\n    output_text TEXT,\n    start_time TIMESTAMP,\n    end_time TIMESTAMP,\n    was_cache_hit BOOLEAN DEFAULT false,\n    seed_value INTEGER DEFAULT 42\n)\nRETURNS VOID AS $$\nDECLARE\n    processing_ms INTEGER;\nBEGIN\n    processing_ms := EXTRACT(epoch FROM (end_time - start_time)) * 1000;\n\n    INSERT INTO api_usage_logs (\n        function_name,\n        input_text,\n        input_length,\n        output_text,\n        output_length,\n        processing_time_ms,\n        cache_hit,\n        seed_used\n    ) VALUES (\n        func_name,\n        input_text,\n        length(input_text),\n        output_text,\n        length(output_text),\n        processing_ms,\n        was_cache_hit,\n        seed_value\n    );\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Enhanced generation function with monitoring\nCREATE OR REPLACE FUNCTION monitored_generate(\n    prompt TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    seed INTEGER DEFAULT 42\n)\nRETURNS TEXT AS $$\nDECLARE\n    start_time TIMESTAMP;\n    end_time TIMESTAMP;\n    result TEXT;\nBEGIN\n    start_time := clock_timestamp();\n\n    result := steadytext_generate(prompt, max_tokens, true, seed);\n\n    end_time := clock_timestamp();\n\n    -- Log the usage\n    PERFORM log_steadytext_usage(\n        'steadytext_generate',\n        prompt,\n        result,\n        start_time,\n        end_time,\n        false, -- We'd need to modify steadytext to return cache hit info\n        seed\n    );\n\n    RETURN result;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Performance analytics views\nCREATE VIEW performance_summary AS\nSELECT \n    function_name,\n    COUNT(*) as total_calls,\n    AVG(processing_time_ms) as avg_time_ms,\n    MIN(processing_time_ms) as min_time_ms,\n    MAX(processing_time_ms) as max_time_ms,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY processing_time_ms) as median_time_ms,\n    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY processing_time_ms) as p95_time_ms,\n    AVG(input_length) as avg_input_length,\n    AVG(output_length) as avg_output_length,\n    COUNT(*) FILTER (WHERE cache_hit) as cache_hits,\n    ROUND(100.0 * COUNT(*) FILTER (WHERE cache_hit) / COUNT(*), 2) as cache_hit_rate_pct\nFROM api_usage_logs\nWHERE created_at &gt; NOW() - INTERVAL '24 hours'\nGROUP BY function_name;\n\nCREATE VIEW hourly_usage AS\nSELECT \n    DATE_TRUNC('hour', created_at) as hour,\n    function_name,\n    COUNT(*) as calls,\n    AVG(processing_time_ms) as avg_time_ms,\n    COUNT(*) FILTER (WHERE cache_hit) as cache_hits\nFROM api_usage_logs\nWHERE created_at &gt; NOW() - INTERVAL '7 days'\nGROUP BY DATE_TRUNC('hour', created_at), function_name\nORDER BY hour DESC;\n\n-- Batch optimization function\nCREATE OR REPLACE FUNCTION optimize_batch_processing(\n    prompts TEXT[],\n    base_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    prompt_index INTEGER,\n    generated_text TEXT,\n    processing_time_ms INTEGER\n) AS $$\nDECLARE\n    prompt_text TEXT;\n    result TEXT;\n    start_time TIMESTAMP;\n    end_time TIMESTAMP;\n    i INTEGER := 1;\nBEGIN\n    FOREACH prompt_text IN ARRAY prompts LOOP\n        start_time := clock_timestamp();\n\n        result := steadytext_generate(\n            prompt_text,\n            max_tokens := 200,\n            seed := base_seed + i\n        );\n\n        end_time := clock_timestamp();\n\n        prompt_index := i;\n        generated_text := result;\n        processing_time_ms := EXTRACT(epoch FROM (end_time - start_time)) * 1000;\n\n        RETURN NEXT;\n        i := i + 1;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage examples and performance testing\nSELECT * FROM performance_summary;\nSELECT * FROM hourly_usage LIMIT 10;\n\n-- Test batch optimization\nSELECT * FROM optimize_batch_processing(ARRAY[\n    'Explain machine learning',\n    'Describe quantum computing',\n    'What is blockchain technology',\n    'How does AI work',\n    'Define neural networks'\n], 70000);\n\n-- Cache warming function\nCREATE OR REPLACE FUNCTION warm_cache(\n    common_prompts TEXT[],\n    base_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    prompt TEXT,\n    cached BOOLEAN\n) AS $$\nDECLARE\n    prompt_text TEXT;\n    result TEXT;\nBEGIN\n    FOREACH prompt_text IN ARRAY common_prompts LOOP\n        -- Generate to populate cache\n        result := steadytext_generate(prompt_text, max_tokens := 100, seed := base_seed);\n\n        prompt := prompt_text;\n        cached := true;\n        RETURN NEXT;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Warm cache with common prompts\nSELECT * FROM warm_cache(ARRAY[\n    'Write a professional email',\n    'Create a product summary',\n    'Explain a technical concept',\n    'Generate a brief description'\n]);\n\n-- Performance monitoring dashboard query\nWITH recent_stats AS (\n    SELECT \n        function_name,\n        COUNT(*) as calls_last_hour,\n        AVG(processing_time_ms) as avg_time_last_hour,\n        COUNT(*) FILTER (WHERE cache_hit) as cache_hits_last_hour\n    FROM api_usage_logs \n    WHERE created_at &gt; NOW() - INTERVAL '1 hour'\n    GROUP BY function_name\n),\ndaily_stats AS (\n    SELECT \n        function_name,\n        COUNT(*) as calls_last_day,\n        AVG(processing_time_ms) as avg_time_last_day\n    FROM api_usage_logs \n    WHERE created_at &gt; NOW() - INTERVAL '24 hours'\n    GROUP BY function_name\n)\nSELECT \n    COALESCE(r.function_name, d.function_name) as function_name,\n    COALESCE(r.calls_last_hour, 0) as calls_last_hour,\n    COALESCE(d.calls_last_day, 0) as calls_last_day,\n    COALESCE(r.avg_time_last_hour, 0) as avg_time_last_hour,\n    COALESCE(d.avg_time_last_day, 0) as avg_time_last_day,\n    COALESCE(r.cache_hits_last_hour, 0) as cache_hits_last_hour,\n    CASE \n        WHEN r.calls_last_hour &gt; 0 THEN \n            ROUND(100.0 * r.cache_hits_last_hour / r.calls_last_hour, 2)\n        ELSE 0\n    END as cache_hit_rate_pct\nFROM recent_stats r\nFULL OUTER JOIN daily_stats d ON r.function_name = d.function_name\nORDER BY calls_last_day DESC;\n</code></pre>"},{"location":"examples/postgresql-integration/#structured-generation-examples","title":"Structured Generation Examples","text":"<p>SteadyText v2.4.1+ supports structured text generation using native llama.cpp grammars. Generate JSON, regex-constrained text, or multiple-choice responses directly in PostgreSQL.</p>"},{"location":"examples/postgresql-integration/#json-generation","title":"JSON Generation","text":"<pre><code>-- Generate structured user profiles\nCREATE TABLE user_profiles (\n    id SERIAL PRIMARY KEY,\n    user_data JSONB,\n    generated_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Function to generate user profiles\nCREATE OR REPLACE FUNCTION generate_user_profile(\n    description TEXT,\n    profile_seed INTEGER DEFAULT 42\n)\nRETURNS JSONB AS $$\nDECLARE\n    schema JSONB;\n    result TEXT;\n    parsed_json JSONB;\nBEGIN\n    -- Define JSON schema\n    schema := '{\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"age\": {\"type\": \"integer\"},\n            \"email\": {\"type\": \"string\"},\n            \"occupation\": {\"type\": \"string\"},\n            \"interests\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"}\n            }\n        }\n    }'::jsonb;\n\n    -- Generate JSON with error handling\n    result := steadytext_generate_json(\n        'Create a user profile: ' || description,\n        schema,\n        max_tokens := 200,\n        seed := profile_seed\n    );\n\n    IF result IS NOT NULL THEN\n        -- Extract JSON from the output (it contains &lt;json-output&gt;...&lt;/json-output&gt;)\n        result := substring(result FROM '&lt;json-output&gt;(.*)&lt;/json-output&gt;');\n        parsed_json := result::jsonb;\n    ELSE\n        -- Fallback on generation failure\n        parsed_json := jsonb_build_object(\n            'name', 'Unknown User',\n            'age', 0,\n            'email', 'unknown@example.com',\n            'error', 'Generation failed'\n        );\n    END IF;\n\n    RETURN parsed_json;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Generate multiple user profiles\nINSERT INTO user_profiles (user_data)\nSELECT generate_user_profile(description, seed)\nFROM (VALUES \n    ('software engineer who loves hiking', 100),\n    ('data scientist interested in ML', 200),\n    ('product manager with startup experience', 300)\n) AS profiles(description, seed);\n\n-- Generate product catalog with structured data\nCREATE OR REPLACE FUNCTION generate_product_listing(\n    product_type TEXT,\n    listing_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    name TEXT,\n    price NUMERIC,\n    specs JSONB,\n    description TEXT\n) AS $$\nDECLARE\n    schema JSONB;\n    result TEXT;\n    product_json JSONB;\nBEGIN\n    schema := '{\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"price\": {\"type\": \"number\"},\n            \"specs\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"weight\": {\"type\": \"string\"},\n                    \"dimensions\": {\"type\": \"string\"},\n                    \"material\": {\"type\": \"string\"}\n                }\n            }\n        }\n    }'::jsonb;\n\n    result := steadytext_generate_json(\n        format('Create a %s product listing', product_type),\n        schema,\n        seed := listing_seed\n    );\n\n    IF result IS NOT NULL THEN\n        result := substring(result FROM '&lt;json-output&gt;(.*)&lt;/json-output&gt;');\n        product_json := result::jsonb;\n\n        name := product_json-&gt;&gt;'name';\n        price := (product_json-&gt;&gt;'price')::numeric;\n        specs := product_json-&gt;'specs';\n\n        -- Generate description separately\n        description := steadytext_generate(\n            format('Write a brief description for %s', product_json-&gt;&gt;'name'),\n            max_tokens := 100,\n            seed := listing_seed + 1000\n        );\n    END IF;\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/postgresql-integration/#regex-constrained-generation","title":"Regex-Constrained Generation","text":"<pre><code>-- Generate formatted data matching patterns\nCREATE TABLE formatted_data (\n    id SERIAL PRIMARY KEY,\n    data_type TEXT,\n    formatted_value TEXT,\n    pattern_used TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Phone number generation\nINSERT INTO formatted_data (data_type, formatted_value, pattern_used)\nSELECT \n    'phone',\n    steadytext_generate_regex(\n        'Customer support number: ',\n        '\\(\\d{3}\\) \\d{3}-\\d{4}',\n        seed := 1000 + generate_series\n    ),\n    '\\(\\d{3}\\) \\d{3}-\\d{4}'\nFROM generate_series(1, 5);\n\n-- Generate SKUs with specific format\nINSERT INTO formatted_data (data_type, formatted_value, pattern_used)\nSELECT \n    'sku',\n    steadytext_generate_regex(\n        'Product SKU: ',\n        '[A-Z]{3}-\\d{4}-[A-Z]\\d{2}',\n        seed := 2000 + generate_series\n    ),\n    '[A-Z]{3}-\\d{4}-[A-Z]\\d{2}'\nFROM generate_series(1, 10);\n\n-- Generate formatted dates\nCREATE OR REPLACE FUNCTION generate_event_schedule(\n    num_events INTEGER,\n    base_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    event_name TEXT,\n    event_date TEXT,\n    event_time TEXT,\n    event_code TEXT\n) AS $$\nBEGIN\n    FOR i IN 1..num_events LOOP\n        event_name := steadytext_generate(\n            'Event name: ',\n            max_tokens := 20,\n            seed := base_seed + i\n        );\n\n        event_date := steadytext_generate_regex(\n            'Date: ',\n            '\\d{4}-\\d{2}-\\d{2}',\n            seed := base_seed + i + 1000\n        );\n\n        event_time := steadytext_generate_regex(\n            'Time: ',\n            '\\d{2}:\\d{2}',\n            seed := base_seed + i + 2000\n        );\n\n        event_code := steadytext_generate_regex(\n            'Code: ',\n            'EVT-[A-Z]{2}\\d{3}',\n            seed := base_seed + i + 3000\n        );\n\n        RETURN NEXT;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/postgresql-integration/#multiple-choice-generation","title":"Multiple Choice Generation","text":"<pre><code>-- Sentiment analysis with choices\nCREATE TABLE review_sentiments (\n    id SERIAL PRIMARY KEY,\n    review_text TEXT,\n    sentiment TEXT,\n    confidence_level TEXT,\n    analyzed_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Analyze sentiments\nINSERT INTO review_sentiments (review_text, sentiment, confidence_level)\nSELECT \n    review,\n    steadytext_generate_choice(\n        'Sentiment of this review: ' || review,\n        ARRAY['positive', 'negative', 'neutral'],\n        seed := 5000 + ROW_NUMBER() OVER()\n    ) AS sentiment,\n    steadytext_generate_choice(\n        'Confidence level for sentiment analysis: ' || review,\n        ARRAY['high', 'medium', 'low'],\n        seed := 6000 + ROW_NUMBER() OVER()\n    ) AS confidence_level\nFROM (VALUES \n    ('This product exceeded my expectations! Highly recommend.'),\n    ('Terrible quality, broke after one day of use.'),\n    ('It works as described, nothing special.')\n) AS reviews(review);\n\n-- Content categorization\nCREATE OR REPLACE FUNCTION categorize_content(\n    content TEXT,\n    categories TEXT[],\n    cat_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    primary_category TEXT,\n    secondary_category TEXT,\n    content_type TEXT\n) AS $$\nBEGIN\n    primary_category := steadytext_generate_choice(\n        'Primary category for: ' || content,\n        categories,\n        seed := cat_seed\n    );\n\n    secondary_category := steadytext_generate_choice(\n        'Secondary category for: ' || content,\n        categories,\n        seed := cat_seed + 1000\n    );\n\n    content_type := steadytext_generate_choice(\n        'Content type: ' || content,\n        ARRAY['article', 'tutorial', 'news', 'opinion', 'guide'],\n        seed := cat_seed + 2000\n    );\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/postgresql-integration/#ai-summarization-examples","title":"AI Summarization Examples","text":"<p>Leverage AI-powered summarization for intelligent data aggregation and insights extraction.</p>"},{"location":"examples/postgresql-integration/#document-summarization","title":"Document Summarization","text":"<pre><code>-- Create a document repository with AI summaries\nCREATE TABLE documents_with_ai (\n    id SERIAL PRIMARY KEY,\n    title TEXT,\n    content TEXT,\n    summary TEXT,\n    key_facts TEXT[],\n    category TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Populate with AI-generated summaries\nINSERT INTO documents_with_ai (title, content, summary, key_facts, category)\nSELECT \n    title,\n    content,\n    ai_summarize_text(content, jsonb_build_object('doc_type', 'technical')) AS summary,\n    ai_extract_facts(content, 5) AS key_facts,\n    steadytext_generate_choice(\n        'Document category: ' || title,\n        ARRAY['technical', 'business', 'research', 'tutorial'],\n        seed := 7000 + id\n    ) AS category\nFROM source_documents\nWHERE length(content) &gt; 500;\n\n-- Create materialized view for category summaries\nCREATE MATERIALIZED VIEW category_summaries AS\nSELECT \n    category,\n    count(*) AS document_count,\n    ai_summarize(\n        content,\n        jsonb_build_object(\n            'category', category,\n            'doc_count', count(*) OVER (PARTITION BY category)\n        )\n    ) AS category_overview,\n    array_agg(DISTINCT key_facts) AS all_key_facts\nFROM documents_with_ai\nGROUP BY category;\n\n-- Real-time log analysis with AI\nCREATE TABLE system_logs (\n    id SERIAL PRIMARY KEY,\n    timestamp TIMESTAMP DEFAULT NOW(),\n    service_name TEXT,\n    log_level TEXT,\n    message TEXT,\n    metadata JSONB\n);\n\n-- Create hourly log summaries\nCREATE OR REPLACE VIEW hourly_log_analysis AS\nSELECT \n    date_trunc('hour', timestamp) AS hour,\n    service_name,\n    log_level,\n    count(*) AS log_count,\n    ai_summarize(\n        message,\n        jsonb_build_object(\n            'service', service_name,\n            'level', log_level,\n            'count', count(*)\n        )\n    ) AS hour_summary,\n    array_agg(DISTINCT \n        CASE \n            WHEN log_level IN ('ERROR', 'CRITICAL') \n            THEN message \n        END\n    ) FILTER (WHERE log_level IN ('ERROR', 'CRITICAL')) AS critical_messages\nFROM system_logs\nWHERE timestamp &gt; NOW() - INTERVAL '24 hours'\nGROUP BY date_trunc('hour', timestamp), service_name, log_level;\n</code></pre>"},{"location":"examples/postgresql-integration/#timescaledb-integration","title":"TimescaleDB Integration","text":"<pre><code>-- Enable TimescaleDB (if available)\nCREATE EXTENSION IF NOT EXISTS timescaledb;\n\n-- Create hypertable for time-series data\nCREATE TABLE metrics (\n    time TIMESTAMPTZ NOT NULL,\n    device_id TEXT,\n    metric_name TEXT,\n    value NUMERIC,\n    description TEXT\n);\n\nSELECT create_hypertable('metrics', 'time');\n\n-- Create continuous aggregate with AI summarization\nCREATE MATERIALIZED VIEW daily_metric_summaries\nWITH (timescaledb.continuous) AS\nSELECT \n    time_bucket('1 day', time) AS day,\n    device_id,\n    metric_name,\n    avg(value) AS avg_value,\n    min(value) AS min_value,\n    max(value) AS max_value,\n    count(*) AS data_points,\n    ai_summarize_partial(\n        description,\n        jsonb_build_object(\n            'metric', metric_name,\n            'device', device_id,\n            'stats', jsonb_build_object(\n                'avg', avg(value),\n                'min', min(value),\n                'max', max(value)\n            )\n        )\n    ) AS partial_summary\nFROM metrics\nGROUP BY day, device_id, metric_name;\n\n-- Query with final summarization\nCREATE OR REPLACE VIEW weekly_insights AS\nSELECT \n    time_bucket('1 week', day) AS week,\n    device_id,\n    ai_summarize_final(partial_summary) AS weekly_summary,\n    jsonb_agg(\n        jsonb_build_object(\n            'metric', metric_name,\n            'avg', avg_value,\n            'min', min_value,\n            'max', max_value\n        )\n    ) AS metric_stats\nFROM daily_metric_summaries\nWHERE day &gt;= NOW() - INTERVAL '4 weeks'\nGROUP BY week, device_id;\n</code></pre>"},{"location":"examples/postgresql-integration/#intelligent-report-generation","title":"Intelligent Report Generation","text":"<pre><code>-- Generate executive summaries from multiple data sources\nCREATE OR REPLACE FUNCTION generate_executive_report(\n    report_date DATE DEFAULT CURRENT_DATE\n)\nRETURNS TABLE(\n    section TEXT,\n    summary TEXT,\n    key_metrics JSONB,\n    recommendations TEXT[]\n) AS $$\nBEGIN\n    -- Sales summary\n    section := 'Sales Performance';\n    SELECT \n        ai_summarize_text(\n            string_agg(\n                format('Product %s: %s units sold for $%s', \n                    product_name, units_sold, revenue),\n                '; '\n            ),\n            jsonb_build_object('report_type', 'sales', 'date', report_date)\n        ),\n        jsonb_agg(\n            jsonb_build_object(\n                'product', product_name,\n                'units', units_sold,\n                'revenue', revenue\n            )\n        )\n    INTO summary, key_metrics\n    FROM sales_data\n    WHERE date = report_date;\n\n    recommendations := ai_extract_facts(\n        format('Based on sales data: %s. What actions should be taken?', summary),\n        3\n    );\n\n    RETURN NEXT;\n\n    -- Customer feedback summary\n    section := 'Customer Feedback';\n    SELECT \n        ai_summarize(\n            feedback_text,\n            jsonb_build_object(\n                'sentiment', sentiment_score,\n                'category', feedback_category\n            )\n        ),\n        jsonb_build_object(\n            'avg_sentiment', avg(sentiment_score),\n            'total_feedback', count(*),\n            'categories', jsonb_agg(DISTINCT feedback_category)\n        )\n    INTO summary, key_metrics\n    FROM customer_feedback\n    WHERE date_trunc('day', created_at) = report_date\n    GROUP BY date_trunc('day', created_at);\n\n    recommendations := ai_extract_facts(\n        format('Customer feedback summary: %s. Recommended improvements?', summary),\n        4\n    );\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/postgresql-integration/#async-functions-examples","title":"Async Functions Examples","text":"<p>Build high-performance applications with non-blocking AI operations.</p>"},{"location":"examples/postgresql-integration/#basic-async-operations","title":"Basic Async Operations","text":"<pre><code>-- Create a task queue for async processing\nCREATE TABLE ai_tasks (\n    id SERIAL PRIMARY KEY,\n    request_id UUID,\n    task_type TEXT,\n    input_data JSONB,\n    status TEXT DEFAULT 'pending',\n    result TEXT,\n    error TEXT,\n    created_at TIMESTAMP DEFAULT NOW(),\n    completed_at TIMESTAMP\n);\n\n-- Function to process multiple prompts asynchronously\nCREATE OR REPLACE FUNCTION process_bulk_content(\n    prompts TEXT[],\n    task_type TEXT DEFAULT 'generation'\n)\nRETURNS TABLE(\n    task_id INTEGER,\n    request_id UUID,\n    prompt TEXT\n) AS $$\nDECLARE\n    prompt_text TEXT;\n    req_id UUID;\n    task_id_val INTEGER;\nBEGIN\n    FOREACH prompt_text IN ARRAY prompts LOOP\n        -- Start async generation\n        req_id := steadytext_generate_async(\n            prompt_text,\n            max_tokens := 200,\n            priority := 5\n        );\n\n        -- Store in task queue\n        INSERT INTO ai_tasks (request_id, task_type, input_data)\n        VALUES (req_id, task_type, jsonb_build_object('prompt', prompt_text))\n        RETURNING id INTO task_id_val;\n\n        task_id := task_id_val;\n        request_id := req_id;\n        prompt := prompt_text;\n\n        RETURN NEXT;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Process results with retry logic\nCREATE OR REPLACE FUNCTION collect_async_results(\n    timeout_seconds INTEGER DEFAULT 30\n)\nRETURNS INTEGER AS $$\nDECLARE\n    pending_task RECORD;\n    result_data RECORD;\n    completed_count INTEGER := 0;\nBEGIN\n    FOR pending_task IN \n        SELECT id, request_id \n        FROM ai_tasks \n        WHERE status = 'pending'\n    LOOP\n        -- Check async status\n        SELECT * INTO result_data\n        FROM steadytext_check_async(pending_task.request_id);\n\n        IF result_data.status = 'completed' THEN\n            UPDATE ai_tasks\n            SET status = 'completed',\n                result = result_data.result,\n                completed_at = NOW()\n            WHERE id = pending_task.id;\n\n            completed_count := completed_count + 1;\n        ELSIF result_data.status = 'failed' THEN\n            UPDATE ai_tasks\n            SET status = 'failed',\n                error = result_data.error,\n                completed_at = NOW()\n            WHERE id = pending_task.id;\n        END IF;\n    END LOOP;\n\n    RETURN completed_count;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/postgresql-integration/#advanced-async-patterns","title":"Advanced Async Patterns","text":"<pre><code>-- Async document processing pipeline\nCREATE TABLE document_pipeline (\n    id SERIAL PRIMARY KEY,\n    document_id INTEGER,\n    stage TEXT,\n    request_id UUID,\n    result TEXT,\n    next_stage TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Multi-stage async processing\nCREATE OR REPLACE FUNCTION process_document_async(\n    doc_id INTEGER,\n    doc_content TEXT\n)\nRETURNS VOID AS $$\nDECLARE\n    summary_req UUID;\n    facts_req UUID;\n    category_req UUID;\nBEGIN\n    -- Stage 1: Generate summary\n    summary_req := steadytext_generate_async(\n        'Summarize: ' || doc_content,\n        max_tokens := 150,\n        priority := 8\n    );\n\n    INSERT INTO document_pipeline (document_id, stage, request_id, next_stage)\n    VALUES (doc_id, 'summary', summary_req, 'facts');\n\n    -- Stage 2: Extract facts (higher priority)\n    facts_req := steadytext_generate_async(\n        'Extract 5 key facts from: ' || doc_content,\n        max_tokens := 100,\n        priority := 9\n    );\n\n    INSERT INTO document_pipeline (document_id, stage, request_id, next_stage)\n    VALUES (doc_id, 'facts', facts_req, 'category');\n\n    -- Stage 3: Categorize\n    category_req := steadytext_generate_choice_async(\n        'Category for document: ' || substring(doc_content, 1, 200),\n        ARRAY['technical', 'business', 'research', 'general'],\n        priority := 7\n    );\n\n    INSERT INTO document_pipeline (document_id, stage, request_id, next_stage)\n    VALUES (doc_id, 'category', category_req, 'complete');\nEND;\n$$ LANGUAGE plpgsql;\n\n-- LISTEN/NOTIFY based processing\nCREATE OR REPLACE FUNCTION setup_async_notifications()\nRETURNS VOID AS $$\nBEGIN\n    -- Create notification trigger\n    CREATE OR REPLACE FUNCTION notify_async_complete()\n    RETURNS TRIGGER AS $trigger$\n    BEGIN\n        IF NEW.status = 'completed' AND OLD.status != 'completed' THEN\n            PERFORM pg_notify(\n                'ai_task_complete',\n                json_build_object(\n                    'task_id', NEW.id,\n                    'request_id', NEW.request_id,\n                    'task_type', NEW.task_type\n                )::text\n            );\n        END IF;\n        RETURN NEW;\n    END;\n    $trigger$ LANGUAGE plpgsql;\n\n    -- Attach trigger\n    DROP TRIGGER IF EXISTS async_complete_notify ON ai_tasks;\n    CREATE TRIGGER async_complete_notify\n        AFTER UPDATE ON ai_tasks\n        FOR EACH ROW\n        EXECUTE FUNCTION notify_async_complete();\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Batch embedding generation with progress tracking\nCREATE OR REPLACE FUNCTION generate_embeddings_async(\n    texts TEXT[],\n    batch_size INTEGER DEFAULT 10\n)\nRETURNS TABLE(\n    batch_id INTEGER,\n    request_ids UUID[],\n    text_count INTEGER\n) AS $$\nDECLARE\n    batch_texts TEXT[];\n    batch_requests UUID[];\n    start_idx INTEGER := 1;\n    end_idx INTEGER;\n    batch_num INTEGER := 1;\nBEGIN\n    WHILE start_idx &lt;= array_length(texts, 1) LOOP\n        end_idx := LEAST(start_idx + batch_size - 1, array_length(texts, 1));\n        batch_texts := texts[start_idx:end_idx];\n\n        -- Generate embeddings for batch\n        batch_requests := steadytext_embed_batch_async(batch_texts);\n\n        -- Store batch info\n        INSERT INTO embedding_batches (batch_id, request_ids, status)\n        VALUES (batch_num, batch_requests, 'processing');\n\n        batch_id := batch_num;\n        request_ids := batch_requests;\n        text_count := array_length(batch_texts, 1);\n\n        RETURN NEXT;\n\n        start_idx := end_idx + 1;\n        batch_num := batch_num + 1;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Worker management\nCREATE OR REPLACE FUNCTION manage_async_workers()\nRETURNS TABLE(\n    action TEXT,\n    status TEXT,\n    details JSONB\n) AS $$\nDECLARE\n    worker_status RECORD;\nBEGIN\n    -- Check worker status\n    SELECT * INTO worker_status FROM steadytext_worker_status();\n\n    action := 'check_status';\n    status := worker_status.status;\n    details := to_jsonb(worker_status);\n    RETURN NEXT;\n\n    -- Start worker if not running\n    IF worker_status.status != 'running' THEN\n        PERFORM steadytext_worker_start();\n\n        action := 'start_worker';\n        status := 'started';\n        details := jsonb_build_object('message', 'Worker started successfully');\n        RETURN NEXT;\n    END IF;\n\n    -- Configure worker settings\n    PERFORM steadytext_config_set('worker_batch_size', '20');\n    PERFORM steadytext_config_set('worker_poll_interval_ms', '500');\n\n    action := 'configure_worker';\n    status := 'configured';\n    details := jsonb_build_object(\n        'batch_size', 20,\n        'poll_interval_ms', 500\n    );\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"examples/postgresql-integration/#real-world-async-use-cases","title":"Real-World Async Use Cases","text":"<pre><code>-- Real-time content moderation system\nCREATE TABLE content_moderation_queue (\n    id SERIAL PRIMARY KEY,\n    content_id INTEGER,\n    content_text TEXT,\n    moderation_request_id UUID,\n    moderation_result TEXT,\n    action_taken TEXT,\n    processed_at TIMESTAMP\n);\n\nCREATE OR REPLACE FUNCTION moderate_content_async(\n    content_items JSONB[]\n)\nRETURNS VOID AS $$\nDECLARE\n    item JSONB;\n    req_id UUID;\nBEGIN\n    FOREACH item IN ARRAY content_items LOOP\n        -- Check content appropriateness\n        req_id := steadytext_generate_choice_async(\n            format('Is this content appropriate for all audiences? Content: %s', \n                   item-&gt;&gt;'text'),\n            ARRAY['appropriate', 'needs_review', 'inappropriate'],\n            priority := 10  -- High priority for moderation\n        );\n\n        INSERT INTO content_moderation_queue (\n            content_id,\n            content_text,\n            moderation_request_id\n        ) VALUES (\n            (item-&gt;&gt;'id')::integer,\n            item-&gt;&gt;'text',\n            req_id\n        );\n    END LOOP;\n\n    -- Notify moderation service\n    PERFORM pg_notify('content_moderation', 'new_items_queued');\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Async email generation system\nCREATE TABLE email_generation_queue (\n    id SERIAL PRIMARY KEY,\n    recipient_id INTEGER,\n    email_type TEXT,\n    context JSONB,\n    subject_request_id UUID,\n    body_request_id UUID,\n    subject TEXT,\n    body TEXT,\n    status TEXT DEFAULT 'queued',\n    created_at TIMESTAMP DEFAULT NOW(),\n    sent_at TIMESTAMP\n);\n\nCREATE OR REPLACE FUNCTION generate_personalized_emails_async(\n    recipient_data JSONB\n)\nRETURNS INTEGER AS $$\nDECLARE\n    recipient JSONB;\n    subject_req UUID;\n    body_req UUID;\n    email_count INTEGER := 0;\nBEGIN\n    FOR recipient IN SELECT * FROM jsonb_array_elements(recipient_data)\n    LOOP\n        -- Generate subject\n        subject_req := steadytext_generate_async(\n            format('Email subject for %s promotion to %s',\n                   recipient-&gt;&gt;'product',\n                   recipient-&gt;&gt;'name'),\n            max_tokens := 20,\n            priority := 6\n        );\n\n        -- Generate body\n        body_req := steadytext_generate_async(\n            format('Professional marketing email about %s for %s who is interested in %s',\n                   recipient-&gt;&gt;'product',\n                   recipient-&gt;&gt;'name',\n                   recipient-&gt;&gt;'interests'),\n            max_tokens := 300,\n            priority := 5\n        );\n\n        INSERT INTO email_generation_queue (\n            recipient_id,\n            email_type,\n            context,\n            subject_request_id,\n            body_request_id\n        ) VALUES (\n            (recipient-&gt;&gt;'id')::integer,\n            'marketing',\n            recipient,\n            subject_req,\n            body_req\n        );\n\n        email_count := email_count + 1;\n    END LOOP;\n\n    RETURN email_count;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Process completed emails\nCREATE OR REPLACE FUNCTION process_completed_emails()\nRETURNS INTEGER AS $$\nDECLARE\n    email RECORD;\n    subject_result RECORD;\n    body_result RECORD;\n    processed_count INTEGER := 0;\nBEGIN\n    FOR email IN \n        SELECT * FROM email_generation_queue \n        WHERE status = 'queued'\n    LOOP\n        -- Check subject generation\n        SELECT * INTO subject_result\n        FROM steadytext_check_async(email.subject_request_id);\n\n        -- Check body generation\n        SELECT * INTO body_result\n        FROM steadytext_check_async(email.body_request_id);\n\n        -- If both completed, update and send\n        IF subject_result.status = 'completed' AND \n           body_result.status = 'completed' THEN\n\n            UPDATE email_generation_queue\n            SET subject = subject_result.result,\n                body = body_result.result,\n                status = 'ready_to_send'\n            WHERE id = email.id;\n\n            processed_count := processed_count + 1;\n\n            -- Trigger email sending\n            PERFORM pg_notify(\n                'send_email',\n                json_build_object(\n                    'email_id', email.id,\n                    'recipient_id', email.recipient_id\n                )::text\n            );\n        END IF;\n    END LOOP;\n\n    RETURN processed_count;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre> <p>This comprehensive guide demonstrates how to integrate SteadyText with PostgreSQL for building powerful AI-enhanced applications. The examples cover everything from basic setup to advanced workflows, providing a solid foundation for developing production-ready systems that combine structured data with AI-generated content and embeddings.</p>"},{"location":"examples/testing/","title":"Testing with AI","text":"<p>Learn how to use SteadyText to build reliable AI tests that never flake.</p>"},{"location":"examples/testing/#the-problem-with-ai-testing","title":"The Problem with AI Testing","text":"<p>Traditional AI testing is challenging because:</p> <ul> <li>Non-deterministic outputs: Same input produces different results</li> <li>Flaky tests: Tests pass sometimes, fail others  </li> <li>Hard to mock: AI services are complex to replicate</li> <li>Unpredictable behavior: Edge cases are difficult to reproduce</li> </ul> <p>SteadyText solves these by providing deterministic AI outputs - same input always produces the same result.</p>"},{"location":"examples/testing/#basic-test-patterns","title":"Basic Test Patterns","text":""},{"location":"examples/testing/#deterministic-assertions","title":"Deterministic Assertions","text":"<pre><code>import steadytext\n\ndef test_ai_code_generation():\n    \"\"\"Test that never flakes - same input, same output.\"\"\"\n\n    def my_ai_function(prompt):\n        # Your actual AI function (GPT-4, Claude, etc.)\n        # For testing, we compare against SteadyText\n        return call_real_ai_service(prompt)\n\n    prompt = \"write a function to reverse a string\"\n    result = my_ai_function(prompt)\n    expected = steadytext.generate(prompt)\n\n    # This assertion is deterministic and reliable\n    assert result.strip() == expected.strip()\n</code></pre>"},{"location":"examples/testing/#embedding-similarity-tests","title":"Embedding Similarity Tests","text":"<pre><code>import numpy as np\n\ndef test_document_similarity():\n    \"\"\"Test semantic similarity calculations.\"\"\"\n\n    def calculate_similarity(doc1, doc2):\n        vec1 = steadytext.embed(doc1)\n        vec2 = steadytext.embed(doc2)\n        return np.dot(vec1, vec2)  # Already normalized\n\n    # These similarities are always the same\n    similarity = calculate_similarity(\n        \"machine learning algorithms\",\n        \"artificial intelligence methods\"\n    )\n\n    assert similarity &gt; 0.7  # Reliable threshold\n    assert similarity &lt; 1.0  # Not identical documents\n</code></pre>"},{"location":"examples/testing/#mock-ai-services","title":"Mock AI Services","text":""},{"location":"examples/testing/#simple-mock","title":"Simple Mock","text":"<pre><code>class MockAI:\n    \"\"\"Deterministic AI mock for testing.\"\"\"\n\n    def complete(self, prompt: str) -&gt; str:\n        return steadytext.generate(prompt)\n\n    def embed(self, text: str) -&gt; np.ndarray:\n        return steadytext.embed(text)\n\n    def chat(self, messages: list) -&gt; str:\n        # Convert chat format to single prompt\n        prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" \n                           for msg in messages])\n        return steadytext.generate(f\"Chat response to: {prompt}\")\n\n# Usage in tests\ndef test_chat_functionality():\n    ai = MockAI()\n    response = ai.chat([\n        {\"role\": \"user\", \"content\": \"Hello\"}\n    ])\n\n    # Response is always the same\n    assert len(response) &gt; 0\n    assert \"hello\" in response.lower()\n</code></pre>"},{"location":"examples/testing/#advanced-mock-with-state","title":"Advanced Mock with State","text":"<pre><code>class StatefulMockAI:\n    \"\"\"Mock AI that maintains conversation state.\"\"\"\n\n    def __init__(self):\n        self.conversation_history = []\n\n    def chat(self, message: str) -&gt; str:\n        # Include history in prompt for context\n        history = \"\\n\".join(self.conversation_history[-5:])  # Last 5 messages\n        full_prompt = f\"History: {history}\\nNew message: {message}\"\n\n        response = steadytext.generate(full_prompt)\n\n        # Update history\n        self.conversation_history.append(f\"User: {message}\")\n        self.conversation_history.append(f\"AI: {response}\")\n\n        return response\n\ndef test_conversation_flow():\n    \"\"\"Test multi-turn conversations.\"\"\"\n    ai = StatefulMockAI()\n\n    response1 = ai.chat(\"What's the weather like?\")\n    response2 = ai.chat(\"What about tomorrow?\")\n\n    # Both responses are deterministic\n    assert len(response1) &gt; 0\n    assert len(response2) &gt; 0\n    # Tomorrow's response considers the context\n    assert response2 != response1\n</code></pre>"},{"location":"examples/testing/#test-data-generation","title":"Test Data Generation","text":""},{"location":"examples/testing/#reproducible-fixtures","title":"Reproducible Fixtures","text":"<pre><code>def generate_test_user(user_id: int) -&gt; dict:\n    \"\"\"Generate consistent test user data.\"\"\"\n    return {\n        \"id\": user_id,\n        \"name\": steadytext.generate(f\"Generate name for user {user_id}\"),\n        \"bio\": steadytext.generate(f\"Write bio for user {user_id}\"),\n        \"interests\": steadytext.generate(f\"List interests for user {user_id}\"),\n        \"embedding\": steadytext.embed(f\"user {user_id} profile\")\n    }\n\ndef test_user_recommendation():\n    \"\"\"Test user recommendation system.\"\"\"\n    # Generate consistent test users\n    users = [generate_test_user(i) for i in range(10)]\n\n    # Test similarity calculations\n    user1 = users[0]\n    user2 = users[1]\n\n    similarity = np.dot(user1[\"embedding\"], user2[\"embedding\"])\n\n    # Similarity is always the same for these users\n    assert isinstance(similarity, float)\n    assert -1.0 &lt;= similarity &lt;= 1.0\n</code></pre>"},{"location":"examples/testing/#fuzz-testing","title":"Fuzz Testing","text":"<pre><code>def generate_fuzz_input(test_name: str, iteration: int) -&gt; str:\n    \"\"\"Generate reproducible fuzz test inputs.\"\"\"\n    seed_prompt = f\"Generate test input for {test_name} iteration {iteration}\"\n    return steadytext.generate(seed_prompt)\n\ndef test_parser_robustness():\n    \"\"\"Fuzz test with reproducible inputs.\"\"\"\n\n    def parse_user_input(text):\n        # Your parsing function\n        return {\"words\": text.split(), \"length\": len(text)}\n\n    # Generate 100 consistent fuzz inputs\n    for i in range(100):\n        fuzz_input = generate_fuzz_input(\"parser_test\", i)\n\n        try:\n            result = parse_user_input(fuzz_input)\n            assert isinstance(result, dict)\n            assert \"words\" in result\n            assert \"length\" in result\n        except Exception as e:\n            # Reproducible error case\n            print(f\"Fuzz input {i} caused error: {e}\")\n            print(f\"Input was: {fuzz_input[:100]}...\")\n</code></pre>"},{"location":"examples/testing/#integration-testing","title":"Integration Testing","text":""},{"location":"examples/testing/#api-testing","title":"API Testing","text":"<pre><code>import requests_mock\n\ndef test_ai_api_integration():\n    \"\"\"Test integration with AI API using deterministic responses.\"\"\"\n\n    with requests_mock.Mocker() as m:\n        # Mock the AI API with deterministic responses\n        def generate_response(request, context):\n            prompt = request.json().get(\"prompt\", \"\")\n            return {\"response\": steadytext.generate(prompt)}\n\n        m.post(\"https://api.ai-service.com/generate\", json=generate_response)\n\n        # Your actual API client code\n        response = requests.post(\"https://api.ai-service.com/generate\", \n                               json={\"prompt\": \"Hello world\"})\n\n        # Response is always the same\n        expected_text = steadytext.generate(\"Hello world\")\n        assert response.json()[\"response\"] == expected_text\n</code></pre>"},{"location":"examples/testing/#database-testing","title":"Database Testing","text":"<pre><code>import sqlite3\n\ndef test_ai_content_storage():\n    \"\"\"Test storing AI-generated content in database.\"\"\"\n\n    # Create in-memory database\n    conn = sqlite3.connect(\":memory:\")\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n        CREATE TABLE content (\n            id INTEGER PRIMARY KEY,\n            prompt TEXT,\n            generated_text TEXT,\n            embedding BLOB\n        )\n    \"\"\")\n\n    # Generate deterministic content\n    prompt = \"Write a short story about AI\"\n    text = steadytext.generate(prompt)\n    embedding = steadytext.embed(text)\n\n    # Store in database\n    cursor.execute(\"\"\"\n        INSERT INTO content (prompt, generated_text, embedding) \n        VALUES (?, ?, ?)\n    \"\"\", (prompt, text, embedding.tobytes()))\n\n    # Verify storage\n    cursor.execute(\"SELECT * FROM content WHERE id = 1\")\n    row = cursor.fetchone()\n\n    assert row[1] == prompt\n    assert row[2] == text\n    assert len(row[3]) == 1024 * 4  # 1024 float32 values\n\n    conn.close()\n</code></pre>"},{"location":"examples/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"examples/testing/#consistency-benchmarks","title":"Consistency Benchmarks","text":"<pre><code>import time\n\ndef test_generation_performance():\n    \"\"\"Test that generation performance is consistent.\"\"\"\n\n    prompt = \"Explain machine learning in one paragraph\"\n    times = []\n\n    # Warm up cache\n    steadytext.generate(prompt)\n\n    # Measure cached performance\n    for _ in range(10):\n        start = time.time()\n        result = steadytext.generate(prompt)\n        end = time.time()\n        times.append(end - start)\n\n    avg_time = sum(times) / len(times)\n\n    # Cached calls should be very fast\n    assert avg_time &lt; 0.1  # Less than 100ms\n\n    # All results should be identical\n    results = [steadytext.generate(prompt) for _ in range(5)]\n    assert all(r == results[0] for r in results)\n</code></pre>"},{"location":"examples/testing/#best-practices","title":"Best Practices","text":"<p>Testing Guidelines</p> <ol> <li>Use deterministic prompts: Keep test prompts simple and specific</li> <li>Cache warmup: Call functions once before timing tests</li> <li>Mock external services: Use SteadyText to replace real AI APIs</li> <li>Test edge cases: Generate consistent edge case inputs</li> <li>Version pin: Keep SteadyText version fixed for test stability</li> </ol> <p>Limitations</p> <ul> <li>Model changes: Updates to SteadyText models will change outputs</li> <li>Creative tasks: SteadyText is optimized for consistency, not creativity</li> <li>Context length: Limited to model's context window</li> </ul>"},{"location":"examples/testing/#complete-example","title":"Complete Example","text":"<pre><code>import unittest\nimport numpy as np\nimport steadytext\n\nclass TestAIFeatures(unittest.TestCase):\n\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.mock_ai = MockAI()\n        self.test_prompts = [\n            \"Write a function to sort a list\",\n            \"Explain what is machine learning\",\n            \"Generate a product description\"\n        ]\n\n    def test_deterministic_generation(self):\n        \"\"\"Test that generation is deterministic.\"\"\"\n        for prompt in self.test_prompts:\n            result1 = steadytext.generate(prompt)\n            result2 = steadytext.generate(prompt)\n            self.assertEqual(result1, result2)\n\n    def test_embedding_consistency(self):\n        \"\"\"Test that embeddings are consistent.\"\"\"\n        text = \"test embedding consistency\"\n        vec1 = steadytext.embed(text)\n        vec2 = steadytext.embed(text)\n        np.testing.assert_array_equal(vec1, vec2)\n\n    def test_mock_ai_service(self):\n        \"\"\"Test mock AI service.\"\"\"\n        response = self.mock_ai.complete(\"Hello\")\n        self.assertIsInstance(response, str)\n        self.assertGreater(len(response), 0)\n\n        # Response should be deterministic\n        response2 = self.mock_ai.complete(\"Hello\")\n        self.assertEqual(response, response2)\n\nif __name__ == \"__main__\":\n    unittest.main()\n</code></pre> <p>This comprehensive testing approach ensures your AI features are reliable, reproducible, and maintainable.</p>"},{"location":"examples/tooling/","title":"CLI Tools &amp; Tooling","text":"<p>Build deterministic command-line tools and development utilities with SteadyText.</p>"},{"location":"examples/tooling/#why-steadytext-for-cli-tools","title":"Why SteadyText for CLI Tools?","text":"<p>Traditional AI-powered CLI tools have problems:</p> <ul> <li>Inconsistent outputs: Same command gives different results</li> <li>Unreliable automation: Scripts break due to changing responses  </li> <li>Hard to test: Non-deterministic behavior makes testing difficult</li> <li>User confusion: Users expect consistent behavior from tools</li> </ul> <p>SteadyText solves these with deterministic outputs - same input always produces the same result.</p>"},{"location":"examples/tooling/#basic-cli-patterns","title":"Basic CLI Patterns","text":""},{"location":"examples/tooling/#simple-command-tools","title":"Simple Command Tools","text":"<pre><code>import click\nimport steadytext\n\n@click.command()\n@click.argument('topic')\ndef motivate(topic):\n    \"\"\"Generate motivational quotes about any topic.\"\"\"\n    prompt = f\"Write an inspiring quote about {topic}\"\n    quote = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udcaa {quote}\")\n\n# Usage: python script.py programming\n# Always generates the same quote for \"programming\"\n</code></pre>"},{"location":"examples/tooling/#error-code-explainer","title":"Error Code Explainer","text":"<pre><code>@click.command()\n@click.argument('error_code')\ndef explain(error_code):\n    \"\"\"Convert error codes to friendly explanations.\"\"\"\n    prompt = f\"Explain error {error_code} in simple, user-friendly terms\"\n    explanation = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udd0d {error_code}: {explanation}\")\n\n# Usage: python script.py ECONNREFUSED\n# Always gives the same explanation for ECONNREFUSED\n</code></pre>"},{"location":"examples/tooling/#command-generator","title":"Command Generator","text":"<pre><code>@click.command()\n@click.argument('task')\ndef git_helper(task):\n    \"\"\"Generate git commands for common tasks.\"\"\"\n    prompt = f\"Git command to {task}. Return only the command.\"\n    command = steadytext.generate(prompt).strip()\n    click.echo(f\"\ud83d\udcbb {command}\")\n\n# Usage: python script.py \"undo last commit\"\n# Always suggests the same git command\n</code></pre>"},{"location":"examples/tooling/#development-tools","title":"Development Tools","text":""},{"location":"examples/tooling/#code-generation-helper","title":"Code Generation Helper","text":"<pre><code>import os\nimport click\n\n@click.group()\ndef codegen():\n    \"\"\"Code generation CLI tool.\"\"\"\n    pass\n\n@codegen.command()\n@click.argument('function_name')\n@click.argument('description')\n@click.option('--language', '-l', default='python', help='Programming language')\ndef function(function_name, description, language):\n    \"\"\"Generate a function from description.\"\"\"\n    prompt = f\"Write a {language} function named {function_name} that {description}\"\n    code = steadytext.generate(prompt)\n\n    # Save to file\n    ext = {'python': 'py', 'javascript': 'js', 'rust': 'rs'}.get(language, 'txt')\n    filename = f\"{function_name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(code)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n    click.echo(f\"\ud83d\udcc4 Preview:\\n{code[:200]}...\")\n\n# Usage: python codegen.py function binary_search \"search for item in sorted array\"\n</code></pre>"},{"location":"examples/tooling/#documentation-generator","title":"Documentation Generator","text":"<pre><code>@codegen.command()\n@click.argument('project_name')\ndef readme(project_name):\n    \"\"\"Generate README.md for a project.\"\"\"\n    prompt = f\"Write a comprehensive README.md for a project called {project_name}\"\n    readme_content = steadytext.generate(prompt)\n\n    with open('README.md', 'w') as f:\n        f.write(readme_content)\n\n    click.echo(\"\u2705 Generated README.md\")\n\n# Usage: python codegen.py readme \"my-awesome-project\"\n</code></pre>"},{"location":"examples/tooling/#testing-qa-tools","title":"Testing &amp; QA Tools","text":""},{"location":"examples/tooling/#test-case-generator","title":"Test Case Generator","text":"<pre><code>@click.command()\n@click.argument('function_description')\ndef test_cases(function_description):\n    \"\"\"Generate test cases for a function.\"\"\"\n    prompt = f\"Generate 5 test cases for a function that {function_description}\"\n    cases = steadytext.generate(prompt)\n\n    # Save to test file\n    with open('test_cases.py', 'w') as f:\n        f.write(f\"# Test cases for: {function_description}\\n\")\n        f.write(cases)\n\n    click.echo(\"\u2705 Generated test_cases.py\")\n    click.echo(f\"\ud83d\udccb Preview:\\n{cases[:300]}...\")\n\n# Usage: python tool.py \"calculates fibonacci numbers\"\n</code></pre>"},{"location":"examples/tooling/#mock-data-generator","title":"Mock Data Generator","text":"<pre><code>@click.command()\n@click.argument('data_type')\n@click.option('--count', '-c', default=10, help='Number of items to generate')\ndef mockdata(data_type, count):\n    \"\"\"Generate mock data for testing.\"\"\"\n    items = []\n\n    for i in range(count):\n        prompt = f\"Generate realistic {data_type} data item {i+1}\"\n        item = steadytext.generate(prompt)\n        items.append(item.strip())\n\n    # Output as JSON\n    import json\n    output = {data_type: items}\n\n    with open(f'mock_{data_type}.json', 'w') as f:\n        json.dump(output, f, indent=2)\n\n    click.echo(f\"\u2705 Generated mock_{data_type}.json with {count} items\")\n\n# Usage: python tool.py user_profiles --count 20\n</code></pre>"},{"location":"examples/tooling/#content-documentation-tools","title":"Content &amp; Documentation Tools","text":""},{"location":"examples/tooling/#commit-message-generator","title":"Commit Message Generator","text":"<pre><code>@click.command()\n@click.argument('changes', nargs=-1)\ndef commit_msg(changes):\n    \"\"\"Generate commit messages from change descriptions.\"\"\"\n    change_list = \" \".join(changes)\n    prompt = f\"Write a concise git commit message for: {change_list}\"\n    message = steadytext.generate(prompt).strip()\n\n    click.echo(f\"\ud83d\udcdd Suggested commit message:\")\n    click.echo(f\"   {message}\")\n\n    # Optionally copy to clipboard or commit directly\n    if click.confirm(\"Use this commit message?\"):\n        os.system(f'git commit -m \"{message}\"')\n        click.echo(\"\u2705 Committed!\")\n\n# Usage: python tool.py \"added user authentication\" \"fixed login bug\"\n</code></pre>"},{"location":"examples/tooling/#api-documentation-generator","title":"API Documentation Generator","text":"<pre><code>@click.command()\n@click.argument('api_endpoint')\n@click.argument('description')\ndef api_docs(api_endpoint, description):\n    \"\"\"Generate API documentation for an endpoint.\"\"\"\n    prompt = f\"\"\"Generate API documentation for endpoint {api_endpoint} that {description}.\n    Include: description, parameters, example request/response, error codes.\"\"\"\n\n    docs = steadytext.generate(prompt)\n\n    # Save to markdown file\n    safe_name = api_endpoint.replace('/', '_').replace('{', '').replace('}', '')\n    filename = f\"api_{safe_name}.md\"\n\n    with open(filename, 'w') as f:\n        f.write(f\"# {api_endpoint}\\n\\n\")\n        f.write(docs)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py \"/users/{id}\" \"returns user profile information\"\n</code></pre>"},{"location":"examples/tooling/#automation-scripting","title":"Automation &amp; Scripting","text":""},{"location":"examples/tooling/#configuration-generator","title":"Configuration Generator","text":"<pre><code>@click.command()\n@click.argument('service_name')\n@click.option('--format', '-f', default='yaml', help='Config format (yaml, json, toml)')\ndef config(service_name, format):\n    \"\"\"Generate configuration files for services.\"\"\"\n    prompt = f\"Generate a {format} configuration file for {service_name} service\"\n    config_content = steadytext.generate(prompt)\n\n    ext = {'yaml': 'yml', 'json': 'json', 'toml': 'toml'}.get(format, 'txt')\n    filename = f\"{service_name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(config_content)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py database --format yaml\n</code></pre>"},{"location":"examples/tooling/#script-template-generator","title":"Script Template Generator","text":"<pre><code>@click.command()\n@click.argument('script_type')\n@click.argument('purpose')\ndef script_template(script_type, purpose):\n    \"\"\"Generate script templates for common tasks.\"\"\"\n    prompt = f\"Generate a {script_type} script template for {purpose}\"\n    script = steadytext.generate(prompt)\n\n    ext = {'bash': 'sh', 'python': 'py', 'powershell': 'ps1'}.get(script_type, 'txt')\n    filename = f\"template.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(script)\n\n    # Make executable if shell script\n    if ext == 'sh':\n        os.chmod(filename, 0o755)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py bash \"automated deployment\"\n</code></pre>"},{"location":"examples/tooling/#complete-cli-tool-example","title":"Complete CLI Tool Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nDevHelper - A deterministic development tool powered by SteadyText\n\"\"\"\n\nimport os\nimport json\nimport click\nimport steadytext\n\n@click.group()\n@click.version_option()\ndef cli():\n    \"\"\"DevHelper - Deterministic development utilities.\"\"\"\n    pass\n\n@cli.group()\ndef generate():\n    \"\"\"Code and content generation commands.\"\"\"\n    pass\n\n@generate.command()\n@click.argument('name')\n@click.argument('description')\n@click.option('--lang', '-l', default='python', help='Programming language')\ndef function(name, description, lang):\n    \"\"\"Generate a function from description.\"\"\"\n    prompt = f\"Write a {lang} function named {name} that {description}\"\n    code = steadytext.generate(prompt)\n\n    ext = {'python': 'py', 'javascript': 'js', 'rust': 'rs'}.get(lang, 'txt')\n    filename = f\"{name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(code)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n@generate.command()\n@click.argument('count', type=int)\n@click.option('--type', '-t', default='user', help='Data type to generate')\ndef testdata(count, type):\n    \"\"\"Generate test data.\"\"\"\n    data = []\n\n    for i in range(count):\n        prompt = f\"Generate realistic {type} test data item {i+1} as JSON\"\n        item = steadytext.generate(prompt)\n        data.append(item.strip())\n\n    output_file = f\"test_{type}_data.json\"\n    with open(output_file, 'w') as f:\n        json.dump({f\"{type}_data\": data}, f, indent=2)\n\n    click.echo(f\"\u2705 Generated {output_file} with {count} items\")\n\n@cli.command()\n@click.argument('error_code')\ndef explain(error_code):\n    \"\"\"Explain error codes in friendly terms.\"\"\"\n    prompt = f\"Explain error {error_code} in simple, user-friendly terms\"\n    explanation = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udd0d {error_code}:\")\n    click.echo(f\"   {explanation}\")\n\n@cli.command()\n@click.argument('task')\ndef git(task):\n    \"\"\"Generate git commands for tasks.\"\"\"\n    prompt = f\"Git command to {task}. Return only the command.\"\n    command = steadytext.generate(prompt).strip()\n    click.echo(f\"\ud83d\udcbb {command}\")\n\n    if click.confirm(\"Execute this command?\"):\n        os.system(command)\n\nif __name__ == '__main__':\n    cli()\n</code></pre> <p>Save this as <code>devhelper.py</code> and use it:</p> <pre><code># Generate a function\npython devhelper.py generate function binary_search \"search sorted array\"\n\n# Generate test data  \npython devhelper.py generate testdata 10 --type user\n\n# Explain error codes\npython devhelper.py explain ECONNREFUSED\n\n# Get git commands\npython devhelper.py git \"undo last commit but keep changes\"\n</code></pre>"},{"location":"examples/tooling/#best-practices","title":"Best Practices","text":"<p>CLI Tool Guidelines</p> <ol> <li>Keep prompts specific: Clear, detailed prompts give better results</li> <li>Add confirmation prompts: For destructive operations, ask before executing</li> <li>Save outputs to files: Generate content to files for later use</li> <li>Use consistent formatting: Same input should always produce same output</li> <li>Add help text: Use Click's built-in help system</li> </ol> <p>Benefits of Deterministic CLI Tools</p> <ul> <li>Reliable automation: Scripts work consistently</li> <li>Easier testing: Predictable outputs make testing simple</li> <li>User trust: Users know what to expect</li> <li>Debugging: Reproducible behavior makes issues easier to track</li> <li>Documentation: Examples in docs always work</li> </ul> <p>Considerations</p> <ul> <li>Creative vs. Deterministic: SteadyText prioritizes consistency over creativity</li> <li>Context limits: Model has limited context window</li> <li>Update impacts: SteadyText updates may change outputs (major versions only)</li> </ul> <p>This approach creates reliable, testable CLI tools that users can depend on for consistent behavior.</p>"},{"location":"integrations/timescaledb/","title":"TimescaleDB + SteadyText: AI-Powered Time-Series Analytics","text":"<p>Combine TimescaleDB's time-series superpowers with SteadyText's AI capabilities for intelligent, automated analytics at scale.</p>"},{"location":"integrations/timescaledb/#overview","title":"Overview","text":"<p>TimescaleDB + SteadyText enables: - Continuous AI aggregates that summarize data automatically - Real-time pattern detection in time-series data - Intelligent alerting based on AI analysis - Automated report generation from historical data - Predictive insights from time-series trends</p>"},{"location":"integrations/timescaledb/#prerequisites","title":"Prerequisites","text":"<pre><code># Option 1: Docker with both extensions\ndocker run -d -p 5432:5432 \\\n  -e POSTGRES_PASSWORD=password \\\n  timescale/timescaledb-ha:pg16 \\\n  -c shared_preload_libraries='timescaledb,pg_steadytext'\n\n# Option 2: Install on existing TimescaleDB\nCREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;\nCREATE EXTENSION IF NOT EXISTS pg_steadytext CASCADE;\n</code></pre>"},{"location":"integrations/timescaledb/#core-concepts","title":"Core Concepts","text":""},{"location":"integrations/timescaledb/#hypertables-meet-ai","title":"Hypertables Meet AI","text":"<pre><code>-- Create a hypertable for sensor data\nCREATE TABLE sensor_data (\n    time TIMESTAMPTZ NOT NULL,\n    sensor_id INTEGER,\n    temperature DOUBLE PRECISION,\n    humidity DOUBLE PRECISION,\n    status TEXT,\n    error_message TEXT\n);\n\nSELECT create_hypertable('sensor_data', 'time');\n\n-- Add AI analysis column\nALTER TABLE sensor_data \nADD COLUMN ai_analysis TEXT;\n\n-- Automatically analyze anomalies on insert\nCREATE OR REPLACE FUNCTION analyze_sensor_reading()\nRETURNS TRIGGER AS $$\nBEGIN\n    IF NEW.temperature &gt; 80 OR NEW.temperature &lt; 20 THEN\n        NEW.ai_analysis := steadytext_generate(\n            format('Analyze abnormal temperature reading: %s\u00b0C at sensor %s. Previous status: %s',\n                NEW.temperature, NEW.sensor_id, NEW.status),\n            max_tokens := 100\n        );\n    END IF;\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER analyze_on_insert\n    BEFORE INSERT ON sensor_data\n    FOR EACH ROW\n    EXECUTE FUNCTION analyze_sensor_reading();\n</code></pre>"},{"location":"integrations/timescaledb/#continuous-ai-aggregates","title":"Continuous AI Aggregates","text":"<p>The killer feature: AI summaries that update automatically!</p>"},{"location":"integrations/timescaledb/#example-1-hourly-log-summaries","title":"Example 1: Hourly Log Summaries","text":"<pre><code>-- Create continuous aggregate with AI summaries\nCREATE MATERIALIZED VIEW hourly_system_insights\nWITH (timescaledb.continuous) AS\nSELECT \n    time_bucket('1 hour'::interval, time) AS hour,\n    count(*) AS event_count,\n    count(*) FILTER (WHERE severity = 'ERROR') AS error_count,\n    count(*) FILTER (WHERE severity = 'WARNING') AS warning_count,\n    steadytext_generate(\n        format('Summarize system status: %s total events, %s errors, %s warnings. Key messages: %s',\n            count(*),\n            count(*) FILTER (WHERE severity = 'ERROR'),\n            count(*) FILTER (WHERE severity = 'WARNING'),\n            string_agg(\n                CASE WHEN severity IN ('ERROR', 'WARNING') \n                THEN message ELSE NULL END, \n                '; ' \n                ORDER BY time\n            )\n        ),\n        max_tokens := 200\n    ) AS ai_summary\nFROM system_logs\nGROUP BY hour;\n\n-- Refresh policy for real-time updates\nSELECT add_continuous_aggregate_policy('hourly_system_insights',\n    start_offset =&gt; INTERVAL '2 hours',\n    end_offset =&gt; INTERVAL '10 minutes',\n    schedule_interval =&gt; INTERVAL '10 minutes'\n);\n</code></pre>"},{"location":"integrations/timescaledb/#example-2-daily-business-metrics","title":"Example 2: Daily Business Metrics","text":"<pre><code>-- Sales analysis with AI insights\nCREATE MATERIALIZED VIEW daily_sales_intelligence\nWITH (timescaledb.continuous) AS\nSELECT \n    time_bucket('1 day'::interval, order_time) AS day,\n    product_category,\n    COUNT(*) AS orders,\n    SUM(amount) AS revenue,\n    AVG(amount) AS avg_order_value,\n    COUNT(DISTINCT customer_id) AS unique_customers,\n    steadytext_generate(\n        format('Analyze sales performance for %s: $%s revenue from %s orders (%s customers). AOV: $%s',\n            product_category,\n            ROUND(SUM(amount), 2),\n            COUNT(*),\n            COUNT(DISTINCT customer_id),\n            ROUND(AVG(amount), 2)\n        ),\n        max_tokens := 150\n    ) AS performance_analysis,\n    steadytext_generate_json(\n        format('Suggest 3 actions to improve %s sales based on: revenue=$%s, orders=%s, AOV=$%s',\n            product_category,\n            ROUND(SUM(amount), 2),\n            COUNT(*),\n            ROUND(AVG(amount), 2)\n        ),\n        '{\"recommendations\": {\"type\": \"array\", \"items\": {\"type\": \"string\"}}}'::json\n    )::jsonb AS ai_recommendations\nFROM orders\nGROUP BY day, product_category;\n\n-- Auto-refresh every hour\nSELECT add_continuous_aggregate_policy('daily_sales_intelligence',\n    start_offset =&gt; INTERVAL '3 days',\n    end_offset =&gt; INTERVAL '1 hour',\n    schedule_interval =&gt; INTERVAL '1 hour'\n);\n</code></pre>"},{"location":"integrations/timescaledb/#real-time-pattern-detection","title":"Real-Time Pattern Detection","text":"<p>Detect complex patterns in streaming data:</p> <pre><code>-- Function to detect patterns across time windows\nCREATE OR REPLACE FUNCTION detect_anomaly_patterns(\n    p_hours_back INTEGER DEFAULT 24\n)\nRETURNS TABLE (\n    pattern_type VARCHAR,\n    severity VARCHAR,\n    affected_sensors INTEGER[],\n    ai_diagnosis TEXT,\n    recommended_action TEXT\n) AS $$\nBEGIN\n    RETURN QUERY\n    WITH recent_data AS (\n        SELECT \n            sensor_id,\n            time,\n            temperature,\n            humidity,\n            LAG(temperature) OVER (PARTITION BY sensor_id ORDER BY time) AS prev_temp,\n            LAG(humidity) OVER (PARTITION BY sensor_id ORDER BY time) AS prev_humidity\n        FROM sensor_data\n        WHERE time &gt; NOW() - (p_hours_back || ' hours')::INTERVAL\n    ),\n    anomalies AS (\n        SELECT \n            sensor_id,\n            COUNT(*) AS anomaly_count,\n            AVG(ABS(temperature - prev_temp)) AS avg_temp_change,\n            MAX(temperature) AS max_temp,\n            MIN(temperature) AS min_temp\n        FROM recent_data\n        WHERE ABS(temperature - prev_temp) &gt; 5  -- Rapid changes\n           OR temperature &gt; 75 \n           OR temperature &lt; 25\n        GROUP BY sensor_id\n        HAVING COUNT(*) &gt; 3  -- Persistent anomalies\n    )\n    SELECT \n        'temperature_instability' AS pattern_type,\n        CASE \n            WHEN MAX(anomaly_count) &gt; 10 THEN 'critical'\n            WHEN MAX(anomaly_count) &gt; 5 THEN 'high'\n            ELSE 'medium'\n        END AS severity,\n        array_agg(sensor_id) AS affected_sensors,\n        steadytext_generate(\n            format('Diagnose temperature instability: %s sensors affected, max variations: %s\u00b0C',\n                COUNT(DISTINCT sensor_id),\n                ROUND(MAX(avg_temp_change), 2)\n            ),\n            max_tokens := 150\n        ) AS ai_diagnosis,\n        steadytext_generate(\n            format('Recommend action for %s sensors with temperature anomalies (severity: %s)',\n                COUNT(DISTINCT sensor_id),\n                CASE \n                    WHEN MAX(anomaly_count) &gt; 10 THEN 'critical'\n                    WHEN MAX(anomaly_count) &gt; 5 THEN 'high'\n                    ELSE 'medium'\n                END\n            ),\n            max_tokens := 100\n        ) AS recommended_action\n    FROM anomalies\n    GROUP BY pattern_type;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"integrations/timescaledb/#intelligent-data-retention","title":"Intelligent Data Retention","text":"<p>Use AI to decide what historical data to keep:</p> <pre><code>-- Intelligent compression policy\nCREATE OR REPLACE FUNCTION intelligent_compression_policy()\nRETURNS VOID AS $$\nDECLARE\n    v_chunk RECORD;\n    v_importance_score DECIMAL;\n    v_compression_decision TEXT;\nBEGIN\n    FOR v_chunk IN \n        SELECT \n            chunk_name,\n            range_start,\n            range_end,\n            chunk_table_size,\n            compression_status\n        FROM timescaledb_information.chunks\n        WHERE hypertable_name = 'sensor_data'\n          AND range_end &lt; NOW() - INTERVAL '7 days'\n          AND compression_status IS NULL\n    LOOP\n        -- AI evaluates chunk importance\n        v_compression_decision := steadytext_generate_choice(\n            format('Should we compress sensor data from %s to %s? Size: %s. Analyze for historical importance.',\n                v_chunk.range_start::date,\n                v_chunk.range_end::date,\n                pg_size_pretty(v_chunk.chunk_table_size)\n            ),\n            ARRAY['compress_aggressive', 'compress_normal', 'keep_uncompressed']\n        );\n\n        -- Execute decision\n        CASE v_compression_decision\n            WHEN 'compress_aggressive' THEN\n                -- Compress with aggressive settings\n                PERFORM compress_chunk(v_chunk.chunk_name::regclass, if_not_compressed =&gt; true);\n\n            WHEN 'compress_normal' THEN\n                -- Standard compression\n                PERFORM compress_chunk(v_chunk.chunk_name::regclass);\n\n            ELSE\n                -- Keep uncompressed for now\n                RAISE NOTICE 'Keeping chunk % uncompressed due to importance', v_chunk.chunk_name;\n        END CASE;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Schedule intelligent compression\nSELECT cron.schedule('intelligent_compression', '0 2 * * *', 'SELECT intelligent_compression_policy()');\n</code></pre>"},{"location":"integrations/timescaledb/#predictive-analytics","title":"Predictive Analytics","text":"<p>Combine time-series data with AI predictions:</p> <pre><code>-- Predictive maintenance system\nCREATE OR REPLACE FUNCTION predict_equipment_failure(\n    p_sensor_id INTEGER,\n    p_hours_ahead INTEGER DEFAULT 24\n)\nRETURNS TABLE (\n    prediction_time TIMESTAMPTZ,\n    failure_probability DECIMAL,\n    risk_factors JSONB,\n    maintenance_recommendation TEXT\n) AS $$\nDECLARE\n    v_recent_patterns TEXT;\n    v_historical_failures INTEGER;\nBEGIN\n    -- Gather recent patterns\n    SELECT \n        format('Recent: Avg temp %s\u00b0C, %s errors in last 24h, %s maintenance events',\n            ROUND(AVG(temperature), 1),\n            COUNT(*) FILTER (WHERE error_message IS NOT NULL),\n            COUNT(DISTINCT maintenance_id)\n        ) INTO v_recent_patterns\n    FROM sensor_data\n    WHERE sensor_id = p_sensor_id\n      AND time &gt; NOW() - INTERVAL '24 hours';\n\n    -- Get historical context\n    SELECT COUNT(*) INTO v_historical_failures\n    FROM equipment_failures\n    WHERE sensor_id = p_sensor_id\n      AND time &gt; NOW() - INTERVAL '90 days';\n\n    RETURN QUERY\n    SELECT \n        NOW() + (p_hours_ahead || ' hours')::INTERVAL AS prediction_time,\n        (steadytext_generate_json(\n            format('Predict failure probability (0-1) for sensor %s: %s. Historical failures: %s',\n                p_sensor_id, v_recent_patterns, v_historical_failures),\n            '{\"probability\": {\"type\": \"number\", \"minimum\": 0, \"maximum\": 1}}'::json\n        )::jsonb-&gt;&gt;'probability')::DECIMAL AS failure_probability,\n        steadytext_generate_json(\n            format('Identify risk factors for sensor %s: %s',\n                p_sensor_id, v_recent_patterns),\n            '{\n                \"temperature_risk\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]},\n                \"usage_pattern_risk\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]},\n                \"age_risk\": {\"type\": \"string\", \"enum\": [\"low\", \"medium\", \"high\"]},\n                \"maintenance_overdue\": {\"type\": \"boolean\"}\n            }'::json\n        )::jsonb AS risk_factors,\n        steadytext_generate(\n            format('Recommend maintenance for sensor %s based on: %s',\n                p_sensor_id, v_recent_patterns),\n            max_tokens := 150\n        ) AS maintenance_recommendation;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"integrations/timescaledb/#performance-optimization","title":"Performance Optimization","text":""},{"location":"integrations/timescaledb/#parallel-ai-processing","title":"Parallel AI Processing","text":"<pre><code>-- Enable parallel processing for large aggregates\nALTER DATABASE mydb SET max_parallel_workers_per_gather = 4;\nALTER DATABASE mydb SET max_parallel_workers = 8;\n\n-- Parallel AI analysis function\nCREATE OR REPLACE FUNCTION parallel_analyze_time_range(\n    start_time TIMESTAMPTZ,\n    end_time TIMESTAMPTZ,\n    bucket_size INTERVAL DEFAULT '1 hour'\n)\nRETURNS TABLE (\n    bucket TIMESTAMPTZ,\n    analysis TEXT\n) AS $$\nBEGIN\n    -- Force parallel execution\n    SET LOCAL max_parallel_workers_per_gather = 4;\n\n    RETURN QUERY\n    SELECT \n        time_bucket(bucket_size, time) AS bucket,\n        steadytext_generate(\n            'Summarize: ' || string_agg(message, '; '),\n            max_tokens := 100\n        ) AS analysis\n    FROM sensor_data\n    WHERE time BETWEEN start_time AND end_time\n    GROUP BY time_bucket(bucket_size, time)\n    ORDER BY bucket;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"integrations/timescaledb/#caching-strategies","title":"Caching Strategies","text":"<pre><code>-- Cache AI results for frequently accessed time periods\nCREATE TABLE ai_analysis_cache (\n    time_bucket TIMESTAMPTZ,\n    cache_key VARCHAR(255),\n    analysis_result TEXT,\n    created_at TIMESTAMPTZ DEFAULT NOW(),\n    PRIMARY KEY (time_bucket, cache_key)\n);\n\n-- Auto-expire old cache entries\nSELECT create_hypertable('ai_analysis_cache', 'time_bucket');\nSELECT add_retention_policy('ai_analysis_cache', INTERVAL '7 days');\n</code></pre>"},{"location":"integrations/timescaledb/#real-world-use-cases","title":"Real-World Use Cases","text":""},{"location":"integrations/timescaledb/#iot-sensor-networks","title":"IoT Sensor Networks","text":"<pre><code>-- Complete IoT monitoring solution\nCREATE MATERIALIZED VIEW iot_fleet_status\nWITH (timescaledb.continuous) AS\nSELECT \n    time_bucket('5 minutes'::interval, time) AS bucket,\n    device_type,\n    COUNT(DISTINCT device_id) AS active_devices,\n    AVG(battery_level) AS avg_battery,\n    COUNT(*) FILTER (WHERE status = 'ERROR') AS errors,\n    steadytext_generate(\n        format('Fleet status: %s %s devices, %s%% avg battery, %s errors',\n            COUNT(DISTINCT device_id),\n            device_type,\n            ROUND(AVG(battery_level)),\n            COUNT(*) FILTER (WHERE status = 'ERROR')\n        ),\n        max_tokens := 100\n    ) AS fleet_summary\nFROM iot_telemetry\nGROUP BY bucket, device_type;\n</code></pre>"},{"location":"integrations/timescaledb/#financial-trading","title":"Financial Trading","text":"<pre><code>-- Market analysis with AI insights\nCREATE MATERIALIZED VIEW market_intelligence\nWITH (timescaledb.continuous) AS\nSELECT \n    time_bucket('1 minute'::interval, time) AS minute,\n    symbol,\n    AVG(price) AS avg_price,\n    SUM(volume) AS total_volume,\n    MAX(price) - MIN(price) AS price_range,\n    steadytext_generate(\n        format('Analyze %s: price movement $%s, volume %s, volatility %s%%',\n            symbol,\n            ROUND(MAX(price) - MIN(price), 2),\n            SUM(volume),\n            ROUND((MAX(price) - MIN(price)) / AVG(price) * 100, 2)\n        ),\n        max_tokens := 100\n    ) AS market_analysis\nFROM trades\nGROUP BY minute, symbol;\n</code></pre>"},{"location":"integrations/timescaledb/#monitoring-alerting","title":"Monitoring &amp; Alerting","text":"<pre><code>-- AI-powered alert system\nCREATE OR REPLACE FUNCTION check_alerts()\nRETURNS VOID AS $$\nDECLARE\n    v_alert RECORD;\nBEGIN\n    FOR v_alert IN \n        SELECT * FROM detect_anomaly_patterns(1)\n        WHERE severity IN ('high', 'critical')\n    LOOP\n        -- Send intelligent alerts\n        PERFORM pg_notify(\n            'ai_alert',\n            jsonb_build_object(\n                'severity', v_alert.severity,\n                'diagnosis', v_alert.ai_diagnosis,\n                'action', v_alert.recommended_action,\n                'timestamp', NOW()\n            )::text\n        );\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Schedule alert checks\nSELECT cron.schedule('ai_alerts', '*/5 * * * *', 'SELECT check_alerts()');\n</code></pre>"},{"location":"integrations/timescaledb/#best-practices","title":"Best Practices","text":"<ol> <li>Chunk Size: Optimize chunk_time_interval for your workload</li> <li>Aggregate Design: Pre-compute AI summaries in continuous aggregates</li> <li>Compression: Use AI to identify compressible chunks</li> <li>Indexes: Create indexes on AI-generated columns for fast queries</li> <li>Parallel Processing: Enable for large-scale AI operations</li> </ol>"},{"location":"integrations/timescaledb/#performance-benchmarks","title":"Performance Benchmarks","text":"<pre><code>-- Benchmark AI processing speed\nDO $$\nDECLARE\n    start_time TIMESTAMP;\n    end_time TIMESTAMP;\n    record_count INTEGER;\nBEGIN\n    start_time := clock_timestamp();\n\n    -- Process 1 million records\n    SELECT COUNT(*) INTO record_count\n    FROM (\n        SELECT steadytext_generate('Analyze: ' || message, 50)\n        FROM system_logs\n        LIMIT 1000000\n    ) t;\n\n    end_time := clock_timestamp();\n\n    RAISE NOTICE 'Processed % records in % seconds (% records/sec)',\n        record_count,\n        EXTRACT(EPOCH FROM (end_time - start_time)),\n        record_count / EXTRACT(EPOCH FROM (end_time - start_time));\nEND $$;\n</code></pre>"},{"location":"integrations/timescaledb/#troubleshooting","title":"Troubleshooting","text":""},{"location":"integrations/timescaledb/#common-issues","title":"Common Issues","text":"<ol> <li>Slow continuous aggregates</li> <li>Solution: Reduce AI token count in aggregates</li> <li> <p>Use sampling for very large time buckets</p> </li> <li> <p>Memory usage</p> </li> <li>Solution: Tune work_mem for AI operations</li> <li> <p>Use batching for large datasets</p> </li> <li> <p>Lock contention</p> </li> <li>Solution: Use CONCURRENTLY option</li> <li>Schedule refreshes during low-traffic periods</li> </ol>"},{"location":"integrations/timescaledb/#next-steps","title":"Next Steps","text":"<ul> <li>Log Analysis Example \u2192</li> <li>Production Deployment \u2192</li> <li>PostgreSQL Extension Docs \u2192</li> </ul> <p>Pro Tip</p> <p>Start with hourly aggregates and tune based on your needs. The combination of TimescaleDB's efficiency and SteadyText's determinism makes even minute-level AI aggregates feasible for many workloads.</p>"},{"location":"migration/from-embeddings-api/","title":"Migrating from Embeddings APIs to SteadyText","text":"<p>Replace external embedding services with fast, deterministic, local embeddings that work directly in your database.</p>"},{"location":"migration/from-embeddings-api/#why-migrate-from-embedding-apis","title":"Why Migrate from Embedding APIs?","text":"External Embedding APIs SteadyText Embeddings Pay per embedding Free after installation Network latency (50-200ms) Local execution (&lt;1ms) Rate limits apply Unlimited embeddings Internet required Works offline Privacy concerns Data never leaves your server Non-deterministic* 100% deterministic <p>*Some providers vary embeddings slightly between calls</p>"},{"location":"migration/from-embeddings-api/#quick-start","title":"Quick Start","text":""},{"location":"migration/from-embeddings-api/#python-migration","title":"Python Migration","text":"<p>Before (OpenAI/Cohere/etc): <pre><code># OpenAI\nimport openai\nopenai.api_key = \"sk-...\"\nresponse = openai.Embedding.create(\n    model=\"text-embedding-ada-002\",\n    input=\"Hello world\"\n)\nembedding = response.data[0].embedding  # 1536 dims\n\n# Cohere\nimport cohere\nco = cohere.Client(\"api-key\")\nresponse = co.embed(texts=[\"Hello world\"])\nembedding = response.embeddings[0]  # 768/1024 dims\n\n# Hugging Face\nfrom transformers import AutoTokenizer, AutoModel\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n# Complex setup...\n</code></pre></p> <p>After (SteadyText): <pre><code>import steadytext\n\n# That's it! No API keys, no setup\nembedding = steadytext.embed(\"Hello world\")  # 1024 dims\n# Always returns the same vector for the same input\n</code></pre></p>"},{"location":"migration/from-embeddings-api/#postgresql-migration","title":"PostgreSQL Migration","text":"<p>Before (Storing external embeddings): <pre><code>-- Complex setup with external calls\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    content TEXT,\n    embedding FLOAT[]  -- Or vector type\n);\n\n-- Need application code to generate embeddings\n-- INSERT happens from Python/Node/etc\n</code></pre></p> <p>After (Native PostgreSQL): <pre><code>CREATE EXTENSION pg_steadytext;\nCREATE EXTENSION pgvector;\n\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    content TEXT,\n    embedding vector(1024)\n);\n\n-- Generate embeddings directly in SQL!\nINSERT INTO documents (content, embedding)\nVALUES \n    ('Hello world', steadytext_embed('Hello world')::vector),\n    ('PostgreSQL rocks', steadytext_embed('PostgreSQL rocks')::vector);\n\n-- Or update existing data\nUPDATE documents \nSET embedding = steadytext_embed(content)::vector\nWHERE embedding IS NULL;\n</code></pre></p>"},{"location":"migration/from-embeddings-api/#common-embedding-api-migrations","title":"Common Embedding API Migrations","text":""},{"location":"migration/from-embeddings-api/#1-openai-text-embedding-ada-002","title":"1. OpenAI text-embedding-ada-002","text":"<p>Dimension Mapping: - OpenAI: 1536 dimensions - SteadyText: 1024 dimensions</p> <p>Migration Script: <pre><code>-- Add new column for SteadyText embeddings\nALTER TABLE documents ADD COLUMN embedding_new vector(1024);\n\n-- Generate new embeddings\nUPDATE documents \nSET embedding_new = steadytext_embed(content)::vector;\n\n-- Once verified, swap columns\nALTER TABLE documents DROP COLUMN embedding;\nALTER TABLE documents RENAME COLUMN embedding_new TO embedding;\n\n-- Update indexes\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);\n</code></pre></p>"},{"location":"migration/from-embeddings-api/#2-cohere-embeddings","title":"2. Cohere Embeddings","text":"<p>Before: <pre><code>import cohere\nco = cohere.Client('api-key')\n\ndef get_embeddings_batch(texts, batch_size=96):\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        response = co.embed(\n            texts=batch,\n            model='embed-english-v3.0',\n            input_type='search_document'\n        )\n        embeddings.extend(response.embeddings)\n    return embeddings\n</code></pre></p> <p>After: <pre><code>import steadytext\n\ndef get_embeddings_batch(texts, batch_size=1000):\n    # No rate limits! Process as fast as your CPU allows\n    return [steadytext.embed(text) for text in texts]\n</code></pre></p>"},{"location":"migration/from-embeddings-api/#3-sentence-transformers","title":"3. Sentence Transformers","text":"<p>Before: <pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer('all-MiniLM-L6-v2')\nembeddings = model.encode(sentences)  # 384 dims\n</code></pre></p> <p>After: <pre><code>import steadytext\n\nembeddings = [steadytext.embed(s) for s in sentences]  # 1024 dims\n# Higher dimensional embeddings often perform better\n</code></pre></p>"},{"location":"migration/from-embeddings-api/#4-voyage-ai","title":"4. Voyage AI","text":"<p>Before: <pre><code>import voyageai\n\nvo = voyageai.Client(api_key=\"...\")\nresult = vo.embed(texts, model=\"voyage-02\")\nembeddings = result.embeddings\n</code></pre></p> <p>After: <pre><code>embeddings = [steadytext.embed(text) for text in texts]\n# No API key needed!\n</code></pre></p>"},{"location":"migration/from-embeddings-api/#semantic-search-migration","title":"Semantic Search Migration","text":""},{"location":"migration/from-embeddings-api/#vector-database-migration","title":"Vector Database Migration","text":"<p>Before (Pinecone/Weaviate/Qdrant): <pre><code>import pinecone\n\npinecone.init(api_key=\"...\", environment=\"...\")\nindex = pinecone.Index(\"my-index\")\n\n# Upload embeddings\nfor i, text in enumerate(texts):\n    embedding = get_external_embedding(text)\n    index.upsert([(str(i), embedding, {\"text\": text})])\n\n# Search\nquery_embedding = get_external_embedding(query)\nresults = index.query(query_embedding, top_k=10)\n</code></pre></p> <p>After (PostgreSQL + pgvector): <pre><code>-- Everything in your database!\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    content TEXT,\n    embedding vector(1024),\n    metadata JSONB\n);\n\n-- Index for fast search\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops);\n\n-- Insert with embeddings\nINSERT INTO documents (content, embedding)\nSELECT content, steadytext_embed(content)::vector\nFROM raw_documents;\n\n-- Search is just SQL\nWITH query AS (\n    SELECT steadytext_embed('search query')::vector AS q_embedding\n)\nSELECT d.*, 1 - (d.embedding &lt;=&gt; q.q_embedding) AS similarity\nFROM documents d, query q\nWHERE d.embedding &lt;=&gt; q.q_embedding &lt; 0.3  -- Distance threshold\nORDER BY d.embedding &lt;=&gt; q.q_embedding\nLIMIT 10;\n</code></pre></p>"},{"location":"migration/from-embeddings-api/#bulk-migration-strategies","title":"Bulk Migration Strategies","text":""},{"location":"migration/from-embeddings-api/#parallel-processing","title":"Parallel Processing","text":"<pre><code>-- Process embeddings in parallel\nCREATE OR REPLACE FUNCTION migrate_embeddings_parallel(\n    batch_size INTEGER DEFAULT 1000\n)\nRETURNS VOID AS $$\nDECLARE\n    v_count INTEGER := 0;\nBEGIN\n    -- Enable parallel processing\n    SET max_parallel_workers_per_gather = 4;\n\n    -- Update in batches\n    LOOP\n        WITH batch AS (\n            SELECT id, content\n            FROM documents\n            WHERE embedding IS NULL\n            LIMIT batch_size\n            FOR UPDATE SKIP LOCKED\n        )\n        UPDATE documents d\n        SET embedding = steadytext_embed(b.content)::vector\n        FROM batch b\n        WHERE d.id = b.id;\n\n        GET DIAGNOSTICS v_count = ROW_COUNT;\n        EXIT WHEN v_count = 0;\n\n        -- Progress notification\n        RAISE NOTICE 'Processed % records', v_count;\n        COMMIT;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"migration/from-embeddings-api/#progress-tracking","title":"Progress Tracking","text":"<pre><code>-- Track migration progress\nCREATE TABLE embedding_migration_progress (\n    id SERIAL PRIMARY KEY,\n    table_name VARCHAR(100),\n    total_records INTEGER,\n    processed_records INTEGER,\n    started_at TIMESTAMPTZ DEFAULT NOW(),\n    completed_at TIMESTAMPTZ,\n    status VARCHAR(20) DEFAULT 'running'\n);\n\n-- Monitor progress\nCREATE OR REPLACE VIEW migration_status AS\nSELECT \n    table_name,\n    processed_records || '/' || total_records AS progress,\n    ROUND(processed_records::NUMERIC / total_records * 100, 2) || '%' AS percentage,\n    CASE \n        WHEN completed_at IS NOT NULL THEN \n            'Completed in ' || (completed_at - started_at)::TEXT\n        ELSE \n            'Running for ' || (NOW() - started_at)::TEXT\n    END AS duration\nFROM embedding_migration_progress;\n</code></pre>"},{"location":"migration/from-embeddings-api/#quality-comparison","title":"Quality Comparison","text":""},{"location":"migration/from-embeddings-api/#ab-testing-embeddings","title":"A/B Testing Embeddings","text":"<pre><code>def compare_embedding_quality(texts, queries):\n    results = {\n        'external': {'precision': [], 'recall': []},\n        'steadytext': {'precision': [], 'recall': []}\n    }\n\n    for query in queries:\n        # External API results\n        external_embedding = get_external_embedding(query)\n        external_results = search_with_embedding(external_embedding)\n\n        # SteadyText results\n        steady_embedding = steadytext.embed(query)\n        steady_results = search_with_embedding(steady_embedding)\n\n        # Compare precision/recall\n        # ... calculation logic ...\n\n    return results\n</code></pre>"},{"location":"migration/from-embeddings-api/#embedding-space-analysis","title":"Embedding Space Analysis","text":"<pre><code>-- Analyze embedding space distribution\nCREATE OR REPLACE FUNCTION analyze_embedding_distribution()\nRETURNS TABLE (\n    metric VARCHAR,\n    value NUMERIC\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT 'avg_magnitude', AVG(sqrt(sum(v * v)))::NUMERIC\n    FROM (\n        SELECT unnest(embedding::float[]) AS v, id\n        FROM documents\n    ) t\n    GROUP BY id\n\n    UNION ALL\n\n    SELECT 'avg_similarity', AVG(1 - (a.embedding &lt;=&gt; b.embedding))::NUMERIC\n    FROM documents a, documents b\n    WHERE a.id &lt; b.id\n    LIMIT 10000;  -- Sample for performance\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"migration/from-embeddings-api/#cost-calculator","title":"Cost Calculator","text":"<pre><code>def calculate_savings(monthly_embeddings, provider=\"openai\"):\n    costs = {\n        \"openai\": 0.0001,      # per 1K tokens\n        \"cohere\": 0.0002,      # per 1K embeddings\n        \"voyage\": 0.00012,     # per 1K embeddings\n        \"anthropic\": 0.0001    # per 1K tokens\n    }\n\n    monthly_cost = (monthly_embeddings / 1000) * costs.get(provider, 0.0001)\n    yearly_cost = monthly_cost * 12\n\n    print(f\"Current {provider} costs:\")\n    print(f\"  Monthly: ${monthly_cost:,.2f}\")\n    print(f\"  Yearly: ${yearly_cost:,.2f}\")\n    print(f\"\\nSteadyText costs:\")\n    print(f\"  Monthly: $0\")\n    print(f\"  Yearly: $0\")\n    print(f\"\\nYearly savings: ${yearly_cost:,.2f}\")\n\n# Example: 10M embeddings/month\ncalculate_savings(10_000_000, \"openai\")\n</code></pre>"},{"location":"migration/from-embeddings-api/#performance-benchmarks","title":"Performance Benchmarks","text":""},{"location":"migration/from-embeddings-api/#speed-comparison","title":"Speed Comparison","text":"<pre><code>import time\nimport statistics\n\ndef benchmark_embedding_speed(texts, iterations=5):\n    # SteadyText\n    steady_times = []\n    for _ in range(iterations):\n        start = time.time()\n        for text in texts:\n            steadytext.embed(text)\n        steady_times.append(time.time() - start)\n\n    # External API (simulated)\n    api_times = []\n    for _ in range(iterations):\n        start = time.time()\n        for text in texts:\n            time.sleep(0.05)  # Simulate 50ms API latency\n        api_times.append(time.time() - start)\n\n    print(f\"SteadyText: {statistics.mean(steady_times):.3f}s \"\n          f\"({len(texts)/statistics.mean(steady_times):.0f} embeddings/sec)\")\n    print(f\"External API: {statistics.mean(api_times):.3f}s \"\n          f\"({len(texts)/statistics.mean(api_times):.0f} embeddings/sec)\")\n</code></pre>"},{"location":"migration/from-embeddings-api/#common-issues-solutions","title":"Common Issues &amp; Solutions","text":""},{"location":"migration/from-embeddings-api/#issue-1-dimension-mismatch","title":"Issue 1: Dimension Mismatch","text":"<pre><code>-- Solution: Re-create vector columns with correct dimensions\nALTER TABLE documents \nALTER COLUMN embedding TYPE vector(1024) \nUSING embedding::vector(1024);\n</code></pre>"},{"location":"migration/from-embeddings-api/#issue-2-different-similarity-scores","title":"Issue 2: Different Similarity Scores","text":"<pre><code># Normalize similarity scores for comparison\ndef normalize_similarity(score, method=\"cosine\"):\n    if method == \"cosine\":\n        return (score + 1) / 2  # Map [-1, 1] to [0, 1]\n    elif method == \"euclidean\":\n        return 1 / (1 + score)  # Map [0, \u221e) to (0, 1]\n</code></pre>"},{"location":"migration/from-embeddings-api/#issue-3-batch-size-optimization","title":"Issue 3: Batch Size Optimization","text":"<pre><code>-- Find optimal batch size for your hardware\nDO $$\nDECLARE\n    batch_sizes INTEGER[] := ARRAY[100, 500, 1000, 5000];\n    size INTEGER;\n    start_time TIMESTAMP;\n    duration INTERVAL;\nBEGIN\n    FOREACH size IN ARRAY batch_sizes\n    LOOP\n        start_time := clock_timestamp();\n\n        PERFORM steadytext_embed(content)\n        FROM documents\n        LIMIT size;\n\n        duration := clock_timestamp() - start_time;\n        RAISE NOTICE 'Batch size %: % seconds', size, duration;\n    END LOOP;\nEND $$;\n</code></pre>"},{"location":"migration/from-embeddings-api/#migration-checklist","title":"Migration Checklist","text":"<ul> <li>[ ] Benchmark current embedding quality</li> <li>[ ] Install SteadyText and pgvector</li> <li>[ ] Create new vector columns with correct dimensions</li> <li>[ ] Migrate embeddings (use parallel processing)</li> <li>[ ] Update application code</li> <li>[ ] Compare search quality (A/B test)</li> <li>[ ] Update vector indexes</li> <li>[ ] Remove external API dependencies</li> <li>[ ] Calculate and celebrate cost savings! \ud83c\udf89</li> </ul>"},{"location":"migration/from-embeddings-api/#next-steps","title":"Next Steps","text":"<ul> <li>PostgreSQL Extension Setup \u2192</li> <li>Customer Intelligence Examples \u2192</li> <li>Performance Tuning \u2192</li> </ul> <p>Pro Tip</p> <p>Start by migrating a small subset of your data to compare quality. SteadyText's embeddings are optimized for semantic similarity and often outperform general-purpose embedding APIs for domain-specific content.</p>"},{"location":"migration/from-langchain/","title":"Migrating from LangChain to SteadyText","text":"<p>Simplify your AI stack by replacing complex LangChain abstractions with SteadyText's straightforward, deterministic approach.</p>"},{"location":"migration/from-langchain/#why-migrate-from-langchain","title":"Why Migrate from LangChain?","text":"LangChain SteadyText Complex abstractions Simple, direct API Multiple dependencies Single lightweight library Non-deterministic chains 100% deterministic outputs Verbose configuration Zero configuration External LLM costs Free local execution Debugging nightmares Predictable behavior"},{"location":"migration/from-langchain/#quick-comparison","title":"Quick Comparison","text":""},{"location":"migration/from-langchain/#text-generation","title":"Text Generation","text":"<p>Before (LangChain): <pre><code>from langchain import PromptTemplate, LLMChain\nfrom langchain.llms import OpenAI\nfrom langchain.callbacks import get_openai_callback\n\nllm = OpenAI(temperature=0, api_key=\"sk-...\")\ntemplate = \"\"\"Summarize the following text:\n{text}\n\nSummary:\"\"\"\nprompt = PromptTemplate(template=template, input_variables=[\"text\"])\nchain = LLMChain(llm=llm, prompt=prompt)\n\nwith get_openai_callback() as cb:\n    summary = chain.run(text=\"Long text here...\")\n    print(f\"Cost: ${cb.total_cost}\")\n</code></pre></p> <p>After (SteadyText): <pre><code>import steadytext\n\nsummary = steadytext.generate(f\"Summarize the following text: {text}\")\n# No chains, no templates, no callbacks, no costs!\n</code></pre></p>"},{"location":"migration/from-langchain/#embeddings-and-vector-stores","title":"Embeddings and Vector Stores","text":"<p>Before (LangChain): <pre><code>from langchain.embeddings import OpenAIEmbeddings\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import CharacterTextSplitter\n\n# Complex setup\nloader = TextLoader(\"document.txt\")\ndocuments = loader.load()\ntext_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\ndocs = text_splitter.split_documents(documents)\n\nembeddings = OpenAIEmbeddings(api_key=\"sk-...\")\ndb = FAISS.from_documents(docs, embeddings)\n\n# Search\nquery = \"What is the main topic?\"\ndocs = db.similarity_search(query, k=4)\n</code></pre></p> <p>After (SteadyText + PostgreSQL): <pre><code>-- Everything in SQL!\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    content TEXT,\n    embedding vector(1024)\n);\n\n-- Load and embed documents\nINSERT INTO documents (content, embedding)\nSELECT \n    content,\n    steadytext_embed(content)::vector\nFROM load_text_file('document.txt');\n\n-- Search\nSELECT content, 1 - (embedding &lt;=&gt; steadytext_embed('What is the main topic?')::vector) AS score\nFROM documents\nORDER BY embedding &lt;=&gt; steadytext_embed('What is the main topic?')::vector\nLIMIT 4;\n</code></pre></p>"},{"location":"migration/from-langchain/#common-langchain-pattern-migrations","title":"Common LangChain Pattern Migrations","text":""},{"location":"migration/from-langchain/#1-prompt-templates-direct-formatting","title":"1. Prompt Templates \u2192 Direct Formatting","text":"<p>Before: <pre><code>from langchain import PromptTemplate, FewShotPromptTemplate\n\nexample_prompt = PromptTemplate(\n    input_variables=[\"input\", \"output\"],\n    template=\"Input: {input}\\nOutput: {output}\"\n)\n\nexamples = [\n    {\"input\": \"2+2\", \"output\": \"4\"},\n    {\"input\": \"3+3\", \"output\": \"6\"}\n]\n\nfew_shot_prompt = FewShotPromptTemplate(\n    examples=examples,\n    example_prompt=example_prompt,\n    prefix=\"Solve math problems:\",\n    suffix=\"Input: {input}\\nOutput:\",\n    input_variables=[\"input\"]\n)\n\nchain = LLMChain(llm=llm, prompt=few_shot_prompt)\nresult = chain.run(input=\"5+5\")\n</code></pre></p> <p>After: <pre><code>def solve_math(problem):\n    prompt = \"\"\"Solve math problems:\nInput: 2+2\nOutput: 4\nInput: 3+3\nOutput: 6\nInput: {problem}\nOutput:\"\"\".format(problem=problem)\n\n    return steadytext.generate(prompt, max_tokens=10)\n\nresult = solve_math(\"5+5\")  # Deterministic: always \"10\"\n</code></pre></p>"},{"location":"migration/from-langchain/#2-chains-simple-functions","title":"2. Chains \u2192 Simple Functions","text":"<p>Before: <pre><code>from langchain.chains import SimpleSequentialChain\n\n# First chain: summarize\nsummarize_chain = LLMChain(llm=llm, prompt=summarize_prompt)\n\n# Second chain: translate\ntranslate_chain = LLMChain(llm=llm, prompt=translate_prompt)\n\n# Combine chains\noverall_chain = SimpleSequentialChain(\n    chains=[summarize_chain, translate_chain],\n    verbose=True\n)\n\nresult = overall_chain.run(long_text)\n</code></pre></p> <p>After: <pre><code>def summarize_and_translate(text, target_lang=\"Spanish\"):\n    # Step 1: Summarize\n    summary = steadytext.generate(f\"Summarize: {text}\", max_tokens=100)\n\n    # Step 2: Translate\n    translation = steadytext.generate(\n        f\"Translate to {target_lang}: {summary}\", \n        max_tokens=150\n    )\n\n    return translation\n\nresult = summarize_and_translate(long_text)\n</code></pre></p>"},{"location":"migration/from-langchain/#3-agents-direct-logic","title":"3. Agents \u2192 Direct Logic","text":"<p>Before: <pre><code>from langchain.agents import load_tools, initialize_agent, AgentType\n\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\nagent = initialize_agent(\n    tools, \n    llm, \n    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n    verbose=True\n)\n\nresult = agent.run(\"What is the weather in NYC and what is 234 * 432?\")\n</code></pre></p> <p>After: <pre><code>def answer_question(question):\n    # Determine what's needed\n    needs = steadytext.generate_json(\n        f\"What tools are needed for: {question}\",\n        schema={\n            \"needs_search\": {\"type\": \"boolean\"},\n            \"needs_calculation\": {\"type\": \"boolean\"},\n            \"calculation\": {\"type\": \"string\"}\n        }\n    )\n\n    results = []\n\n    if needs[\"needs_calculation\"]:\n        # Direct calculation\n        calc_result = eval(needs[\"calculation\"])  # In production, use safe eval\n        results.append(f\"Calculation: {calc_result}\")\n\n    if needs[\"needs_search\"]:\n        # Your search logic here\n        results.append(\"Search: [Results]\")\n\n    return steadytext.generate(\n        f\"Answer based on: {results}\\nQuestion: {question}\"\n    )\n</code></pre></p>"},{"location":"migration/from-langchain/#4-document-qa-sql-queries","title":"4. Document QA \u2192 SQL Queries","text":"<p>Before: <pre><code>from langchain.chains import RetrievalQA\nfrom langchain.document_loaders import DirectoryLoader\nfrom langchain.indexes import VectorstoreIndexCreator\n\nloader = DirectoryLoader('docs/', glob=\"*.txt\")\nindex = VectorstoreIndexCreator(\n    embedding=OpenAIEmbeddings(),\n    text_splitter=CharacterTextSplitter(chunk_size=1000)\n).from_loaders([loader])\n\nqa = RetrievalQA.from_chain_type(\n    llm=llm,\n    chain_type=\"stuff\",\n    retriever=index.vectorstore.as_retriever()\n)\n\nanswer = qa.run(\"What is the main topic?\")\n</code></pre></p> <p>After: <pre><code>-- Create document chunks table\nCREATE TABLE document_chunks (\n    id SERIAL PRIMARY KEY,\n    document_name VARCHAR(255),\n    chunk_number INTEGER,\n    content TEXT,\n    embedding vector(1024)\n);\n\n-- Load and chunk documents\nINSERT INTO document_chunks (document_name, chunk_number, content, embedding)\nSELECT \n    filename,\n    chunk_number,\n    chunk_text,\n    steadytext_embed(chunk_text)::vector\nFROM chunk_documents('docs/*.txt', 1000);\n\n-- Question answering function\nCREATE FUNCTION answer_question(question TEXT)\nRETURNS TEXT AS $$\nDECLARE\n    context TEXT;\nBEGIN\n    -- Get relevant chunks\n    SELECT string_agg(content, E'\\n\\n') INTO context\n    FROM (\n        SELECT content\n        FROM document_chunks\n        WHERE embedding &lt;=&gt; steadytext_embed(question)::vector &lt; 0.3\n        ORDER BY embedding &lt;=&gt; steadytext_embed(question)::vector\n        LIMIT 4\n    ) relevant_chunks;\n\n    -- Generate answer\n    RETURN steadytext_generate(\n        format('Based on this context: %s\\n\\nAnswer: %s', \n               context, question)\n    );\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre></p>"},{"location":"migration/from-langchain/#5-output-parsers-structured-generation","title":"5. Output Parsers \u2192 Structured Generation","text":"<p>Before: <pre><code>from langchain.output_parsers import PydanticOutputParser, OutputFixingParser\nfrom pydantic import BaseModel, Field\n\nclass Person(BaseModel):\n    name: str = Field(description=\"person's name\")\n    age: int = Field(description=\"person's age\")\n\nparser = PydanticOutputParser(pydantic_object=Person)\nfixing_parser = OutputFixingParser.from_llm(parser=parser, llm=llm)\n\nprompt = PromptTemplate(\n    template=\"Extract person info:\\n{format_instructions}\\n{text}\",\n    input_variables=[\"text\"],\n    partial_variables={\"format_instructions\": parser.get_format_instructions()}\n)\n\nchain = LLMChain(llm=llm, prompt=prompt)\noutput = chain.run(text=\"John is 30 years old\")\nperson = fixing_parser.parse(output)  # Might fail and retry!\n</code></pre></p> <p>After: <pre><code>from pydantic import BaseModel\n\nclass Person(BaseModel):\n    name: str\n    age: int\n\n# Guaranteed to return valid Person object!\nresult = steadytext.generate(\"Extract: John is 30 years old\", schema=Person)\n# Parses automatically from: \"...&lt;json-output&gt;{\"name\": \"John\", \"age\": 30}&lt;/json-output&gt;\"\n</code></pre></p>"},{"location":"migration/from-langchain/#memory-and-state-management","title":"Memory and State Management","text":""},{"location":"migration/from-langchain/#conversation-memory","title":"Conversation Memory","text":"<p>Before (LangChain): <pre><code>from langchain.memory import ConversationBufferMemory\nfrom langchain.chains import ConversationChain\n\nmemory = ConversationBufferMemory()\nconversation = ConversationChain(\n    llm=llm,\n    memory=memory,\n    verbose=True\n)\n\nresponse1 = conversation.predict(input=\"Hi, my name is John\")\nresponse2 = conversation.predict(input=\"What's my name?\")\n</code></pre></p> <p>After (Simple Python): <pre><code>class SimpleConversation:\n    def __init__(self):\n        self.history = []\n\n    def chat(self, user_input):\n        # Build context from history\n        context = \"\\n\".join([\n            f\"User: {h['user']}\\nAssistant: {h['assistant']}\"\n            for h in self.history[-5:]  # Keep last 5 turns\n        ])\n\n        prompt = f\"{context}\\nUser: {user_input}\\nAssistant:\"\n        response = steadytext.generate(prompt, max_tokens=100)\n\n        # Save to history\n        self.history.append({\n            \"user\": user_input,\n            \"assistant\": response\n        })\n\n        return response\n\nconv = SimpleConversation()\nresponse1 = conv.chat(\"Hi, my name is John\")\nresponse2 = conv.chat(\"What's my name?\")  # Will remember \"John\"\n</code></pre></p>"},{"location":"migration/from-langchain/#cost-and-performance-comparison","title":"Cost and Performance Comparison","text":""},{"location":"migration/from-langchain/#langchain-openai-costs","title":"LangChain + OpenAI Costs","text":"<pre><code># Typical LangChain application\ndef calculate_langchain_costs(daily_requests):\n    # Multiple LLM calls per request due to chains\n    avg_calls_per_request = 3  # Chain steps\n    tokens_per_call = 500\n    cost_per_1k_tokens = 0.002  # GPT-3.5\n\n    daily_cost = (daily_requests * avg_calls_per_request * \n                  tokens_per_call / 1000 * cost_per_1k_tokens)\n\n    print(f\"Daily: ${daily_cost:.2f}\")\n    print(f\"Monthly: ${daily_cost * 30:.2f}\")\n    print(f\"Yearly: ${daily_cost * 365:.2f}\")\n</code></pre>"},{"location":"migration/from-langchain/#steadytext-costs","title":"SteadyText Costs","text":"<pre><code># SteadyText: Always $0 after installation\nprint(\"Daily: $0\")\nprint(\"Monthly: $0\") \nprint(\"Yearly: $0\")\nprint(\"Plus: 100x faster, 100% deterministic!\")\n</code></pre>"},{"location":"migration/from-langchain/#testing-strategies","title":"Testing Strategies","text":""},{"location":"migration/from-langchain/#making-tests-deterministic","title":"Making Tests Deterministic","text":"<p>Before (LangChain - Flaky): <pre><code>def test_qa_chain():\n    # This test might fail randomly!\n    qa_chain = create_qa_chain()\n    answer = qa_chain.run(\"What is the capital of France?\")\n    assert \"Paris\" in answer  # Sometimes fails!\n</code></pre></p> <p>After (SteadyText - Reliable): <pre><code>def test_qa_function():\n    # Always passes with deterministic output\n    answer = answer_question(\"What is the capital of France?\")\n    assert answer == \"The capital of France is Paris.\"  # Exact match!\n</code></pre></p>"},{"location":"migration/from-langchain/#migration-strategy","title":"Migration Strategy","text":""},{"location":"migration/from-langchain/#phase-1-replace-simple-chains","title":"Phase 1: Replace Simple Chains","text":"<pre><code># Start with single-step operations\n# Replace: LLMChain \u2192 steadytext.generate()\n# Replace: embedding + vectorstore \u2192 PostgreSQL + pgvector\n</code></pre>"},{"location":"migration/from-langchain/#phase-2-simplify-complex-chains","title":"Phase 2: Simplify Complex Chains","text":"<pre><code># Convert multi-step chains to simple functions\n# Remove unnecessary abstractions\n# Use straightforward Python logic\n</code></pre>"},{"location":"migration/from-langchain/#phase-3-eliminate-external-dependencies","title":"Phase 3: Eliminate External Dependencies","text":"<pre><code># Remove API-based tools\n# Implement simple alternatives\n# Use PostgreSQL for persistence\n</code></pre>"},{"location":"migration/from-langchain/#common-pitfalls-solutions","title":"Common Pitfalls &amp; Solutions","text":""},{"location":"migration/from-langchain/#1-over-engineering","title":"1. Over-Engineering","text":"<pre><code># \u274c LangChain habit: Creating chains for everything\nchain = LLMChain(llm=llm, prompt=PromptTemplate(...))\n\n# \u2705 SteadyText: Just call the function\nresult = steadytext.generate(\"Your prompt here\")\n</code></pre>"},{"location":"migration/from-langchain/#2-callback-complexity","title":"2. Callback Complexity","text":"<pre><code># \u274c LangChain: Complex callback systems\ncallbacks = [StreamingStdOutCallbackHandler(), CustomCallback()]\n\n# \u2705 SteadyText: Simple iteration\nfor token in steadytext.generate_iter(\"Your prompt\"):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"migration/from-langchain/#3-configuration-overload","title":"3. Configuration Overload","text":"<pre><code># \u274c LangChain: Tons of configuration\nllm = ChatOpenAI(\n    model_name=\"gpt-3.5-turbo\",\n    temperature=0,\n    max_tokens=100,\n    model_kwargs={\"top_p\": 0.9},\n    callbacks=callbacks,\n    cache=True\n)\n\n# \u2705 SteadyText: Sensible defaults\nresult = steadytext.generate(\"prompt\", max_tokens=100)\n</code></pre>"},{"location":"migration/from-langchain/#real-world-migration-example","title":"Real-World Migration Example","text":"<p>Here's a complete migration of a document QA system:</p>"},{"location":"migration/from-langchain/#before-langchain-150-lines","title":"Before (LangChain - 150+ lines)","text":"<pre><code># Complex setup with multiple files, classes, and configurations\n# Vector stores, embeddings, chains, callbacks, etc.\n</code></pre>"},{"location":"migration/from-langchain/#after-steadytext-20-lines","title":"After (SteadyText - 20 lines)","text":"<pre><code>-- Complete QA system in PostgreSQL\nCREATE OR REPLACE FUNCTION qa_system(question TEXT)\nRETURNS TEXT AS $$\nDECLARE\n    context TEXT;\nBEGIN\n    -- Find relevant content\n    SELECT string_agg(content, E'\\n') INTO context\n    FROM documents\n    WHERE embedding &lt;=&gt; steadytext_embed(question)::vector &lt; 0.3\n    ORDER BY embedding &lt;=&gt; steadytext_embed(question)::vector\n    LIMIT 3;\n\n    -- Generate answer\n    RETURN steadytext_generate(\n        format('Context: %s\\n\\nQuestion: %s\\n\\nAnswer:', \n               context, question)\n    );\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage\nSELECT qa_system('What is the main topic?');\n</code></pre>"},{"location":"migration/from-langchain/#migration-checklist","title":"Migration Checklist","text":"<ul> <li>[ ] List all LangChain components in use</li> <li>[ ] Identify which can be replaced with simple functions</li> <li>[ ] Install SteadyText</li> <li>[ ] Migrate embeddings to PostgreSQL</li> <li>[ ] Replace chains with direct function calls</li> <li>[ ] Remove prompt templates (use f-strings)</li> <li>[ ] Simplify memory management</li> <li>[ ] Update tests for deterministic outputs</li> <li>[ ] Remove all API key management</li> <li>[ ] Delete unused dependencies</li> <li>[ ] Celebrate simpler code! \ud83c\udf89</li> </ul>"},{"location":"migration/from-langchain/#next-steps","title":"Next Steps","text":"<ul> <li>PostgreSQL Extension Guide \u2192</li> <li>Simple Examples \u2192</li> <li>API Reference \u2192</li> </ul> <p>The Beauty of Simplicity</p> <p>Most LangChain applications can be reduced to 10% of their original code size while gaining determinism, speed, and reliability. Less abstraction = fewer bugs!</p>"},{"location":"migration/from-openai/","title":"Migrating from OpenAI API to SteadyText","text":"<p>A practical guide to replacing OpenAI API calls with SteadyText's deterministic, local AI capabilities.</p>"},{"location":"migration/from-openai/#why-migrate","title":"Why Migrate?","text":"OpenAI API SteadyText $0.01-0.12 per 1K tokens $0 after installation 100-500ms latency &lt;1ms local execution Rate limits and quotas Unlimited local usage Non-deterministic outputs 100% deterministic Internet required Works offline API key management No keys needed"},{"location":"migration/from-openai/#quick-comparison","title":"Quick Comparison","text":""},{"location":"migration/from-openai/#text-generation","title":"Text Generation","text":"<p>Before (OpenAI): <pre><code>import openai\n\nopenai.api_key = \"sk-...\"\n\nresponse = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\"role\": \"user\", \"content\": \"Summarize this text: \" + text}],\n    temperature=0,  # Still not deterministic!\n    max_tokens=150\n)\nsummary = response.choices[0].message.content\n</code></pre></p> <p>After (SteadyText): <pre><code>import steadytext\n\n# No API key needed!\nsummary = steadytext.generate(\n    f\"Summarize this text: {text}\",\n    max_tokens=150\n)\n# Same input ALWAYS produces same output\n</code></pre></p>"},{"location":"migration/from-openai/#embeddings","title":"Embeddings","text":"<p>Before (OpenAI): <pre><code>response = openai.Embedding.create(\n    model=\"text-embedding-ada-002\",\n    input=\"Hello world\"\n)\nembedding = response.data[0].embedding  # 1536 dimensions\n</code></pre></p> <p>After (SteadyText): <pre><code>embedding = steadytext.embed(\"Hello world\")  # 1024 dimensions\n# Deterministic - same text always produces same vector\n</code></pre></p>"},{"location":"migration/from-openai/#postgresql-migration","title":"PostgreSQL Migration","text":""},{"location":"migration/from-openai/#database-functions","title":"Database Functions","text":"<p>Before (OpenAI via HTTP): <pre><code>-- Complex function making HTTP requests\nCREATE OR REPLACE FUNCTION summarize_with_openai(text_input TEXT)\nRETURNS TEXT AS $$\nDECLARE\n    api_response JSONB;\nBEGIN\n    -- Using pg_http or similar\n    SELECT content::JSONB INTO api_response\n    FROM http_post(\n        'https://api.openai.com/v1/chat/completions',\n        jsonb_build_object(\n            'model', 'gpt-3.5-turbo',\n            'messages', jsonb_build_array(\n                jsonb_build_object('role', 'user', 'content', text_input)\n            )\n        )::TEXT,\n        'application/json',\n        ARRAY[['Authorization', 'Bearer sk-...']]\n    );\n\n    RETURN api_response-&gt;'choices'-&gt;0-&gt;'message'-&gt;&gt;'content';\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre></p> <p>After (SteadyText): <pre><code>-- Simple, fast, deterministic\nCREATE EXTENSION pg_steadytext;\n\n-- That's it! Now just use:\nSELECT steadytext_generate('Summarize: ' || text_column) \nFROM your_table;\n</code></pre></p>"},{"location":"migration/from-openai/#batch-processing","title":"Batch Processing","text":"<p>Before (OpenAI with rate limits): <pre><code>import time\nimport openai\nfrom openai.error import RateLimitError\n\nsummaries = []\nfor i, text in enumerate(texts):\n    try:\n        response = openai.ChatCompletion.create(\n            model=\"gpt-3.5-turbo\",\n            messages=[{\"role\": \"user\", \"content\": f\"Summarize: {text}\"}]\n        )\n        summaries.append(response.choices[0].message.content)\n\n        # Respect rate limits\n        if i % 10 == 0:\n            time.sleep(1)\n\n    except RateLimitError:\n        time.sleep(60)  # Wait a minute\n        # Retry logic here...\n</code></pre></p> <p>After (SteadyText unlimited): <pre><code>-- Process millions of rows without rate limits\nUPDATE articles \nSET summary = steadytext_generate('Summarize: ' || content)\nWHERE summary IS NULL;\n\n-- Or in Python with no rate limits\nsummaries = [steadytext.generate(f\"Summarize: {text}\") for text in texts]\n</code></pre></p>"},{"location":"migration/from-openai/#common-use-case-migrations","title":"Common Use Case Migrations","text":""},{"location":"migration/from-openai/#1-content-moderation","title":"1. Content Moderation","text":"<p>Before: <pre><code>def moderate_content_openai(text):\n    response = openai.Moderation.create(input=text)\n    return response.results[0].flagged\n</code></pre></p> <p>After: <pre><code>def moderate_content_steadytext(text):\n    result = steadytext.generate_choice(\n        f\"Is this content inappropriate: {text}\",\n        choices=[\"safe\", \"inappropriate\"]\n    )\n    return result == \"inappropriate\"\n</code></pre></p>"},{"location":"migration/from-openai/#2-structured-data-extraction","title":"2. Structured Data Extraction","text":"<p>Before: <pre><code>response = openai.ChatCompletion.create(\n    model=\"gpt-3.5-turbo\",\n    messages=[{\n        \"role\": \"system\", \n        \"content\": \"Extract JSON data\"\n    }, {\n        \"role\": \"user\",\n        \"content\": text\n    }],\n    response_format={\"type\": \"json_object\"}  # Still can fail!\n)\n</code></pre></p> <p>After: <pre><code># Guaranteed valid JSON with schema\nresult = steadytext.generate_json(\n    text,\n    schema={\n        \"type\": \"object\",\n        \"properties\": {\n            \"name\": {\"type\": \"string\"},\n            \"email\": {\"type\": \"string\"},\n            \"phone\": {\"type\": \"string\"}\n        }\n    }\n)\n</code></pre></p>"},{"location":"migration/from-openai/#3-semantic-search","title":"3. Semantic Search","text":"<p>Before: <pre><code># Store OpenAI embeddings\ndef create_embedding_openai(text):\n    response = openai.Embedding.create(\n        model=\"text-embedding-ada-002\",\n        input=text\n    )\n    return response.data[0].embedding\n\n# Search with cosine similarity\nquery_embedding = create_embedding_openai(query)\n# Complex vector similarity search...\n</code></pre></p> <p>After: <pre><code>-- Native PostgreSQL with pgvector\nALTER TABLE documents ADD COLUMN embedding vector(1024);\n\nUPDATE documents \nSET embedding = steadytext_embed(content)::vector;\n\n-- Search is just SQL\nSELECT * FROM documents\nWHERE embedding &lt;=&gt; steadytext_embed('search query')::vector &lt; 0.3\nORDER BY embedding &lt;=&gt; steadytext_embed('search query')::vector\nLIMIT 10;\n</code></pre></p>"},{"location":"migration/from-openai/#cost-analysis","title":"Cost Analysis","text":""},{"location":"migration/from-openai/#openai-costs-monthly","title":"OpenAI Costs (Monthly)","text":"<pre><code>10M tokens/day \u00d7 30 days = 300M tokens/month\n\nGPT-3.5 Turbo: 300M \u00d7 $0.001/1K = $300/month\nGPT-4: 300M \u00d7 $0.03/1K = $9,000/month\nEmbeddings: 100M \u00d7 $0.0001/1K = $10/month\n\nTotal: $310-9,010/month + rate limit delays\n</code></pre>"},{"location":"migration/from-openai/#steadytext-costs","title":"SteadyText Costs","text":"<pre><code>One-time: $0 (open source)\nMonthly: $0 (runs on your infrastructure)\nRate limits: None\nLatency: &lt;1ms (vs 100-500ms)\n</code></pre>"},{"location":"migration/from-openai/#testing-strategy","title":"Testing Strategy","text":""},{"location":"migration/from-openai/#making-tests-deterministic","title":"Making Tests Deterministic","text":"<p>Before (Flaky): <pre><code>def test_summarization():\n    # This test randomly fails!\n    summary = call_openai_api(\"Summarize: \" + text)\n    assert \"important point\" in summary  # Sometimes true, sometimes false\n</code></pre></p> <p>After (Reliable): <pre><code>def test_summarization():\n    # Always passes with same input\n    summary = steadytext.generate(\"Summarize: \" + text)\n    assert summary == \"Expected exact output\"  # Deterministic!\n</code></pre></p>"},{"location":"migration/from-openai/#migration-checklist","title":"Migration Checklist","text":"<ul> <li>[ ] Install SteadyText (<code>pip install steadytext</code>)</li> <li>[ ] For PostgreSQL: Install pg_steadytext extension</li> <li>[ ] Replace OpenAI initialization with SteadyText import</li> <li>[ ] Update function calls (see mapping below)</li> <li>[ ] Remove API key management code</li> <li>[ ] Remove rate limit handling</li> <li>[ ] Update error handling (no more network errors!)</li> <li>[ ] Update tests to expect deterministic outputs</li> <li>[ ] Calculate cost savings \ud83c\udf89</li> </ul>"},{"location":"migration/from-openai/#function-mapping-reference","title":"Function Mapping Reference","text":"OpenAI Function SteadyText Equivalent <code>ChatCompletion.create()</code> <code>steadytext.generate()</code> <code>Embedding.create()</code> <code>steadytext.embed()</code> <code>ChatCompletion.create(stream=True)</code> <code>steadytext.generate_iter()</code> <code>response_format={\"type\": \"json_object\"}</code> <code>steadytext.generate_json()</code> <code>functions=[...]</code> <code>steadytext.generate_json(schema=...)</code> <code>Moderation.create()</code> <code>steadytext.generate_choice()</code>"},{"location":"migration/from-openai/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"migration/from-openai/#caching-layer","title":"Caching Layer","text":"<p>Before (Complex Redis setup): <pre><code>def get_summary_with_cache(text):\n    cache_key = hashlib.md5(text.encode()).hexdigest()\n\n    # Check Redis\n    cached = redis_client.get(cache_key)\n    if cached:\n        return cached\n\n    # Call OpenAI\n    summary = call_openai_api(text)\n\n    # Cache with TTL\n    redis_client.setex(cache_key, 3600, summary)\n    return summary\n</code></pre></p> <p>After (Built-in caching): <pre><code># SteadyText automatically caches deterministic outputs\nsummary = steadytext.generate(f\"Summarize: {text}\")\n# Subsequent calls with same input are instant!\n</code></pre></p>"},{"location":"migration/from-openai/#async-operations","title":"Async Operations","text":"<p>Before: <pre><code>async def process_batch_openai(texts):\n    async with aiohttp.ClientSession() as session:\n        tasks = []\n        for text in texts:\n            task = call_openai_async(session, text)\n            tasks.append(task)\n        return await asyncio.gather(*tasks)\n</code></pre></p> <p>After (PostgreSQL): <pre><code>-- Process asynchronously in database\nSELECT steadytext_generate_async(\n    'Summarize: ' || content\n) FROM articles;\n\n-- Check results\nSELECT * FROM steadytext_check_async_batch(array_of_ids);\n</code></pre></p>"},{"location":"migration/from-openai/#gradual-migration-strategy","title":"Gradual Migration Strategy","text":""},{"location":"migration/from-openai/#phase-1-development-environment","title":"Phase 1: Development Environment","text":"<ol> <li>Install SteadyText alongside OpenAI</li> <li>A/B test outputs for quality</li> <li>Measure performance improvements</li> </ol>"},{"location":"migration/from-openai/#phase-2-non-critical-features","title":"Phase 2: Non-Critical Features","text":"<ol> <li>Migrate internal tools first</li> <li>Move test environments</li> <li>Validate deterministic behavior</li> </ol>"},{"location":"migration/from-openai/#phase-3-production-migration","title":"Phase 3: Production Migration","text":"<ol> <li>Start with read-heavy workloads</li> <li>Migrate batch processing</li> <li>Finally migrate real-time features</li> </ol>"},{"location":"migration/from-openai/#rollback-plan","title":"Rollback Plan","text":"<pre><code># Feature flag approach\nUSE_STEADYTEXT = os.getenv(\"USE_STEADYTEXT\", \"false\") == \"true\"\n\ndef generate_text(prompt):\n    if USE_STEADYTEXT:\n        return steadytext.generate(prompt)\n    else:\n        return call_openai_api(prompt)\n</code></pre>"},{"location":"migration/from-openai/#common-gotchas","title":"Common Gotchas","text":"<ol> <li>Output Length: SteadyText defaults to 512 tokens (configurable)</li> <li>Model Size: 2GB download on first use</li> <li>Embedding Dimensions: 1024 vs OpenAI's 1536</li> <li>JSON Mode: Use <code>generate_json()</code> with schema for guaranteed structure</li> </ol>"},{"location":"migration/from-openai/#support-community","title":"Support &amp; Community","text":"<ul> <li>GitHub Issues: github.com/julep-ai/steadytext/issues</li> <li>Discord: discord.gg/steadytext</li> <li>More Questions: See our FAQ</li> </ul> <p>Ready to Save Money?</p> <p>Most teams see 100% cost reduction and 100x performance improvement after migration. Start with a small proof-of-concept and scale from there!</p>"}]}