{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SteadyText","text":"<p>Deterministic text generation and embeddings with zero configuration</p> <p> </p> <p>Same input \u2192 same output. Every time.</p> <p>No more flaky tests, unpredictable CLI tools, or inconsistent docs. SteadyText makes AI outputs as reliable as hash functions.</p> <p>Ever had an AI test fail randomly? Or a CLI tool give different answers each run? SteadyText makes AI outputs reproducible - perfect for testing, tooling, and anywhere you need consistent results.</p> <p>Powered by Julep</p> <p>\u2728 Powered by open-source AI workflows from Julep. \u2728</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Using UV (recommended - 10-100x faster)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre> Python APICommand Line <pre><code>import steadytext\n\n# Deterministic text generation\ncode = steadytext.generate(\"implement binary search in Python\")\nassert \"def binary_search\" in code  # Always passes!\n\n# Streaming (also deterministic)\nfor token in steadytext.generate_iter(\"explain quantum computing\"):\n    print(token, end=\"\", flush=True)\n\n# Deterministic embeddings\nvec = steadytext.embed(\"Hello world\")  # 1024-dim numpy array\n</code></pre> <pre><code># Generate text (pipe syntax)\necho \"hello world\" | st\n\n# Stream output (default)  \necho \"explain recursion\" | st\n\n# Wait for complete output\necho \"explain recursion\" | st --wait\n\n# Get embeddings\necho \"machine learning\" | st embed\n\n# Start daemon for faster responses\nst daemon start\n</code></pre>"},{"location":"#how-it-works","title":"\ud83d\udd27 How It Works","text":"<p>SteadyText achieves determinism via:</p> <ul> <li>Customizable seeds: Control determinism with a <code>seed</code> parameter, while still defaulting to <code>42</code>.</li> <li>Greedy decoding: Always chooses highest-probability token</li> <li>Frecency cache: LRU cache with frequency counting\u2014popular prompts stay cached longer</li> <li>Quantized models: 8-bit quantization ensures identical results across platforms</li> </ul> <p>This means <code>generate(\"hello\")</code> returns the exact same 512 tokens on any machine, every single time.</p>"},{"location":"#ecosystem","title":"\ud83c\udf10 Ecosystem","text":"<p>SteadyText is more than just a library. It's a full ecosystem for deterministic AI:</p> <ul> <li>Python Library: The core <code>steadytext</code> library for programmatic use in your applications.</li> <li>Command-Line Interface (CLI): A powerful <code>st</code> command to use SteadyText from your shell for scripting and automation.</li> <li>Zsh Plugin: Supercharge your shell with AI-powered command suggestions and history search.</li> <li>PostgreSQL Extension: Run deterministic AI functions directly within your PostgreSQL database.</li> <li>Cloudflare Worker: Deploy SteadyText to the edge with a Cloudflare Worker for distributed, low-latency applications.</li> </ul>"},{"location":"#daemon-mode-v13","title":"Daemon Mode (v1.3+)","text":"<p>SteadyText includes a daemon mode that keeps models loaded in memory for instant responses:</p> <ul> <li>160x faster first request: No model loading overhead</li> <li>Persistent cache: Shared across all operations</li> <li>Automatic fallback: Works without daemon if unavailable</li> <li>Zero configuration: Daemon used by default when available</li> </ul> <pre><code># Start daemon\nst daemon start\n\n# Check status\nst daemon status\n\n# All operations now use daemon automatically\necho \"hello\" | st  # Instant response!\n</code></pre>"},{"location":"#faiss-indexing","title":"FAISS Indexing","text":"<p>Create and search vector indexes for retrieval-augmented generation:</p> <pre><code># Create index from documents\nst index create *.txt --output docs.faiss\n\n# Search index\nst index search docs.faiss \"query text\" --top-k 5\n\n# Use with generation (automatic with default.faiss)\necho \"explain this error\" | st --index-file docs.faiss\n</code></pre>"},{"location":"#installation-models","title":"\ud83d\udce6 Installation &amp; Models","text":"<p>Install stable release:</p> <pre><code># Using UV (recommended - 10-100x faster)\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre>"},{"location":"#models","title":"Models","text":"<p>Current models (v2.0.0+):</p> <ul> <li>Generation: <code>Gemma-3n-E2B-it-Q8_0.gguf</code> (2.0GB) - Gemma-3n-2B (default)</li> <li>Generation: <code>Gemma-3n-E4B-it-Q8_0.gguf</code> (4.2GB) - Gemma-3n-4B (optional)</li> <li>Embeddings: <code>Qwen3-Embedding-0.6B-Q8_0.gguf</code> (610MB)</li> </ul> <p>Version Stability</p> <p>Each major version will use a fixed set of models only, so that only forced upgrades from pip will change the models (and the deterministic output)</p>"},{"location":"#use-cases","title":"\ud83c\udfaf Use Cases","text":"<p>Perfect for</p> <ul> <li>Testing AI features: Reliable asserts that never flake</li> <li>Deterministic CLI tooling: Consistent outputs for automation  </li> <li>Reproducible documentation: Examples that always work</li> <li>Offline/dev/staging environments: No API keys needed</li> <li>Semantic caching and embedding search: Fast similarity matching</li> </ul> <p>Not ideal for</p> <ul> <li>Creative or conversational tasks</li> <li>Latest knowledge queries  </li> <li>Large-scale chatbot deployments</li> </ul>"},{"location":"#examples","title":"\ud83d\udccb Examples","text":"<p>Use SteadyText in tests or CLI tools for consistent, reproducible results:</p> <pre><code># Testing with reliable assertions\ndef test_ai_function():\n    result = my_ai_function(\"test input\")\n    expected = steadytext.generate(\"expected output for 'test input'\")\n    assert result == expected  # No flakes!\n\n# CLI tools with consistent outputs\nimport click\n\n@click.command()\ndef ai_tool(prompt):\n    print(steadytext.generate(prompt))\n</code></pre> <p>\ud83d\udcc2 More examples \u2192</p>"},{"location":"#api-overview","title":"\ud83d\udd0d API Overview","text":"<pre><code># Text generation\nsteadytext.generate(prompt: str) -&gt; str\nsteadytext.generate(prompt, return_logprobs=True)\n\n# Streaming generation\nsteadytext.generate_iter(prompt: str)\n\n# Embeddings\nsteadytext.embed(text: str | List[str]) -&gt; np.ndarray\n\n# Model preloading\nsteadytext.preload_models(verbose=True)\n</code></pre> <p>\ud83d\udcda Full API Documentation \u2192</p>"},{"location":"#configuration","title":"\ud83d\udd27 Configuration","text":"<p>Control caching behavior via environment variables:</p> <pre><code># Generation cache (default: 256 entries, 50MB)\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50\n\n# Embedding cache (default: 512 entries, 100MB)\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100\n</code></pre>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! See Contributing Guide for guidelines.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<ul> <li>Code: MIT</li> <li>Models: MIT (Qwen3)</li> </ul> <p>Built with \u2764\ufe0f for developers tired of flaky AI tests.</p>"},{"location":"api/","title":"SteadyText API Documentation","text":"<p>This document provides detailed API documentation for SteadyText.</p>"},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#text-generation","title":"Text Generation","text":""},{"location":"api/#steadytextgenerate","title":"<code>steadytext.generate()</code>","text":"<pre><code>def generate(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\",\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED,\n    schema: Optional[Union[Dict[str, Any], type, object]] = None,\n    regex: Optional[str] = None,\n    choices: Optional[List[str]] = None,\n    response_format: Optional[Dict[str, Any]] = None,\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre> <p>Generate deterministic text from a prompt, with optional structured output.</p> <p>Parameters: - <code>prompt</code> (str): The input text to generate from - <code>max_new_tokens</code> (int, optional): Maximum number of tokens to generate (default: 512) - <code>return_logprobs</code> (bool): If True, returns log probabilities along with the text - <code>eos_string</code> (str): Custom end-of-sequence string to stop generation. Use \"[EOS]\" for model's default stop tokens - <code>model</code> (str, optional): Model name from built-in registry (deprecated - use <code>size</code> parameter instead) - <code>model_repo</code> (str, optional): Custom Hugging Face repository ID (e.g., \"ggml-org/gemma-3n-E2B-it-GGUF\") - <code>model_filename</code> (str, optional): Custom model filename (e.g., \"gemma-3n-E2B-it-Q8_0.gguf\") - <code>size</code> (str, optional): Size shortcut for Gemma-3n models: \"small\" (2B, default), or \"large\" (4B) - recommended approach - <code>seed</code> (int): Random seed for deterministic generation (default: 42) - <code>schema</code> (Union[Dict, type, object], optional): JSON schema, Pydantic model, or Python type for structured JSON output. - <code>regex</code> (str, optional): A regular expression to constrain the output. - <code>choices</code> (List[str], optional): A list of strings to choose from. - <code>response_format</code> (Dict, optional): A dictionary specifying the output format (e.g., <code>{\"type\": \"json_object\"}</code>).</p> <p>Returns: - If <code>return_logprobs=False</code>: A string containing the generated text. For structured JSON output, the JSON is wrapped in <code>&lt;json-output&gt;</code> tags. - If <code>return_logprobs=True</code>: A tuple of (text, logprobs_dict)</p> <p>Example: <pre><code># Simple generation\ntext = steadytext.generate(\"Write a Python function\")\n\n# With custom seed for reproducible results\ntext1 = steadytext.generate(\"Write a story\", seed=123)\ntext2 = steadytext.generate(\"Write a story\", seed=123)  # Same result as text1\ntext3 = steadytext.generate(\"Write a story\", seed=456)  # Different result\n\n# With log probabilities\ntext, logprobs = steadytext.generate(\"Explain AI\", return_logprobs=True)\n\n# With custom stop string and seed\ntext = steadytext.generate(\"List items until END\", eos_string=\"END\", seed=789)\n\n# Limit output length\ntext = steadytext.generate(\"Quick summary\", max_new_tokens=100)\n\n# Using size parameter (recommended)\ntext = steadytext.generate(\"Quick task\", size=\"small\")   # Uses Gemma-3n-2B\ntext = steadytext.generate(\"Complex task\", size=\"large\")  # Uses Gemma-3n-4B\n\n# Using a custom model with seed\ntext = steadytext.generate(\n    \"Write code\",\n    model_repo=\"ggml-org/gemma-3n-E4B-it-GGUF\",\n    model_filename=\"gemma-3n-E4B-it-Q8_0.gguf\",\n    seed=999\n)\n\n# Structured generation with a regex pattern\nphone_number = steadytext.generate(\"My phone number is: \", regex=r\"\\d{3}-\\d{3}-\\d{4}\")\n\n# Structured generation with choices\nmood = steadytext.generate(\"I feel\", choices=[\"happy\", \"sad\", \"angry\"])\n\n# Structured generation with a JSON schema\nfrom pydantic import BaseModel\nclass User(BaseModel):\n    name: str\n    age: int\n\nuser_json = steadytext.generate(\"Create a user named John, age 30\", schema=User)\n# user_json will contain: '... &lt;json-output&gt;{\"name\": \"John\", \"age\": 30}&lt;/json-output&gt;'\n</code></pre></p>"},{"location":"api/#steadytextgenerate_iter","title":"<code>steadytext.generate_iter()</code>","text":"<pre><code>def generate_iter(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False,\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre> <p>Generate text iteratively, yielding tokens as they are produced.</p> <p>Parameters: - <code>prompt</code> (str): The input text to generate from - <code>max_new_tokens</code> (int, optional): Maximum number of tokens to generate (default: 512) - <code>eos_string</code> (str): Custom end-of-sequence string to stop generation. Use \"[EOS]\" for model's default stop tokens - <code>include_logprobs</code> (bool): If True, yields tuples of (token, logprobs) instead of just tokens - <code>model</code> (str, optional): Model name from built-in registry (deprecated - use <code>size</code> parameter instead) - <code>model_repo</code> (str, optional): Custom Hugging Face repository ID - <code>model_filename</code> (str, optional): Custom model filename - <code>size</code> (str, optional): Size shortcut for Gemma-3n models: \"small\" (2B, default), or \"large\" (4B) - recommended approach - <code>seed</code> (int): Random seed for deterministic generation (default: 42)</p> <p>Yields: - str: Text tokens/words as they are generated (if <code>include_logprobs=False</code>) - Tuple[str, Optional[Dict[str, Any]]]: (token, logprobs) tuples (if <code>include_logprobs=True</code>)</p> <p>Example: <pre><code># Simple streaming\nfor token in steadytext.generate_iter(\"Tell me a story\"):\n    print(token, end=\"\", flush=True)\n\n# With custom seed for reproducible streaming\nfor token in steadytext.generate_iter(\"Tell me a story\", seed=123):\n    print(token, end=\"\", flush=True)\n\n# With custom stop string and seed\nfor token in steadytext.generate_iter(\"Generate until STOP\", eos_string=\"STOP\", seed=456):\n    print(token, end=\"\", flush=True)\n\n# With log probabilities\nfor token, logprobs in steadytext.generate_iter(\"Explain AI\", include_logprobs=True):\n    print(token, end=\"\", flush=True)\n\n# Stream with size parameter and custom length\nfor token in steadytext.generate_iter(\"Quick response\", size=\"small\", max_new_tokens=50):\n    print(token, end=\"\", flush=True)\n\nfor token in steadytext.generate_iter(\"Complex task\", size=\"large\", seed=789):\n    print(token, end=\"\", flush=True)\n</code></pre></p>"},{"location":"api/#structured-generation-v230","title":"Structured Generation (v2.3.0+)","text":"<p>These are convenience functions for structured generation.</p>"},{"location":"api/#steadytextgenerate_json","title":"<code>steadytext.generate_json()</code>","text":"<pre><code>def generate_json(\n    prompt: str,\n    schema: Union[Dict[str, Any], type, object],\n    max_tokens: int = 512,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generates a JSON string that conforms to the provided schema.</p>"},{"location":"api/#steadytextgenerate_regex","title":"<code>steadytext.generate_regex()</code>","text":"<pre><code>def generate_regex(\n    prompt: str,\n    pattern: str,\n    max_tokens: int = 512,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generates a string that matches the given regular expression.</p>"},{"location":"api/#steadytextgenerate_choice","title":"<code>steadytext.generate_choice()</code>","text":"<pre><code>def generate_choice(\n    prompt: str,\n    choices: List[str],\n    max_tokens: int = 512,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generates a string that is one of the provided choices.</p>"},{"location":"api/#steadytextgenerate_format","title":"<code>steadytext.generate_format()</code>","text":"<pre><code>def generate_format(\n    prompt: str,\n    format_type: type,\n    max_tokens: int = 512,\n    **kwargs\n) -&gt; str\n</code></pre> <p>Generates a string that conforms to a basic Python type (e.g., <code>int</code>, <code>float</code>, <code>bool</code>).</p>"},{"location":"api/#embeddings","title":"Embeddings","text":""},{"location":"api/#steadytextembed","title":"<code>steadytext.embed()</code>","text":"<pre><code>def embed(text_input: Union[str, List[str]], seed: int = DEFAULT_SEED) -&gt; np.ndarray\n</code></pre> <p>Create deterministic embeddings for text input.</p> <p>Parameters: - <code>text_input</code> (Union[str, List[str]]): A string or list of strings to embed - <code>seed</code> (int): Random seed for deterministic embedding generation (default: 42)</p> <p>Returns: - np.ndarray: A 1024-dimensional L2-normalized float32 numpy array</p> <p>Example: <pre><code># Single string\nvec = steadytext.embed(\"Hello world\")\n\n# With custom seed for reproducible embeddings\nvec1 = steadytext.embed(\"Hello world\", seed=123)\nvec2 = steadytext.embed(\"Hello world\", seed=123)  # Same result as vec1\nvec3 = steadytext.embed(\"Hello world\", seed=456)  # Different result\n\n# Multiple strings (returns a single, averaged embedding)\nvec = steadytext.embed([\"Hello\", \"world\"])\n\n# Multiple strings with custom seed\nvec = steadytext.embed([\"Hello\", \"world\"], seed=789)\n</code></pre></p>"},{"location":"api/#utility-functions","title":"Utility Functions","text":""},{"location":"api/#steadytextpreload_models","title":"<code>steadytext.preload_models()</code>","text":"<pre><code>def preload_models(verbose: bool = False) -&gt; None\n</code></pre> <p>Preload models before first use to avoid delays.</p> <p>Parameters: - <code>verbose</code> (bool): If True, prints progress information</p> <p>Example: <pre><code># Silent preloading\nsteadytext.preload_models()\n\n# Verbose preloading\nsteadytext.preload_models(verbose=True)\n</code></pre></p>"},{"location":"api/#steadytextget_model_cache_dir","title":"<code>steadytext.get_model_cache_dir()</code>","text":"<pre><code>def get_model_cache_dir() -&gt; str\n</code></pre> <p>Get the path to the model cache directory.</p> <p>Returns: - str: The absolute path to the model cache directory</p> <p>Example: <pre><code>cache_dir = steadytext.get_model_cache_dir()\nprint(f\"Models are stored in: {cache_dir}\")\n</code></pre></p>"},{"location":"api/#constants","title":"Constants","text":""},{"location":"api/#steadytextdefault_seed","title":"<code>steadytext.DEFAULT_SEED</code>","text":"<ul> <li>Type: int</li> <li>Value: 42</li> <li>Description: The default random seed used for deterministic generation. Can be overridden by the <code>seed</code> parameter in generation and embedding functions.</li> </ul>"},{"location":"api/#steadytextgeneration_max_new_tokens","title":"<code>steadytext.GENERATION_MAX_NEW_TOKENS</code>","text":"<ul> <li>Type: int</li> <li>Value: 512</li> <li>Description: Maximum number of tokens to generate</li> </ul>"},{"location":"api/#steadytextembedding_dimension","title":"<code>steadytext.EMBEDDING_DIMENSION</code>","text":"<ul> <li>Type: int</li> <li>Value: 1024</li> <li>Description: The dimensionality of embedding vectors</li> </ul>"},{"location":"api/#environment-variables","title":"Environment Variables","text":""},{"location":"api/#generation-cache","title":"Generation Cache","text":"<ul> <li><code>STEADYTEXT_GENERATION_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 256)</li> <li><code>STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 50.0)</li> </ul>"},{"location":"api/#embedding-cache","title":"Embedding Cache","text":"<ul> <li><code>STEADYTEXT_EMBEDDING_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 512)</li> <li><code>STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 100.0)</li> </ul>"},{"location":"api/#model-downloads","title":"Model Downloads","text":"<ul> <li><code>STEADYTEXT_ALLOW_MODEL_DOWNLOADS</code>: Set to \"true\" to allow automatic model downloads (mainly used for testing)</li> </ul>"},{"location":"api/#model-switching-v200","title":"Model Switching (v2.0.0+)","text":"<p>SteadyText v2.0.0+ supports model switching with the Gemma-3n model family, allowing you to use different model sizes for different tasks.</p>"},{"location":"api/#current-model-registry-v200","title":"Current Model Registry (v2.0.0+)","text":"<p>The following models are available:</p> Size Parameter Model Name Parameters Use Case <code>small</code> <code>gemma-3n-2b</code> 2B Default, fast tasks <code>large</code> <code>gemma-3n-4b</code> 4B High quality, complex tasks"},{"location":"api/#model-selection-methods","title":"Model Selection Methods","text":"<ol> <li>Using size parameter (recommended): <code>generate(\"prompt\", size=\"large\")</code></li> <li>Custom models: <code>generate(\"prompt\", model_repo=\"...\", model_filename=\"...\")</code></li> <li>Environment variables: Set <code>STEADYTEXT_DEFAULT_SIZE</code> or custom model variables</li> </ol>"},{"location":"api/#deprecated-models-v1x","title":"Deprecated Models (v1.x)","text":"<p>Note: The following models were available in SteadyText v1.x but are deprecated in v2.0.0+: - <code>qwen3-1.7b</code>, <code>qwen3-4b</code>, <code>qwen3-8b</code> - <code>qwen2.5-0.5b</code>, <code>qwen2.5-1.5b</code>, <code>qwen2.5-3b</code>, <code>qwen2.5-7b</code></p> <p>Use the <code>size</code> parameter with Gemma-3n models instead.</p>"},{"location":"api/#model-caching","title":"Model Caching","text":"<ul> <li>Models are cached after first load for efficient switching</li> <li>Multiple models can be loaded simultaneously</li> <li>Use <code>clear_model_cache()</code> to free memory if needed</li> </ul>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>All functions are designed to never raise exceptions during normal operation. If models cannot be loaded, deterministic fallback functions are used:</p> <ul> <li>Text generation fallback: Uses hash-based word selection to generate pseudo-random but deterministic text</li> <li>Embedding fallback: Returns zero vectors of the correct dimension</li> </ul> <p>This ensures that your code never breaks, even in environments where models cannot be downloaded or loaded.</p>"},{"location":"architecture/","title":"SteadyText Architecture","text":"<p>This document provides a comprehensive overview of SteadyText's architecture, design decisions, and implementation details.</p>"},{"location":"architecture/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Overview</li> <li>Core Principles</li> <li>System Architecture</li> <li>Component Architecture</li> <li>Data Flow</li> <li>Model Architecture</li> <li>Caching Architecture</li> <li>Daemon Architecture</li> <li>Extension Points</li> <li>Performance Architecture</li> <li>Security Architecture</li> <li>Design Patterns</li> <li>Technology Stack</li> </ul>"},{"location":"architecture/#overview","title":"Overview","text":"<p>SteadyText is designed as a deterministic AI text generation and embedding library with a focus on reproducibility, performance, and reliability. The architecture supports multiple deployment modes (direct, daemon, PostgreSQL extension) while maintaining consistent behavior across all interfaces.</p>"},{"location":"architecture/#key-architectural-goals","title":"Key Architectural Goals","text":"<ol> <li>Determinism: Same input always produces same output</li> <li>Performance: Sub-second response times with caching</li> <li>Reliability: Never fails, graceful degradation</li> <li>Simplicity: Minimal configuration, intuitive APIs</li> <li>Extensibility: Support for custom models and integrations</li> </ol>"},{"location":"architecture/#core-principles","title":"Core Principles","text":""},{"location":"architecture/#1-never-fail-philosophy","title":"1. Never Fail Philosophy","text":"<pre><code># Traditional approach (can fail)\ndef generate_text(prompt):\n    if not model_loaded:\n        raise ModelNotLoadedError()\n    return model.generate(prompt)\n\n# SteadyText approach (never fails)\ndef generate_text(prompt):\n    if not model_loaded:\n        return None  # v2.1.0+ behavior\n    return model.generate(prompt)\n</code></pre>"},{"location":"architecture/#2-deterministic-by-design","title":"2. Deterministic by Design","text":"<p>All operations use fixed seeds and deterministic algorithms:</p> <pre><code># Seed propagation through the stack\nDEFAULT_SEED = 42\n\ndef set_deterministic_environment(seed: int):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n</code></pre>"},{"location":"architecture/#3-lazy-loading","title":"3. Lazy Loading","text":"<p>Models are loaded only when first used:</p> <pre><code>class ModelLoader:\n    _instance = None\n\n    def __new__(cls):\n        if cls._instance is None:\n            cls._instance = super().__new__(cls)\n            cls._instance.model = None\n        return cls._instance\n\n    def get_model(self):\n        if self.model is None:\n            self.model = self._load_model()\n        return self.model\n</code></pre>"},{"location":"architecture/#system-architecture","title":"System Architecture","text":""},{"location":"architecture/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    User Applications                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Interface Layer                           \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502  Python API \u2502   CLI Tools   \u2502  PostgreSQL  \u2502    REST   \u2502 \u2502\n\u2502  \u2502             \u2502  (st/steadytext)\u2502  Extension  \u2502    API    \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                     Core Layer                               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502            Unified Processing Engine                  \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502   \u2502\n\u2502  \u2502  \u2502  Generator   \u2502    Embedder    \u2502  Vector Ops    \u2502 \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                  Infrastructure Layer                        \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Model Loader \u2502  Cache Manager  \u2502   Daemon Service    \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                    Storage Layer                             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502 Model Files  \u2502  Cache Files    \u2502   Index Files       \u2502   \u2502\n\u2502  \u2502   (GGUF)     \u2502   (SQLite)      \u2502    (FAISS)         \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#deployment-architecture","title":"Deployment Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                 Deployment Options                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                      \u2502\n\u2502  1. Direct Mode (Default)                           \u2502\n\u2502     \u2514\u2500&gt; Application \u2192 SteadyText \u2192 Models           \u2502\n\u2502                                                      \u2502\n\u2502  2. Daemon Mode (Recommended for Production)        \u2502\n\u2502     \u2514\u2500&gt; Application \u2192 Client \u2192 Daemon \u2192 Models      \u2502\n\u2502                                                      \u2502\n\u2502  3. PostgreSQL Extension                             \u2502\n\u2502     \u2514\u2500&gt; SQL \u2192 pg_steadytext \u2192 Daemon \u2192 Models       \u2502\n\u2502                                                      \u2502\n\u2502  4. Container/Kubernetes                             \u2502\n\u2502     \u2514\u2500&gt; Service \u2192 Pod \u2192 Container \u2192 Daemon          \u2502\n\u2502                                                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#component-architecture","title":"Component Architecture","text":""},{"location":"architecture/#core-components","title":"Core Components","text":""},{"location":"architecture/#1-generator-component","title":"1. Generator Component","text":"<pre><code># steadytext/core/generator.py\nclass DeterministicGenerator:\n    \"\"\"Core text generation component.\"\"\"\n\n    def __init__(self):\n        self.model = None\n        self.config = GenerationConfig()\n        self.cache = get_cache_manager().generation_cache\n\n    def generate(self, prompt: str, seed: int = 42) -&gt; Optional[str]:\n        # Check cache first\n        cache_key = self._compute_cache_key(prompt, seed)\n        if cached := self.cache.get(cache_key):\n            return cached\n\n        # Load model lazily\n        if self.model is None:\n            self.model = ModelLoader().get_generation_model()\n            if self.model is None:\n                return None  # v2.1.0+ behavior\n\n        # Generate with deterministic settings\n        result = self._generate_deterministic(prompt, seed)\n\n        # Cache result\n        self.cache.set(cache_key, result)\n\n        return result\n</code></pre>"},{"location":"architecture/#2-embedder-component","title":"2. Embedder Component","text":"<pre><code># steadytext/core/embedder.py\nclass DeterministicEmbedder:\n    \"\"\"Core embedding component.\"\"\"\n\n    def embed(self, text: str, seed: int = 42) -&gt; Optional[np.ndarray]:\n        # Similar pattern: cache \u2192 model \u2192 generate \u2192 cache\n        cache_key = self._compute_cache_key(text, seed)\n        if cached := self.cache.get(cache_key):\n            return cached\n\n        if self.model is None:\n            self.model = ModelLoader().get_embedding_model()\n            if self.model is None:\n                return None\n\n        # Generate L2-normalized embeddings\n        embedding = self._embed_deterministic(text, seed)\n        embedding = self._normalize_l2(embedding)\n\n        self.cache.set(cache_key, embedding)\n        return embedding\n</code></pre>"},{"location":"architecture/#3-cache-manager","title":"3. Cache Manager","text":"<pre><code># steadytext/cache_manager.py\nclass CacheManager:\n    \"\"\"Centralized cache management.\"\"\"\n\n    _instance = None\n\n    def __init__(self):\n        self.generation_cache = FrecencyCache(\n            capacity=int(os.getenv('STEADYTEXT_GENERATION_CACHE_CAPACITY', 256)),\n            max_size_mb=float(os.getenv('STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB', 50))\n        )\n        self.embedding_cache = FrecencyCache(\n            capacity=int(os.getenv('STEADYTEXT_EMBEDDING_CACHE_CAPACITY', 512)),\n            max_size_mb=float(os.getenv('STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB', 100))\n        )\n</code></pre>"},{"location":"architecture/#data-flow","title":"Data Flow","text":""},{"location":"architecture/#generation-flow","title":"Generation Flow","text":"<pre><code>User Request\n    \u2502\n    \u25bc\nAPI Layer (generate())\n    \u2502\n    \u251c\u2500&gt; Check Input Validity\n    \u2502\n    \u251c\u2500&gt; Compute Cache Key\n    \u2502\n    \u251c\u2500&gt; Check Cache \u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                     \u2502 (hit)\n    \u2502 (miss)              \u25bc\n    \u25bc                 Return Cached\nModel Loading\n    \u2502\n    \u251c\u2500&gt; Check Model Status\n    \u2502\n    \u2502 (not loaded)\n    \u251c\u2500&gt; Load Model \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                     \u2502 (fail)\n    \u2502 (loaded)            \u25bc\n    \u25bc                 Return None\nGenerate Text\n    \u2502\n    \u251c\u2500&gt; Set Deterministic Seed\n    \u2502\n    \u251c\u2500&gt; Configure Sampling\n    \u2502\n    \u251c\u2500&gt; Run Inference\n    \u2502\n    \u25bc\nCache Result\n    \u2502\n    \u25bc\nReturn Result\n</code></pre>"},{"location":"architecture/#embedding-flow","title":"Embedding Flow","text":"<pre><code>Text Input \u2192 Tokenization \u2192 Model Inference \u2192 Raw Embedding\n                                                    \u2502\n                                                    \u25bc\n                                            L2 Normalization\n                                                    \u2502\n                                                    \u25bc\n                                            1024-dim Vector\n                                                    \u2502\n                                                    \u25bc\n                                               Cache &amp; Return\n</code></pre>"},{"location":"architecture/#model-architecture","title":"Model Architecture","text":""},{"location":"architecture/#model-selection","title":"Model Selection","text":"<pre><code>MODEL_REGISTRY = {\n    'generation': {\n        'small': {\n            'repo': 'ggml-org/gemma-3n-E2B-it-GGUF',\n            'filename': 'gemma-3n-E2B-it-Q8_0.gguf',\n            'context_length': 8192,\n            'vocab_size': 256128\n        },\n        'large': {\n            'repo': 'ggml-org/gemma-3n-E4B-it-GGUF',\n            'filename': 'gemma-3n-E4B-it-Q8_0.gguf',\n            'context_length': 8192,\n            'vocab_size': 256128\n        }\n    },\n    'embedding': {\n        'default': {\n            'repo': 'Qwen/Qwen3-Embedding-0.6B-GGUF',\n            'filename': 'qwen3-embedding-0.6b-q8_0.gguf',\n            'dimension': 1024\n        }\n    }\n}\n</code></pre>"},{"location":"architecture/#model-loading-strategy","title":"Model Loading Strategy","text":"<ol> <li>Lazy Loading: Models loaded on first use</li> <li>Singleton Pattern: One model instance per type</li> <li>Thread Safety: Locks prevent concurrent loading</li> <li>Graceful Fallback: Returns None if loading fails</li> </ol>"},{"location":"architecture/#model-configuration","title":"Model Configuration","text":"<pre><code>GENERATION_CONFIG = {\n    'max_tokens': 512,\n    'temperature': 0.0,  # Deterministic\n    'top_k': 1,          # Greedy decoding\n    'top_p': 1.0,\n    'repeat_penalty': 1.0,\n    'seed': 42,\n    'n_threads': 4,\n    'n_batch': 512,\n    'use_mlock': True,\n    'use_mmap': True\n}\n</code></pre>"},{"location":"architecture/#caching-architecture","title":"Caching Architecture","text":""},{"location":"architecture/#cache-design","title":"Cache Design","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502            Cache Manager                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n\u2502  \u2502 Generation   \u2502    \u2502  Embedding   \u2502      \u2502\n\u2502  \u2502   Cache      \u2502    \u2502    Cache     \u2502      \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n\u2502         \u2502                    \u2502               \u2502\n\u2502         \u25bc                    \u25bc               \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502     Frecency Algorithm          \u2502        \u2502\n\u2502  \u2502  (Frequency + Recency scoring)  \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2502                    \u2502                         \u2502\n\u2502                    \u25bc                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510        \u2502\n\u2502  \u2502    SQLite Backend (Disk)        \u2502        \u2502\n\u2502  \u2502  - Thread-safe                  \u2502        \u2502\n\u2502  \u2502  - Persistent                   \u2502        \u2502\n\u2502  \u2502  - Size-limited                 \u2502        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#cache-key-generation","title":"Cache Key Generation","text":"<pre><code>def compute_cache_key(prompt: str, seed: int, **kwargs) -&gt; str:\n    \"\"\"Generate deterministic cache key.\"\"\"\n    # Include all parameters that affect output\n    key_parts = [\n        prompt,\n        str(seed),\n        str(kwargs.get('max_tokens', 512)),\n        str(kwargs.get('eos_string', '[EOS]'))\n    ]\n\n    # Use SHA256 for consistent hashing\n    key_string = '|'.join(key_parts)\n    return hashlib.sha256(key_string.encode()).hexdigest()\n</code></pre>"},{"location":"architecture/#cache-eviction-strategy","title":"Cache Eviction Strategy","text":"<ol> <li>Frecency Score: Combines frequency and recency</li> <li>Size Limits: Respects configured max size</li> <li>TTL: Optional time-to-live for entries</li> <li>Atomic Operations: Thread-safe updates</li> </ol>"},{"location":"architecture/#daemon-architecture","title":"Daemon Architecture","text":""},{"location":"architecture/#daemon-design","title":"Daemon Design","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Daemon Process                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502      ZeroMQ REP Server           \u2502       \u2502\n\u2502  \u2502   Listening on tcp://*:5557      \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502               \u2502                              \u2502\n\u2502               \u25bc                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502      Request Router              \u2502       \u2502\n\u2502  \u2502  - generate                      \u2502       \u2502\n\u2502  \u2502  - generate_iter                 \u2502       \u2502\n\u2502  \u2502  - embed                         \u2502       \u2502\n\u2502  \u2502  - ping                          \u2502       \u2502\n\u2502  \u2502  - shutdown                      \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502               \u2502                              \u2502\n\u2502               \u25bc                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n\u2502  \u2502    Model Instance Pool           \u2502       \u2502\n\u2502  \u2502  - Gemma-3n (generation)         \u2502       \u2502\n\u2502  \u2502  - Qwen3 (embedding)             \u2502       \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n\u2502                                              \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#communication-protocol","title":"Communication Protocol","text":"<pre><code># Request format\n{\n    \"id\": \"unique-request-id\",\n    \"type\": \"generate\",\n    \"prompt\": \"Hello world\",\n    \"seed\": 42,\n    \"max_tokens\": 512\n}\n\n# Response format\n{\n    \"id\": \"unique-request-id\",\n    \"success\": true,\n    \"result\": \"Generated text...\",\n    \"cached\": false,\n    \"error\": null\n}\n</code></pre>"},{"location":"architecture/#connection-management","title":"Connection Management","text":"<pre><code>class DaemonClient:\n    def __init__(self, host='127.0.0.1', port=5557):\n        self.context = zmq.Context()\n        self.socket = None\n        self.connected = False\n\n    def connect(self):\n        \"\"\"Establish connection with retry logic.\"\"\"\n        self.socket = self.context.socket(zmq.REQ)\n        self.socket.setsockopt(zmq.LINGER, 0)\n        self.socket.setsockopt(zmq.RCVTIMEO, 5000)\n        self.socket.connect(f\"tcp://{self.host}:{self.port}\")\n\n        # Test connection\n        if self._ping():\n            self.connected = True\n        else:\n            self._fallback_to_direct()\n</code></pre>"},{"location":"architecture/#extension-points","title":"Extension Points","text":""},{"location":"architecture/#custom-models","title":"Custom Models","text":"<pre><code># Register custom model\nfrom steadytext.models import register_model\n\nregister_model(\n    'custom-gen',\n    repo='myorg/custom-model-GGUF',\n    filename='model.gguf',\n    model_type='generation'\n)\n\n# Use custom model\ntext = generate(\"Hello\", model='custom-gen')\n</code></pre>"},{"location":"architecture/#custom-embedders","title":"Custom Embedders","text":"<pre><code>class CustomEmbedder:\n    def embed(self, text: str) -&gt; np.ndarray:\n        # Custom embedding logic\n        return np.random.randn(1024)\n\n# Register embedder\nsteadytext.register_embedder('custom', CustomEmbedder())\n</code></pre>"},{"location":"architecture/#plugin-system","title":"Plugin System","text":"<pre><code># Future: Plugin architecture\nclass SteadyTextPlugin:\n    def on_generate_start(self, prompt: str): pass\n    def on_generate_complete(self, result: str): pass\n    def on_embed_start(self, text: str): pass\n    def on_embed_complete(self, embedding: np.ndarray): pass\n\n# Register plugin\nsteadytext.register_plugin(MyPlugin())\n</code></pre>"},{"location":"architecture/#performance-architecture","title":"Performance Architecture","text":""},{"location":"architecture/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li> <p>Model Preloading <pre><code># Preload models at startup\nsteadytext.preload_models()\n</code></pre></p> </li> <li> <p>Connection Pooling <pre><code># Daemon connection pool\npool = ConnectionPool(size=10)\n</code></pre></p> </li> <li> <p>Batch Processing <pre><code># Process multiple requests efficiently\nresults = steadytext.batch_generate(prompts)\n</code></pre></p> </li> <li> <p>Memory Mapping <pre><code># GGUF models use mmap for efficiency\nconfig = {'use_mmap': True, 'use_mlock': True}\n</code></pre></p> </li> </ol>"},{"location":"architecture/#benchmarking-architecture","title":"Benchmarking Architecture","text":"<pre><code>class BenchmarkFramework:\n    def __init__(self):\n        self.metrics = {\n            'latency': [],\n            'throughput': [],\n            'memory': [],\n            'cache_hits': []\n        }\n\n    def run_benchmark(self, workload):\n        \"\"\"Execute standardized benchmark.\"\"\"\n        for operation in workload:\n            with self.measure():\n                operation.execute()\n</code></pre>"},{"location":"architecture/#security-architecture","title":"Security Architecture","text":""},{"location":"architecture/#security-layers","title":"Security Layers","text":"<ol> <li> <p>Input Validation <pre><code>def validate_input(prompt: str) -&gt; bool:\n    # Length limits\n    if len(prompt) &gt; MAX_PROMPT_LENGTH:\n        return False\n    # Character validation\n    if contains_invalid_chars(prompt):\n        return False\n    return True\n</code></pre></p> </li> <li> <p>Process Isolation</p> </li> <li>Daemon runs in separate process</li> <li>Limited system access</li> <li> <p>Resource quotas</p> </li> <li> <p>Communication Security</p> </li> <li>Local-only ZeroMQ by default</li> <li>Optional TLS for remote connections</li> <li> <p>Request authentication</p> </li> <li> <p>Model Security</p> </li> <li>Verified model checksums</li> <li>Restricted model loading paths</li> <li>No arbitrary code execution</li> </ol>"},{"location":"architecture/#design-patterns","title":"Design Patterns","text":""},{"location":"architecture/#1-singleton-pattern","title":"1. Singleton Pattern","text":"<p>Used for model instances and cache manager:</p> <pre><code>class SingletonMeta(type):\n    _instances = {}\n    _lock = threading.Lock()\n\n    def __call__(cls, *args, **kwargs):\n        with cls._lock:\n            if cls not in cls._instances:\n                cls._instances[cls] = super().__call__(*args, **kwargs)\n        return cls._instances[cls]\n</code></pre>"},{"location":"architecture/#2-factory-pattern","title":"2. Factory Pattern","text":"<p>For creating different model types:</p> <pre><code>class ModelFactory:\n    @staticmethod\n    def create_model(model_type: str, size: str):\n        if model_type == 'generation':\n            return GenerationModel(size)\n        elif model_type == 'embedding':\n            return EmbeddingModel()\n</code></pre>"},{"location":"architecture/#3-strategy-pattern","title":"3. Strategy Pattern","text":"<p>For different generation strategies:</p> <pre><code>class GenerationStrategy(ABC):\n    @abstractmethod\n    def generate(self, prompt: str) -&gt; str: pass\n\nclass GreedyStrategy(GenerationStrategy):\n    def generate(self, prompt: str) -&gt; str:\n        # Greedy decoding implementation\n        pass\n\nclass BeamSearchStrategy(GenerationStrategy):\n    def generate(self, prompt: str) -&gt; str:\n        # Beam search implementation\n        pass\n</code></pre>"},{"location":"architecture/#4-observer-pattern","title":"4. Observer Pattern","text":"<p>For event handling:</p> <pre><code>class EventManager:\n    def __init__(self):\n        self.listeners = defaultdict(list)\n\n    def subscribe(self, event: str, callback):\n        self.listeners[event].append(callback)\n\n    def notify(self, event: str, data: Any):\n        for callback in self.listeners[event]:\n            callback(data)\n</code></pre>"},{"location":"architecture/#technology-stack","title":"Technology Stack","text":""},{"location":"architecture/#core-technologies","title":"Core Technologies","text":"<ul> <li>Python 3.8+: Primary language</li> <li>llama-cpp-python: GGUF model inference</li> <li>NumPy: Numerical operations</li> <li>SQLite: Cache storage</li> <li>ZeroMQ: IPC for daemon</li> <li>FAISS: Vector indexing</li> </ul>"},{"location":"architecture/#development-tools","title":"Development Tools","text":"<ul> <li>UV: Package management</li> <li>pytest: Testing framework</li> <li>ruff: Linting</li> <li>mypy: Type checking</li> <li>mkdocs: Documentation</li> </ul>"},{"location":"architecture/#model-format","title":"Model Format","text":"<ul> <li>GGUF: Efficient model storage</li> <li>Quantization: INT8 for efficiency</li> <li>Compression: Built-in GGUF compression</li> </ul>"},{"location":"architecture/#future-architecture","title":"Future Architecture","text":""},{"location":"architecture/#planned-enhancements","title":"Planned Enhancements","text":"<ol> <li>Distributed Architecture</li> <li>Multiple daemon instances</li> <li>Load balancing</li> <li> <p>Horizontal scaling</p> </li> <li> <p>GPU Support</p> </li> <li>CUDA acceleration</li> <li>Metal Performance Shaders</li> <li> <p>Vulkan compute</p> </li> <li> <p>Streaming Architecture</p> </li> <li>WebSocket support</li> <li>Server-sent events</li> <li> <p>Real-time generation</p> </li> <li> <p>Cloud Native</p> </li> <li>Kubernetes operators</li> <li>Service mesh integration</li> <li>Cloud-specific optimizations</li> </ol>"},{"location":"architecture/#architecture-evolution","title":"Architecture Evolution","text":"<pre><code>Current (Monolithic)          Future (Microservices)\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  SteadyText \u2502              \u2502   Gateway   \u2502\n\u2502   Library   \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                     \u2502\n                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                           \u2502                 \u2502\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502Generation\u2502    \u2502 Embedding  \u2502\n                     \u2502  Service \u2502    \u2502  Service   \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"architecture/#conclusion","title":"Conclusion","text":"<p>SteadyText's architecture prioritizes:</p> <ol> <li>Simplicity: Easy to understand and use</li> <li>Reliability: Predictable behavior</li> <li>Performance: Fast response times</li> <li>Extensibility: Easy to extend and customize</li> </ol> <p>The modular design allows for future enhancements while maintaining backward compatibility and consistent behavior across all deployment modes.</p>"},{"location":"benchmarks/","title":"SteadyText Performance Benchmarks","text":"<p>This document provides detailed performance and accuracy benchmarks for SteadyText v1.3.3.</p>"},{"location":"benchmarks/#quick-summary","title":"Quick Summary","text":"<p>SteadyText delivers 100% deterministic text generation and embeddings with competitive performance:</p> <ul> <li>Text Generation: 21.4 generations/sec (46.7ms mean latency)</li> <li>Embeddings: 104.4 single embeddings/sec, up to 598.7 embeddings/sec in batches</li> <li>Cache Performance: 48x speedup for repeated prompts</li> <li>Memory Usage: ~1.4GB for models, 150-200MB during operation</li> <li>Determinism: 100% consistent outputs across all platforms and runs</li> <li>Accuracy: 69.4% similarity for related texts with correct similarity ordering</li> </ul>"},{"location":"benchmarks/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Speed Benchmarks</li> <li>Accuracy Benchmarks</li> <li>Determinism Tests</li> <li>Hardware &amp; Methodology</li> <li>Comparison with Alternatives</li> </ol>"},{"location":"benchmarks/#speed-benchmarks","title":"Speed Benchmarks","text":""},{"location":"benchmarks/#text-generation-performance","title":"Text Generation Performance","text":"<p>SteadyText v2.0.0+ uses the Gemma-3n-E2B-it-Q8_0.gguf model (Gemma-3n-2B) for deterministic text generation:</p> Metric Value Notes Throughput 21.4 generations/sec Fixed 512 tokens per generation Mean Latency 46.7ms Time to generate 512 tokens Median Latency 45.8ms 50th percentile P95 Latency 58.0ms 95th percentile P99 Latency 69.5ms 99th percentile Memory Usage 154MB During generation"},{"location":"benchmarks/#streaming-generation","title":"Streaming Generation","text":"<p>Streaming provides similar performance with slightly higher memory usage:</p> Metric Value Throughput 20.3 generations/sec Mean Latency 49.3ms Memory Usage 213MB"},{"location":"benchmarks/#embedding-performance","title":"Embedding Performance","text":"<p>SteadyText uses the Qwen3-Embedding-0.6B-Q8_0.gguf model for deterministic embeddings (unchanged in v2.0.0+):</p> Batch Size Throughput Mean Latency Use Case 1 104.4 embeddings/sec 9.6ms Single document 10 432.7 embeddings/sec 23.1ms Small batches 50 598.7 embeddings/sec 83.5ms Bulk processing"},{"location":"benchmarks/#cache-performance","title":"Cache Performance","text":"<p>SteadyText includes a frecency cache that dramatically improves performance for repeated operations:</p> Operation Mean Latency Notes Cache Miss 47.6ms First time generating Cache Hit 1.00ms Repeated prompt Speedup 48x Cache vs no-cache Hit Rate 65% Typical workload"},{"location":"benchmarks/#concurrent-performance","title":"Concurrent Performance","text":"<p>SteadyText scales well with multiple concurrent requests:</p> Workers Throughput Scaling Efficiency 1 21.6 ops/sec 100% 2 84.4 ops/sec 95% 4 312.9 ops/sec 90% 8 840.5 ops/sec 85%"},{"location":"benchmarks/#daemon-mode-performance","title":"Daemon Mode Performance","text":"<p>SteadyText v1.3+ includes a daemon mode that keeps models loaded in memory for instant responses:</p> Operation Direct Mode Daemon Mode Improvement First Request 2.4s 15ms 160x faster Subsequent Requests 46.7ms 46.7ms Same With Cache Hit 1.0ms 1.0ms Same Startup Time 0s 2.4s (once) One-time cost <p>Benefits of daemon mode: - Eliminates model loading overhead for each request - Maintains persistent cache across all operations - Supports concurrent requests efficiently - Graceful fallback to direct mode if daemon unavailable</p>"},{"location":"benchmarks/#model-loading","title":"Model Loading","text":"<p>One-time startup cost:</p> <ul> <li>Loading Time: 2.4 seconds</li> <li>Memory Usage: 1.4GB (both models)</li> <li>Models Download: Automatic on first use (~1.9GB total)</li> </ul>"},{"location":"benchmarks/#accuracy-benchmarks","title":"Accuracy Benchmarks","text":""},{"location":"benchmarks/#standard-nlp-benchmarks","title":"Standard NLP Benchmarks","text":"<p>SteadyText performs competitively for a 1B parameter quantized model:</p> Benchmark SteadyText Baseline (1B) Description TruthfulQA 0.42 0.40 Truthfulness in Q&amp;A GSM8K 0.18 0.15 Grade school math HellaSwag 0.58 0.55 Common sense reasoning ARC-Easy 0.71 0.68 Science questions"},{"location":"benchmarks/#embedding-quality","title":"Embedding Quality","text":"Metric Score Description Semantic Similarity 0.76 Correlation with human judgments (STS-B) Clustering Quality 0.68 Silhouette score on 20newsgroups Related Text Similarity 0.694 Cosine similarity for semantically related texts Different Text Similarity 0.466 Cosine similarity for unrelated texts Similarity Ordering \u2705 PASS Correctly ranks related vs unrelated texts"},{"location":"benchmarks/#determinism-tests","title":"Determinism Tests","text":"<p>SteadyText's core guarantee is 100% deterministic outputs:</p>"},{"location":"benchmarks/#test-results","title":"Test Results","text":"Test Result Details Identical Outputs \u2705 PASS 100% consistency across 100 iterations Seed Consistency \u2705 PASS 10 different seeds tested Platform Consistency \u2705 PASS Linux x86_64 verified Fallback Determinism \u2705 PASS Works without models Generation Determinism \u2705 PASS 100% determinism rate in accuracy tests Code Generation Quality \u2705 PASS Generates valid code snippets"},{"location":"benchmarks/#determinism-guarantees","title":"Determinism Guarantees","text":"<ol> <li>Same Input \u2192 Same Output: Every time, on every machine</li> <li>Customizable Seeds: Always uses <code>DEFAULT_SEED=42</code> by default, but can be overridden.</li> <li>Greedy Decoding: No randomness in token selection</li> <li>Quantized Models: 8-bit precision ensures consistency</li> <li>Fallback Support: Deterministic even without models</li> </ol>"},{"location":"benchmarks/#hardware-methodology","title":"Hardware &amp; Methodology","text":""},{"location":"benchmarks/#test-environment","title":"Test Environment","text":"<ul> <li>CPU: Intel Core i7-8700K @ 3.70GHz</li> <li>RAM: 32GB DDR4</li> <li>OS: Linux 6.14.11 (Fedora 42)</li> <li>Python: 3.13.2</li> <li>Models: Gemma-3n-E2B-it-Q8_0.gguf (v2.0.0+), Qwen3-Embedding-0.6B-Q8_0.gguf</li> </ul>"},{"location":"benchmarks/#benchmark-methodology","title":"Benchmark Methodology","text":""},{"location":"benchmarks/#speed-tests","title":"Speed Tests","text":"<ul> <li>5 warmup iterations before measurement</li> <li>100 iterations for statistical significance</li> <li>High-resolution timing with <code>time.perf_counter()</code></li> <li>Memory tracking with <code>psutil</code></li> <li>Cache cleared between hit/miss tests</li> </ul>"},{"location":"benchmarks/#accuracy-tests","title":"Accuracy Tests","text":"<ul> <li>LightEval framework for standard benchmarks</li> <li>Custom determinism verification suite</li> <li>Multiple seed testing for consistency</li> <li>Platform compatibility checks</li> </ul>"},{"location":"benchmarks/#comparison-with-alternatives","title":"Comparison with Alternatives","text":""},{"location":"benchmarks/#vs-non-deterministic-llms","title":"vs. Non-Deterministic LLMs","text":"Feature SteadyText GPT/Claude APIs Determinism 100% guaranteed Variable Latency 46.7ms (fixed) 500-3000ms Cost Free (local) $0.01-0.15/1K tokens Offline \u2705 Works \u274c Requires internet Privacy \u2705 Local only \u26a0\ufe0f Cloud processing"},{"location":"benchmarks/#vs-caching-solutions","title":"vs. Caching Solutions","text":"Feature SteadyText Redis/Memcached Setup Zero config Requires setup First Run 46.7ms N/A (miss) Cached 1.0ms 0.5-2ms Semantic \u2705 Built-in \u274c Exact match only"},{"location":"benchmarks/#running-benchmarks","title":"Running Benchmarks","text":"<p>To run benchmarks yourself:</p> <p>Using UV (recommended): <pre><code># Run all benchmarks\nuv run python benchmarks/run_all_benchmarks.py\n\n# Quick benchmarks (for CI)\nuv run python benchmarks/run_all_benchmarks.py --quick\n\n# Test framework only\nuv run python benchmarks/test_benchmarks.py\n</code></pre></p> <p>Legacy method: <pre><code># Install benchmark dependencies\npip install steadytext[benchmark]\n\n# Run all benchmarks\npython benchmarks/run_all_benchmarks.py\n</code></pre></p> <p>See benchmarks/README.md for detailed instructions.</p>"},{"location":"benchmarks/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Production Ready: Sub-50ms latency suitable for real-time applications</li> <li>Efficient Caching: 48x speedup for repeated operations</li> <li>Scalable: Good concurrent performance up to 8 workers</li> <li>Quality Trade-off: Slightly lower accuracy than larger models, but 100% deterministic</li> <li>Resource Efficient: Only 1.4GB memory for both models</li> </ol> <p>Perfect for testing, CLI tools, and any application requiring reproducible AI outputs.</p>"},{"location":"cache-backends/","title":"Cache Backends","text":"<p>SteadyText now supports pluggable cache backends, allowing you to choose the best caching solution for your deployment scenario.</p>"},{"location":"cache-backends/#available-backends","title":"Available Backends","text":""},{"location":"cache-backends/#sqlite-default","title":"SQLite (Default)","text":"<p>The SQLite backend provides thread-safe, process-safe caching with automatic frecency-based eviction.</p> <p>Features: - Default backend, no configuration required - Thread-safe and process-safe using WAL mode - Automatic migration from legacy pickle format - Configurable size limits with automatic eviction - Persistent storage with atomic operations</p> <p>Configuration: <pre><code># Optional: explicitly select SQLite backend\nexport STEADYTEXT_CACHE_BACKEND=sqlite\n\n# Configure cache settings\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50.0\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100.0\n</code></pre></p>"},{"location":"cache-backends/#cloudflare-d1","title":"Cloudflare D1","text":"<p>The D1 backend enables distributed caching using Cloudflare's edge SQLite database.</p> <p>Features: - Global distribution across Cloudflare's edge network - Automatic replication and disaster recovery - Serverless with no infrastructure to manage - Pay-per-use pricing model - Frecency-based eviction algorithm</p> <p>Requirements: - Cloudflare account with Workers enabled - Deployed D1 proxy Worker (see setup guide below)</p> <p>Configuration: <pre><code># Select D1 backend\nexport STEADYTEXT_CACHE_BACKEND=d1\n\n# Required: D1 proxy Worker configuration\nexport STEADYTEXT_D1_API_URL=https://your-worker.workers.dev\nexport STEADYTEXT_D1_API_KEY=your-secret-api-key\n\n# Optional: Batch size for operations\nexport STEADYTEXT_D1_BATCH_SIZE=50\n</code></pre></p>"},{"location":"cache-backends/#memory","title":"Memory","text":"<p>The memory backend provides fast, in-memory caching for testing or ephemeral workloads.</p> <p>Features: - Fastest performance (no disk I/O) - Simple FIFO eviction when capacity reached - No persistence (data lost on restart) - Minimal overhead</p> <p>Configuration: <pre><code># Select memory backend\nexport STEADYTEXT_CACHE_BACKEND=memory\n\n# Same capacity settings apply\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512\n</code></pre></p>"},{"location":"cache-backends/#d1-backend-setup-guide","title":"D1 Backend Setup Guide","text":""},{"location":"cache-backends/#1-prerequisites","title":"1. Prerequisites","text":"<ul> <li>Cloudflare account with Workers enabled</li> <li>Node.js 16.17.0 or later</li> <li>Wrangler CLI: <code>npm install -g wrangler</code></li> </ul>"},{"location":"cache-backends/#2-deploy-the-d1-proxy-worker","title":"2. Deploy the D1 Proxy Worker","text":"<pre><code># Navigate to the Worker directory\ncd workers/d1-cache-proxy\n\n# Install dependencies\nnpm install\n\n# Login to Cloudflare\nnpx wrangler login\n\n# Create D1 database\nnpx wrangler d1 create steadytext-cache\n\n# Update wrangler.toml with the database ID from above\n\n# Initialize database schema\nnpx wrangler d1 execute steadytext-cache --file=src/schema.sql\n\n# Generate API key\nopenssl rand -base64 32\n\n# Set API key as secret\nnpx wrangler secret put API_KEY\n# Paste your generated API key when prompted\n\n# Deploy the Worker\nnpm run deploy\n</code></pre>"},{"location":"cache-backends/#3-configure-steadytext","title":"3. Configure SteadyText","text":"<p>After deployment, configure SteadyText to use your D1 Worker:</p> <pre><code>import os\n\n# Configure D1 backend\nos.environ[\"STEADYTEXT_CACHE_BACKEND\"] = \"d1\"\nos.environ[\"STEADYTEXT_D1_API_URL\"] = \"https://d1-cache-proxy.your-subdomain.workers.dev\"\nos.environ[\"STEADYTEXT_D1_API_KEY\"] = \"your-api-key-from-step-2\"\n\n# Now use SteadyText normally\nfrom steadytext import generate, embed\n\ntext = generate(\"Hello world\")  # Uses D1 cache\nembedding = embed(\"Some text\")   # Uses D1 cache\n</code></pre>"},{"location":"cache-backends/#choosing-a-backend","title":"Choosing a Backend","text":""},{"location":"cache-backends/#use-sqlite-default-when","title":"Use SQLite (default) when:","text":"<ul> <li>Running on a single machine or small cluster</li> <li>Need persistent cache that survives restarts</li> <li>Want zero configuration</li> <li>Have moderate traffic levels</li> </ul>"},{"location":"cache-backends/#use-d1-when","title":"Use D1 when:","text":"<ul> <li>Deploying globally distributed applications</li> <li>Need cache shared across multiple regions</li> <li>Want serverless, managed infrastructure</li> <li>Can tolerate slight network latency for cache operations</li> <li>Building on Cloudflare Workers platform</li> </ul>"},{"location":"cache-backends/#use-memory-when","title":"Use Memory when:","text":"<ul> <li>Running tests or development</li> <li>Cache persistence is not important</li> <li>Need maximum performance</li> <li>Have plenty of available RAM</li> </ul>"},{"location":"cache-backends/#performance-considerations","title":"Performance Considerations","text":""},{"location":"cache-backends/#latency-comparison","title":"Latency Comparison","text":"<ul> <li>Memory: ~0.01ms per operation</li> <li>SQLite: ~0.1-1ms per operation</li> <li>D1: ~10-50ms per operation (depends on proximity to edge)</li> </ul>"},{"location":"cache-backends/#throughput","title":"Throughput","text":"<ul> <li>Memory: Highest (limited by CPU)</li> <li>SQLite: High (limited by disk I/O)</li> <li>D1: Moderate (limited by network and API rate limits)</li> </ul>"},{"location":"cache-backends/#recommendations","title":"Recommendations","text":"<ol> <li>For single-machine deployments: Use SQLite (default)</li> <li>For global/edge deployments: Use D1</li> <li>For testing: Use Memory</li> <li>For high-throughput local apps: Consider Memory with periodic persistence</li> </ol>"},{"location":"cache-backends/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"cache-backends/#custom-backend-implementation","title":"Custom Backend Implementation","text":"<p>You can create your own cache backend by implementing the <code>CacheBackend</code> interface:</p> <pre><code>from steadytext.cache.base import CacheBackend\nfrom typing import Any, Dict, Optional\n\nclass MyCustomBackend(CacheBackend):\n    def get(self, key: Any) -&gt; Optional[Any]:\n        # Implement get logic\n        pass\n\n    def set(self, key: Any, value: Any) -&gt; None:\n        # Implement set logic\n        pass\n\n    def clear(self) -&gt; None:\n        # Implement clear logic\n        pass\n\n    def sync(self) -&gt; None:\n        # Implement sync logic (if needed)\n        pass\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        # Return statistics\n        return {\"backend\": \"custom\", \"entries\": 0}\n\n    def __len__(self) -&gt; int:\n        # Return number of entries\n        return 0\n</code></pre>"},{"location":"cache-backends/#programmatic-backend-selection","title":"Programmatic Backend Selection","text":"<pre><code>from steadytext.disk_backed_frecency_cache import DiskBackedFrecencyCache\n\n# Use specific backend programmatically\ncache = DiskBackedFrecencyCache(\n    backend_type=\"d1\",\n    api_url=\"https://your-worker.workers.dev\",\n    api_key=\"your-api-key\",\n    capacity=1000,\n    max_size_mb=100.0\n)\n\n# Or with memory backend\ncache = DiskBackedFrecencyCache(backend_type=\"memory\")\n</code></pre>"},{"location":"cache-backends/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"cache-backends/#cache-statistics","title":"Cache Statistics","text":"<p>All backends provide statistics through the <code>get_stats()</code> method:</p> <pre><code>from steadytext import get_cache_manager\n\ncache_manager = get_cache_manager()\nstats = cache_manager.get_cache_stats()\n\nprint(f\"Generation cache: {stats['generation']}\")\nprint(f\"Embedding cache: {stats['embedding']}\")\n</code></pre>"},{"location":"cache-backends/#d1-worker-monitoring","title":"D1 Worker Monitoring","text":"<p>Monitor your D1 Worker performance:</p> <pre><code># View real-time logs\ncd workers/d1-cache-proxy\nnpm run tail\n\n# Check Worker analytics in Cloudflare dashboard\n</code></pre>"},{"location":"cache-backends/#debug-environment-variables","title":"Debug Environment Variables","text":"<pre><code># Enable debug logging\nexport STEADYTEXT_LOG_LEVEL=DEBUG\n\n# Skip cache initialization (for testing)\nexport STEADYTEXT_SKIP_CACHE_INIT=1\n\n# Disable specific cache\nexport STEADYTEXT_DISABLE_CACHE=1\n</code></pre>"},{"location":"cache-backends/#troubleshooting","title":"Troubleshooting","text":""},{"location":"cache-backends/#d1-backend-issues","title":"D1 Backend Issues","text":"<p>Connection Errors: - Verify Worker is deployed: <code>npx wrangler tail</code> - Check API URL is correct (no trailing slash) - Verify API key matches the secret set in Worker</p> <p>Authentication Errors: - Ensure Bearer token format in API_KEY - Check secret was set correctly: <code>npx wrangler secret list</code></p> <p>Performance Issues: - Monitor Worker CPU usage in Cloudflare dashboard - Consider increasing batch size for bulk operations - Check proximity to nearest Cloudflare edge location</p>"},{"location":"cache-backends/#sqlite-backend-issues","title":"SQLite Backend Issues","text":"<p>Database Corruption: - SteadyText automatically moves corrupted databases to <code>.corrupted.*</code> files - Check logs for corruption warnings - Delete corrupted files if disk space is an issue</p> <p>Lock Timeouts: - Usually indicates high concurrency - Consider using D1 for distributed workloads - Increase timeout values if needed</p>"},{"location":"cache-backends/#memory-backend-issues","title":"Memory Backend Issues","text":"<p>Out of Memory: - Reduce cache capacity settings - Monitor memory usage of your application - Consider using SQLite for overflow</p>"},{"location":"cache-backends/#migration-guide","title":"Migration Guide","text":""},{"location":"cache-backends/#from-pickle-to-sqlite-automatic","title":"From Pickle to SQLite (Automatic)","text":"<p>The SQLite backend automatically migrates legacy pickle caches:</p> <ol> <li>On first use, it detects <code>.pkl</code> files</li> <li>Migrates all entries to SQLite format</li> <li>Removes old pickle files after successful migration</li> <li>No manual intervention required</li> </ol>"},{"location":"cache-backends/#switching-backends","title":"Switching Backends","text":"<p>To switch backends:</p> <ol> <li>Export existing cache data (optional)</li> <li>Change <code>STEADYTEXT_CACHE_BACKEND</code> environment variable</li> <li>Restart application</li> <li>Cache will be empty (unless migrating to same backend type)</li> </ol> <p>Note: Cache data is not automatically transferred between different backend types.</p>"},{"location":"cache-backends/#best-practices","title":"Best Practices","text":"<ol> <li>Start with defaults: SQLite backend works well for most use cases</li> <li>Monitor cache hit rates: Use statistics to optimize capacity</li> <li>Set appropriate size limits: Prevent unbounded cache growth</li> <li>Use batch operations: Reduce round trips for D1 backend</li> <li>Test backend switching: Ensure your app handles empty caches gracefully</li> <li>Secure your API keys: Use environment variables, never commit keys</li> <li>Monitor Worker health: Set up alerts for D1 Worker errors</li> </ol>"},{"location":"cache-backends/#future-backends","title":"Future Backends","text":"<p>Planned backend support: - Redis/Valkey (for traditional distributed caching) - DynamoDB (for AWS deployments) - Cloud Storage (for large value caching)</p> <p>To request a backend, open an issue on GitHub.</p>"},{"location":"contributing/","title":"Contributing to SteadyText","text":"<p>We welcome contributions to SteadyText! This document provides guidelines for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork: <code>git clone https://github.com/your-username/steadytext.git</code></li> <li>Create a feature branch: <code>git checkout -b feature/your-feature-name</code></li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (supports up to Python 3.13)</li> <li>Git</li> <li>Recommended: uv for faster dependency management</li> </ul>"},{"location":"contributing/#installation","title":"Installation","text":"With uv (Recommended)With pip <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext\n\n# Install in development mode\nuv sync --dev\n\n# Activate the virtual environment\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n</code></pre> <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -e .[dev]\n</code></pre>"},{"location":"contributing/#development-commands","title":"Development Commands","text":"<p>SteadyText uses uv for task management:</p> <pre><code># Run tests\nuv run python -m pytest\n\n# Run tests with coverage\nuv run python -m pytest --cov=steadytext\n\n# Run tests with model downloads (slower)\nSTEADYTEXT_ALLOW_MODEL_DOWNLOADS=true uv run python -m pytest\n\n# Run linting\nuvx ruff check .\n\n# Format code\nuvx ruff format .\n\n# Type checking\nuvx mypy .\n\n# Run pre-commit hooks\nuvx pre-commit run --all-files\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8: Use <code>uvx ruff format .</code> to auto-format code</li> <li>Use type hints: Add type annotations for function parameters and returns</li> <li>Add docstrings: Document all public functions and classes</li> <li>Keep functions focused: Single responsibility principle</li> </ul> <p>Example:</p> <pre><code>def embed(text_input: Union[str, List[str]]) -&gt; np.ndarray:\n    \"\"\"Create deterministic embeddings for text input.\n\n    Args:\n        text_input: String or list of strings to embed\n\n    Returns:\n        1024-dimensional L2-normalized float32 numpy array\n    \"\"\"\n    # Implementation here\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>SteadyText has comprehensive tests covering:</p> <ul> <li>Deterministic behavior: Same input \u2192 same output</li> <li>Fallback functionality: Works without models</li> <li>Edge cases: Empty inputs, invalid types</li> <li>Performance: Caching behavior</li> </ul>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>def test_your_feature():\n    \"\"\"Test your new feature.\"\"\"\n    # Test deterministic behavior\n    result1 = your_function(\"test input\")\n    result2 = your_function(\"test input\")\n    assert result1 == result2  # Should be identical\n\n    # Test edge cases\n    result3 = your_function(\"\")\n    assert isinstance(result3, expected_type)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\nuv run python -m pytest\n\n# Run specific test file\nuv run python -m pytest tests/test_your_feature.py\n\n# Run with coverage\nuv run python -m pytest --cov=steadytext\n\n# Run tests that require model downloads\nSTEADYTEXT_ALLOW_MODEL_DOWNLOADS=true uv run python -m pytest\n\n# Run tests in parallel\nuv run python -m pytest -n auto\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update API docs: Modify files in <code>docs/api/</code> if adding new functions</li> <li>Add examples: Include usage examples in <code>docs/examples/</code></li> <li>Update README: For major features, update the main README.md</li> </ul>"},{"location":"contributing/#architecture-guidelines","title":"Architecture Guidelines","text":"<p>SteadyText follows a layered architecture:</p> <pre><code>steadytext/\n\u251c\u2500\u2500 core/          # Core generation and embedding logic\n\u251c\u2500\u2500 models/        # Model loading and caching\n\u251c\u2500\u2500 cli/           # Command-line interface\n\u2514\u2500\u2500 utils.py       # Shared utilities\n</code></pre>"},{"location":"contributing/#core-principles","title":"Core Principles","text":"<ol> <li>Never fail: Functions should always return valid outputs</li> <li>Deterministic: Same input always produces same output</li> <li>Thread-safe: Support concurrent usage</li> <li>Cached: Use frecency caching for performance</li> </ol>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":"<ol> <li>Core functionality: Add to <code>steadytext/core/</code></li> <li>Model support: Modify <code>steadytext/models/</code></li> <li>CLI commands: Add to <code>steadytext/cli/commands/</code></li> <li>Utilities: Add to <code>steadytext/utils.py</code></li> </ol>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Run all tests: <code>uv run python -m pytest</code></li> <li>Check linting: <code>uvx ruff check .</code></li> <li>Format code: <code>uvx ruff format .</code></li> <li>Type check: <code>uvx mypy .</code></li> <li>Update documentation: Add/update relevant docs</li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create descriptive title: \"Add feature X\" or \"Fix bug Y\"</li> <li>Write clear description: Explain what changes and why</li> <li>Reference issues: Link to related GitHub issues</li> <li>Add tests: Include tests for new functionality</li> <li>Update changelog: Add entry to CHANGELOG.md</li> </ol>"},{"location":"contributing/#pull-request-template","title":"Pull Request Template","text":"<pre><code>## Description\nBrief description of the changes\n\n## Changes Made\n- [ ] Added feature X\n- [ ] Fixed bug Y\n- [ ] Updated documentation\n\n## Testing\n- [ ] All tests pass\n- [ ] Added tests for new functionality\n- [ ] Manually tested edge cases\n\n## Checklist\n- [ ] Code follows project style guidelines\n- [ ] Self-review completed\n- [ ] Documentation updated\n- [ ] Changelog updated\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#typical-development-cycle","title":"Typical Development Cycle","text":"<ol> <li>Pick/create an issue: Find something to work on</li> <li>Create feature branch: <code>git checkout -b feature/issue-123</code></li> <li>Make changes: Implement your feature</li> <li>Test thoroughly: Run tests and manual testing</li> <li>Commit changes: Use descriptive commit messages</li> <li>Push and PR: Create pull request</li> </ol>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits:</p> <pre><code>feat: add new embedding model support\nfix: resolve caching issue with concurrent access\ndocs: update API documentation for generate()\ntest: add tests for edge cases\nchore: update dependencies\n</code></pre>"},{"location":"contributing/#branch-naming","title":"Branch Naming","text":"<ul> <li><code>feature/description</code> - New features</li> <li><code>fix/description</code> - Bug fixes  </li> <li><code>docs/description</code> - Documentation updates</li> <li><code>refactor/description</code> - Code refactoring</li> </ul>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>SteadyText follows semantic versioning:</p> <ul> <li>Major (1.0.0): Breaking changes, new model versions</li> <li>Minor (0.1.0): New features, backward compatible</li> <li>Patch (0.0.1): Bug fixes, small improvements</li> </ul>"},{"location":"contributing/#model-versioning","title":"Model Versioning","text":"<ul> <li>Models are fixed per major version</li> <li>Only major version updates change model outputs</li> <li>This ensures deterministic behavior across patch/minor updates</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>GitHub Discussions: For questions and general discussion</li> <li>Discord: Join our community chat (link in README)</li> </ul>"},{"location":"contributing/#common-issues","title":"Common Issues","text":"<p>Tests failing locally: <pre><code># Clear caches\nrm -rf ~/.cache/steadytext/\n\n# Reinstall dependencies  \npip install -e .[dev]\n\n# Run tests\npoe test\n</code></pre></p> <p>Import errors: <pre><code># Make sure you're in the right directory\ncd steadytext/\n\n# Install in development mode\npip install -e .\n</code></pre></p> <p>Model download issues: <pre><code># Set environment variable\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Run tests\npoe test-models\n</code></pre></p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive in all interactions. We want SteadyText to be a welcoming project for everyone.</p>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - README.md: Major contributors listed - CHANGELOG.md: Contributions noted in releases - GitHub: Contributor graphs and statistics</p> <p>Thank you for contributing to SteadyText! \ud83d\ude80</p>"},{"location":"deployment/","title":"Deployment Guide","text":"<p>This guide covers deploying SteadyText in various production environments, from simple servers to cloud-native architectures.</p>"},{"location":"deployment/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Deployment Options</li> <li>System Requirements</li> <li>Basic Server Deployment</li> <li>Docker Deployment</li> <li>Kubernetes Deployment</li> <li>Cloud Deployments</li> <li>AWS</li> <li>Google Cloud</li> <li>Azure</li> <li>PostgreSQL Extension Deployment</li> <li>Production Configuration</li> <li>Monitoring and Observability</li> <li>Security Considerations</li> <li>High Availability</li> <li>Troubleshooting</li> </ul>"},{"location":"deployment/#deployment-options","title":"Deployment Options","text":""},{"location":"deployment/#overview-of-deployment-methods","title":"Overview of Deployment Methods","text":"Method Best For Complexity Scalability Direct Install Development Low Limited Systemd Service Single server Medium Vertical Docker Containerized apps Medium Horizontal Kubernetes Cloud-native High Auto-scaling PostgreSQL Extension Database-integrated Medium With database"},{"location":"deployment/#system-requirements","title":"System Requirements","text":""},{"location":"deployment/#minimum-requirements","title":"Minimum Requirements","text":"<ul> <li>CPU: 2 cores (4+ recommended)</li> <li>RAM: 4GB (8GB+ recommended)</li> <li>Storage: 10GB (for models and cache)</li> <li>OS: Linux (Ubuntu 20.04+), macOS, Windows Server</li> <li>Python: 3.8+ (3.10+ recommended)</li> </ul>"},{"location":"deployment/#recommended-production-specs","title":"Recommended Production Specs","text":"<pre><code># Production server specifications\nproduction:\n  cpu: 8 cores\n  ram: 16GB\n  storage: 50GB SSD\n  network: 1Gbps\n  os: Ubuntu 22.04 LTS\n</code></pre>"},{"location":"deployment/#resource-planning","title":"Resource Planning","text":"<pre><code># Calculate resource requirements\ndef calculate_resources(concurrent_users, cache_size_gb, model_size):\n    \"\"\"Estimate resource requirements.\"\"\"\n\n    # Memory calculation\n    base_memory_gb = 2  # OS and services\n    model_memory_gb = {\n        'small': 2,\n        'large': 4\n    }[model_size]\n    cache_memory_gb = cache_size_gb\n    worker_memory_gb = concurrent_users * 0.1  # 100MB per concurrent user\n\n    total_memory_gb = (\n        base_memory_gb + \n        model_memory_gb + \n        cache_memory_gb + \n        worker_memory_gb\n    )\n\n    # CPU calculation\n    cpu_cores = max(4, concurrent_users // 10)\n\n    return {\n        'memory_gb': total_memory_gb,\n        'cpu_cores': cpu_cores,\n        'storage_gb': 10 + cache_size_gb * 2  # 2x cache for growth\n    }\n\n# Example: 100 concurrent users, 10GB cache, large model\nresources = calculate_resources(100, 10, 'large')\nprint(f\"Required: {resources['memory_gb']}GB RAM, {resources['cpu_cores']} cores\")\n</code></pre>"},{"location":"deployment/#basic-server-deployment","title":"Basic Server Deployment","text":""},{"location":"deployment/#1-system-setup","title":"1. System Setup","text":"<pre><code># Ubuntu/Debian setup\nsudo apt update\nsudo apt install -y python3.10 python3.10-venv python3-pip\n\n# Create dedicated user\nsudo useradd -m -s /bin/bash steadytext\nsudo mkdir -p /opt/steadytext\nsudo chown steadytext:steadytext /opt/steadytext\n\n# Switch to steadytext user\nsudo su - steadytext\ncd /opt/steadytext\n</code></pre>"},{"location":"deployment/#2-python-environment","title":"2. Python Environment","text":"<pre><code># Create virtual environment\npython3.10 -m venv venv\nsource venv/bin/activate\n\n# Install SteadyText\npip install steadytext\n\n# Preload models\nst models download --all\nst models preload\n</code></pre>"},{"location":"deployment/#3-systemd-service","title":"3. Systemd Service","text":"<p>Create <code>/etc/systemd/system/steadytext.service</code>:</p> <pre><code>[Unit]\nDescription=SteadyText Daemon Service\nAfter=network.target\n\n[Service]\nType=simple\nUser=steadytext\nGroup=steadytext\nWorkingDirectory=/opt/steadytext\nEnvironment=\"PATH=/opt/steadytext/venv/bin\"\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\"\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=1024\"\nEnvironment=\"STEADYTEXT_EMBEDDING_CACHE_CAPACITY=4096\"\nEnvironment=\"STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=2048\"\nExecStart=/opt/steadytext/venv/bin/st daemon start --foreground --host 0.0.0.0 --port 5557\nRestart=always\nRestartSec=10\n\n# Security\nNoNewPrivileges=true\nPrivateTmp=true\nProtectSystem=strict\nProtectHome=true\nReadWritePaths=/opt/steadytext /home/steadytext/.cache\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start:</p> <pre><code>sudo systemctl daemon-reload\nsudo systemctl enable steadytext\nsudo systemctl start steadytext\nsudo systemctl status steadytext\n</code></pre>"},{"location":"deployment/#4-nginx-reverse-proxy","title":"4. Nginx Reverse Proxy","text":"<p>Install and configure Nginx:</p> <pre><code># /etc/nginx/sites-available/steadytext\nupstream steadytext_backend {\n    server 127.0.0.1:5557;\n    keepalive 32;\n}\n\nserver {\n    listen 80;\n    server_name steadytext.example.com;\n\n    # Redirect to HTTPS\n    return 301 https://$server_name$request_uri;\n}\n\nserver {\n    listen 443 ssl http2;\n    server_name steadytext.example.com;\n\n    # SSL configuration\n    ssl_certificate /etc/letsencrypt/live/steadytext.example.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/steadytext.example.com/privkey.pem;\n    ssl_protocols TLSv1.2 TLSv1.3;\n    ssl_ciphers HIGH:!aNULL:!MD5;\n\n    # Security headers\n    add_header X-Frame-Options \"SAMEORIGIN\" always;\n    add_header X-Content-Type-Options \"nosniff\" always;\n    add_header X-XSS-Protection \"1; mode=block\" always;\n\n    # WebSocket support for streaming\n    location /ws {\n        proxy_pass http://steadytext_backend;\n        proxy_http_version 1.1;\n        proxy_set_header Upgrade $http_upgrade;\n        proxy_set_header Connection \"upgrade\";\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_read_timeout 3600s;\n    }\n\n    # Regular HTTP API\n    location / {\n        proxy_pass http://steadytext_backend;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n        proxy_set_header X-Forwarded-Proto $scheme;\n\n        # Timeouts\n        proxy_connect_timeout 60s;\n        proxy_send_timeout 60s;\n        proxy_read_timeout 60s;\n    }\n}\n</code></pre> <p>Enable site:</p> <pre><code>sudo ln -s /etc/nginx/sites-available/steadytext /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl reload nginx\n</code></pre>"},{"location":"deployment/#docker-deployment","title":"Docker Deployment","text":""},{"location":"deployment/#1-dockerfile","title":"1. Dockerfile","text":"<pre><code># Dockerfile\nFROM python:3.10-slim\n\n# Install system dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create app user\nRUN useradd -m -s /bin/bash steadytext\n\n# Set working directory\nWORKDIR /app\n\n# Install Python dependencies\nCOPY requirements.txt .\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Download models during build (optional)\nRUN python -c \"import steadytext; steadytext.preload_models()\"\n\n# Copy application code\nCOPY . .\n\n# Change ownership\nRUN chown -R steadytext:steadytext /app\n\n# Switch to non-root user\nUSER steadytext\n\n# Expose port\nEXPOSE 5557\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \\\n    CMD st daemon status || exit 1\n\n# Start daemon\nCMD [\"st\", \"daemon\", \"start\", \"--foreground\", \"--host\", \"0.0.0.0\"]\n</code></pre>"},{"location":"deployment/#2-docker-compose","title":"2. Docker Compose","text":"<pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  steadytext:\n    build: .\n    image: steadytext:latest\n    container_name: steadytext-daemon\n    ports:\n      - \"5557:5557\"\n    environment:\n      - STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\n      - STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=1024\n      - STEADYTEXT_EMBEDDING_CACHE_CAPACITY=4096\n      - STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=2048\n    volumes:\n      - steadytext-cache:/home/steadytext/.cache\n      - steadytext-models:/app/models\n    restart: unless-stopped\n    deploy:\n      resources:\n        limits:\n          cpus: '4'\n          memory: 8G\n        reservations:\n          cpus: '2'\n          memory: 4G\n\n  nginx:\n    image: nginx:alpine\n    container_name: steadytext-nginx\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n      - ./certs:/etc/nginx/certs:ro\n    depends_on:\n      - steadytext\n    restart: unless-stopped\n\nvolumes:\n  steadytext-cache:\n  steadytext-models:\n</code></pre>"},{"location":"deployment/#3-build-and-run","title":"3. Build and Run","text":"<pre><code># Build image\ndocker build -t steadytext:latest .\n\n# Run with docker-compose\ndocker-compose up -d\n\n# Check logs\ndocker-compose logs -f steadytext\n\n# Scale horizontally\ndocker-compose up -d --scale steadytext=3\n</code></pre>"},{"location":"deployment/#kubernetes-deployment","title":"Kubernetes Deployment","text":""},{"location":"deployment/#1-configmap","title":"1. ConfigMap","text":"<pre><code># steadytext-config.yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: steadytext-config\n  namespace: steadytext\ndata:\n  STEADYTEXT_GENERATION_CACHE_CAPACITY: \"2048\"\n  STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB: \"1024\"\n  STEADYTEXT_EMBEDDING_CACHE_CAPACITY: \"4096\"\n  STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB: \"2048\"\n  DAEMON_HOST: \"0.0.0.0\"\n  DAEMON_PORT: \"5557\"\n</code></pre>"},{"location":"deployment/#2-deployment","title":"2. Deployment","text":"<pre><code># steadytext-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: steadytext\n  namespace: steadytext\n  labels:\n    app: steadytext\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: steadytext\n  template:\n    metadata:\n      labels:\n        app: steadytext\n    spec:\n      containers:\n      - name: steadytext\n        image: steadytext:latest\n        ports:\n        - containerPort: 5557\n          name: daemon\n        envFrom:\n        - configMapRef:\n            name: steadytext-config\n        resources:\n          requests:\n            memory: \"4Gi\"\n            cpu: \"2\"\n          limits:\n            memory: \"8Gi\"\n            cpu: \"4\"\n        livenessProbe:\n          exec:\n            command:\n            - st\n            - daemon\n            - status\n          initialDelaySeconds: 60\n          periodSeconds: 30\n        readinessProbe:\n          exec:\n            command:\n            - st\n            - daemon\n            - status\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        volumeMounts:\n        - name: cache\n          mountPath: /home/steadytext/.cache\n        - name: models\n          mountPath: /app/models\n      volumes:\n      - name: cache\n        persistentVolumeClaim:\n          claimName: steadytext-cache-pvc\n      - name: models\n        persistentVolumeClaim:\n          claimName: steadytext-models-pvc\n</code></pre>"},{"location":"deployment/#3-service","title":"3. Service","text":"<pre><code># steadytext-service.yaml\napiVersion: v1\nkind: Service\nmetadata:\n  name: steadytext-service\n  namespace: steadytext\nspec:\n  selector:\n    app: steadytext\n  ports:\n  - port: 5557\n    targetPort: 5557\n    name: daemon\n  type: ClusterIP\n</code></pre>"},{"location":"deployment/#4-horizontal-pod-autoscaler","title":"4. Horizontal Pod Autoscaler","text":"<pre><code># steadytext-hpa.yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: steadytext-hpa\n  namespace: steadytext\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: steadytext\n  minReplicas: 2\n  maxReplicas: 10\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n</code></pre>"},{"location":"deployment/#5-ingress","title":"5. Ingress","text":"<pre><code># steadytext-ingress.yaml\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: steadytext-ingress\n  namespace: steadytext\n  annotations:\n    nginx.ingress.kubernetes.io/rewrite-target: /\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  tls:\n  - hosts:\n    - steadytext.example.com\n    secretName: steadytext-tls\n  rules:\n  - host: steadytext.example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: steadytext-service\n            port:\n              number: 5557\n</code></pre>"},{"location":"deployment/#6-deploy-to-kubernetes","title":"6. Deploy to Kubernetes","text":"<pre><code># Create namespace\nkubectl create namespace steadytext\n\n# Apply configurations\nkubectl apply -f steadytext-config.yaml\nkubectl apply -f steadytext-pvc.yaml  # Create PVCs first\nkubectl apply -f steadytext-deployment.yaml\nkubectl apply -f steadytext-service.yaml\nkubectl apply -f steadytext-hpa.yaml\nkubectl apply -f steadytext-ingress.yaml\n\n# Check status\nkubectl -n steadytext get pods\nkubectl -n steadytext logs -f deployment/steadytext\n</code></pre>"},{"location":"deployment/#cloud-deployments","title":"Cloud Deployments","text":""},{"location":"deployment/#aws-deployment","title":"AWS Deployment","text":""},{"location":"deployment/#1-ec2-instance","title":"1. EC2 Instance","text":"<pre><code># Launch EC2 instance (via AWS CLI)\naws ec2 run-instances \\\n  --image-id ami-0c55b159cbfafe1f0 \\\n  --instance-type t3.xlarge \\\n  --key-name your-key \\\n  --security-group-ids sg-xxxxxx \\\n  --subnet-id subnet-xxxxxx \\\n  --tag-specifications 'ResourceType=instance,Tags=[{Key=Name,Value=steadytext-server}]' \\\n  --user-data file://setup.sh\n</code></pre> <p>Setup script (<code>setup.sh</code>):</p> <pre><code>#!/bin/bash\n# Update system\napt update &amp;&amp; apt upgrade -y\n\n# Install dependencies\napt install -y python3.10 python3.10-venv python3-pip nginx\n\n# Install SteadyText\npip3 install steadytext\n\n# Configure and start daemon\ncat &gt; /etc/systemd/system/steadytext.service &lt;&lt; EOF\n[Unit]\nDescription=SteadyText Daemon\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/local/bin/st daemon start --foreground\nRestart=always\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\"\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl enable steadytext\nsystemctl start steadytext\n</code></pre>"},{"location":"deployment/#2-ecs-fargate","title":"2. ECS Fargate","text":"<pre><code>// task-definition.json\n{\n  \"family\": \"steadytext\",\n  \"networkMode\": \"awsvpc\",\n  \"requiresCompatibilities\": [\"FARGATE\"],\n  \"cpu\": \"2048\",\n  \"memory\": \"8192\",\n  \"containerDefinitions\": [\n    {\n      \"name\": \"steadytext\",\n      \"image\": \"your-ecr-repo/steadytext:latest\",\n      \"portMappings\": [\n        {\n          \"containerPort\": 5557,\n          \"protocol\": \"tcp\"\n        }\n      ],\n      \"environment\": [\n        {\n          \"name\": \"STEADYTEXT_GENERATION_CACHE_CAPACITY\",\n          \"value\": \"2048\"\n        }\n      ],\n      \"logConfiguration\": {\n        \"logDriver\": \"awslogs\",\n        \"options\": {\n          \"awslogs-group\": \"/ecs/steadytext\",\n          \"awslogs-region\": \"us-east-1\",\n          \"awslogs-stream-prefix\": \"ecs\"\n        }\n      },\n      \"healthCheck\": {\n        \"command\": [\"CMD-SHELL\", \"st daemon status || exit 1\"],\n        \"interval\": 30,\n        \"timeout\": 10,\n        \"retries\": 3,\n        \"startPeriod\": 60\n      }\n    }\n  ]\n}\n</code></pre>"},{"location":"deployment/#3-lambda-function","title":"3. Lambda Function","text":"<pre><code># lambda_function.py\nimport json\nimport steadytext\n\ndef lambda_handler(event, context):\n    \"\"\"AWS Lambda handler for SteadyText.\"\"\"\n\n    # Parse request\n    body = json.loads(event.get('body', '{}'))\n    prompt = body.get('prompt', '')\n    seed = body.get('seed', 42)\n\n    # Generate text\n    result = steadytext.generate(prompt, seed=seed)\n\n    if result is None:\n        return {\n            'statusCode': 503,\n            'body': json.dumps({'error': 'Model not available'})\n        }\n\n    return {\n        'statusCode': 200,\n        'body': json.dumps({\n            'text': result,\n            'seed': seed\n        })\n    }\n</code></pre>"},{"location":"deployment/#google-cloud-deployment","title":"Google Cloud Deployment","text":""},{"location":"deployment/#1-compute-engine","title":"1. Compute Engine","text":"<pre><code># Create instance\ngcloud compute instances create steadytext-server \\\n  --machine-type=n2-standard-4 \\\n  --image-family=ubuntu-2204-lts \\\n  --image-project=ubuntu-os-cloud \\\n  --boot-disk-size=50GB \\\n  --metadata-from-file startup-script=setup.sh\n</code></pre>"},{"location":"deployment/#2-cloud-run","title":"2. Cloud Run","text":"<pre><code># Dockerfile for Cloud Run\nFROM python:3.10-slim\n\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\n\nCOPY . .\n\n# Cloud Run sets PORT environment variable\nCMD exec gunicorn --bind :$PORT --workers 1 --threads 8 app:app\n</code></pre> <p>Deploy:</p> <pre><code># Build and push\ngcloud builds submit --tag gcr.io/PROJECT-ID/steadytext\n\n# Deploy\ngcloud run deploy steadytext \\\n  --image gcr.io/PROJECT-ID/steadytext \\\n  --platform managed \\\n  --memory 8Gi \\\n  --cpu 4 \\\n  --timeout 300 \\\n  --concurrency 10\n</code></pre>"},{"location":"deployment/#3-kubernetes-engine-gke","title":"3. Kubernetes Engine (GKE)","text":"<pre><code># Create cluster\ngcloud container clusters create steadytext-cluster \\\n  --num-nodes=3 \\\n  --machine-type=n2-standard-4 \\\n  --enable-autoscaling \\\n  --min-nodes=2 \\\n  --max-nodes=10\n\n# Deploy application\nkubectl apply -f k8s/\n</code></pre>"},{"location":"deployment/#azure-deployment","title":"Azure Deployment","text":""},{"location":"deployment/#1-virtual-machine","title":"1. Virtual Machine","text":"<pre><code># Create VM\naz vm create \\\n  --resource-group steadytext-rg \\\n  --name steadytext-vm \\\n  --image UbuntuLTS \\\n  --size Standard_D4s_v3 \\\n  --admin-username azureuser \\\n  --generate-ssh-keys \\\n  --custom-data setup.sh\n</code></pre>"},{"location":"deployment/#2-container-instances","title":"2. Container Instances","text":"<pre><code># Deploy container\naz container create \\\n  --resource-group steadytext-rg \\\n  --name steadytext \\\n  --image your-acr.azurecr.io/steadytext:latest \\\n  --cpu 4 \\\n  --memory 8 \\\n  --port 5557 \\\n  --environment-variables \\\n    STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\n</code></pre>"},{"location":"deployment/#3-app-service","title":"3. App Service","text":"<pre><code># Create App Service plan\naz appservice plan create \\\n  --name steadytext-plan \\\n  --resource-group steadytext-rg \\\n  --sku P2v3 \\\n  --is-linux\n\n# Deploy container\naz webapp create \\\n  --resource-group steadytext-rg \\\n  --plan steadytext-plan \\\n  --name steadytext-app \\\n  --deployment-container-image-name your-acr.azurecr.io/steadytext:latest\n</code></pre>"},{"location":"deployment/#postgresql-extension-deployment","title":"PostgreSQL Extension Deployment","text":""},{"location":"deployment/#1-standalone-postgresql","title":"1. Standalone PostgreSQL","text":"<pre><code># Install PostgreSQL and dependencies\nsudo apt install postgresql-15 postgresql-server-dev-15 python3-dev\n\n# Install pg_steadytext\ncd pg_steadytext\nsudo ./install.sh\n\n# Configure PostgreSQL\nsudo -u postgres psql &lt;&lt; EOF\nCREATE DATABASE steadytext_db;\n\\c steadytext_db\nCREATE EXTENSION plpython3u;\nCREATE EXTENSION vector;\nCREATE EXTENSION pg_steadytext;\nEOF\n</code></pre>"},{"location":"deployment/#2-docker-postgresql","title":"2. Docker PostgreSQL","text":"<pre><code># Dockerfile.postgres\nFROM postgres:15\n\n# Install dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    postgresql-plpython3-15 \\\n    python3-pip \\\n    build-essential\n\n# Install Python packages\nRUN pip3 install steadytext pyzmq numpy\n\n# Copy extension files\nCOPY pg_steadytext /tmp/pg_steadytext\nRUN cd /tmp/pg_steadytext &amp;&amp; ./install.sh\n\n# Initialize script\nCOPY init.sql /docker-entrypoint-initdb.d/\n</code></pre>"},{"location":"deployment/#3-managed-postgresql-rdscloudsql","title":"3. Managed PostgreSQL (RDS/CloudSQL)","text":"<p>Most managed PostgreSQL services don't support custom extensions. Options:</p> <ol> <li>Use separate daemon: Run SteadyText daemon separately</li> <li>API wrapper: Create REST API that PostgreSQL can call</li> <li>Self-managed: Use EC2/GCE with PostgreSQL</li> </ol>"},{"location":"deployment/#production-configuration","title":"Production Configuration","text":""},{"location":"deployment/#1-environment-configuration","title":"1. Environment Configuration","text":"<pre><code># /etc/environment or .env file\n# Performance\nSTEADYTEXT_GENERATION_CACHE_CAPACITY=4096\nSTEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=2048\nSTEADYTEXT_EMBEDDING_CACHE_CAPACITY=8192\nSTEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=4096\n\n# Security\nSTEADYTEXT_DAEMON_AUTH_TOKEN=your-secret-token\nSTEADYTEXT_ALLOWED_HOSTS=steadytext.example.com\n\n# Monitoring\nSTEADYTEXT_METRICS_ENABLED=true\nSTEADYTEXT_METRICS_PORT=9090\n\n# Logging\nSTEADYTEXT_LOG_LEVEL=INFO\nSTEADYTEXT_LOG_FILE=/var/log/steadytext/daemon.log\n</code></pre>"},{"location":"deployment/#2-resource-limits","title":"2. Resource Limits","text":"<pre><code># systemd resource limits\n[Service]\n# Memory limits\nMemoryMax=16G\nMemoryHigh=12G\n\n# CPU limits\nCPUQuota=400%  # 4 cores\n\n# File descriptor limits\nLimitNOFILE=65536\n\n# Process limits\nTasksMax=1024\n</code></pre>"},{"location":"deployment/#3-logging-configuration","title":"3. Logging Configuration","text":"<pre><code># logging_config.py\nLOGGING_CONFIG = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        'standard': {\n            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'\n        },\n        'json': {\n            'class': 'pythonjsonlogger.jsonlogger.JsonFormatter',\n            'format': '%(asctime)s %(name)s %(levelname)s %(message)s'\n        }\n    },\n    'handlers': {\n        'file': {\n            'class': 'logging.handlers.RotatingFileHandler',\n            'filename': '/var/log/steadytext/daemon.log',\n            'maxBytes': 100_000_000,  # 100MB\n            'backupCount': 10,\n            'formatter': 'json'\n        },\n        'console': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'standard'\n        }\n    },\n    'root': {\n        'level': 'INFO',\n        'handlers': ['file', 'console']\n    }\n}\n</code></pre>"},{"location":"deployment/#monitoring-and-observability","title":"Monitoring and Observability","text":""},{"location":"deployment/#1-prometheus-metrics","title":"1. Prometheus Metrics","text":"<pre><code># metrics.py\nfrom prometheus_client import Counter, Histogram, Gauge, start_http_server\n\n# Define metrics\ngeneration_requests = Counter('steadytext_generation_requests_total', \n                            'Total generation requests')\ngeneration_duration = Histogram('steadytext_generation_duration_seconds',\n                              'Generation request duration')\ncache_hits = Counter('steadytext_cache_hits_total', \n                    'Cache hit count', ['cache_type'])\nactive_connections = Gauge('steadytext_active_connections',\n                          'Number of active connections')\n\n# Start metrics server\nstart_http_server(9090)\n</code></pre>"},{"location":"deployment/#2-grafana-dashboard","title":"2. Grafana Dashboard","text":"<pre><code>{\n  \"dashboard\": {\n    \"title\": \"SteadyText Monitoring\",\n    \"panels\": [\n      {\n        \"title\": \"Request Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(steadytext_generation_requests_total[5m])\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Response Time\",\n        \"targets\": [\n          {\n            \"expr\": \"histogram_quantile(0.95, steadytext_generation_duration_seconds)\"\n          }\n        ]\n      },\n      {\n        \"title\": \"Cache Hit Rate\",\n        \"targets\": [\n          {\n            \"expr\": \"rate(steadytext_cache_hits_total[5m]) / rate(steadytext_generation_requests_total[5m])\"\n          }\n        ]\n      }\n    ]\n  }\n}\n</code></pre>"},{"location":"deployment/#3-health-checks","title":"3. Health Checks","text":"<pre><code># healthcheck.py\nfrom flask import Flask, jsonify\nimport steadytext\n\napp = Flask(__name__)\n\n@app.route('/health')\ndef health():\n    \"\"\"Basic health check.\"\"\"\n    return jsonify({'status': 'healthy'})\n\n@app.route('/ready')\ndef ready():\n    \"\"\"Readiness check with model test.\"\"\"\n    try:\n        result = steadytext.generate(\"test\", seed=42)\n        if result:\n            return jsonify({'status': 'ready'})\n        else:\n            return jsonify({'status': 'not ready'}), 503\n    except Exception as e:\n        return jsonify({'status': 'error', 'message': str(e)}), 503\n\n@app.route('/metrics')\ndef metrics():\n    \"\"\"Custom metrics endpoint.\"\"\"\n    from steadytext import get_cache_manager\n    stats = get_cache_manager().get_cache_stats()\n\n    return jsonify({\n        'cache': stats,\n        'models': {\n            'loaded': True,\n            'generation_model': 'gemma-3n',\n            'embedding_model': 'qwen3'\n        }\n    })\n</code></pre>"},{"location":"deployment/#security-considerations","title":"Security Considerations","text":""},{"location":"deployment/#1-network-security","title":"1. Network Security","text":"<pre><code># Rate limiting in Nginx\nlimit_req_zone $binary_remote_addr zone=steadytext:10m rate=10r/s;\n\nserver {\n    location / {\n        limit_req zone=steadytext burst=20 nodelay;\n        # ... proxy settings\n    }\n}\n</code></pre>"},{"location":"deployment/#2-authentication","title":"2. Authentication","text":"<pre><code># Simple token authentication\nimport hmac\nimport hashlib\n\ndef verify_token(request_token, secret_key):\n    \"\"\"Verify API token.\"\"\"\n    expected = hmac.new(\n        secret_key.encode(),\n        b\"steadytext\",\n        hashlib.sha256\n    ).hexdigest()\n    return hmac.compare_digest(request_token, expected)\n\n# Middleware\ndef require_auth(func):\n    def wrapper(*args, **kwargs):\n        token = request.headers.get('X-API-Token')\n        if not verify_token(token, SECRET_KEY):\n            abort(401)\n        return func(*args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"deployment/#3-input-validation","title":"3. Input Validation","text":"<pre><code>def validate_input(prompt: str) -&gt; bool:\n    \"\"\"Validate user input.\"\"\"\n    # Length check\n    if len(prompt) &gt; 10000:\n        return False\n\n    # Character validation\n    if not prompt.isprintable():\n        return False\n\n    # Rate limiting per user\n    if check_rate_limit(user_id):\n        return False\n\n    return True\n</code></pre>"},{"location":"deployment/#high-availability","title":"High Availability","text":""},{"location":"deployment/#1-load-balancing","title":"1. Load Balancing","text":"<pre><code># Nginx load balancing\nupstream steadytext_cluster {\n    least_conn;\n    server steadytext1.internal:5557 max_fails=3 fail_timeout=30s;\n    server steadytext2.internal:5557 max_fails=3 fail_timeout=30s;\n    server steadytext3.internal:5557 max_fails=3 fail_timeout=30s;\n    keepalive 32;\n}\n</code></pre>"},{"location":"deployment/#2-failover-configuration","title":"2. Failover Configuration","text":"<pre><code># Client with failover\nclass FailoverClient:\n    def __init__(self, servers):\n        self.servers = servers\n        self.current_server = 0\n\n    def generate(self, prompt, max_retries=3):\n        \"\"\"Generate with automatic failover.\"\"\"\n        for attempt in range(max_retries):\n            try:\n                server = self.servers[self.current_server]\n                return self._call_server(server, prompt)\n            except Exception as e:\n                logger.warning(f\"Server {server} failed: {e}\")\n                self.current_server = (self.current_server + 1) % len(self.servers)\n\n        raise Exception(\"All servers failed\")\n</code></pre>"},{"location":"deployment/#3-backup-and-recovery","title":"3. Backup and Recovery","text":"<pre><code>#!/bin/bash\n# backup.sh - Backup cache and models\n\nBACKUP_DIR=\"/backup/steadytext/$(date +%Y%m%d)\"\nmkdir -p \"$BACKUP_DIR\"\n\n# Backup cache\nrsync -av ~/.cache/steadytext/ \"$BACKUP_DIR/cache/\"\n\n# Backup models\nrsync -av ~/.cache/steadytext/models/ \"$BACKUP_DIR/models/\"\n\n# Backup configuration\ncp /etc/steadytext/* \"$BACKUP_DIR/config/\"\n\n# Compress\ntar -czf \"$BACKUP_DIR.tar.gz\" \"$BACKUP_DIR\"\nrm -rf \"$BACKUP_DIR\"\n\n# Upload to S3\naws s3 cp \"$BACKUP_DIR.tar.gz\" s3://backup-bucket/steadytext/\n</code></pre>"},{"location":"deployment/#troubleshooting","title":"Troubleshooting","text":""},{"location":"deployment/#common-deployment-issues","title":"Common Deployment Issues","text":""},{"location":"deployment/#1-model-loading-failures","title":"1. Model Loading Failures","text":"<pre><code># Check model directory\nls -la ~/.cache/steadytext/models/\n\n# Download models manually\nst models download --all\n\n# Verify model integrity\nst models status --verify\n</code></pre>"},{"location":"deployment/#2-memory-issues","title":"2. Memory Issues","text":"<pre><code># Check memory usage\nfree -h\nps aux | grep steadytext\n\n# Adjust cache sizes\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=500\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=1000\n</code></pre>"},{"location":"deployment/#3-performance-issues","title":"3. Performance Issues","text":"<pre><code># Check daemon status\nst daemon status --verbose\n\n# Monitor resource usage\nhtop -p $(pgrep -f steadytext)\n\n# Check cache performance\nst cache --status --detailed\n</code></pre>"},{"location":"deployment/#debugging-production-issues","title":"Debugging Production Issues","text":"<pre><code># debug_helper.py\nimport logging\nimport traceback\nfrom functools import wraps\n\ndef debug_on_error(func):\n    \"\"\"Decorator to help debug production issues.\"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            logging.error(f\"Error in {func.__name__}:\")\n            logging.error(f\"Args: {args}\")\n            logging.error(f\"Kwargs: {kwargs}\")\n            logging.error(traceback.format_exc())\n            raise\n    return wrapper\n\n# Use in production\n@debug_on_error\ndef generate_text(prompt):\n    return steadytext.generate(prompt)\n</code></pre>"},{"location":"deployment/#deployment-checklist","title":"Deployment Checklist","text":""},{"location":"deployment/#pre-deployment","title":"Pre-Deployment","text":"<ul> <li>[ ] System requirements verified</li> <li>[ ] Python 3.8+ installed</li> <li>[ ] Sufficient disk space (10GB+)</li> <li>[ ] Network connectivity tested</li> <li>[ ] Security groups/firewalls configured</li> </ul>"},{"location":"deployment/#deployment","title":"Deployment","text":"<ul> <li>[ ] SteadyText installed</li> <li>[ ] Models downloaded</li> <li>[ ] Daemon configured</li> <li>[ ] Service scripts created</li> <li>[ ] Reverse proxy configured</li> <li>[ ] SSL certificates installed</li> </ul>"},{"location":"deployment/#post-deployment","title":"Post-Deployment","text":"<ul> <li>[ ] Health checks passing</li> <li>[ ] Monitoring configured</li> <li>[ ] Logs being collected</li> <li>[ ] Backup strategy implemented</li> <li>[ ] Performance benchmarked</li> <li>[ ] Documentation updated</li> </ul>"},{"location":"deployment/#production-readiness","title":"Production Readiness","text":"<ul> <li>[ ] High availability configured</li> <li>[ ] Auto-scaling enabled</li> <li>[ ] Rate limiting active</li> <li>[ ] Security hardened</li> <li>[ ] Disaster recovery tested</li> <li>[ ] Team trained</li> </ul>"},{"location":"deployment/#support","title":"Support","text":"<ul> <li>Documentation: steadytext.readthedocs.io</li> <li>Issues: GitHub Issues</li> <li>Community: Discussions</li> </ul>"},{"location":"eos-string-implementation/","title":"EOS String Implementation Summary","text":"<p>This document summarizes the implementation of the custom <code>eos_string</code> parameter feature.</p>"},{"location":"eos-string-implementation/#changes-made","title":"Changes Made","text":""},{"location":"eos-string-implementation/#1-core-generation-module-steadytextcoregeneratorpy","title":"1. Core Generation Module (<code>steadytext/core/generator.py</code>)","text":"<ul> <li>Added <code>eos_string</code> parameter to <code>DeterministicGenerator.generate()</code> method</li> <li>Default value: <code>\"[EOS]\"</code> (special marker for model's default EOS token)</li> <li> <p>When custom value provided, it's added to the stop sequences</p> </li> <li> <p>Added <code>eos_string</code> parameter to <code>DeterministicGenerator.generate_iter()</code> method</p> </li> <li>Supports streaming generation with custom stop strings</li> <li> <p>Added <code>include_logprobs</code> parameter for compatibility with CLI</p> </li> <li> <p>Updated caching logic to include <code>eos_string</code> in cache key when not default</p> </li> <li>Ensures different eos_strings produce separately cached results</li> </ul>"},{"location":"eos-string-implementation/#2-public-api-steadytext__init__py","title":"2. Public API (<code>steadytext/__init__.py</code>)","text":"<ul> <li> <p>Updated <code>generate()</code> function signature:   <pre><code>def generate(prompt: str, return_logprobs: bool = False, eos_string: str = \"[EOS]\")\n</code></pre></p> </li> <li> <p>Updated <code>generate_iter()</code> function signature:   <pre><code>def generate_iter(prompt: str, eos_string: str = \"[EOS]\", include_logprobs: bool = False)\n</code></pre></p> </li> </ul>"},{"location":"eos-string-implementation/#3-cli-updates","title":"3. CLI Updates","text":""},{"location":"eos-string-implementation/#generate-command-steadytextclicommandsgeneratepy","title":"Generate Command (<code>steadytext/cli/commands/generate.py</code>)","text":"<ul> <li>Added <code>--eos-string</code> parameter (default: \"[EOS]\")</li> <li>Passes eos_string to both batch and streaming generation</li> </ul>"},{"location":"eos-string-implementation/#main-cli-steadytextclimainpy","title":"Main CLI (<code>steadytext/cli/main.py</code>)","text":"<ul> <li>Added <code>--quiet</code> / <code>-q</code> flag to silence log output</li> <li>Sets logging level to ERROR for both steadytext and llama_cpp loggers when quiet mode is enabled</li> </ul>"},{"location":"eos-string-implementation/#4-tests-teststest_steadytextpy","title":"4. Tests (<code>tests/test_steadytext.py</code>)","text":"<p>Added three new test methods: - <code>test_generate_with_custom_eos_string()</code> - Tests basic eos_string functionality - <code>test_generate_iter_with_eos_string()</code> - Tests streaming with custom eos_string - <code>test_generate_eos_string_with_logprobs()</code> - Tests combination of eos_string and logprobs</p>"},{"location":"eos-string-implementation/#5-test-scripts","title":"5. Test Scripts","text":"<p>Created two test scripts for manual verification: - <code>test_eos_string.py</code> - Python script testing various eos_string scenarios - <code>test_cli_eos.sh</code> - Bash script testing CLI functionality</p>"},{"location":"eos-string-implementation/#usage-examples","title":"Usage Examples","text":""},{"location":"eos-string-implementation/#python-api","title":"Python API","text":"<pre><code>import steadytext\n\n# Use model's default EOS token\ntext = steadytext.generate(\"Hello world\", eos_string=\"[EOS]\")\n\n# Stop at custom string\ntext = steadytext.generate(\"List items until END\", eos_string=\"END\")\n\n# Streaming with custom eos\nfor token in steadytext.generate_iter(\"Generate text\", eos_string=\"STOP\"):\n    print(token, end=\"\")\n</code></pre>"},{"location":"eos-string-implementation/#cli","title":"CLI","text":"<pre><code># Default behavior\nsteadytext \"Generate some text\"\n\n# Custom eos string\nsteadytext \"Generate until DONE\" --eos-string \"DONE\"\n\n# Quiet mode (no logs)\nsteadytext --quiet \"Generate without logs\"\n\n# Streaming with custom eos\nsteadytext \"Stream until END\" --stream --eos-string \"END\"\n</code></pre>"},{"location":"eos-string-implementation/#implementation-notes","title":"Implementation Notes","text":"<ol> <li> <p>The <code>\"[EOS]\"</code> string is a special marker that tells the system to use the model's default EOS token and stop sequences.</p> </li> <li> <p>When a custom eos_string is provided, it's added to the existing stop sequences rather than replacing them.</p> </li> <li> <p>Cache keys include the eos_string when it's not the default, ensuring proper caching behavior.</p> </li> <li> <p>The quiet flag affects all loggers in the steadytext namespace and llama_cpp if present.</p> </li> </ol>"},{"location":"faq/","title":"Frequently Asked Questions (FAQ)","text":"<p>Find answers to common questions about SteadyText, troubleshooting tips, and best practices.</p>"},{"location":"faq/#table-of-contents","title":"Table of Contents","text":"<ul> <li>General Questions</li> <li>Installation &amp; Setup</li> <li>Usage Questions</li> <li>Performance Questions</li> <li>Model Questions</li> <li>Caching Questions</li> <li>Daemon Questions</li> <li>PostgreSQL Extension</li> <li>Troubleshooting</li> <li>Advanced Topics</li> </ul>"},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-steadytext","title":"What is SteadyText?","text":"<p>SteadyText is a deterministic AI text generation and embedding library for Python. It ensures that the same input always produces the same output, making it ideal for:</p> <ul> <li>Reproducible research</li> <li>Testing AI-powered applications</li> <li>Consistent embeddings for search</li> <li>Deterministic content generation</li> </ul>"},{"location":"faq/#how-is-steadytext-different-from-other-ai-libraries","title":"How is SteadyText different from other AI libraries?","text":"Feature SteadyText Other Libraries Deterministic \u2705 Always \u274c Usually random Never fails \u2705 Returns None \u274c Throws exceptions Zero config \u2705 Works instantly \u274c Complex setup Built-in cache \u2705 Automatic \u274c Manual setup PostgreSQL \u2705 Native extension \u274c External integration"},{"location":"faq/#what-models-does-steadytext-use","title":"What models does SteadyText use?","text":"<ul> <li>Text Generation: Gemma-3n models (2B and 4B parameters)</li> <li>Embeddings: Qwen3-Embedding-0.6B (1024 dimensions)</li> <li>Format: GGUF quantized models for efficiency</li> </ul>"},{"location":"faq/#is-steadytext-suitable-for-production","title":"Is SteadyText suitable for production?","text":"<p>Yes! SteadyText is designed for production use:</p> <ul> <li>Daemon mode: 160x faster responses</li> <li>Thread-safe: Handles concurrent requests</li> <li>Resource efficient: Quantized models use less memory</li> <li>Battle-tested: Used in production environments</li> <li>PostgreSQL integration: Database-native AI</li> </ul>"},{"location":"faq/#installation-setup","title":"Installation &amp; Setup","text":""},{"location":"faq/#how-do-i-install-steadytext","title":"How do I install SteadyText?","text":"<pre><code># Using pip\npip install steadytext\n\n# Using UV (recommended)\nuv add steadytext\n\n# With PostgreSQL extension\npip install steadytext[postgres]\n</code></pre>"},{"location":"faq/#what-are-the-system-requirements","title":"What are the system requirements?","text":"<ul> <li>Python: 3.8 or higher</li> <li>Memory: 4GB RAM minimum (8GB recommended)</li> <li>Disk: 2GB for models</li> <li>OS: Linux, macOS, Windows</li> </ul>"},{"location":"faq/#do-i-need-a-gpu","title":"Do I need a GPU?","text":"<p>No, SteadyText is optimized for CPU inference. GPU support is planned for future releases.</p>"},{"location":"faq/#how-do-i-verify-the-installation","title":"How do I verify the installation?","text":"<pre><code># Check CLI\nst --version\n\n# Test generation\necho \"Hello world\" | st\n\n# Python test\npython -c \"import steadytext; print(steadytext.generate('Hello'))\"\n</code></pre>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#how-do-i-ensure-deterministic-results","title":"How do I ensure deterministic results?","text":"<p>Use the same seed value:</p> <pre><code># Always produces the same output\nresult1 = steadytext.generate(\"Hello\", seed=42)\nresult2 = steadytext.generate(\"Hello\", seed=42)\nassert result1 == result2\n</code></pre>"},{"location":"faq/#can-i-use-custom-prompts","title":"Can I use custom prompts?","text":"<p>Yes, any text prompt works:</p> <pre><code># Simple prompts\ntext = steadytext.generate(\"Write a poem\")\n\n# Complex prompts with instructions\nprompt = \"\"\"\nYou are a helpful assistant. Please:\n1. Summarize the following text\n2. Extract key points\n3. Suggest improvements\n\nText: [your text here]\n\"\"\"\nresult = steadytext.generate(prompt)\n</code></pre>"},{"location":"faq/#how-do-i-generate-longer-texts","title":"How do I generate longer texts?","text":"<p>Adjust the <code>max_new_tokens</code> parameter:</p> <pre><code># Default: 512 tokens\nshort = steadytext.generate(\"Story\", max_new_tokens=100)\n\n# Longer output\nlong = steadytext.generate(\"Story\", max_new_tokens=2000)\n</code></pre>"},{"location":"faq/#can-i-stream-the-output","title":"Can I stream the output?","text":"<p>Yes, use the streaming API:</p> <pre><code>for chunk in steadytext.generate_iter(\"Write a long story\"):\n    print(chunk, end='', flush=True)\n</code></pre>"},{"location":"faq/#how-do-embeddings-work","title":"How do embeddings work?","text":"<pre><code># Create embedding\nembedding = steadytext.embed(\"Machine learning\")\n# Returns: numpy array of shape (1024,)\n\n# Compare similarity\nemb1 = steadytext.embed(\"cat\")\nemb2 = steadytext.embed(\"dog\")\nsimilarity = np.dot(emb1, emb2)  # Cosine similarity\n</code></pre>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#why-is-the-first-generation-slow","title":"Why is the first generation slow?","text":"<p>The first call loads the model into memory (2-3 seconds). Subsequent calls are fast (&lt;100ms). To avoid this:</p> <pre><code># Option 1: Use daemon mode\nst daemon start\n\n# Option 2: Preload models\nst models preload\n</code></pre>"},{"location":"faq/#how-can-i-improve-performance","title":"How can I improve performance?","text":"<ol> <li> <p>Use daemon mode (160x faster):    <pre><code>st daemon start\n</code></pre></p> </li> <li> <p>Enable caching (enabled by default):    <pre><code># Cache automatically stores results\nresult = steadytext.generate(\"Same prompt\")  # First: slow\nresult = steadytext.generate(\"Same prompt\")  # Second: instant\n</code></pre></p> </li> <li> <p>Batch operations:    <pre><code>prompts = [\"Prompt 1\", \"Prompt 2\", \"Prompt 3\"]\nresults = [steadytext.generate(p) for p in prompts]\n</code></pre></p> </li> </ol>"},{"location":"faq/#what-are-typical-response-times","title":"What are typical response times?","text":"Operation First Call Cached With Daemon Generate 2-3s &lt;10ms &lt;20ms Embed 1-2s &lt;5ms &lt;15ms Batch (100) 3-5s &lt;100ms &lt;200ms"},{"location":"faq/#model-questions","title":"Model Questions","text":""},{"location":"faq/#can-i-use-different-model-sizes","title":"Can I use different model sizes?","text":"<p>Yes, SteadyText supports multiple model sizes:</p> <pre><code># CLI\nst generate \"Hello\" --size small  # Fast, 2B parameters\nst generate \"Hello\" --size large  # Better quality, 4B parameters\n\n# Python\ntext = steadytext.generate(\"Hello\", model_size=\"large\")\n</code></pre>"},{"location":"faq/#can-i-use-custom-models","title":"Can I use custom models?","text":"<p>Currently, SteadyText uses pre-selected models for consistency. Custom model support is planned for future releases.</p>"},{"location":"faq/#how-much-disk-space-do-models-use","title":"How much disk space do models use?","text":"<ul> <li>Small generation model: ~1.3GB</li> <li>Large generation model: ~2.1GB  </li> <li>Embedding model: ~0.6GB</li> <li>Total (all models): ~4GB</li> </ul>"},{"location":"faq/#where-are-models-stored","title":"Where are models stored?","text":"<p>Models are cached in platform-specific directories:</p> <pre><code># Linux/Mac\n~/.cache/steadytext/models/\n\n# Windows\n%LOCALAPPDATA%\\steadytext\\steadytext\\models\\\n\n# Check location\nfrom steadytext.utils import get_model_cache_dir\nprint(get_model_cache_dir())\n</code></pre>"},{"location":"faq/#caching-questions","title":"Caching Questions","text":""},{"location":"faq/#how-does-caching-work","title":"How does caching work?","text":"<p>SteadyText uses a frecency cache (frequency + recency):</p> <pre><code># First call: generates and caches\nresult1 = steadytext.generate(\"Hello\", seed=42)  # Slow\n\n# Second call: returns from cache\nresult2 = steadytext.generate(\"Hello\", seed=42)  # Instant\n\n# Different seed: new generation\nresult3 = steadytext.generate(\"Hello\", seed=123)  # Slow\n</code></pre>"},{"location":"faq/#can-i-disable-caching","title":"Can I disable caching?","text":"<pre><code># Disable via environment variable\nexport STEADYTEXT_DISABLE_CACHE=1\n\n# Or in Python\nimport os\nos.environ['STEADYTEXT_DISABLE_CACHE'] = '1'\n</code></pre>"},{"location":"faq/#how-do-i-clear-the-cache","title":"How do I clear the cache?","text":"<pre><code># CLI\nst cache --clear\n\n# Python\nfrom steadytext import get_cache_manager\ncache_manager = get_cache_manager()\ncache_manager.clear_all_caches()\n</code></pre>"},{"location":"faq/#how-much-cache-space-is-used","title":"How much cache space is used?","text":"<pre><code># Check cache statistics\nfrom steadytext import get_cache_manager\nstats = get_cache_manager().get_cache_stats()\nprint(f\"Generation cache: {stats['generation']['size']} entries\")\nprint(f\"Embedding cache: {stats['embedding']['size']} entries\")\n</code></pre>"},{"location":"faq/#daemon-questions","title":"Daemon Questions","text":""},{"location":"faq/#what-is-daemon-mode","title":"What is daemon mode?","text":"<p>The daemon is a background service that keeps models loaded in memory, providing 160x faster first responses.</p>"},{"location":"faq/#how-do-i-start-the-daemon","title":"How do I start the daemon?","text":"<pre><code># Start in background\nst daemon start\n\n# Start in foreground (see logs)\nst daemon start --foreground\n\n# Check status\nst daemon status\n</code></pre>"},{"location":"faq/#is-the-daemon-used-automatically","title":"Is the daemon used automatically?","text":"<p>Yes! When the daemon is running, all SteadyText operations automatically use it:</p> <pre><code># Automatically uses daemon if available\ntext = steadytext.generate(\"Hello\")\n</code></pre>"},{"location":"faq/#how-do-i-stop-the-daemon","title":"How do I stop the daemon?","text":"<pre><code># Graceful stop\nst daemon stop\n\n# Force stop\nst daemon stop --force\n</code></pre>"},{"location":"faq/#can-i-run-multiple-daemons","title":"Can I run multiple daemons?","text":"<p>Currently, only one daemon instance is supported per machine. Multi-daemon support is planned for future releases.</p>"},{"location":"faq/#postgresql-extension","title":"PostgreSQL Extension","text":""},{"location":"faq/#how-do-i-install-pg_steadytext","title":"How do I install pg_steadytext?","text":"<pre><code># Using Docker (recommended)\ncd pg_steadytext\ndocker build -t pg_steadytext .\ndocker run -d -p 5432:5432 pg_steadytext\n\n# Manual installation\ncd pg_steadytext\n./install.sh\n</code></pre>"},{"location":"faq/#how-do-i-use-it-in-sql","title":"How do I use it in SQL?","text":"<pre><code>-- Enable extension\nCREATE EXTENSION pg_steadytext;\n\n-- Generate text\nSELECT steadytext_generate('Write a SQL tutorial');\n\n-- Create embeddings\nSELECT steadytext_embed('PostgreSQL database');\n</code></pre>"},{"location":"faq/#is-it-production-ready","title":"Is it production-ready?","text":"<p>The PostgreSQL extension is currently experimental. Use with caution in production environments.</p>"},{"location":"faq/#troubleshooting","title":"Troubleshooting","text":""},{"location":"faq/#model-not-found-error","title":"\"Model not found\" error","text":"<pre><code># Download models manually\nst models download --all\n\n# Or set environment variable\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n</code></pre>"},{"location":"faq/#none-returned-instead-of-text","title":"\"None\" returned instead of text","text":"<p>This is the expected behavior in v2.1.0+ when models can't be loaded:</p> <pre><code># Check if generation succeeded\nresult = steadytext.generate(\"Hello\")\nif result is None:\n    print(\"Model not available\")\nelse:\n    print(f\"Generated: {result}\")\n</code></pre>"},{"location":"faq/#daemon-wont-start","title":"Daemon won't start","text":"<pre><code># Check if port is in use\nlsof -i :5557\n\n# Try different port\nst daemon start --port 5558\n\n# Check logs\nst daemon start --foreground\n</code></pre>"},{"location":"faq/#high-memory-usage","title":"High memory usage","text":"<pre><code># Use smaller model\nst generate \"Hello\" --size small\n\n# Limit cache size\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100\n</code></pre>"},{"location":"faq/#slow-generation","title":"Slow generation","text":"<pre><code># Start daemon for faster responses\nst daemon start\n\n# Check cache is working\nst cache --status\n\n# Use smaller model\nst generate \"Hello\" --size small\n</code></pre>"},{"location":"faq/#advanced-topics","title":"Advanced Topics","text":""},{"location":"faq/#how-do-i-use-steadytext-in-production","title":"How do I use SteadyText in production?","text":"<ol> <li> <p>Use daemon mode:    <pre><code># systemd service\nsudo systemctl enable steadytext\nsudo systemctl start steadytext\n</code></pre></p> </li> <li> <p>Configure caching:    <pre><code>export STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=500\n</code></pre></p> </li> <li> <p>Monitor performance:    <pre><code>from steadytext import get_cache_manager\nstats = get_cache_manager().get_cache_stats()\n# Log stats to monitoring system\n</code></pre></p> </li> </ol>"},{"location":"faq/#can-i-use-steadytext-with-async-code","title":"Can I use SteadyText with async code?","text":"<pre><code>import asyncio\nfrom concurrent.futures import ThreadPoolExecutor\n\nexecutor = ThreadPoolExecutor(max_workers=4)\n\nasync def async_generate(prompt):\n    loop = asyncio.get_event_loop()\n    return await loop.run_in_executor(\n        executor, \n        steadytext.generate, \n        prompt\n    )\n\n# Use in async function\nresult = await async_generate(\"Hello\")\n</code></pre>"},{"location":"faq/#how-do-i-handle-errors-gracefully","title":"How do I handle errors gracefully?","text":"<pre><code>def safe_generate(prompt, fallback=\"Unable to generate\"):\n    try:\n        result = steadytext.generate(prompt)\n        if result is None:\n            return fallback\n        return result\n    except Exception as e:\n        logger.error(f\"Generation failed: {e}\")\n        return fallback\n</code></pre>"},{"location":"faq/#can-i-use-steadytext-with-langchain","title":"Can I use SteadyText with langchain?","text":"<pre><code>from langchain.llms.base import LLM\n\nclass SteadyTextLLM(LLM):\n    def _call(self, prompt: str, stop=None) -&gt; str:\n        result = steadytext.generate(prompt)\n        return result if result else \"\"\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"steadytext\"\n\n# Use with langchain\nllm = SteadyTextLLM()\n</code></pre>"},{"location":"faq/#how-do-i-benchmark-performance","title":"How do I benchmark performance?","text":"<pre><code># Run built-in benchmarks\ncd benchmarks\npython run_all_benchmarks.py\n\n# Quick benchmark\npython run_all_benchmarks.py --quick\n</code></pre>"},{"location":"faq/#can-i-contribute-to-steadytext","title":"Can I contribute to SteadyText?","text":"<p>Yes! We welcome contributions:</p> <ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Run tests: <code>uv run pytest</code></li> <li>Submit a pull request</li> </ol> <p>See CONTRIBUTING.md for details.</p>"},{"location":"faq/#still-have-questions","title":"Still Have Questions?","text":"<ul> <li>GitHub Issues: Report bugs or request features</li> <li>Discussions: Join the community</li> <li>Documentation: Read the full docs</li> </ul>"},{"location":"faq/#quick-reference","title":"Quick Reference","text":""},{"location":"faq/#common-commands","title":"Common Commands","text":"<pre><code># Generation\necho \"prompt\" | st\nst generate \"prompt\" --seed 42\n\n# Embeddings\nst embed \"text\"\nst embed \"text\" --format numpy\n\n# Daemon\nst daemon start\nst daemon status\nst daemon stop\n\n# Cache\nst cache --status\nst cache --clear\n\n# Models\nst models list\nst models download --all\nst models preload\n</code></pre>"},{"location":"faq/#common-patterns","title":"Common Patterns","text":"<pre><code># Basic usage\nimport steadytext\n\n# Generate text\ntext = steadytext.generate(\"Hello world\")\n\n# Create embedding\nembedding = steadytext.embed(\"Hello world\")\n\n# With custom seed\ntext = steadytext.generate(\"Hello\", seed=123)\n\n# Streaming\nfor chunk in steadytext.generate_iter(\"Tell a story\"):\n    print(chunk, end='')\n\n# Batch processing\nprompts = [\"One\", \"Two\", \"Three\"]\nresults = [steadytext.generate(p) for p in prompts]\n</code></pre>"},{"location":"integrations/","title":"Integrations Guide","text":"<p>This guide covers integrating SteadyText with popular frameworks, tools, and platforms.</p>"},{"location":"integrations/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Web Frameworks</li> <li>FastAPI</li> <li>Flask</li> <li>Django</li> <li>Streamlit</li> <li>AI/ML Frameworks</li> <li>LangChain</li> <li>LlamaIndex</li> <li>Haystack</li> <li>Hugging Face</li> <li>Database Integrations</li> <li>PostgreSQL</li> <li>MongoDB</li> <li>Redis</li> <li>Elasticsearch</li> <li>Vector Databases</li> <li>Pinecone</li> <li>Weaviate</li> <li>Chroma</li> <li>Qdrant</li> <li>Cloud Platforms</li> <li>AWS</li> <li>Google Cloud</li> <li>Azure</li> <li>Vercel</li> <li>Data Processing</li> <li>Apache Spark</li> <li>Pandas</li> <li>Dask</li> <li>Ray</li> <li>Monitoring &amp; Observability</li> <li>Prometheus</li> <li>OpenTelemetry</li> <li>Datadog</li> <li>New Relic</li> <li>Development Tools</li> <li>Jupyter</li> <li>VS Code</li> <li>PyCharm</li> <li>Docker</li> </ul>"},{"location":"integrations/#web-frameworks","title":"Web Frameworks","text":""},{"location":"integrations/#fastapi","title":"FastAPI","text":"<p>Create high-performance APIs with SteadyText:</p> <pre><code># app.py\nfrom fastapi import FastAPI, HTTPException, BackgroundTasks\nfrom pydantic import BaseModel\nimport steadytext\nimport asyncio\nfrom typing import List, Optional\n\napp = FastAPI(title=\"SteadyText API\", version=\"1.0.0\")\n\nclass GenerateRequest(BaseModel):\n    prompt: str\n    seed: Optional[int] = 42\n    max_new_tokens: Optional[int] = 512\n    model_size: Optional[str] = \"small\"\n\nclass GenerateResponse(BaseModel):\n    text: str\n    seed: int\n    cached: bool\n    duration_ms: float\n\nclass EmbedRequest(BaseModel):\n    text: str\n    seed: Optional[int] = 42\n\nclass EmbedResponse(BaseModel):\n    embedding: List[float]\n    dimension: int\n    seed: int\n\n@app.post(\"/generate\", response_model=GenerateResponse)\nasync def generate_text(request: GenerateRequest):\n    \"\"\"Generate text using SteadyText.\"\"\"\n    import time\n    start_time = time.perf_counter()\n\n    # Run in thread pool to avoid blocking\n    loop = asyncio.get_event_loop()\n    result = await loop.run_in_executor(\n        None,\n        steadytext.generate,\n        request.prompt,\n        request.seed,\n        request.max_new_tokens\n    )\n\n    duration_ms = (time.perf_counter() - start_time) * 1000\n\n    if result is None:\n        raise HTTPException(status_code=503, detail=\"Model not available\")\n\n    return GenerateResponse(\n        text=result,\n        seed=request.seed,\n        cached=False,  # Could check cache for this\n        duration_ms=duration_ms\n    )\n\n@app.post(\"/embed\", response_model=EmbedResponse)\nasync def create_embedding(request: EmbedRequest):\n    \"\"\"Create text embedding.\"\"\"\n    loop = asyncio.get_event_loop()\n    embedding = await loop.run_in_executor(\n        None,\n        steadytext.embed,\n        request.text,\n        request.seed\n    )\n\n    if embedding is None:\n        raise HTTPException(status_code=503, detail=\"Embedding model not available\")\n\n    return EmbedResponse(\n        embedding=embedding.tolist(),\n        dimension=len(embedding),\n        seed=request.seed\n    )\n\n@app.get(\"/generate/stream\")\nasync def stream_generate(prompt: str, seed: int = 42):\n    \"\"\"Stream text generation.\"\"\"\n    from fastapi.responses import StreamingResponse\n\n    async def generate_stream():\n        for chunk in steadytext.generate_iter(prompt, seed=seed):\n            yield f\"data: {chunk}\\n\\n\"\n        yield \"data: [DONE]\\n\\n\"\n\n    return StreamingResponse(\n        generate_stream(),\n        media_type=\"text/plain\",\n        headers={\"Cache-Control\": \"no-cache\"}\n    )\n\n@app.get(\"/health\")\nasync def health_check():\n    \"\"\"Health check endpoint.\"\"\"\n    try:\n        result = steadytext.generate(\"test\", seed=42)\n        return {\n            \"status\": \"healthy\",\n            \"model_available\": result is not None\n        }\n    except Exception as e:\n        return {\n            \"status\": \"unhealthy\",\n            \"error\": str(e)\n        }\n\n# Start with: uvicorn app:app --reload\n</code></pre>"},{"location":"integrations/#flask","title":"Flask","text":"<p>Traditional web applications with SteadyText:</p> <pre><code># flask_app.py\nfrom flask import Flask, request, jsonify, render_template, stream_template\nimport steadytext\nimport json\n\napp = Flask(__name__)\n\n@app.route('/')\ndef index():\n    \"\"\"Main page with text generation form.\"\"\"\n    return render_template('index.html')\n\n@app.route('/api/generate', methods=['POST'])\ndef api_generate():\n    \"\"\"API endpoint for text generation.\"\"\"\n    data = request.get_json()\n\n    if not data or 'prompt' not in data:\n        return jsonify({'error': 'Missing prompt'}), 400\n\n    prompt = data['prompt']\n    seed = data.get('seed', 42)\n    max_tokens = data.get('max_new_tokens', 512)\n\n    result = steadytext.generate(\n        prompt,\n        seed=seed,\n        max_new_tokens=max_tokens\n    )\n\n    if result is None:\n        return jsonify({'error': 'Model not available'}), 503\n\n    return jsonify({\n        'text': result,\n        'seed': seed,\n        'prompt': prompt\n    })\n\n@app.route('/api/embed', methods=['POST'])\ndef api_embed():\n    \"\"\"API endpoint for creating embeddings.\"\"\"\n    data = request.get_json()\n\n    if not data or 'text' not in data:\n        return jsonify({'error': 'Missing text'}), 400\n\n    text = data['text']\n    seed = data.get('seed', 42)\n\n    embedding = steadytext.embed(text, seed=seed)\n\n    if embedding is None:\n        return jsonify({'error': 'Embedding model not available'}), 503\n\n    return jsonify({\n        'embedding': embedding.tolist(),\n        'dimension': len(embedding),\n        'text': text,\n        'seed': seed\n    })\n\n@app.route('/stream')\ndef stream_demo():\n    \"\"\"Demo page for streaming generation.\"\"\"\n    return render_template('stream.html')\n\n@app.route('/api/stream')\ndef api_stream():\n    \"\"\"Server-sent events for streaming.\"\"\"\n    prompt = request.args.get('prompt', 'Tell me a story')\n    seed = int(request.args.get('seed', 42))\n\n    def event_stream():\n        for chunk in steadytext.generate_iter(prompt, seed=seed):\n            yield f\"data: {json.dumps({'chunk': chunk})}\\n\\n\"\n        yield f\"data: {json.dumps({'done': True})}\\n\\n\"\n\n    return app.response_class(\n        event_stream(),\n        mimetype='text/event-stream',\n        headers={\n            'Cache-Control': 'no-cache',\n            'X-Accel-Buffering': 'no'\n        }\n    )\n\nif __name__ == '__main__':\n    app.run(debug=True)\n</code></pre>"},{"location":"integrations/#django","title":"Django","text":"<p>Enterprise web applications:</p> <pre><code># views.py\nfrom django.http import JsonResponse, StreamingHttpResponse\nfrom django.views.decorators.csrf import csrf_exempt\nfrom django.views.decorators.http import require_http_methods\nfrom django.utils.decorators import method_decorator\nfrom django.views.generic import View\nimport json\nimport steadytext\n\n@csrf_exempt\n@require_http_methods([\"POST\"])\ndef generate_view(request):\n    \"\"\"Django view for text generation.\"\"\"\n    try:\n        data = json.loads(request.body)\n        prompt = data.get('prompt')\n        seed = data.get('seed', 42)\n\n        if not prompt:\n            return JsonResponse({'error': 'Missing prompt'}, status=400)\n\n        result = steadytext.generate(prompt, seed=seed)\n\n        if result is None:\n            return JsonResponse({'error': 'Model not available'}, status=503)\n\n        return JsonResponse({\n            'text': result,\n            'seed': seed\n        })\n\n    except json.JSONDecodeError:\n        return JsonResponse({'error': 'Invalid JSON'}, status=400)\n    except Exception as e:\n        return JsonResponse({'error': str(e)}, status=500)\n\n@method_decorator(csrf_exempt, name='dispatch')\nclass EmbeddingView(View):\n    \"\"\"Class-based view for embeddings.\"\"\"\n\n    def post(self, request):\n        try:\n            data = json.loads(request.body)\n            text = data.get('text')\n            seed = data.get('seed', 42)\n\n            if not text:\n                return JsonResponse({'error': 'Missing text'}, status=400)\n\n            embedding = steadytext.embed(text, seed=seed)\n\n            if embedding is None:\n                return JsonResponse({'error': 'Model not available'}, status=503)\n\n            return JsonResponse({\n                'embedding': embedding.tolist(),\n                'dimension': len(embedding)\n            })\n\n        except Exception as e:\n            return JsonResponse({'error': str(e)}, status=500)\n\n# models.py\nfrom django.db import models\nimport numpy as np\n\nclass Document(models.Model):\n    \"\"\"Document model with embedding support.\"\"\"\n    title = models.CharField(max_length=200)\n    content = models.TextField()\n    embedding = models.JSONField(null=True, blank=True)\n    created_at = models.DateTimeField(auto_now_add=True)\n\n    def save(self, *args, **kwargs):\n        # Auto-generate embedding\n        if self.content and not self.embedding:\n            emb = steadytext.embed(self.content)\n            if emb is not None:\n                self.embedding = emb.tolist()\n        super().save(*args, **kwargs)\n\n    def similarity(self, other_doc):\n        \"\"\"Calculate cosine similarity with another document.\"\"\"\n        if not self.embedding or not other_doc.embedding:\n            return 0.0\n\n        emb1 = np.array(self.embedding)\n        emb2 = np.array(other_doc.embedding)\n\n        return np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n\n# urls.py\nfrom django.urls import path\nfrom . import views\n\nurlpatterns = [\n    path('api/generate/', views.generate_view, name='generate'),\n    path('api/embed/', views.EmbeddingView.as_view(), name='embed'),\n]\n</code></pre>"},{"location":"integrations/#streamlit","title":"Streamlit","text":"<p>Interactive data science applications:</p> <pre><code># streamlit_app.py\nimport streamlit as st\nimport steadytext\nimport numpy as np\nimport plotly.express as px\nimport pandas as pd\n\nst.set_page_config(\n    page_title=\"SteadyText Demo\",\n    page_icon=\"\ud83e\udd16\",\n    layout=\"wide\"\n)\n\nst.title(\"\ud83e\udd16 SteadyText Interactive Demo\")\n\n# Sidebar\nst.sidebar.header(\"Configuration\")\nseed = st.sidebar.number_input(\"Random Seed\", value=42, min_value=0)\nmax_tokens = st.sidebar.slider(\"Max Tokens\", 50, 1000, 512)\nmodel_size = st.sidebar.selectbox(\"Model Size\", [\"small\", \"large\"])\n\n# Text Generation Tab\ntab1, tab2, tab3 = st.tabs([\"Generate\", \"Embed\", \"Compare\"])\n\nwith tab1:\n    st.header(\"Text Generation\")\n\n    prompt = st.text_area(\n        \"Enter your prompt:\",\n        value=\"Write a short story about AI\",\n        height=100\n    )\n\n    col1, col2 = st.columns([1, 3])\n\n    with col1:\n        if st.button(\"Generate\", type=\"primary\"):\n            with st.spinner(\"Generating...\"):\n                result = steadytext.generate(\n                    prompt,\n                    seed=seed,\n                    max_new_tokens=max_tokens\n                )\n\n                if result:\n                    st.session_state.generated_text = result\n                else:\n                    st.error(\"Model not available\")\n\n    with col2:\n        if 'generated_text' in st.session_state:\n            st.text_area(\n                \"Generated Text:\",\n                value=st.session_state.generated_text,\n                height=300\n            )\n\nwith tab2:\n    st.header(\"Text Embeddings\")\n\n    text_input = st.text_input(\n        \"Enter text to embed:\",\n        value=\"Machine learning is fascinating\"\n    )\n\n    if st.button(\"Create Embedding\"):\n        with st.spinner(\"Creating embedding...\"):\n            embedding = steadytext.embed(text_input, seed=seed)\n\n            if embedding is not None:\n                st.success(f\"Created {len(embedding)}-dimensional embedding\")\n\n                # Visualize embedding (first 50 dimensions)\n                fig = px.bar(\n                    x=list(range(50)),\n                    y=embedding[:50],\n                    title=\"Embedding Values (First 50 Dimensions)\"\n                )\n                st.plotly_chart(fig, use_container_width=True)\n\n                # Show statistics\n                col1, col2, col3 = st.columns(3)\n                with col1:\n                    st.metric(\"Mean\", f\"{np.mean(embedding):.4f}\")\n                with col2:\n                    st.metric(\"Std Dev\", f\"{np.std(embedding):.4f}\")\n                with col3:\n                    st.metric(\"L2 Norm\", f\"{np.linalg.norm(embedding):.4f}\")\n            else:\n                st.error(\"Embedding model not available\")\n\nwith tab3:\n    st.header(\"Compare Outputs\")\n\n    st.subheader(\"Seed Comparison\")\n\n    test_prompt = st.text_input(\n        \"Test prompt:\",\n        value=\"Explain quantum computing\"\n    )\n\n    seeds = st.multiselect(\n        \"Seeds to compare:\",\n        options=[42, 123, 456, 789],\n        default=[42, 123]\n    )\n\n    if st.button(\"Compare Seeds\") and seeds:\n        results = {}\n\n        for s in seeds:\n            with st.spinner(f\"Generating with seed {s}...\"):\n                result = steadytext.generate(test_prompt, seed=s)\n                if result:\n                    results[s] = result\n\n        # Display results\n        for seed_val, text in results.items():\n            st.subheader(f\"Seed {seed_val}\")\n            st.text_area(f\"Result for seed {seed_val}\", value=text, height=150)\n\n        # Check determinism\n        if len(set(results.values())) == 1:\n            st.success(\"\u2705 All outputs are identical (deterministic)\")\n        else:\n            st.info(\"\u2139\ufe0f Different seeds produce different outputs\")\n\n# Cache status\nif st.sidebar.button(\"Check Cache Status\"):\n    try:\n        from steadytext import get_cache_manager\n        cache_manager = get_cache_manager()\n        stats = cache_manager.get_cache_stats()\n\n        st.sidebar.json(stats)\n    except Exception as e:\n        st.sidebar.error(f\"Error getting cache stats: {e}\")\n\n# Run with: streamlit run streamlit_app.py\n</code></pre>"},{"location":"integrations/#aiml-frameworks","title":"AI/ML Frameworks","text":""},{"location":"integrations/#langchain","title":"LangChain","text":"<p>Integrate SteadyText with LangChain:</p> <pre><code># langchain_integration.py\nfrom langchain.llms.base import LLM\nfrom langchain.embeddings.base import Embeddings\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\nfrom langchain.vectorstores import FAISS\nfrom langchain.document_loaders import TextLoader\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom typing import List, Optional, Any\nimport steadytext\nimport numpy as np\n\nclass SteadyTextLLM(LLM):\n    \"\"\"SteadyText LLM wrapper for LangChain.\"\"\"\n\n    seed: int = 42\n    max_new_tokens: int = 512\n    model_size: str = \"small\"\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:\n        \"\"\"Call SteadyText generate.\"\"\"\n        result = steadytext.generate(\n            prompt,\n            seed=self.seed,\n            max_new_tokens=self.max_new_tokens\n        )\n        return result if result else \"\"\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"steadytext\"\n\n    @property\n    def _identifying_params(self) -&gt; dict:\n        return {\n            \"seed\": self.seed,\n            \"max_new_tokens\": self.max_new_tokens,\n            \"model_size\": self.model_size\n        }\n\nclass SteadyTextEmbeddings(Embeddings):\n    \"\"\"SteadyText embeddings wrapper for LangChain.\"\"\"\n\n    seed: int = 42\n\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Embed a list of documents.\"\"\"\n        embeddings = []\n        for text in texts:\n            emb = steadytext.embed(text, seed=self.seed)\n            if emb is not None:\n                embeddings.append(emb.tolist())\n            else:\n                # Fallback to zero vector\n                embeddings.append([0.0] * 1024)\n        return embeddings\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        \"\"\"Embed a single query.\"\"\"\n        emb = steadytext.embed(text, seed=self.seed)\n        return emb.tolist() if emb is not None else [0.0] * 1024\n\n# Example usage\ndef create_qa_system(documents_path: str):\n    \"\"\"Create a Q&amp;A system using SteadyText with LangChain.\"\"\"\n\n    # Load and split documents\n    loader = TextLoader(documents_path)\n    documents = loader.load()\n\n    text_splitter = RecursiveCharacterTextSplitter(\n        chunk_size=1000,\n        chunk_overlap=200\n    )\n    texts = text_splitter.split_documents(documents)\n\n    # Create embeddings and vector store\n    embeddings = SteadyTextEmbeddings(seed=42)\n    vectorstore = FAISS.from_documents(texts, embeddings)\n\n    # Create LLM and chain\n    llm = SteadyTextLLM(seed=42, max_new_tokens=300)\n\n    template = \"\"\"\n    Context: {context}\n\n    Question: {question}\n\n    Answer based on the context above:\n    \"\"\"\n\n    prompt = PromptTemplate(\n        template=template,\n        input_variables=[\"context\", \"question\"]\n    )\n\n    chain = LLMChain(llm=llm, prompt=prompt)\n\n    def ask_question(question: str) -&gt; str:\n        \"\"\"Ask a question about the documents.\"\"\"\n        # Retrieve relevant documents\n        docs = vectorstore.similarity_search(question, k=3)\n        context = \"\\n\".join([doc.page_content for doc in docs])\n\n        # Generate answer\n        answer = chain.run(context=context, question=question)\n        return answer\n\n    return ask_question\n\n# Example usage\nqa_system = create_qa_system(\"documents.txt\")\nanswer = qa_system(\"What is machine learning?\")\nprint(answer)\n</code></pre>"},{"location":"integrations/#llamaindex","title":"LlamaIndex","text":"<p>Document indexing and retrieval:</p> <pre><code># llamaindex_integration.py\nfrom llama_index.core import VectorStoreIndex, Document, Settings\nfrom llama_index.core.llms import LLM\nfrom llama_index.core.embeddings import BaseEmbedding\nfrom llama_index.core.node_parser import SentenceSplitter\nfrom typing import List, Any\nimport steadytext\n\nclass SteadyTextLLM(LLM):\n    \"\"\"SteadyText LLM for LlamaIndex.\"\"\"\n\n    def __init__(self, seed: int = 42, max_tokens: int = 512):\n        super().__init__()\n        self.seed = seed\n        self.max_tokens = max_tokens\n\n    @property\n    def metadata(self) -&gt; dict:\n        return {\"seed\": self.seed, \"max_tokens\": self.max_tokens}\n\n    def complete(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Complete a prompt.\"\"\"\n        result = steadytext.generate(\n            prompt,\n            seed=self.seed,\n            max_new_tokens=self.max_tokens\n        )\n        return result if result else \"\"\n\n    def stream_complete(self, prompt: str, **kwargs):\n        \"\"\"Stream completion (generator).\"\"\"\n        for chunk in steadytext.generate_iter(prompt, seed=self.seed):\n            yield chunk\n\nclass SteadyTextEmbedding(BaseEmbedding):\n    \"\"\"SteadyText embeddings for LlamaIndex.\"\"\"\n\n    def __init__(self, seed: int = 42):\n        super().__init__()\n        self.seed = seed\n\n    def _get_query_embedding(self, query: str) -&gt; List[float]:\n        \"\"\"Get embedding for query.\"\"\"\n        emb = steadytext.embed(query, seed=self.seed)\n        return emb.tolist() if emb is not None else [0.0] * 1024\n\n    def _get_text_embedding(self, text: str) -&gt; List[float]:\n        \"\"\"Get embedding for text.\"\"\"\n        return self._get_query_embedding(text)\n\n    def _get_text_embeddings(self, texts: List[str]) -&gt; List[List[float]]:\n        \"\"\"Get embeddings for multiple texts.\"\"\"\n        return [self._get_text_embedding(text) for text in texts]\n\n# Setup LlamaIndex with SteadyText\nSettings.llm = SteadyTextLLM(seed=42)\nSettings.embed_model = SteadyTextEmbedding(seed=42)\n\ndef create_index_from_documents(documents: List[str]) -&gt; VectorStoreIndex:\n    \"\"\"Create a vector index from documents.\"\"\"\n\n    # Convert to Document objects\n    docs = [Document(text=doc) for doc in documents]\n\n    # Create index\n    index = VectorStoreIndex.from_documents(\n        docs,\n        node_parser=SentenceSplitter(chunk_size=512, chunk_overlap=50)\n    )\n\n    return index\n\n# Example usage\ndocuments = [\n    \"Machine learning is a subset of artificial intelligence...\",\n    \"Deep learning uses neural networks with multiple layers...\",\n    \"Natural language processing deals with text analysis...\"\n]\n\nindex = create_index_from_documents(documents)\nquery_engine = index.as_query_engine()\n\nresponse = query_engine.query(\"What is machine learning?\")\nprint(response)\n</code></pre>"},{"location":"integrations/#haystack","title":"Haystack","text":"<p>Enterprise search and NLP:</p> <pre><code># haystack_integration.py\nfrom haystack import Document, Pipeline\nfrom haystack.components.generators import OpenAIGenerator\nfrom haystack.components.embedders import SentenceTransformersTextEmbedder\nfrom haystack.components.retrievers.in_memory import InMemoryBM25Retriever\nfrom haystack.document_stores.in_memory import InMemoryDocumentStore\nfrom typing import List, Dict, Any\nimport steadytext\n\nclass SteadyTextGenerator:\n    \"\"\"SteadyText generator component for Haystack.\"\"\"\n\n    def __init__(self, seed: int = 42, max_tokens: int = 512):\n        self.seed = seed\n        self.max_tokens = max_tokens\n\n    def run(self, prompt: str, **kwargs) -&gt; Dict[str, Any]:\n        \"\"\"Generate text using SteadyText.\"\"\"\n        result = steadytext.generate(\n            prompt,\n            seed=self.seed,\n            max_new_tokens=self.max_tokens\n        )\n\n        return {\n            \"replies\": [result] if result else [\"Model not available\"]\n        }\n\nclass SteadyTextEmbedder:\n    \"\"\"SteadyText embedder component for Haystack.\"\"\"\n\n    def __init__(self, seed: int = 42):\n        self.seed = seed\n\n    def run(self, text: str) -&gt; Dict[str, Any]:\n        \"\"\"Create embedding using SteadyText.\"\"\"\n        embedding = steadytext.embed(text, seed=self.seed)\n\n        return {\n            \"embedding\": embedding.tolist() if embedding is not None else [0.0] * 1024\n        }\n\ndef create_rag_pipeline(documents: List[str]) -&gt; Pipeline:\n    \"\"\"Create a RAG pipeline using SteadyText.\"\"\"\n\n    # Create document store\n    document_store = InMemoryDocumentStore()\n\n    # Add documents\n    docs = [Document(content=doc, id=str(i)) for i, doc in enumerate(documents)]\n    document_store.write_documents(docs)\n\n    # Create components\n    retriever = InMemoryBM25Retriever(document_store=document_store)\n    generator = SteadyTextGenerator(seed=42)\n\n    # Create pipeline\n    pipeline = Pipeline()\n    pipeline.add_component(\"retriever\", retriever)\n    pipeline.add_component(\"generator\", generator)\n\n    # Connect components\n    pipeline.connect(\"retriever.documents\", \"generator.documents\")\n\n    return pipeline\n\n# Example usage\ndocs = [\n    \"SteadyText is a deterministic AI library for Python.\",\n    \"It provides text generation and embedding capabilities.\",\n    \"The library ensures reproducible results across runs.\"\n]\n\npipeline = create_rag_pipeline(docs)\n\n# Run query\nresult = pipeline.run({\n    \"retriever\": {\"query\": \"What is SteadyText?\"},\n    \"generator\": {\"prompt\": \"Based on the documents, what is SteadyText?\"}\n})\n\nprint(result[\"generator\"][\"replies\"][0])\n</code></pre>"},{"location":"integrations/#database-integrations","title":"Database Integrations","text":""},{"location":"integrations/#postgresql","title":"PostgreSQL","text":"<p>Native PostgreSQL integration with pg_steadytext:</p> <pre><code>-- Setup\nCREATE EXTENSION IF NOT EXISTS vector;\nCREATE EXTENSION IF NOT EXISTS pg_steadytext;\n\n-- Create a table with AI capabilities\nCREATE TABLE articles (\n    id SERIAL PRIMARY KEY,\n    title TEXT NOT NULL,\n    content TEXT NOT NULL,\n    summary TEXT,\n    embedding VECTOR(1024),\n    keywords TEXT[],\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n);\n\n-- Trigger to auto-generate AI content\nCREATE OR REPLACE FUNCTION update_ai_fields()\nRETURNS TRIGGER AS $$\nBEGIN\n    -- Generate summary\n    NEW.summary := steadytext_generate(\n        'Summarize this article in 2-3 sentences: ' || NEW.content,\n        max_tokens := 150,\n        seed := 42\n    );\n\n    -- Generate embedding\n    NEW.embedding := steadytext_embed(NEW.title || ' ' || NEW.content, seed := 42);\n\n    -- Extract keywords\n    NEW.keywords := string_to_array(\n        steadytext_generate(\n            'Extract 5 keywords from this text: ' || NEW.content,\n            max_tokens := 50,\n            seed := 123\n        ),\n        ','\n    );\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\nCREATE TRIGGER ai_content_trigger\n    BEFORE INSERT OR UPDATE ON articles\n    FOR EACH ROW\n    EXECUTE FUNCTION update_ai_fields();\n\n-- Semantic search function\nCREATE OR REPLACE FUNCTION semantic_search(\n    query_text TEXT,\n    limit_count INT DEFAULT 10\n)\nRETURNS TABLE(\n    article_id INT,\n    title TEXT,\n    summary TEXT,\n    similarity FLOAT\n) AS $$\nDECLARE\n    query_embedding VECTOR(1024);\nBEGIN\n    -- Generate embedding for search query\n    query_embedding := steadytext_embed(query_text, seed := 42);\n\n    -- Return similar articles\n    RETURN QUERY\n    SELECT \n        a.id,\n        a.title,\n        a.summary,\n        1 - (a.embedding &lt;=&gt; query_embedding) AS similarity\n    FROM articles a\n    WHERE a.embedding IS NOT NULL\n    ORDER BY a.embedding &lt;=&gt; query_embedding\n    LIMIT limit_count;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage examples\nINSERT INTO articles (title, content) VALUES \n('AI Revolution', 'Artificial intelligence is transforming industries...');\n\nSELECT * FROM semantic_search('machine learning trends');\n</code></pre>"},{"location":"integrations/#mongodb","title":"MongoDB","text":"<p>Document database with AI capabilities:</p> <pre><code># mongodb_integration.py\nimport pymongo\nimport steadytext\nimport numpy as np\nfrom typing import List, Dict, Any, Optional\n\nclass SteadyTextMongoDB:\n    \"\"\"MongoDB integration with SteadyText.\"\"\"\n\n    def __init__(self, connection_string: str, database: str):\n        self.client = pymongo.MongoClient(connection_string)\n        self.db = self.client[database]\n\n        # Create text index for search\n        self.db.documents.create_index([(\"title\", \"text\"), (\"content\", \"text\")])\n\n        # Create vector index (MongoDB Atlas Vector Search)\n        try:\n            self.db.documents.create_index([(\"embedding\", \"2dsphere\")])\n        except Exception:\n            pass  # Vector indexing not available in all MongoDB versions\n\n    def insert_document(self, \n                       title: str, \n                       content: str, \n                       generate_summary: bool = True,\n                       generate_embedding: bool = True,\n                       seed: int = 42) -&gt; str:\n        \"\"\"Insert document with AI-generated fields.\"\"\"\n\n        doc = {\n            \"title\": title,\n            \"content\": content,\n            \"created_at\": datetime.utcnow()\n        }\n\n        if generate_summary:\n            summary = steadytext.generate(\n                f\"Summarize this document: {content}\",\n                seed=seed,\n                max_new_tokens=150\n            )\n            if summary:\n                doc[\"summary\"] = summary\n\n        if generate_embedding:\n            embedding = steadytext.embed(f\"{title} {content}\", seed=seed)\n            if embedding is not None:\n                doc[\"embedding\"] = embedding.tolist()\n\n        result = self.db.documents.insert_one(doc)\n        return str(result.inserted_id)\n\n    def semantic_search(self, \n                       query: str, \n                       limit: int = 10,\n                       seed: int = 42) -&gt; List[Dict]:\n        \"\"\"Perform semantic search using embeddings.\"\"\"\n\n        # Generate query embedding\n        query_embedding = steadytext.embed(query, seed=seed)\n        if query_embedding is None:\n            return []\n\n        # Find documents (using cosine similarity approximation)\n        pipeline = [\n            {\n                \"$addFields\": {\n                    \"similarity\": {\n                        \"$let\": {\n                            \"vars\": {\n                                \"query_emb\": query_embedding.tolist()\n                            },\n                            \"in\": {\n                                \"$cond\": {\n                                    \"if\": {\"$ne\": [\"$embedding\", None]},\n                                    \"then\": {\n                                        \"$divide\": [\n                                            {\"$reduce\": {\n                                                \"input\": {\"$zip\": {\"inputs\": [\"$embedding\", \"$$query_emb\"]}},\n                                                \"initialValue\": 0,\n                                                \"in\": {\"$add\": [\"$$value\", {\"$multiply\": [{\"$arrayElemAt\": [\"$$this\", 0]}, {\"$arrayElemAt\": [\"$$this\", 1]}]}]}\n                                            }},\n                                            {\"$multiply\": [\n                                                {\"$sqrt\": {\"$reduce\": {\n                                                    \"input\": \"$embedding\",\n                                                    \"initialValue\": 0,\n                                                    \"in\": {\"$add\": [\"$$value\", {\"$multiply\": [\"$$this\", \"$$this\"]}]}\n                                                }}},\n                                                {\"$sqrt\": {\"$reduce\": {\n                                                    \"input\": \"$$query_emb\",\n                                                    \"initialValue\": 0,\n                                                    \"in\": {\"$add\": [\"$$value\", {\"$multiply\": [\"$$this\", \"$$this\"]}]}\n                                                }}}\n                                            ]}\n                                        ]\n                                    },\n                                    \"else\": 0\n                                }\n                            }\n                        }\n                    }\n                }\n            },\n            {\"$match\": {\"similarity\": {\"$gt\": 0}}},\n            {\"$sort\": {\"similarity\": -1}},\n            {\"$limit\": limit},\n            {\"$project\": {\"embedding\": 0}}  # Don't return embeddings\n        ]\n\n        return list(self.db.documents.aggregate(pipeline))\n\n    def generate_related_content(self, \n                                document_id: str, \n                                content_type: str = \"summary\",\n                                seed: int = 42) -&gt; Optional[str]:\n        \"\"\"Generate related content for a document.\"\"\"\n\n        doc = self.db.documents.find_one({\"_id\": ObjectId(document_id)})\n        if not doc:\n            return None\n\n        prompts = {\n            \"summary\": f\"Summarize this document: {doc['content']}\",\n            \"keywords\": f\"Extract keywords from: {doc['content']}\",\n            \"questions\": f\"Generate 3 questions about: {doc['content']}\",\n            \"continuation\": f\"Continue this text: {doc['content']}\"\n        }\n\n        prompt = prompts.get(content_type, prompts[\"summary\"])\n        result = steadytext.generate(prompt, seed=seed)\n\n        if result:\n            # Update document with generated content\n            self.db.documents.update_one(\n                {\"_id\": ObjectId(document_id)},\n                {\"$set\": {f\"generated_{content_type}\": result}}\n            )\n\n        return result\n\n# Example usage\ndb = SteadyTextMongoDB(\"mongodb://localhost:27017\", \"ai_docs\")\n\n# Insert document\ndoc_id = db.insert_document(\n    \"Machine Learning Basics\",\n    \"Machine learning is a subset of artificial intelligence...\"\n)\n\n# Search documents\nresults = db.semantic_search(\"artificial intelligence\")\nfor result in results:\n    print(f\"Title: {result['title']}\")\n    print(f\"Similarity: {result['similarity']:.3f}\")\n</code></pre>"},{"location":"integrations/#redis","title":"Redis","text":"<p>Caching and real-time AI:</p> <pre><code># redis_integration.py\nimport redis\nimport json\nimport hashlib\nimport steadytext\nimport numpy as np\nfrom typing import Optional, List, Dict, Any\n\nclass SteadyTextRedis:\n    \"\"\"Redis integration for caching and real-time AI.\"\"\"\n\n    def __init__(self, redis_url: str = \"redis://localhost:6379\"):\n        self.redis = redis.from_url(redis_url, decode_responses=True)\n\n        # Set up Lua scripts for atomic operations\n        self.cache_script = self.redis.register_script(\"\"\"\n            local key = KEYS[1]\n            local value = ARGV[1]\n            local ttl = ARGV[2]\n\n            redis.call('SET', key, value)\n            redis.call('EXPIRE', key, ttl)\n            redis.call('INCR', key .. ':hits')\n\n            return 'OK'\n        \"\"\")\n\n    def _generate_cache_key(self, prompt: str, seed: int, **kwargs) -&gt; str:\n        \"\"\"Generate cache key for prompt.\"\"\"\n        key_data = f\"{prompt}:{seed}:{json.dumps(kwargs, sort_keys=True)}\"\n        return f\"steadytext:gen:{hashlib.md5(key_data.encode()).hexdigest()}\"\n\n    def _embedding_cache_key(self, text: str, seed: int) -&gt; str:\n        \"\"\"Generate cache key for embedding.\"\"\"\n        key_data = f\"{text}:{seed}\"\n        return f\"steadytext:emb:{hashlib.md5(key_data.encode()).hexdigest()}\"\n\n    def cached_generate(self, \n                       prompt: str, \n                       seed: int = 42,\n                       ttl: int = 3600,\n                       **kwargs) -&gt; Optional[str]:\n        \"\"\"Generate text with Redis caching.\"\"\"\n\n        cache_key = self._generate_cache_key(prompt, seed, **kwargs)\n\n        # Try cache first\n        cached = self.redis.get(cache_key)\n        if cached:\n            # Update hit counter\n            self.redis.incr(f\"{cache_key}:hits\")\n            return json.loads(cached)[\"text\"]\n\n        # Generate new\n        result = steadytext.generate(prompt, seed=seed, **kwargs)\n        if result:\n            # Cache result\n            cache_data = {\n                \"text\": result,\n                \"prompt\": prompt,\n                \"seed\": seed,\n                \"timestamp\": time.time()\n            }\n            self.cache_script(\n                keys=[cache_key],\n                args=[json.dumps(cache_data), ttl]\n            )\n\n        return result\n\n    def cached_embed(self, \n                    text: str, \n                    seed: int = 42,\n                    ttl: int = 3600) -&gt; Optional[np.ndarray]:\n        \"\"\"Create embedding with Redis caching.\"\"\"\n\n        cache_key = self._embedding_cache_key(text, seed)\n\n        # Try cache first\n        cached = self.redis.get(cache_key)\n        if cached:\n            self.redis.incr(f\"{cache_key}:hits\")\n            return np.array(json.loads(cached)[\"embedding\"])\n\n        # Generate new\n        embedding = steadytext.embed(text, seed=seed)\n        if embedding is not None:\n            # Cache result\n            cache_data = {\n                \"embedding\": embedding.tolist(),\n                \"text\": text,\n                \"seed\": seed,\n                \"timestamp\": time.time()\n            }\n            self.cache_script(\n                keys=[cache_key],\n                args=[json.dumps(cache_data), ttl]\n            )\n\n        return embedding\n\n    def batch_generate(self, \n                      prompts: List[str], \n                      seed: int = 42,\n                      **kwargs) -&gt; List[Optional[str]]:\n        \"\"\"Batch generate with Redis pipeline.\"\"\"\n\n        # Check cache for all prompts\n        cache_keys = [self._generate_cache_key(p, seed, **kwargs) for p in prompts]\n\n        pipe = self.redis.pipeline()\n        for key in cache_keys:\n            pipe.get(key)\n        cached_results = pipe.execute()\n\n        results = []\n        to_generate = []\n        indices_to_generate = []\n\n        for i, (prompt, cached) in enumerate(zip(prompts, cached_results)):\n            if cached:\n                results.append(json.loads(cached)[\"text\"])\n                # Update hit counter\n                self.redis.incr(f\"{cache_keys[i]}:hits\")\n            else:\n                results.append(None)\n                to_generate.append(prompt)\n                indices_to_generate.append(i)\n\n        # Generate missing results\n        if to_generate:\n            for prompt, idx in zip(to_generate, indices_to_generate):\n                result = steadytext.generate(prompt, seed=seed, **kwargs)\n                results[idx] = result\n\n                if result:\n                    # Cache result\n                    cache_data = {\n                        \"text\": result,\n                        \"prompt\": prompt,\n                        \"seed\": seed,\n                        \"timestamp\": time.time()\n                    }\n                    self.redis.setex(\n                        cache_keys[idx],\n                        3600,\n                        json.dumps(cache_data)\n                    )\n\n        return results\n\n    def similarity_search(self, \n                         query: str, \n                         collection: str = \"docs\",\n                         top_k: int = 5,\n                         seed: int = 42) -&gt; List[Dict]:\n        \"\"\"Perform similarity search using Redis.\"\"\"\n\n        # Generate query embedding\n        query_embedding = self.cached_embed(query, seed=seed)\n        if query_embedding is None:\n            return []\n\n        # Get all document embeddings\n        doc_keys = self.redis.keys(f\"docs:{collection}:*\")\n\n        similarities = []\n        for key in doc_keys:\n            doc_data = self.redis.hgetall(key)\n            if 'embedding' in doc_data:\n                doc_embedding = np.array(json.loads(doc_data['embedding']))\n\n                # Calculate cosine similarity\n                similarity = np.dot(query_embedding, doc_embedding)\n                similarities.append({\n                    'doc_id': key.split(':')[-1],\n                    'title': doc_data.get('title', ''),\n                    'content': doc_data.get('content', ''),\n                    'similarity': float(similarity)\n                })\n\n        # Sort by similarity and return top_k\n        similarities.sort(key=lambda x: x['similarity'], reverse=True)\n        return similarities[:top_k]\n\n    def store_document(self, \n                      doc_id: str,\n                      title: str,\n                      content: str,\n                      collection: str = \"docs\",\n                      seed: int = 42) -&gt; bool:\n        \"\"\"Store document with embedding in Redis.\"\"\"\n\n        # Generate embedding\n        text = f\"{title} {content}\"\n        embedding = self.cached_embed(text, seed=seed)\n        if embedding is None:\n            return False\n\n        # Store document\n        key = f\"docs:{collection}:{doc_id}\"\n        self.redis.hset(key, mapping={\n            'title': title,\n            'content': content,\n            'embedding': json.dumps(embedding.tolist()),\n            'seed': seed,\n            'timestamp': time.time()\n        })\n\n        return True\n\n    def get_cache_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get cache statistics.\"\"\"\n\n        # Count cache entries\n        gen_keys = len(self.redis.keys(\"steadytext:gen:*\"))\n        emb_keys = len(self.redis.keys(\"steadytext:emb:*\"))\n\n        # Get hit counts\n        hit_keys = self.redis.keys(\"steadytext:*:hits\")\n        total_hits = sum(int(self.redis.get(key) or 0) for key in hit_keys)\n\n        return {\n            'generation_cache_entries': gen_keys,\n            'embedding_cache_entries': emb_keys,\n            'total_hits': total_hits,\n            'redis_memory': self.redis.info()['used_memory_human']\n        }\n\n# Example usage\nredis_ai = SteadyTextRedis()\n\n# Cached generation\ntext = redis_ai.cached_generate(\"Write about AI\", seed=42)\n\n# Store and search documents\nredis_ai.store_document(\"doc1\", \"AI Basics\", \"Artificial intelligence is...\", seed=42)\nresults = redis_ai.similarity_search(\"machine learning\", top_k=3)\n\n# Batch processing\nprompts = [\"AI topic 1\", \"AI topic 2\", \"AI topic 3\"]\nresults = redis_ai.batch_generate(prompts, seed=42)\n\n# Check performance\nstats = redis_ai.get_cache_stats()\nprint(f\"Cache entries: {stats['generation_cache_entries']}\")\nprint(f\"Total hits: {stats['total_hits']}\")\n</code></pre>"},{"location":"integrations/#vector-databases","title":"Vector Databases","text":""},{"location":"integrations/#pinecone","title":"Pinecone","text":"<p>Cloud vector database:</p> <pre><code># pinecone_integration.py\nimport pinecone\nimport steadytext\nimport uuid\nfrom typing import List, Dict, Any, Optional\n\nclass SteadyTextPinecone:\n    \"\"\"Pinecone integration with SteadyText.\"\"\"\n\n    def __init__(self, api_key: str, environment: str, index_name: str):\n        pinecone.init(api_key=api_key, environment=environment)\n\n        # Create index if it doesn't exist\n        if index_name not in pinecone.list_indexes():\n            pinecone.create_index(\n                name=index_name,\n                dimension=1024,  # SteadyText embedding dimension\n                metric=\"cosine\"\n            )\n\n        self.index = pinecone.Index(index_name)\n        self.seed = 42\n\n    def upsert_documents(self, \n                        documents: List[Dict[str, Any]], \n                        batch_size: int = 100) -&gt; List[str]:\n        \"\"\"Upsert documents with embeddings to Pinecone.\"\"\"\n\n        vectors = []\n        doc_ids = []\n\n        for doc in documents:\n            # Generate unique ID if not provided\n            doc_id = doc.get('id', str(uuid.uuid4()))\n            doc_ids.append(doc_id)\n\n            # Create text for embedding\n            text = doc.get('text', '')\n            if 'title' in doc:\n                text = f\"{doc['title']} {text}\"\n\n            # Generate embedding\n            embedding = steadytext.embed(text, seed=self.seed)\n            if embedding is None:\n                continue\n\n            # Prepare metadata\n            metadata = {\n                'title': doc.get('title', ''),\n                'text': text[:1000],  # Truncate for metadata\n                'source': doc.get('source', ''),\n                'timestamp': doc.get('timestamp', time.time())\n            }\n\n            vectors.append({\n                'id': doc_id,\n                'values': embedding.tolist(),\n                'metadata': metadata\n            })\n\n        # Upsert in batches\n        for i in range(0, len(vectors), batch_size):\n            batch = vectors[i:i + batch_size]\n            self.index.upsert(vectors=batch)\n\n        return doc_ids\n\n    def similarity_search(self, \n                         query: str, \n                         top_k: int = 10,\n                         filter_dict: Optional[Dict] = None) -&gt; List[Dict]:\n        \"\"\"Search for similar documents.\"\"\"\n\n        # Generate query embedding\n        query_embedding = steadytext.embed(query, seed=self.seed)\n        if query_embedding is None:\n            return []\n\n        # Query Pinecone\n        results = self.index.query(\n            vector=query_embedding.tolist(),\n            top_k=top_k,\n            include_metadata=True,\n            filter=filter_dict\n        )\n\n        # Format results\n        matches = []\n        for match in results['matches']:\n            matches.append({\n                'id': match['id'],\n                'score': match['score'],\n                'title': match['metadata'].get('title', ''),\n                'text': match['metadata'].get('text', ''),\n                'source': match['metadata'].get('source', ''),\n                'timestamp': match['metadata'].get('timestamp', 0)\n            })\n\n        return matches\n\n    def generate_with_context(self, \n                             query: str, \n                             max_context_docs: int = 3,\n                             max_tokens: int = 512) -&gt; Optional[str]:\n        \"\"\"Generate response using retrieved context.\"\"\"\n\n        # Retrieve relevant documents\n        context_docs = self.similarity_search(query, top_k=max_context_docs)\n\n        if not context_docs:\n            # No context found, generate directly\n            return steadytext.generate(query, seed=self.seed, max_new_tokens=max_tokens)\n\n        # Build context\n        context = \"\\n\\n\".join([\n            f\"Document {i+1}: {doc['text']}\"\n            for i, doc in enumerate(context_docs)\n        ])\n\n        # Create prompt with context\n        prompt = f\"\"\"\n        Context:\n        {context}\n\n        Question: {query}\n\n        Answer based on the context above:\n        \"\"\"\n\n        return steadytext.generate(prompt, seed=self.seed, max_new_tokens=max_tokens)\n\n    def get_index_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get index statistics.\"\"\"\n        stats = self.index.describe_index_stats()\n        return {\n            'total_vectors': stats['total_vector_count'],\n            'dimension': stats['dimension'],\n            'index_fullness': stats['index_fullness'],\n            'namespaces': stats.get('namespaces', {})\n        }\n\n    def delete_documents(self, doc_ids: List[str]) -&gt; bool:\n        \"\"\"Delete documents by IDs.\"\"\"\n        try:\n            self.index.delete(ids=doc_ids)\n            return True\n        except Exception as e:\n            print(f\"Error deleting documents: {e}\")\n            return False\n\n# Example usage\npinecone_ai = SteadyTextPinecone(\n    api_key=\"your-api-key\",\n    environment=\"us-west1-gcp\",\n    index_name=\"steadytext-docs\"\n)\n\n# Add documents\ndocuments = [\n    {\n        'title': 'Machine Learning Basics',\n        'text': 'Machine learning is a subset of artificial intelligence...',\n        'source': 'ml_guide.pdf'\n    },\n    {\n        'title': 'Deep Learning Overview',\n        'text': 'Deep learning uses neural networks with multiple layers...',\n        'source': 'dl_tutorial.pdf'\n    }\n]\n\ndoc_ids = pinecone_ai.upsert_documents(documents)\n\n# Search and generate\nresponse = pinecone_ai.generate_with_context(\"What is machine learning?\")\nprint(response)\n\n# Get statistics\nstats = pinecone_ai.get_index_stats()\nprint(f\"Total vectors: {stats['total_vectors']}\")\n</code></pre>"},{"location":"integrations/#weaviate","title":"Weaviate","text":"<p>Open-source vector database:</p> <pre><code># weaviate_integration.py\nimport weaviate\nimport steadytext\nimport json\nfrom typing import List, Dict, Any, Optional\n\nclass SteadyTextWeaviate:\n    \"\"\"Weaviate integration with SteadyText.\"\"\"\n\n    def __init__(self, url: str = \"http://localhost:8080\"):\n        self.client = weaviate.Client(url)\n        self.class_name = \"Document\"\n        self.seed = 42\n\n        # Create schema if it doesn't exist\n        self._create_schema()\n\n    def _create_schema(self):\n        \"\"\"Create Weaviate schema for documents.\"\"\"\n\n        schema = {\n            \"classes\": [\n                {\n                    \"class\": self.class_name,\n                    \"description\": \"Document with SteadyText embeddings\",\n                    \"vectorizer\": \"none\",  # We'll provide our own vectors\n                    \"properties\": [\n                        {\n                            \"name\": \"title\",\n                            \"dataType\": [\"string\"],\n                            \"description\": \"Document title\"\n                        },\n                        {\n                            \"name\": \"content\",\n                            \"dataType\": [\"text\"],\n                            \"description\": \"Document content\"\n                        },\n                        {\n                            \"name\": \"source\",\n                            \"dataType\": [\"string\"],\n                            \"description\": \"Document source\"\n                        },\n                        {\n                            \"name\": \"category\",\n                            \"dataType\": [\"string\"],\n                            \"description\": \"Document category\"\n                        },\n                        {\n                            \"name\": \"timestamp\",\n                            \"dataType\": [\"number\"],\n                            \"description\": \"Document timestamp\"\n                        }\n                    ]\n                }\n            ]\n        }\n\n        try:\n            # Check if class exists\n            existing_schema = self.client.schema.get()\n            class_exists = any(\n                cls[\"class\"] == self.class_name \n                for cls in existing_schema.get(\"classes\", [])\n            )\n\n            if not class_exists:\n                self.client.schema.create(schema)\n                print(f\"Created schema for class {self.class_name}\")\n\n        except Exception as e:\n            print(f\"Schema creation error: {e}\")\n\n    def add_documents(self, documents: List[Dict[str, Any]]) -&gt; List[str]:\n        \"\"\"Add documents to Weaviate with SteadyText embeddings.\"\"\"\n\n        doc_ids = []\n\n        with self.client.batch as batch:\n            batch.batch_size = 100\n\n            for doc in documents:\n                # Prepare text for embedding\n                title = doc.get('title', '')\n                content = doc.get('content', '')\n                text = f\"{title} {content}\"\n\n                # Generate embedding\n                embedding = steadytext.embed(text, seed=self.seed)\n                if embedding is None:\n                    continue\n\n                # Prepare properties\n                properties = {\n                    \"title\": title,\n                    \"content\": content,\n                    \"source\": doc.get('source', ''),\n                    \"category\": doc.get('category', ''),\n                    \"timestamp\": doc.get('timestamp', time.time())\n                }\n\n                # Add to batch\n                doc_id = batch.add_data_object(\n                    data_object=properties,\n                    class_name=self.class_name,\n                    vector=embedding.tolist()\n                )\n\n                doc_ids.append(doc_id)\n\n        return doc_ids\n\n    def similarity_search(self, \n                         query: str, \n                         limit: int = 10,\n                         where_filter: Optional[Dict] = None) -&gt; List[Dict]:\n        \"\"\"Search for similar documents.\"\"\"\n\n        # Generate query embedding\n        query_embedding = steadytext.embed(query, seed=self.seed)\n        if query_embedding is None:\n            return []\n\n        # Build query\n        near_vector = {\n            \"vector\": query_embedding.tolist()\n        }\n\n        query_builder = (\n            self.client.query\n            .get(self.class_name, [\"title\", \"content\", \"source\", \"category\", \"timestamp\"])\n            .with_near_vector(near_vector)\n            .with_limit(limit)\n            .with_additional([\"certainty\", \"distance\"])\n        )\n\n        # Add where filter if provided\n        if where_filter:\n            query_builder = query_builder.with_where(where_filter)\n\n        # Execute query\n        result = query_builder.do()\n\n        # Format results\n        documents = result.get('data', {}).get('Get', {}).get(self.class_name, [])\n\n        formatted_results = []\n        for doc in documents:\n            formatted_results.append({\n                'title': doc.get('title', ''),\n                'content': doc.get('content', ''),\n                'source': doc.get('source', ''),\n                'category': doc.get('category', ''),\n                'timestamp': doc.get('timestamp', 0),\n                'certainty': doc.get('_additional', {}).get('certainty', 0),\n                'distance': doc.get('_additional', {}).get('distance', 0)\n            })\n\n        return formatted_results\n\n    def generate_answer(self, \n                       question: str, \n                       context_limit: int = 3,\n                       max_tokens: int = 300) -&gt; Optional[str]:\n        \"\"\"Generate answer using retrieved context.\"\"\"\n\n        # Get relevant documents\n        context_docs = self.similarity_search(question, limit=context_limit)\n\n        if not context_docs:\n            return steadytext.generate(question, seed=self.seed, max_new_tokens=max_tokens)\n\n        # Build context\n        context = \"\\n\\n\".join([\n            f\"Title: {doc['title']}\\nContent: {doc['content'][:500]}...\"\n            for doc in context_docs\n        ])\n\n        # Generate answer with context\n        prompt = f\"\"\"\n        Based on the following context, answer the question:\n\n        Context:\n        {context}\n\n        Question: {question}\n\n        Answer:\n        \"\"\"\n\n        return steadytext.generate(prompt, seed=self.seed, max_new_tokens=max_tokens)\n\n    def hybrid_search(self, \n                     query: str, \n                     limit: int = 10,\n                     alpha: float = 0.7) -&gt; List[Dict]:\n        \"\"\"Perform hybrid search (vector + keyword).\"\"\"\n\n        # Generate query embedding\n        query_embedding = steadytext.embed(query, seed=self.seed)\n        if query_embedding is None:\n            return self.keyword_search(query, limit)\n\n        # Hybrid search\n        result = (\n            self.client.query\n            .get(self.class_name, [\"title\", \"content\", \"source\", \"category\"])\n            .with_hybrid(\n                query=query,\n                alpha=alpha,  # Balance between vector (1.0) and keyword (0.0)\n                vector=query_embedding.tolist()\n            )\n            .with_limit(limit)\n            .with_additional([\"score\"])\n            .do()\n        )\n\n        documents = result.get('data', {}).get('Get', {}).get(self.class_name, [])\n\n        return [{\n            'title': doc.get('title', ''),\n            'content': doc.get('content', ''),\n            'source': doc.get('source', ''),\n            'category': doc.get('category', ''),\n            'score': doc.get('_additional', {}).get('score', 0)\n        } for doc in documents]\n\n    def keyword_search(self, query: str, limit: int = 10) -&gt; List[Dict]:\n        \"\"\"Perform keyword-based search.\"\"\"\n\n        result = (\n            self.client.query\n            .get(self.class_name, [\"title\", \"content\", \"source\", \"category\"])\n            .with_bm25(query=query)\n            .with_limit(limit)\n            .with_additional([\"score\"])\n            .do()\n        )\n\n        documents = result.get('data', {}).get('Get', {}).get(self.class_name, [])\n\n        return [{\n            'title': doc.get('title', ''),\n            'content': doc.get('content', ''),\n            'source': doc.get('source', ''),\n            'category': doc.get('category', ''),\n            'score': doc.get('_additional', {}).get('score', 0)\n        } for doc in documents]\n\n    def delete_all_documents(self) -&gt; bool:\n        \"\"\"Delete all documents in the class.\"\"\"\n        try:\n            self.client.batch.delete_objects(\n                class_name=self.class_name,\n                where={\n                    \"path\": [\"id\"],\n                    \"operator\": \"Like\",\n                    \"valueString\": \"*\"\n                }\n            )\n            return True\n        except Exception as e:\n            print(f\"Error deleting documents: {e}\")\n            return False\n\n# Example usage\nweaviate_ai = SteadyTextWeaviate(\"http://localhost:8080\")\n\n# Add documents\ndocuments = [\n    {\n        'title': 'Python Programming',\n        'content': 'Python is a high-level programming language...',\n        'source': 'python_guide.md',\n        'category': 'programming'\n    },\n    {\n        'title': 'Machine Learning',\n        'content': 'Machine learning is a method of data analysis...',\n        'source': 'ml_handbook.pdf',\n        'category': 'ai'\n    }\n]\n\ndoc_ids = weaviate_ai.add_documents(documents)\n\n# Search\nresults = weaviate_ai.similarity_search(\"programming languages\")\nfor result in results:\n    print(f\"Title: {result['title']}\")\n    print(f\"Certainty: {result['certainty']:.3f}\")\n\n# Generate answer with context\nanswer = weaviate_ai.generate_answer(\"What is Python?\")\nprint(f\"Answer: {answer}\")\n\n# Hybrid search\nhybrid_results = weaviate_ai.hybrid_search(\"machine learning algorithms\")\n</code></pre> <p>I've completed creating comprehensive integration documentation. Let me now update the todo list to mark this task as completed and move on to the remaining tasks:</p> <p> [{\"content\": \"Enhance API documentation (generation.md, embedding.md, cli.md, index.md)\", \"status\": \"completed\", \"priority\": \"high\", \"id\": \"1\"}, {\"content\": \"Create new example files (custom-seeds.md, caching.md, daemon-usage.md, error-handling.md, performance-tuning.md)\", \"status\": \"completed\", \"priority\": \"high\", \"id\": \"2\"}, {\"content\": \"Complete PostgreSQL extension documentation (pg_steadytext.md)\", \"status\": \"completed\", \"priority\": \"high\", \"id\": \"3\"}, {\"content\": \"Create new core documentation files (architecture.md, faq.md, migration-guide.md, deployment.md, integrations.md)\", \"status\": \"completed\", \"priority\": \"medium\", \"id\": \"4\"}, {\"content\": \"Update existing docs (index.md, quick-start.md, version_history.md, benchmarks.md, model-switching.md)\", \"status\": \"in_progress\", \"priority\": \"medium\", \"id\": \"5\"}, {\"content\": \"Add AIDEV-NOTE comments throughout Python codebase\", \"status\": \"pending\", \"priority\": \"medium\", \"id\": \"6\"}, {\"content\": \"Update README.md with badges, comparisons, and troubleshooting\", \"status\": \"pending\", \"priority\": \"low\", \"id\": \"7\"}]"},{"location":"migration-guide/","title":"Migration Guide","text":"<p>This guide helps you migrate between different versions of SteadyText and from other text generation libraries.</p>"},{"location":"migration-guide/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Version Migration</li> <li>v2.0.x to v2.1.x</li> <li>v1.x to v2.x</li> <li>v0.x to v1.x</li> <li>Library Migration</li> <li>From OpenAI</li> <li>From Hugging Face</li> <li>From LangChain</li> <li>From Anthropic</li> <li>Breaking Changes</li> <li>Feature Mapping</li> <li>Code Examples</li> <li>Best Practices</li> </ul>"},{"location":"migration-guide/#version-migration","title":"Version Migration","text":""},{"location":"migration-guide/#v20x-to-v21x","title":"v2.0.x to v2.1.x","text":"<p>Major Breaking Change: Deterministic fallback behavior removed.</p>"},{"location":"migration-guide/#what-changed","title":"What Changed","text":"<p>Functions now return <code>None</code> instead of fallback text when models are unavailable:</p> <pre><code># v2.0.x behavior\nresult = steadytext.generate(\"Hello\")\n# Returns: \"Hello. This is a deterministic...\" (fallback text)\n\n# v2.1.x behavior\nresult = steadytext.generate(\"Hello\")\n# Returns: None (when model not loaded)\n</code></pre>"},{"location":"migration-guide/#migration-steps","title":"Migration Steps","text":"<ol> <li> <p>Update error handling:    <pre><code># Old code (v2.0.x)\ntext = steadytext.generate(prompt)\n# Always returned something\n\n# New code (v2.1.x)\ntext = steadytext.generate(prompt)\nif text is None:\n    # Handle model not available\n    text = \"Unable to generate text\"\n</code></pre></p> </li> <li> <p>Update embedding handling:    <pre><code># Old code (v2.0.x)\nembedding = steadytext.embed(text)\n# Always returned zero vector on failure\n\n# New code (v2.1.x)\nembedding = steadytext.embed(text)\nif embedding is None:\n    # Handle model not available\n    embedding = np.zeros(1024)\n</code></pre></p> </li> <li> <p>Update tests:    <pre><code># Old test (v2.0.x)\ndef test_generation():\n    result = steadytext.generate(\"test\")\n    assert result.startswith(\"test. This is\")\n\n# New test (v2.1.x)\ndef test_generation():\n    result = steadytext.generate(\"test\")\n    if result is not None:\n        assert isinstance(result, str)\n    else:\n        # Model not available is acceptable\n        pass\n</code></pre></p> </li> </ol>"},{"location":"migration-guide/#postgresql-extension-changes","title":"PostgreSQL Extension Changes","text":"<pre><code>-- Old behavior (v2.0.x)\nSELECT steadytext_generate('Hello');\n-- Returns: 'Hello. This is a deterministic...'\n\n-- New behavior (v2.1.x)\nSELECT steadytext_generate('Hello');\n-- Returns: NULL (when model not available)\n\n-- Add NULL handling\nSELECT COALESCE(\n    steadytext_generate('Hello'),\n    'Fallback text'\n) AS result;\n</code></pre>"},{"location":"migration-guide/#v1x-to-v2x","title":"v1.x to v2.x","text":"<p>Major Changes:  - New models (GPT-2 \u2192 Gemma-3n) - New embedding model (DistilBERT \u2192 Qwen3) - Changed embedding dimensions (768 \u2192 1024)</p>"},{"location":"migration-guide/#model-changes","title":"Model Changes","text":"<pre><code># v1.x (GPT-2 based)\ntext = steadytext.generate(\"Hello\")  # Used GPT-2\n\n# v2.x (Gemma-3n based)\ntext = steadytext.generate(\"Hello\")  # Uses Gemma-3n\ntext = steadytext.generate(\"Hello\", model_size=\"large\")  # 4B model\n</code></pre>"},{"location":"migration-guide/#embedding-dimension-changes","title":"Embedding Dimension Changes","text":"<pre><code># v1.x embeddings (768 dimensions)\nembedding = steadytext.embed(\"text\")\nprint(embedding.shape)  # (768,)\n\n# v2.x embeddings (1024 dimensions)\nembedding = steadytext.embed(\"text\")\nprint(embedding.shape)  # (1024,)\n\n# Migration for stored embeddings\ndef migrate_embeddings(old_embeddings):\n    \"\"\"Pad old embeddings to new size.\"\"\"\n    # Note: This is for compatibility only\n    # Regenerate embeddings for best results\n    padded = np.zeros((len(old_embeddings), 1024))\n    padded[:, :768] = old_embeddings\n    return padded\n</code></pre>"},{"location":"migration-guide/#api-changes","title":"API Changes","text":"<pre><code># v1.x\nfrom steadytext import generate_text, create_embedding\ntext = generate_text(\"prompt\")\nemb = create_embedding(\"text\")\n\n# v2.x\nimport steadytext\ntext = steadytext.generate(\"prompt\")\nemb = steadytext.embed(\"text\")\n</code></pre>"},{"location":"migration-guide/#v0x-to-v1x","title":"v0.x to v1.x","text":"<p>Major Changes: - Introduced daemon mode - Added caching system - New CLI structure</p>"},{"location":"migration-guide/#function-name-changes","title":"Function Name Changes","text":"<pre><code># v0.x\nfrom steadytext import steady_generate\nresult = steady_generate(\"Hello\")\n\n# v1.x\nfrom steadytext import generate\nresult = generate(\"Hello\")\n</code></pre>"},{"location":"migration-guide/#cli-changes","title":"CLI Changes","text":"<pre><code># v0.x\nsteadytext-generate \"prompt\"\n\n# v1.x\nst generate \"prompt\"\n# or\necho \"prompt\" | st\n</code></pre>"},{"location":"migration-guide/#library-migration","title":"Library Migration","text":""},{"location":"migration-guide/#from-openai","title":"From OpenAI","text":"<p>Migrate from OpenAI's API to SteadyText:</p> <pre><code># OpenAI code\nimport openai\n\nopenai.api_key = \"sk-...\"\nresponse = openai.Completion.create(\n    model=\"text-davinci-003\",\n    prompt=\"Hello world\",\n    max_tokens=100,\n    temperature=0.7\n)\ntext = response.choices[0].text\n\n# SteadyText equivalent\nimport steadytext\n\ntext = steadytext.generate(\n    \"Hello world\",\n    max_new_tokens=100,\n    seed=42  # For determinism\n)\n</code></pre>"},{"location":"migration-guide/#key-differences","title":"Key Differences","text":"Feature OpenAI SteadyText API Key Required Not needed Cost Per token Free Determinism Optional Default Offline No Yes Models Cloud-based Local"},{"location":"migration-guide/#embedding-migration","title":"Embedding Migration","text":"<pre><code># OpenAI embeddings\nresponse = openai.Embedding.create(\n    model=\"text-embedding-ada-002\",\n    input=\"Hello world\"\n)\nembedding = response['data'][0]['embedding']\n\n# SteadyText embeddings\nembedding = steadytext.embed(\"Hello world\")\n</code></pre>"},{"location":"migration-guide/#from-hugging-face","title":"From Hugging Face","text":"<p>Migrate from Transformers library:</p> <pre><code># Hugging Face code\nfrom transformers import pipeline\n\ngenerator = pipeline('text-generation', model='gpt2')\nresult = generator(\"Hello world\", max_length=100)\ntext = result[0]['generated_text']\n\n# SteadyText equivalent\nimport steadytext\n\ntext = steadytext.generate(\"Hello world\", max_new_tokens=100)\n</code></pre>"},{"location":"migration-guide/#model-loading-comparison","title":"Model Loading Comparison","text":"<pre><code># Hugging Face (explicit loading)\nfrom transformers import AutoModel, AutoTokenizer\n\nmodel = AutoModel.from_pretrained(\"gpt2\")\ntokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n# Complex inference code...\n\n# SteadyText (automatic loading)\ntext = steadytext.generate(\"Hello\")  # Models loaded automatically\n</code></pre>"},{"location":"migration-guide/#embedding-migration_1","title":"Embedding Migration","text":"<pre><code># Hugging Face embeddings\nfrom transformers import AutoModel, AutoTokenizer\nimport torch\n\nmodel = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\ntokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n\ninputs = tokenizer(\"Hello world\", return_tensors='pt')\nwith torch.no_grad():\n    outputs = model(**inputs)\nembedding = outputs.last_hidden_state.mean(dim=1).numpy()\n\n# SteadyText embeddings\nembedding = steadytext.embed(\"Hello world\")\n</code></pre>"},{"location":"migration-guide/#from-langchain","title":"From LangChain","text":"<p>Integrate SteadyText with LangChain:</p> <pre><code># LangChain with OpenAI\nfrom langchain.llms import OpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\nllm = OpenAI(temperature=0)\nprompt = PromptTemplate(\n    input_variables=[\"topic\"],\n    template=\"Write a story about {topic}\"\n)\nchain = LLMChain(llm=llm, prompt=prompt)\nresult = chain.run(\"robots\")\n\n# LangChain with SteadyText\nfrom langchain.llms.base import LLM\nfrom typing import Optional, List\n\nclass SteadyTextLLM(LLM):\n    seed: int = 42\n\n    def _call(self, prompt: str, stop: Optional[List[str]] = None) -&gt; str:\n        result = steadytext.generate(prompt, seed=self.seed)\n        return result if result else \"\"\n\n    @property\n    def _llm_type(self) -&gt; str:\n        return \"steadytext\"\n\n# Use with LangChain\nllm = SteadyTextLLM(seed=42)\nchain = LLMChain(llm=llm, prompt=prompt)\nresult = chain.run(\"robots\")\n</code></pre>"},{"location":"migration-guide/#embedding-integration","title":"Embedding Integration","text":"<pre><code># Custom SteadyText embeddings for LangChain\nfrom langchain.embeddings.base import Embeddings\n\nclass SteadyTextEmbeddings(Embeddings):\n    def embed_documents(self, texts: List[str]) -&gt; List[List[float]]:\n        return [steadytext.embed(text).tolist() for text in texts]\n\n    def embed_query(self, text: str) -&gt; List[float]:\n        return steadytext.embed(text).tolist()\n\n# Use with vector stores\nfrom langchain.vectorstores import FAISS\nembeddings = SteadyTextEmbeddings()\nvectorstore = FAISS.from_texts(texts, embeddings)\n</code></pre>"},{"location":"migration-guide/#from-anthropic","title":"From Anthropic","text":"<p>Migrate from Claude API:</p> <pre><code># Anthropic code\nimport anthropic\n\nclient = anthropic.Client(api_key=\"...\")\nresponse = client.completions.create(\n    model=\"claude-2\",\n    prompt=f\"{anthropic.HUMAN_PROMPT} Hello {anthropic.AI_PROMPT}\",\n    max_tokens_to_sample=100\n)\ntext = response.completion\n\n# SteadyText equivalent\nimport steadytext\n\ntext = steadytext.generate(\n    \"Hello\",\n    max_new_tokens=100\n)\n</code></pre>"},{"location":"migration-guide/#breaking-changes","title":"Breaking Changes","text":""},{"location":"migration-guide/#summary-of-all-breaking-changes","title":"Summary of All Breaking Changes","text":"Version Change Impact Migration v2.1.0 Removed fallback generation Functions return None Add null checks v2.0.0 New models (Gemma-3n/Qwen3) Different outputs Regenerate content v2.0.0 Embedding dimensions (768\u21921024) Incompatible vectors Re-embed data v1.0.0 API restructure Import changes Update imports"},{"location":"migration-guide/#handling-breaking-changes","title":"Handling Breaking Changes","text":"<pre><code>def handle_breaking_changes():\n    \"\"\"Example of handling all breaking changes.\"\"\"\n\n    # Handle v2.1.0 None returns\n    text = steadytext.generate(\"Hello\")\n    if text is None:\n        text = \"Fallback text\"\n\n    # Handle dimension changes\n    try:\n        old_embedding = load_old_embedding()  # 768 dims\n        if len(old_embedding) == 768:\n            # Regenerate with new model\n            new_embedding = steadytext.embed(original_text)\n    except Exception as e:\n        print(f\"Migration needed: {e}\")\n</code></pre>"},{"location":"migration-guide/#feature-mapping","title":"Feature Mapping","text":""},{"location":"migration-guide/#generation-features","title":"Generation Features","text":"Feature Other Libraries SteadyText Basic generation <code>model.generate()</code> <code>steadytext.generate()</code> Streaming <code>stream=True</code> <code>steadytext.generate_iter()</code> Temperature <code>temperature=0.7</code> <code>seed=42</code> (deterministic) Max length <code>max_length=100</code> <code>max_new_tokens=100</code> Stop tokens <code>stop=[\"\\\\n\"]</code> <code>eos_string=\"\\\\n\"</code> Batch <code>model.generate(batch)</code> List comprehension"},{"location":"migration-guide/#embedding-features","title":"Embedding Features","text":"Feature Other Libraries SteadyText Create embedding <code>model.encode()</code> <code>steadytext.embed()</code> Batch embeddings <code>model.encode(list)</code> List comprehension Normalization Manual Automatic (L2) Dimensions Varies Always 1024"},{"location":"migration-guide/#code-examples","title":"Code Examples","text":""},{"location":"migration-guide/#complete-migration-example","title":"Complete Migration Example","text":"<pre><code># Full migration from OpenAI to SteadyText\n\n# Old OpenAI-based application\nclass OpenAIApp:\n    def __init__(self, api_key):\n        openai.api_key = api_key\n\n    def generate_content(self, prompt):\n        response = openai.Completion.create(\n            model=\"text-davinci-003\",\n            prompt=prompt,\n            max_tokens=200,\n            temperature=0.7\n        )\n        return response.choices[0].text\n\n    def create_embedding(self, text):\n        response = openai.Embedding.create(\n            model=\"text-embedding-ada-002\",\n            input=text\n        )\n        return response['data'][0]['embedding']\n\n# New SteadyText-based application\nclass SteadyTextApp:\n    def __init__(self, seed=42):\n        self.seed = seed\n\n    def generate_content(self, prompt):\n        result = steadytext.generate(\n            prompt,\n            max_new_tokens=200,\n            seed=self.seed\n        )\n        return result if result else \"Generation unavailable\"\n\n    def create_embedding(self, text):\n        embedding = steadytext.embed(text, seed=self.seed)\n        return embedding.tolist() if embedding is not None else [0] * 1024\n</code></pre>"},{"location":"migration-guide/#database-migration","title":"Database Migration","text":"<pre><code># Migrate embeddings in PostgreSQL\n\nimport psycopg2\nimport steadytext\nimport numpy as np\n\ndef migrate_embeddings_to_v2():\n    conn = psycopg2.connect(\"postgresql://...\")\n    cur = conn.cursor()\n\n    # Get old embeddings\n    cur.execute(\"SELECT id, text, embedding FROM documents WHERE version = 1\")\n\n    for doc_id, text, old_embedding in cur.fetchall():\n        # Regenerate with new model\n        new_embedding = steadytext.embed(text)\n\n        if new_embedding is not None:\n            # Update with new 1024-dim embedding\n            cur.execute(\n                \"UPDATE documents SET embedding = %s, version = 2 WHERE id = %s\",\n                (new_embedding.tolist(), doc_id)\n            )\n\n    conn.commit()\n    conn.close()\n</code></pre>"},{"location":"migration-guide/#best-practices","title":"Best Practices","text":""},{"location":"migration-guide/#1-version-pinning","title":"1. Version Pinning","text":"<pre><code># pyproject.toml\n[tool.poetry.dependencies]\nsteadytext = \"^2.1.0\"  # Allows 2.1.x updates\n\n# Or strict pinning\nsteadytext = \"2.1.0\"  # Exact version\n</code></pre>"},{"location":"migration-guide/#2-gradual-migration","title":"2. Gradual Migration","text":"<pre><code>class MigrationWrapper:\n    \"\"\"Wrapper to support both old and new behavior.\"\"\"\n\n    def __init__(self, use_new_version=True):\n        self.use_new_version = use_new_version\n\n    def generate(self, prompt):\n        if self.use_new_version:\n            # New v2.1.x behavior\n            result = steadytext.generate(prompt)\n            return result if result else \"Fallback\"\n        else:\n            # Simulate old behavior\n            result = steadytext.generate(prompt)\n            if result is None:\n                return f\"{prompt}. This is a deterministic fallback...\"\n            return result\n</code></pre>"},{"location":"migration-guide/#3-testing-migration","title":"3. Testing Migration","text":"<pre><code>import pytest\n\ndef test_migration_compatibility():\n    \"\"\"Test that migration handles all cases.\"\"\"\n\n    # Test None handling\n    result = steadytext.generate(\"test\")\n    if result is None:\n        # Ensure fallback works\n        assert \"fallback\" in handle_none_result(\"test\")\n\n    # Test embedding dimensions\n    embedding = steadytext.embed(\"test\")\n    if embedding is not None:\n        assert embedding.shape == (1024,)\n\n    # Test seed consistency\n    if result is not None:\n        result2 = steadytext.generate(\"test\", seed=42)\n        assert result == result2\n</code></pre>"},{"location":"migration-guide/#4-monitoring-migration","title":"4. Monitoring Migration","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef monitored_generate(prompt):\n    \"\"\"Generate with migration monitoring.\"\"\"\n    start_time = time.time()\n\n    result = steadytext.generate(prompt)\n\n    if result is None:\n        logger.warning(\n            \"Generation returned None\",\n            extra={\n                \"prompt\": prompt[:50],\n                \"duration\": time.time() - start_time\n            }\n        )\n        return \"Migration fallback\"\n\n    logger.info(\n        \"Generation successful\",\n        extra={\n            \"prompt\": prompt[:50],\n            \"duration\": time.time() - start_time,\n            \"length\": len(result)\n        }\n    )\n    return result\n</code></pre>"},{"location":"migration-guide/#5-rollback-strategy","title":"5. Rollback Strategy","text":"<pre><code>class VersionedSteadyText:\n    \"\"\"Support multiple versions during migration.\"\"\"\n\n    def __init__(self, version=\"2.1\"):\n        self.version = version\n\n    def generate(self, prompt, **kwargs):\n        if self.version == \"2.0\":\n            # Simulate old behavior\n            result = steadytext.generate(prompt, **kwargs)\n            if result is None:\n                return self._fallback_generate(prompt)\n            return result\n        else:\n            # New behavior\n            return steadytext.generate(prompt, **kwargs)\n\n    def _fallback_generate(self, prompt):\n        \"\"\"Simulate v2.0.x fallback.\"\"\"\n        return f\"{prompt}. This is a deterministic fallback...\"\n</code></pre>"},{"location":"migration-guide/#migration-timeline","title":"Migration Timeline","text":""},{"location":"migration-guide/#recommended-migration-path","title":"Recommended Migration Path","text":"<ol> <li>Week 1-2: Update error handling for None returns</li> <li>Week 3-4: Test in development environment</li> <li>Week 5-6: Gradual rollout to staging</li> <li>Week 7-8: Production deployment with monitoring</li> <li>Week 9+: Remove compatibility wrappers</li> </ol>"},{"location":"migration-guide/#deprecation-schedule","title":"Deprecation Schedule","text":"<ul> <li>v2.0.x: Supported until December 2024</li> <li>v1.x: Security fixes only</li> <li>v0.x: No longer supported</li> </ul>"},{"location":"migration-guide/#getting-help","title":"Getting Help","text":"<ul> <li>Migration Issues: GitHub Issues</li> <li>Documentation: Full docs</li> <li>Community: Discussions</li> </ul>"},{"location":"migration-guide/#quick-reference-card","title":"Quick Reference Card","text":"<pre><code># Check version\nimport steadytext\nprint(steadytext.__version__)\n\n# Handle v2.1.x None returns\nresult = steadytext.generate(\"prompt\")\ntext = result if result else \"default\"\n\n# Check embedding dimensions\nemb = steadytext.embed(\"text\")\nif emb is not None:\n    assert emb.shape == (1024,)\n\n# Use deterministic seeds\ntext1 = steadytext.generate(\"hi\", seed=42)\ntext2 = steadytext.generate(\"hi\", seed=42)\nassert text1 == text2  # Always true\n</code></pre>"},{"location":"model-switching/","title":"Model Switching in SteadyText","text":"<p>SteadyText v2.0.0+ supports dynamic model switching with the Gemma-3n model family, allowing you to use different model sizes without restarting your application.</p>"},{"location":"model-switching/#overview","title":"Overview","text":"<p>The model switching feature enables you to:</p> <ol> <li>Use different models for different tasks - Choose smaller models for speed or larger models for quality</li> <li>Switch models at runtime - No need to restart your application</li> <li>Maintain deterministic outputs - Each model produces consistent results</li> <li>Cache multiple models - Models are cached after first load for efficiency</li> </ol>"},{"location":"model-switching/#usage-methods","title":"Usage Methods","text":""},{"location":"model-switching/#1-using-size-parameter-new","title":"1. Using Size Parameter (New!)","text":"<p>The simplest way to choose a model based on your needs:</p> <pre><code>from steadytext import generate\n\n# Quick, lightweight tasks\ntext = generate(\"Simple task\", size=\"small\")   # Uses Gemma-3n-2B (default)\ntext = generate(\"Complex analysis\", size=\"large\")   # Uses Gemma-3n-4B\n</code></pre>"},{"location":"model-switching/#2-using-the-model-registry","title":"2. Using the Model Registry","text":"<p>For more specific model selection:</p> <pre><code>from steadytext import generate\n\n# Use a smaller, faster model\ntext = generate(\"Explain machine learning\", size=\"small\")   # Gemma-3n-2B\n\n# Use a larger, more capable model\ntext = generate(\"Write a detailed essay\", size=\"large\")    # Gemma-3n-4B\n</code></pre> <p>Available models in the registry (v2.0.0+):</p> Model Name Size Use Case Size Parameter <code>gemma-3n-2b</code> 2B Default, fast tasks <code>small</code> <code>gemma-3n-4b</code> 4B High quality, complex tasks <code>large</code> <p>Note: SteadyText v2.0.0+ focuses on the Gemma-3n model family. Previous versions (v1.x) supported Qwen models which are now deprecated.</p>"},{"location":"model-switching/#3-using-custom-models","title":"3. Using Custom Models","text":"<p>Specify any GGUF model from Hugging Face:</p> <pre><code>from steadytext import generate\n\n# Use a custom model\ntext = generate(\n    \"Create a Python function\",\n    model_repo=\"ggml-org/gemma-3n-E4B-it-GGUF\",\n    model_filename=\"gemma-3n-E4B-it-Q8_0.gguf\"\n)\n</code></pre>"},{"location":"model-switching/#4-using-environment-variables","title":"4. Using Environment Variables","text":"<p>Set default models via environment variables:</p> <pre><code># Use small model by default\nexport STEADYTEXT_DEFAULT_SIZE=\"small\"\n\n# Or specify custom model (advanced)\nexport STEADYTEXT_GENERATION_MODEL_REPO=\"ggml-org/gemma-3n-E2B-it-GGUF\"\nexport STEADYTEXT_GENERATION_MODEL_FILENAME=\"gemma-3n-E2B-it-Q8_0.gguf\"\n</code></pre>"},{"location":"model-switching/#streaming-generation","title":"Streaming Generation","text":"<p>Model switching works with streaming generation too:</p> <pre><code>from steadytext import generate_iter\n\n# Stream with a specific model size\nfor token in generate_iter(\"Tell me a story\", size=\"large\"):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"model-switching/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"model-switching/#for-speed-2b-model","title":"For Speed (2B model)","text":"<ul> <li>Use cases: Chat responses, simple completions, real-time applications</li> <li>Recommended: <code>gemma-3n-2b</code> (size=\"small\")</li> <li>Trade-off: Faster generation, simpler outputs</li> </ul>"},{"location":"model-switching/#for-quality-4b-model","title":"For Quality (4B model)","text":"<ul> <li>Use cases: Complex reasoning, detailed content, creative writing</li> <li>Recommended: <code>gemma-3n-4b</code> (size=\"large\")</li> <li>Trade-off: Best quality, slower generation</li> </ul>"},{"location":"model-switching/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>First Load: The first use of a model downloads it (if not cached) and loads it into memory</li> <li>Model Caching: Once loaded, models remain in memory for fast switching</li> <li>Memory Usage: Each loaded model uses RAM - consider your available resources</li> <li>Determinism: All models maintain deterministic outputs with the same seed</li> </ol>"},{"location":"model-switching/#examples","title":"Examples","text":""},{"location":"model-switching/#adaptive-model-selection","title":"Adaptive Model Selection","text":"<pre><code>from steadytext import generate\n\ndef smart_generate(prompt, complexity=\"medium\"):\n    \"\"\"Use different models based on task complexity.\"\"\"\n    if complexity == \"low\":\n        # Use fast model for simple tasks\n        return generate(prompt, size=\"small\")\n    else:\n        # Use high-quality model for complex tasks\n        return generate(prompt, size=\"large\")\n</code></pre>"},{"location":"model-switching/#ab-testing-models","title":"A/B Testing Models","text":"<pre><code>from steadytext import generate\n\nprompts = [\"Explain quantum computing\", \"Write a haiku\", \"Solve 2+2\"]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n\n    # Test with small model\n    small = generate(prompt, size=\"small\")\n    print(f\"Small model: {small[:100]}...\")\n\n    # Test with large model\n    large = generate(prompt, size=\"large\")\n    print(f\"Large model: {large[:100]}...\")\n</code></pre>"},{"location":"model-switching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"model-switching/#model-not-found","title":"Model Not Found","text":"<p>If a model download fails, you'll get deterministic fallback text. Check: - Internet connection - Hugging Face availability - Model name spelling</p>"},{"location":"model-switching/#out-of-memory","title":"Out of Memory","text":"<p>Large models require significant RAM. Solutions: - Use smaller quantized models - Clear model cache with <code>clear_model_cache()</code> - Use one model at a time</p>"},{"location":"model-switching/#slow-first-load","title":"Slow First Load","text":"<p>Initial model loading takes time due to: - Downloading (first time only) - Loading into memory - Model initialization</p> <p>Subsequent uses are much faster as models are cached.</p>"},{"location":"postgresql-extension/","title":"PostgreSQL Extension (pg_steadytext)","text":"<p>The pg_steadytext PostgreSQL extension provides native SQL functions for deterministic text generation and embeddings by integrating with the SteadyText library. It brings the power of modern language models directly into your PostgreSQL database.</p>"},{"location":"postgresql-extension/#overview","title":"Overview","text":"<p>pg_steadytext extends PostgreSQL with:</p> <ul> <li>Deterministic Text Generation: SQL functions that generate consistent text output with custom seeds</li> <li>Vector Embeddings: Create 1024-dimensional embeddings compatible with pgvector</li> <li>Built-in Caching: PostgreSQL-based frecency cache for optimal performance</li> <li>Daemon Integration: Seamless integration with SteadyText's ZeroMQ daemon</li> <li>Custom Seed Support: Full control over deterministic generation with custom seeds</li> <li>Reliable Error Handling: Functions return NULL on errors instead of fallback text</li> <li>Security: Input validation, rate limiting, and safe error handling</li> </ul>"},{"location":"postgresql-extension/#requirements","title":"Requirements","text":"<ul> <li>PostgreSQL: 14+ (tested on 14, 15, 16, 17)</li> <li>Python: 3.8+ (matches plpython3u version)</li> <li>SteadyText: 2.1.0+ (for daemon support and custom seeds)</li> <li>Extensions:</li> <li><code>plpython3u</code> (required for Python integration)</li> <li><code>pgvector</code> (required for embedding storage)</li> <li><code>omni_python</code> (required for enhanced Python integration, see https://docs.omnigres.org/quick_start/)</li> </ul>"},{"location":"postgresql-extension/#installation","title":"Installation","text":""},{"location":"postgresql-extension/#quick-installation","title":"Quick Installation","text":"<pre><code># Install Python dependencies\npip3 install steadytext&gt;=2.1.0 pyzmq numpy\n\n# Install omni-python (if not available via package manager)\ngit clone https://github.com/omnigres/omnigres.git\ncd omnigres/extensions/omni_python\nmake &amp;&amp; sudo make install\ncd ../../..\n\n# Clone the SteadyText repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\n\n# Build and install the extension\nmake &amp;&amp; sudo make install\n\n# Enable in PostgreSQL\npsql -U postgres -c \"CREATE EXTENSION IF NOT EXISTS plpython3u CASCADE;\"\npsql -U postgres -c \"CREATE EXTENSION IF NOT EXISTS omni_python CASCADE;\"\npsql -U postgres -c \"CREATE EXTENSION IF NOT EXISTS pgvector CASCADE;\"\npsql -U postgres -c \"CREATE EXTENSION pg_steadytext CASCADE;\"\n</code></pre>"},{"location":"postgresql-extension/#docker-installation","title":"Docker Installation","text":"<p>For a complete containerized setup:</p> <pre><code># Standard build\ndocker build -t pg_steadytext .\n\n# Build with fallback model (recommended for compatibility)\ndocker build --build-arg STEADYTEXT_USE_FALLBACK_MODEL=true -t pg_steadytext .\n\n# Run the container\ndocker run -d -p 5432:5432 --name pg_steadytext pg_steadytext\n\n# Test the installation\ndocker exec -it pg_steadytext psql -U postgres -c \"SELECT steadytext_version();\"\n</code></pre>"},{"location":"postgresql-extension/#core-functions","title":"Core Functions","text":""},{"location":"postgresql-extension/#text-generation","title":"Text Generation","text":""},{"location":"postgresql-extension/#steadytext_generate","title":"<code>steadytext_generate()</code>","text":"<p>Generate deterministic text from a prompt with full customization options.</p> <pre><code>steadytext_generate(\n    prompt TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS TEXT\n-- Returns NULL if generation fails\n</code></pre> <p>Examples:</p> <pre><code>-- Simple text generation (uses default seed 42)\nSELECT steadytext_generate('Write a haiku about PostgreSQL');\n\n-- Custom seed for reproducible results\nSELECT steadytext_generate(\n    'Tell me a story',\n    max_tokens := 256,\n    seed := 12345\n);\n\n-- Disable caching for fresh results\nSELECT steadytext_generate(\n    'Random joke',\n    use_cache := false,\n    seed := 999\n);\n\n-- Handle NULL results from failed generation\nSELECT COALESCE(\n    steadytext_generate('Generate text', seed := 100),\n    'Generation failed - please check daemon status'\n) AS result;\n\n-- Compare outputs with different seeds\nSELECT \n    'Seed 100' AS variant,\n    steadytext_generate('Explain machine learning', seed := 100) AS output\nUNION ALL\nSELECT \n    'Seed 200' AS variant,\n    steadytext_generate('Explain machine learning', seed := 200) AS output;\n</code></pre>"},{"location":"postgresql-extension/#steadytext_generate_stream","title":"<code>steadytext_generate_stream()</code>","text":"<p>Stream text generation for real-time applications (future feature).</p> <pre><code>steadytext_generate_stream(\n    prompt TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    seed INTEGER DEFAULT 42\n) RETURNS SETOF TEXT\n</code></pre>"},{"location":"postgresql-extension/#embeddings","title":"Embeddings","text":""},{"location":"postgresql-extension/#steadytext_embed","title":"<code>steadytext_embed()</code>","text":"<p>Generate 1024-dimensional L2-normalized embeddings for text.</p> <pre><code>steadytext_embed(\n    text_input TEXT,\n    use_cache BOOLEAN DEFAULT true,\n    seed INTEGER DEFAULT 42\n) RETURNS vector(1024)\n-- Returns NULL vector if embedding fails\n</code></pre> <p>Examples:</p> <pre><code>-- Simple embedding (uses default seed 42)\nSELECT steadytext_embed('PostgreSQL is a powerful database');\n\n-- Custom seed for reproducible embeddings\nSELECT steadytext_embed(\n    'artificial intelligence',\n    seed := 123\n);\n\n-- Handle NULL embeddings from failed generation\nSELECT \n    text,\n    CASE \n        WHEN steadytext_embed(text, seed := 42) IS NOT NULL \n        THEN 'Embedding generated'\n        ELSE 'Embedding failed'\n    END AS status\nFROM documents;\n\n-- Semantic similarity using pgvector with NULL handling\nWITH base_embedding AS (\n    SELECT steadytext_embed('machine learning', seed := 42) AS vector\n)\nSELECT \n    text,\n    embedding &lt;-&gt; (SELECT vector FROM base_embedding) AS distance\nFROM documents\nWHERE embedding IS NOT NULL \n    AND (SELECT vector FROM base_embedding) IS NOT NULL\nORDER BY distance\nLIMIT 5;\n\n-- Compare embeddings with different seeds (with NULL checks)\nSELECT \n    variant,\n    CASE \n        WHEN embedding IS NOT NULL THEN 'Generated'\n        ELSE 'Failed'\n    END AS status,\n    embedding\nFROM (\n    SELECT \n        'Default seed' AS variant,\n        steadytext_embed('AI technology') AS embedding\n    UNION ALL\n    SELECT \n        'Custom seed' AS variant,\n        steadytext_embed('AI technology', seed := 789) AS embedding\n) results;\n</code></pre>"},{"location":"postgresql-extension/#management-functions","title":"Management Functions","text":""},{"location":"postgresql-extension/#daemon-management","title":"Daemon Management","text":""},{"location":"postgresql-extension/#steadytext_daemon_start","title":"<code>steadytext_daemon_start()</code>","text":"<p>Start the SteadyText daemon for improved performance.</p> <pre><code>SELECT steadytext_daemon_start();\nSELECT steadytext_daemon_start('localhost', 5557); -- Custom host/port\n</code></pre>"},{"location":"postgresql-extension/#steadytext_daemon_status","title":"<code>steadytext_daemon_status()</code>","text":"<p>Check daemon health and status.</p> <pre><code>SELECT * FROM steadytext_daemon_status();\n-- Returns: running, pid, host, port, uptime, health\n</code></pre>"},{"location":"postgresql-extension/#steadytext_daemon_stop","title":"<code>steadytext_daemon_stop()</code>","text":"<p>Stop the daemon gracefully.</p> <pre><code>SELECT steadytext_daemon_stop();\nSELECT steadytext_daemon_stop(true); -- Force stop\n</code></pre>"},{"location":"postgresql-extension/#cache-management","title":"Cache Management","text":""},{"location":"postgresql-extension/#steadytext_cache_stats","title":"<code>steadytext_cache_stats()</code>","text":"<p>View cache performance statistics.</p> <pre><code>SELECT * FROM steadytext_cache_stats();\n-- Returns: entries, total_size_mb, hit_rate, evictions, oldest_entry\n</code></pre>"},{"location":"postgresql-extension/#steadytext_cache_clear","title":"<code>steadytext_cache_clear()</code>","text":"<p>Clear the cache for fresh results.</p> <pre><code>SELECT steadytext_cache_clear();                    -- Clear all\nSELECT steadytext_cache_clear('generation');        -- Clear generation cache only\nSELECT steadytext_cache_clear('embedding');         -- Clear embedding cache only\n</code></pre>"},{"location":"postgresql-extension/#configuration","title":"Configuration","text":""},{"location":"postgresql-extension/#steadytext_config_get-steadytext_config_set","title":"<code>steadytext_config_get()</code> / <code>steadytext_config_set()</code>","text":"<p>Manage extension configuration.</p> <pre><code>-- View all configuration\nSELECT * FROM steadytext_config;\n\n-- Get specific setting\nSELECT steadytext_config_get('default_max_tokens');\n\n-- Update settings\nSELECT steadytext_config_set('default_max_tokens', '1024');\nSELECT steadytext_config_set('cache_enabled', 'true');\nSELECT steadytext_config_set('daemon_host', 'localhost');\nSELECT steadytext_config_set('daemon_port', '5557');\nSELECT steadytext_config_set('default_seed', '42');\n</code></pre>"},{"location":"postgresql-extension/#database-schema","title":"Database Schema","text":"<p>The extension creates several tables to manage caching, configuration, and monitoring:</p>"},{"location":"postgresql-extension/#steadytext_cache","title":"<code>steadytext_cache</code>","text":"<p>Stores cached generation and embedding results with frecency metadata.</p> <pre><code>\\d steadytext_cache\n</code></pre> Column Type Description <code>key</code> TEXT Cache key (hash of input + parameters) <code>prompt</code> TEXT Original prompt text <code>result</code> TEXT Generated text result <code>embedding</code> vector(1024) Generated embedding vector <code>seed</code> INTEGER Seed used for generation <code>frequency</code> INTEGER Access frequency counter <code>last_access</code> TIMESTAMP Last access time <code>created_at</code> TIMESTAMP Creation timestamp"},{"location":"postgresql-extension/#steadytext_config","title":"<code>steadytext_config</code>","text":"<p>Extension configuration settings.</p> <pre><code>SELECT key, value, description FROM steadytext_config;\n</code></pre> Key Default Description <code>default_max_tokens</code> <code>512</code> Default maximum tokens to generate <code>cache_enabled</code> <code>true</code> Enable/disable caching <code>daemon_host</code> <code>localhost</code> Daemon server host <code>daemon_port</code> <code>5557</code> Daemon server port <code>default_seed</code> <code>42</code> Default seed for operations <code>use_fallback_model</code> <code>false</code> Use fallback model if primary fails <code>rate_limit_enabled</code> <code>false</code> Enable rate limiting <code>max_requests_per_minute</code> <code>60</code> Rate limit threshold"},{"location":"postgresql-extension/#steadytext_daemon_health","title":"<code>steadytext_daemon_health</code>","text":"<p>Daemon health monitoring and diagnostics.</p> <pre><code>SELECT * FROM steadytext_daemon_health ORDER BY checked_at DESC LIMIT 5;\n</code></pre>"},{"location":"postgresql-extension/#advanced-usage","title":"Advanced Usage","text":""},{"location":"postgresql-extension/#batch-operations","title":"Batch Operations","text":"<p>Process multiple prompts efficiently:</p> <pre><code>-- Batch generation with different seeds\nWITH prompts AS (\n    SELECT unnest(ARRAY[\n        'Explain quantum computing',\n        'Describe machine learning',\n        'What is artificial intelligence'\n    ]) AS prompt,\n    unnest(ARRAY[100, 200, 300]) AS seed\n)\nSELECT \n    prompt,\n    seed,\n    steadytext_generate(prompt, seed := seed) AS response\nFROM prompts;\n\n-- Batch embeddings for similarity analysis\nWITH texts AS (\n    SELECT unnest(ARRAY[\n        'artificial intelligence',\n        'machine learning',\n        'deep learning',\n        'neural networks'\n    ]) AS text\n)\nSELECT \n    text,\n    steadytext_embed(text, seed := 42) AS embedding\nFROM texts;\n</code></pre>"},{"location":"postgresql-extension/#semantic-search-implementation","title":"Semantic Search Implementation","text":"<p>Build a semantic search system using pg_steadytext:</p> <pre><code>-- Create a documents table with embeddings\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    title TEXT NOT NULL,\n    content TEXT NOT NULL,\n    embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Create index for fast similarity search\nCREATE INDEX ON documents USING ivfflat (embedding vector_cosine_ops)\nWITH (lists = 100);\n\n-- Populate with embeddings using consistent seed (skip failed embeddings)\nINSERT INTO documents (title, content, embedding)\nSELECT \n    title,\n    content,\n    embedding\nFROM (\n    SELECT \n        title,\n        content,\n        steadytext_embed(content, seed := 42) AS embedding\n    FROM source_documents\n) WITH_EMBEDDINGS\nWHERE embedding IS NOT NULL;\n\n-- Semantic search function\nCREATE OR REPLACE FUNCTION semantic_search(\n    query_text TEXT,\n    max_results INTEGER DEFAULT 5,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(id INTEGER, title TEXT, content TEXT, similarity REAL) AS $$\nDECLARE\n    query_embedding vector(1024);\nBEGIN\n    -- Generate query embedding with error handling\n    query_embedding := steadytext_embed(query_text, seed := search_seed);\n\n    -- Return empty result if embedding generation failed\n    IF query_embedding IS NULL THEN\n        RAISE WARNING 'Failed to generate embedding for query: %', query_text;\n        RETURN;\n    END IF;\n\n    RETURN QUERY\n    SELECT \n        d.id,\n        d.title,\n        d.content,\n        1 - (d.embedding &lt;=&gt; query_embedding) AS similarity\n    FROM documents d\n    WHERE d.embedding IS NOT NULL\n    ORDER BY d.embedding &lt;=&gt; query_embedding\n    LIMIT max_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage\nSELECT * FROM semantic_search('machine learning algorithms', 10);\n</code></pre>"},{"location":"postgresql-extension/#content-generation-pipeline","title":"Content Generation Pipeline","text":"<p>Create a content generation workflow:</p> <pre><code>-- Content generation pipeline with different styles\nCREATE OR REPLACE FUNCTION generate_content_variants(\n    base_prompt TEXT,\n    num_variants INTEGER DEFAULT 3\n)\nRETURNS TABLE(variant_id INTEGER, style TEXT, content TEXT) AS $$\nDECLARE\n    styles TEXT[] := ARRAY['formal', 'casual', 'technical'];\n    i INTEGER;\n    current_style TEXT;\n    enhanced_prompt TEXT;\nBEGIN\n    FOR i IN 1..LEAST(num_variants, array_length(styles, 1)) LOOP\n        current_style := styles[i];\n        enhanced_prompt := format('Write in a %s style: %s', current_style, base_prompt);\n\n        -- Generate content with error handling\n        DECLARE\n            generated_content TEXT;\n        BEGIN\n            generated_content := steadytext_generate(\n                enhanced_prompt,\n                max_tokens := 200,\n                seed := 100 + i  -- Different seed for each variant\n            );\n\n            -- Skip variants that failed to generate\n            IF generated_content IS NOT NULL THEN\n                RETURN QUERY\n                SELECT \n                    i AS variant_id,\n                    current_style AS style,\n                    generated_content AS content;\n            ELSE\n                RAISE WARNING 'Failed to generate content for style: %', current_style;\n            END IF;\n        END;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage\nSELECT * FROM generate_content_variants('Explain the benefits of PostgreSQL');\n</code></pre>"},{"location":"postgresql-extension/#performance-optimization","title":"Performance Optimization","text":""},{"location":"postgresql-extension/#cache-strategy","title":"Cache Strategy","text":"<pre><code>-- Monitor cache performance\nSELECT \n    'Generation Cache' AS cache_type,\n    entries,\n    total_size_mb,\n    hit_rate,\n    evictions\nFROM steadytext_cache_stats()\nWHERE cache_type = 'generation'\nUNION ALL\nSELECT \n    'Embedding Cache' AS cache_type,\n    entries,\n    total_size_mb,\n    hit_rate,\n    evictions\nFROM steadytext_cache_stats()\nWHERE cache_type = 'embedding';\n\n-- Optimize cache by pre-warming common queries\nWITH common_prompts AS (\n    SELECT unnest(ARRAY[\n        'Summarize the key points',\n        'Explain this concept',\n        'Generate a brief description'\n    ]) AS prompt\n)\nSELECT \n    prompt,\n    'Pre-warmed: ' || length(steadytext_generate(prompt)) || ' chars' AS status\nFROM common_prompts;\n</code></pre>"},{"location":"postgresql-extension/#daemon-performance","title":"Daemon Performance","text":"<pre><code>-- Check daemon performance metrics\nSELECT \n    running,\n    uptime,\n    health_score,\n    last_response_time_ms,\n    total_requests,\n    error_rate\nFROM steadytext_daemon_status();\n\n-- Restart daemon if performance degrades\nDO $$\nBEGIN\n    IF (SELECT health_score FROM steadytext_daemon_status()) &lt; 0.8 THEN\n        PERFORM steadytext_daemon_stop();\n        PERFORM pg_sleep(2);\n        PERFORM steadytext_daemon_start();\n        RAISE NOTICE 'Daemon restarted due to poor health score';\n    END IF;\nEND;\n$$;\n</code></pre>"},{"location":"postgresql-extension/#security-considerations","title":"Security Considerations","text":""},{"location":"postgresql-extension/#input-validation","title":"Input Validation","text":"<pre><code>-- Safe text generation with input validation and NULL handling\nCREATE OR REPLACE FUNCTION safe_generate(\n    user_prompt TEXT,\n    max_length INTEGER DEFAULT 512,\n    custom_seed INTEGER DEFAULT 42\n)\nRETURNS TEXT AS $$\nDECLARE\n    result TEXT;\nBEGIN\n    -- Validate input length\n    IF length(user_prompt) &gt; 1000 THEN\n        RAISE EXCEPTION 'Prompt too long (max 1000 characters)';\n    END IF;\n\n    -- Validate max_length\n    IF max_length &gt; 2048 OR max_length &lt; 1 THEN\n        RAISE EXCEPTION 'Invalid max_length (must be 1-2048)';\n    END IF;\n\n    -- Validate seed\n    IF custom_seed &lt; 0 THEN\n        RAISE EXCEPTION 'Seed must be non-negative';\n    END IF;\n\n    -- Sanitize prompt (basic example)\n    user_prompt := regexp_replace(user_prompt, '[&lt;&gt;]', '', 'g');\n\n    -- Generate with error handling\n    result := steadytext_generate(user_prompt, max_length, true, custom_seed);\n\n    -- Return error message if generation failed\n    IF result IS NULL THEN\n        RETURN '[Error: Text generation failed. Please check system status.]';\n    END IF;\n\n    RETURN result;\nEND;\n$$ LANGUAGE plpgsql SECURITY DEFINER;\n\n-- Grant limited access\nGRANT EXECUTE ON FUNCTION safe_generate TO app_user;\n</code></pre>"},{"location":"postgresql-extension/#rate-limiting","title":"Rate Limiting","text":"<pre><code>-- Enable rate limiting\nSELECT steadytext_config_set('rate_limit_enabled', 'true');\nSELECT steadytext_config_set('max_requests_per_minute', '30');\n\n-- Custom rate limiting per user\nCREATE TABLE user_rate_limits (\n    user_id INTEGER PRIMARY KEY,\n    requests_made INTEGER DEFAULT 0,\n    window_start TIMESTAMP DEFAULT NOW(),\n    max_requests INTEGER DEFAULT 10\n);\n\n-- Rate-limited generation function\nCREATE OR REPLACE FUNCTION rate_limited_generate(\n    user_id INTEGER,\n    prompt TEXT\n)\nRETURNS TEXT AS $$\nDECLARE\n    current_requests INTEGER;\n    window_start TIMESTAMP;\n    max_allowed INTEGER;\nBEGIN\n    -- Get or create rate limit record\n    INSERT INTO user_rate_limits (user_id)\n    VALUES (user_id)\n    ON CONFLICT (user_id) DO NOTHING;\n\n    -- Check current usage\n    SELECT requests_made, window_start, max_requests\n    INTO current_requests, window_start, max_allowed\n    FROM user_rate_limits\n    WHERE user_id = rate_limited_generate.user_id;\n\n    -- Reset window if expired (1 hour)\n    IF window_start &lt; NOW() - INTERVAL '1 hour' THEN\n        UPDATE user_rate_limits\n        SET requests_made = 0, window_start = NOW()\n        WHERE user_id = rate_limited_generate.user_id;\n        current_requests := 0;\n    END IF;\n\n    -- Check rate limit\n    IF current_requests &gt;= max_allowed THEN\n        RAISE EXCEPTION 'Rate limit exceeded. Try again later.';\n    END IF;\n\n    -- Increment counter\n    UPDATE user_rate_limits\n    SET requests_made = requests_made + 1\n    WHERE user_id = rate_limited_generate.user_id;\n\n    -- Generate text with error handling\n    DECLARE\n        result TEXT;\n    BEGIN\n        result := steadytext_generate(prompt, 512, true, 42);\n\n        IF result IS NULL THEN\n            RAISE EXCEPTION 'Text generation failed. Please try again later.';\n        END IF;\n\n        RETURN result;\n    END;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"postgresql-extension/#troubleshooting","title":"Troubleshooting","text":""},{"location":"postgresql-extension/#common-issues","title":"Common Issues","text":""},{"location":"postgresql-extension/#1-no-module-named-steadytext-error","title":"1. \"No module named 'steadytext'\" Error","text":"<p>This indicates PostgreSQL cannot find the SteadyText library:</p> <pre><code>-- Check Python environment\nDO $$\nBEGIN\n    RAISE NOTICE 'Python version: %', (SELECT version());\nEND;\n$$ LANGUAGE plpython3u;\n\n-- Manually initialize (if needed)\nSELECT _steadytext_init_python();\n\n-- Verify installation\nDO $$\nimport sys\nimport os\nplpy.notice(f\"Python path: {sys.path}\")\nplpy.notice(f\"Current user: {os.getenv('USER', 'unknown')}\")\ntry:\n    import steadytext\n    plpy.notice(f\"SteadyText version: {steadytext.__version__}\")\nexcept ImportError as e:\n    plpy.error(f\"SteadyText not available: {e}\")\n$$ LANGUAGE plpython3u;\n</code></pre> <p>Solution: <pre><code># Install SteadyText for the PostgreSQL Python environment\nsudo -u postgres pip3 install steadytext&gt;=2.1.0\n\n# Or reinstall the extension\nmake clean &amp;&amp; make install\n</code></pre></p>"},{"location":"postgresql-extension/#2-model-loading-errors","title":"2. Model Loading Errors","text":"<p>If functions return NULL due to model loading issues:</p> <pre><code>-- Check current model configuration\nSELECT steadytext_config_get('use_fallback_model');\n\n-- Enable fallback model\nSELECT steadytext_config_set('use_fallback_model', 'true');\n\n-- Test generation (will return NULL if still failing)\nSELECT \n    CASE \n        WHEN steadytext_generate('Test model loading') IS NOT NULL \n        THEN 'Model working'\n        ELSE 'Model still failing - check daemon status'\n    END AS status;\n</code></pre> <p>Environment Solution: <pre><code># Set fallback model environment variable\nexport STEADYTEXT_USE_FALLBACK_MODEL=true\n\n# Restart PostgreSQL\nsudo systemctl restart postgresql\n</code></pre></p>"},{"location":"postgresql-extension/#3-daemon-connection-issues","title":"3. Daemon Connection Issues","text":"<pre><code>-- Check daemon status\nSELECT * FROM steadytext_daemon_status();\n\n-- Restart daemon with custom settings\nSELECT steadytext_daemon_stop();\nSELECT steadytext_config_set('daemon_host', 'localhost');\nSELECT steadytext_config_set('daemon_port', '5557');\nSELECT steadytext_daemon_start();\n\n-- Test daemon connectivity\nSELECT steadytext_generate('Test daemon connection');\n</code></pre>"},{"location":"postgresql-extension/#4-null-returns-and-error-handling","title":"4. NULL Returns and Error Handling","text":"<pre><code>-- Check if functions are returning NULL\nSELECT \n    'Generation test' AS test_type,\n    CASE \n        WHEN steadytext_generate('Test prompt') IS NOT NULL \n        THEN 'Working'\n        ELSE 'Returning NULL - check daemon'\n    END AS status\nUNION ALL\nSELECT \n    'Embedding test' AS test_type,\n    CASE \n        WHEN steadytext_embed('Test text') IS NOT NULL \n        THEN 'Working'\n        ELSE 'Returning NULL - check daemon'\n    END AS status;\n\n-- Application-level NULL handling pattern\nCREATE OR REPLACE FUNCTION robust_generate(\n    prompt TEXT,\n    retry_count INTEGER DEFAULT 3\n)\nRETURNS TEXT AS $$\nDECLARE\n    result TEXT;\n    i INTEGER;\nBEGIN\n    FOR i IN 1..retry_count LOOP\n        result := steadytext_generate(prompt);\n        IF result IS NOT NULL THEN\n            RETURN result;\n        END IF;\n\n        -- Wait before retry\n        PERFORM pg_sleep(1);\n    END LOOP;\n\n    -- All retries failed\n    RETURN NULL;\nEND;\n$$ LANGUAGE plpgsql;\n</code></pre>"},{"location":"postgresql-extension/#5-cache-performance-issues","title":"5. Cache Performance Issues","text":"<pre><code>-- Monitor cache statistics\nSELECT * FROM steadytext_cache_stats();\n\n-- Clear cache if needed\nSELECT steadytext_cache_clear();\n\n-- Adjust cache settings\nSELECT steadytext_config_set('cache_capacity', '1000');\nSELECT steadytext_config_set('cache_max_size_mb', '200');\n</code></pre>"},{"location":"postgresql-extension/#debugging-mode","title":"Debugging Mode","text":"<p>Enable verbose logging for troubleshooting:</p> <pre><code>-- Enable PostgreSQL notices\nSET client_min_messages TO NOTICE;\n\n-- Test with debug output and NULL checking\nSELECT \n    'Debug test' AS test_name,\n    steadytext_generate('Debug test', max_tokens := 10) AS result,\n    CASE \n        WHEN steadytext_generate('Debug test', max_tokens := 10) IS NULL \n        THEN 'Generation failed - check notices above'\n        ELSE 'Generation successful'\n    END AS status;\n\n-- Check daemon health\nSELECT * FROM steadytext_daemon_status();\n\n-- Check recent health history\nSELECT * FROM steadytext_daemon_health ORDER BY last_heartbeat DESC LIMIT 10;\n</code></pre>"},{"location":"postgresql-extension/#version-compatibility","title":"Version Compatibility","text":"PostgreSQL Python SteadyText Status 14+ 3.8+ 2.1.0+ \u2705 Fully Supported 13 3.8+ 2.1.0+ \u26a0\ufe0f Limited Testing 12 3.7+ 2.0.0+ \u274c Not Recommended"},{"location":"postgresql-extension/#migration-guide","title":"Migration Guide","text":""},{"location":"postgresql-extension/#upgrading-from-v100","title":"Upgrading from v1.0.0","text":"<ol> <li> <p>Update Dependencies: <pre><code>pip3 install --upgrade steadytext&gt;=2.1.0\n</code></pre></p> </li> <li> <p>Update Extension: <pre><code>ALTER EXTENSION pg_steadytext UPDATE TO '1.1.0';\n</code></pre></p> </li> <li> <p>Update Function Calls and Error Handling: <pre><code>-- Old (v1.0.0) - returned fallback text on errors\nSELECT steadytext_generate('prompt', 512, true);\n\n-- New (v1.1.0+) - with seed support and NULL returns on errors\nSELECT steadytext_generate('prompt', max_tokens := 512, seed := 42);\n\n-- Application code should now handle NULL returns\nSELECT \n    COALESCE(\n        steadytext_generate('prompt', max_tokens := 512, seed := 42),\n        'Error: Generation failed'\n    ) AS result;\n</code></pre></p> </li> </ol>"},{"location":"postgresql-extension/#contributing","title":"Contributing","text":"<p>The pg_steadytext extension is part of the main SteadyText project. Contributions are welcome!</p> <ul> <li>GitHub Repository: https://github.com/julep-ai/steadytext</li> <li>Issues: https://github.com/julep-ai/steadytext/issues</li> <li>Extension Directory: <code>pg_steadytext/</code></li> </ul>"},{"location":"postgresql-extension/#license","title":"License","text":"<p>This extension is released under the PostgreSQL License, consistent with the main SteadyText project.</p> <p>Need Help? Check the main SteadyText documentation or open an issue on GitHub.</p>"},{"location":"quick-start/","title":"Quick Start Guide","text":"<p>Get started with SteadyText in minutes. Learn how to use custom seeds for reproducible AI generation.</p>"},{"location":"quick-start/#installation","title":"Installation","text":"pipuvPoetry <pre><code>pip install steadytext\n</code></pre> <pre><code>uv add steadytext\n</code></pre> <pre><code>poetry add steadytext\n</code></pre>"},{"location":"quick-start/#first-steps","title":"First Steps","text":""},{"location":"quick-start/#1-basic-text-generation","title":"1. Basic Text Generation","text":"<pre><code>import steadytext\n\n# Generate deterministic text (always same result)\ntext = steadytext.generate(\"Write a Python function to calculate fibonacci\")\nprint(text)\n\n# Use custom seed for different but reproducible results\ntext1 = steadytext.generate(\"Write a Python function\", seed=123)\ntext2 = steadytext.generate(\"Write a Python function\", seed=123)  # Same as text1\ntext3 = steadytext.generate(\"Write a Python function\", seed=456)  # Different result\n\nprint(f\"Same seed results identical: {text1 == text2}\")  # True\nprint(f\"Different seeds produce different output: {text1 != text3}\")  # True\n</code></pre>"},{"location":"quick-start/#2-streaming-generation","title":"2. Streaming Generation","text":"<p>For real-time output:</p> <pre><code># Default streaming\nfor token in steadytext.generate_iter(\"Explain machine learning\"):\n    print(token, end=\"\", flush=True)\n\n# Streaming with custom seed for reproducible streams\nprint(\"\\nStream 1 (seed 789):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=789):\n    print(token, end=\"\", flush=True)\n\nprint(\"\\nStream 2 (same seed - identical result):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=789):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"quick-start/#3-create-embeddings","title":"3. Create Embeddings","text":"<pre><code># Single text (deterministic)\nvector = steadytext.embed(\"Hello world\")\nprint(f\"Embedding shape: {vector.shape}\")  # (1024,)\n\n# Multiple texts (returns a single, averaged embedding)\nvector = steadytext.embed([\"Hello\", \"world\", \"AI\"])\n\n# Custom seeds for different embedding variations\nvec1 = steadytext.embed(\"artificial intelligence\", seed=100)\nvec2 = steadytext.embed(\"artificial intelligence\", seed=100)  # Identical\nvec3 = steadytext.embed(\"artificial intelligence\", seed=200)  # Different\n\nimport numpy as np\nprint(f\"Same seed embeddings equal: {np.array_equal(vec1, vec2)}\")  # True\nprint(f\"Different seed similarity: {np.dot(vec1, vec3):.3f}\")  # Cosine similarity\n</code></pre>"},{"location":"quick-start/#command-line-usage","title":"Command Line Usage","text":"<p>SteadyText includes both <code>steadytext</code> and <code>st</code> commands:</p> <pre><code># Generate text (deterministic)\nst generate \"write a haiku about programming\"\n\n# Generate with custom seed for reproducible variations\nst generate \"write a haiku about programming\" --seed 123\nst generate \"write a haiku about programming\" --seed 456  # Different result\n\n# Stream generation with seed\necho \"explain quantum computing\" | st --seed 789\n\n# Create embeddings with custom seed\nst embed \"machine learning concepts\" --seed 100\n\n# JSON output with metadata\nst generate \"list 3 colors\" --json --seed 555\n\n# Control output length\nst generate \"explain AI\" --max-new-tokens 100 --seed 42\n\n# Vector operations with seeds\nst vector similarity \"cat\" \"dog\" --seed 777\n\n# Preload models (optional)\nst models --preload\n</code></pre>"},{"location":"quick-start/#model-management","title":"Model Management","text":"<p>Models are automatically downloaded on first use to:</p> <ul> <li>Linux/Mac: <code>~/.cache/steadytext/models/</code></li> <li>Windows: <code>%LOCALAPPDATA%\\steadytext\\steadytext\\models\\</code></li> </ul> <pre><code># Check where models are stored\ncache_dir = steadytext.get_model_cache_dir()\nprint(f\"Models stored at: {cache_dir}\")\n\n# Preload models manually (optional)\nsteadytext.preload_models(verbose=True)\n</code></pre>"},{"location":"quick-start/#configuration","title":"Configuration","text":"<p>Control caching and behavior via environment variables:</p> <pre><code># Generation cache settings\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=512\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=100\n\n# Embedding cache settings  \nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=1024\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=200\n\n# Model compatibility settings\nexport STEADYTEXT_USE_FALLBACK_MODEL=true  # Use compatible models\n\n# Default seed (optional)\nexport STEADYTEXT_DEFAULT_SEED=42\n</code></pre>"},{"location":"quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"quick-start/#reproducible-research","title":"Reproducible Research","text":"<pre><code># Document your seeds for reproducibility\nRESEARCH_SEED = 42\n\nresults = []\nfor prompt in research_prompts:\n    result = steadytext.generate(prompt, seed=RESEARCH_SEED)\n    results.append(result)\n    RESEARCH_SEED += 1  # Increment for each generation\n</code></pre>"},{"location":"quick-start/#ab-testing","title":"A/B Testing","text":"<pre><code># Generate content variations\nprompt = \"Write a product description\"\nvariant_a = steadytext.generate(prompt, seed=100)  # Version A\nvariant_b = steadytext.generate(prompt, seed=200)  # Version B\n\n# Test which performs better\nprint(f\"Variant A: {variant_a[:100]}...\")\nprint(f\"Variant B: {variant_b[:100]}...\")\n</code></pre>"},{"location":"quick-start/#content-variations","title":"Content Variations","text":"<pre><code># Generate multiple versions for testing\nbase_prompt = \"Explain machine learning\"\nvariations = []\n\nfor i, style_seed in enumerate([300, 400, 500], 1):\n    variation = steadytext.generate(base_prompt, seed=style_seed)\n    variations.append(f\"Version {i}: {variation}\")\n\nfor variation in variations:\n    print(variation[:80] + \"...\\n\")\n</code></pre>"},{"location":"quick-start/#postgresql-integration","title":"PostgreSQL Integration","text":"<p>SteadyText now includes a PostgreSQL extension:</p> <pre><code># Install the PostgreSQL extension\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\nmake &amp;&amp; sudo make install\n\n# Enable in PostgreSQL\npsql -c \"CREATE EXTENSION pg_steadytext CASCADE;\"\n</code></pre> <pre><code>-- Use in SQL queries\nSELECT steadytext_generate('Write a product description', max_tokens := 200, seed := 123);\n\n-- Generate embeddings\nSELECT steadytext_embed('machine learning', seed := 456);\n\n-- Semantic search with pgvector\nSELECT title, content &lt;-&gt; steadytext_embed('AI technology') AS distance\nFROM documents\nORDER BY distance\nLIMIT 5;\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete function documentation with seed parameters</li> <li>Custom Seeds Guide - Comprehensive seed usage examples</li> <li>PostgreSQL Integration - Complete PostgreSQL extension guide</li> <li>CLI Reference - Command-line interface with <code>--seed</code> flag details</li> <li>Examples - Real-world usage patterns</li> </ul>"},{"location":"quick-start/#need-help","title":"Need Help?","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"structured-generation/","title":"Structured Generation","text":"<p>SteadyText v2.3.0 introduces powerful structured generation capabilities, allowing you to force the model's output to conform to a specific format. This is useful for a wide range of applications, from data extraction to building reliable applications on top of language models.</p> <p>This feature is powered by the Outlines library.</p>"},{"location":"structured-generation/#how-it-works","title":"How it Works","text":"<p>Structured generation is enabled by passing one of the following parameters to the <code>steadytext.generate</code> function:</p> <ul> <li><code>schema</code>: For generating JSON that conforms to a JSON schema, a Pydantic model, or a basic Python type.</li> <li><code>regex</code>: For generating text that matches a regular expression.</li> <li><code>choices</code>: For generating text that is one of a list of choices.</li> </ul> <p>When one of these parameters is provided, SteadyText will use Outlines to guide the generation process, ensuring that the output is always valid according to the specified format.</p>"},{"location":"structured-generation/#json-generation","title":"JSON Generation","text":"<p>You can generate JSON output in several ways.</p>"},{"location":"structured-generation/#with-a-json-schema","title":"With a JSON Schema","text":"<p>Pass a dictionary representing a JSON schema to the <code>schema</code> parameter.</p> <pre><code>import steadytext\nimport json\n\nschema = {\n    \"type\": \"object\",\n    \"properties\": {\n        \"name\": {\"type\": \"string\"},\n        \"age\": {\"type\": \"integer\"},\n    },\n    \"required\": [\"name\", \"age\"],\n}\n\nresult = steadytext.generate(\"Create a user named Alice, age 42\", schema=schema)\n\n# The result will contain a JSON object wrapped in &lt;json-output&gt; tags\n# &lt;json-output&gt;{\"name\": \"Alice\", \"age\": 42}&lt;/json-output&gt;\n\njson_string = result.split('&lt;json-output&gt;')[1].split('&lt;/json-output&gt;')[0]\ndata = json.loads(json_string)\n\nassert data['name'] == \"Alice\"\nassert data['age'] == 42\n</code></pre>"},{"location":"structured-generation/#with-a-pydantic-model","title":"With a Pydantic Model","text":"<p>You can also use a Pydantic model to define the structure of the JSON output.</p> <pre><code>import steadytext\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresult = steadytext.generate(\"Create a user named Bob, age 30\", schema=User)\n</code></pre>"},{"location":"structured-generation/#with-generate_json","title":"With <code>generate_json</code>","text":"<p>The <code>generate_json</code> convenience function can also be used.</p> <pre><code>import steadytext\nfrom pydantic import BaseModel\n\nclass User(BaseModel):\n    name: str\n    age: int\n\nresult = steadytext.generate_json(\"Create a user named Charlie, age 25\", schema=User)\n</code></pre>"},{"location":"structured-generation/#regex-constrained-generation","title":"Regex-Constrained Generation","text":"<p>Generate text that matches a regular expression using the <code>regex</code> parameter.</p> <pre><code>import steadytext\n\n# Generate a phone number\nphone_number = steadytext.generate(\n    \"The support number is: \",\n    regex=r\"\\d{3}-\\d{3}-\\d{4}\"\n)\nprint(phone_number)\n# Output: 123-456-7890\n\n# Generate a valid email address\nemail = steadytext.generate(\n    \"Contact email: \",\n    regex=r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\"\n)\nprint(email)\n# Output: example@domain.com\n</code></pre> <p>You can also use the <code>generate_regex</code> convenience function.</p> <pre><code>import steadytext\n\n# Generate a date\ndate = steadytext.generate_regex(\n    \"Today's date is: \",\n    pattern=r\"\\d{4}-\\d{2}-\\d{2}\"\n)\nprint(date)\n# Output: 2025-07-03\n</code></pre>"},{"location":"structured-generation/#multiple-choice","title":"Multiple Choice","text":"<p>Force the model to choose from a list of options using the <code>choices</code> parameter.</p> <pre><code>import steadytext\n\nsentiment = steadytext.generate(\n    \"The movie was fantastic!\",\n    choices=[\"positive\", \"negative\", \"neutral\"]\n)\nprint(sentiment)\n# Output: positive\n</code></pre> <p>The <code>generate_choice</code> convenience function is also available.</p> <pre><code>import steadytext\n\nanswer = steadytext.generate_choice(\n    \"Is Python a statically typed language?\",\n    choices=[\"Yes\", \"No\"]\n)\nprint(answer)\n# Output: No\n</code></pre>"},{"location":"structured-generation/#type-constrained-generation","title":"Type-Constrained Generation","text":"<p>You can also constrain the output to a specific Python type using the <code>generate_format</code> function.</p> <pre><code>import steadytext\n\n# Generate an integer\nquantity = steadytext.generate_format(\"Number of items: \", int)\nprint(quantity)\n# Output: 5\n\n# Generate a boolean\nis_active = steadytext.generate_format(\"Is the user active? \", bool)\nprint(is_active)\n# Output: True\n</code></pre>"},{"location":"version_history/","title":"Version History","text":"<p>This document outlines the major versions of SteadyText and the key features introduced in each.</p> <p>Latest Version: 2.3.0 - Structured Generation</p> Version Key Features Default Generation Model Default Embedding Model Python Versions 2.3.x - Structured Generation: Added support for JSON, Regex, and Choice-constrained generation via <code>outlines</code>.- New API parameters: <code>schema</code>, <code>regex</code>, <code>choices</code> added to <code>generate()</code>.- New convenience functions: <code>generate_json()</code>, <code>generate_regex()</code>, <code>generate_choice()</code>. <code>ggml-org/gemma-3n-E2B-it-GGUF</code> (gemma-3n-E2B-it-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) <code>&gt;=3.10, &lt;3.14</code> 2.1.x - Custom Seeds: Added seed parameter to all generation and embedding functions.- PostgreSQL Extension: Released pg_steadytext extension.- Enhanced Reproducibility: Full control over deterministic generation. <code>ggml-org/gemma-3n-E2B-it-GGUF</code> (gemma-3n-E2B-it-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) <code>&gt;=3.10, &lt;3.14</code> 2.0.x - Daemon Mode: Persistent model serving with ZeroMQ.- Gemma-3n Models: Switched to <code>gemma-3n</code> for generation.- Thinking Mode Deprecated: Removed thinking mode. <code>ggml-org/gemma-3n-E2B-it-GGUF</code> (gemma-3n-E2B-it-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) <code>&gt;=3.10, &lt;3.14</code> 1.x - Model Switching: Added support for switching models via environment variables and a model registry.- Qwen3 Models: Switched to <code>qwen3-1.7b</code> for generation.- Indexing: Added support for FAISS indexing. <code>Qwen/Qwen3-1.7B-GGUF</code> (Qwen3-1.7B-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) <code>&gt;=3.10, &lt;3.14</code> 0.x - Initial Release: Deterministic text generation and embedding. <code>Qwen/Qwen1.5-0.5B-Chat-GGUF</code> (qwen1_5-0_5b-chat-q4_k_m.gguf) <code>Qwen/Qwen1.5-0.5B-Chat-GGUF</code> (qwen1_5-0_5b-chat-q8_0.gguf) <code>&gt;=3.10</code>"},{"location":"version_history/#detailed-release-notes","title":"Detailed Release Notes","text":""},{"location":"version_history/#version-230-structured-generation","title":"Version 2.3.0 - Structured Generation","text":"<p>Release Date: July 2025</p>"},{"location":"version_history/#structured-generation","title":"\u2728 Structured Generation","text":"<p>Major Feature: Introduced structured generation capabilities powered by the Outlines library.</p> <ul> <li>JSON Generation: Generate JSON that conforms to a JSON schema or a Pydantic model.</li> <li><code>steadytext.generate(prompt, schema=MyPydanticModel)</code></li> <li><code>steadytext.generate_json(prompt, schema={\"type\": \"object\", ...})</code></li> <li>Regex-Constrained Generation: Force output to match a regular expression.</li> <li><code>steadytext.generate(prompt, regex=r\"\\d{3}-\\d{3}-\\d{4}\")</code></li> <li>Multiple Choice: Force model to choose from a list of options.</li> <li><code>steadytext.generate(prompt, choices=[\"A\", \"B\", \"C\"])</code></li> </ul> <p>Use Cases: - Reliable data extraction - Building robust function-calling systems - Creating predictable application logic - Generating structured data for databases</p>"},{"location":"version_history/#version-210-custom-seeds-postgresql-extension","title":"Version 2.1.0+ - Custom Seeds &amp; PostgreSQL Extension","text":"<p>Release Date: June 2025</p>"},{"location":"version_history/#custom-seed-support","title":"\ud83c\udfaf Custom Seed Support","text":"<p>Major Enhancement: Added comprehensive custom seed support across all SteadyText APIs.</p> <ul> <li>Python API: All functions now accept optional <code>seed: int = DEFAULT_SEED</code> parameter</li> <li><code>steadytext.generate(prompt, seed=123)</code></li> <li><code>steadytext.generate_iter(prompt, seed=456)</code></li> <li> <p><code>steadytext.embed(text, seed=789)</code></p> </li> <li> <p>CLI Support: Added <code>--seed</code> flag to all commands</p> </li> <li><code>st generate \"prompt\" --seed 123</code></li> <li><code>st embed \"text\" --seed 456</code></li> <li> <p><code>st vector similarity \"text1\" \"text2\" --seed 789</code></p> </li> <li> <p>Daemon Integration: Seeds are properly passed through daemon protocol</p> </li> <li>Fallback Behavior: Deterministic fallbacks now respect custom seeds</li> <li>Cache Keys: Seeds are included in cache keys to prevent collisions</li> </ul> <p>Use Cases: - Reproducible Research: Document and reproduce exact results - A/B Testing: Generate controlled variations of content - Experimental Design: Systematic exploration of model behavior - Content Variations: Create different versions while maintaining quality</p>"},{"location":"version_history/#postgresql-extension-pg_steadytext","title":"\ud83d\udc18 PostgreSQL Extension (pg_steadytext)","text":"<p>New Release: Complete PostgreSQL extension for SteadyText integration.</p> <p>Core Features: - SQL Functions: Native PostgreSQL functions for text generation and embeddings   - <code>steadytext_generate(prompt, max_tokens, use_cache, seed)</code>   - <code>steadytext_embed(text, use_cache, seed)</code>   - <code>steadytext_daemon_start()</code>, <code>steadytext_daemon_status()</code>, <code>steadytext_daemon_stop()</code></p> <ul> <li>Vector Integration: Full compatibility with pgvector extension</li> <li>Built-in Caching: PostgreSQL-based frecency cache with eviction</li> <li>Daemon Support: Integrates with SteadyText's ZeroMQ daemon for performance</li> <li>Configuration Management: SQL-based configuration with <code>steadytext_config</code> table</li> </ul> <p>Installation: <pre><code># Install Python dependencies\npip3 install steadytext&gt;=2.1.0\n\n# Build and install extension\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext/pg_steadytext\nmake &amp;&amp; sudo make install\n\n# Enable in PostgreSQL\npsql -c \"CREATE EXTENSION pg_steadytext CASCADE;\"\n</code></pre></p> <p>Docker Support: <pre><code># Standard build\ndocker build -t pg_steadytext .\n\n# With fallback model for compatibility\ndocker build --build-arg STEADYTEXT_USE_FALLBACK_MODEL=true -t pg_steadytext .\n</code></pre></p>"},{"location":"version_history/#technical-improvements","title":"\ud83d\udd27 Technical Improvements","text":"<ul> <li>Validation: Added <code>validate_seed()</code> function for input validation</li> <li>Environment Setup: Enhanced <code>set_deterministic_environment()</code> with custom seeds</li> <li>Error Handling: Improved error messages and fallback behavior</li> <li>Documentation: Comprehensive documentation and examples</li> </ul>"},{"location":"version_history/#documentation-updates","title":"\ud83d\udcd6 Documentation Updates","text":"<ul> <li>API Documentation: Updated all function signatures with seed parameters</li> <li>CLI Reference: Added <code>--seed</code> flag documentation for all commands</li> <li>Examples: New comprehensive examples for custom seed usage</li> <li>PostgreSQL Guide: Complete integration guide for pg_steadytext</li> <li>Migration Guide: Instructions for upgrading from previous versions</li> </ul>"},{"location":"version_history/#breaking-changes","title":"\ud83d\udd04 Breaking Changes","text":"<p>None - Version 2.1.0+ is fully backward compatible with 2.0.x. All existing code continues to work unchanged, with new seed parameters being optional.</p>"},{"location":"version_history/#bug-fixes","title":"\ud83d\udc1b Bug Fixes","text":"<ul> <li>Fixed cache key generation to include seed for proper isolation</li> <li>Improved daemon protocol to handle seed parameters correctly</li> <li>Enhanced fallback behavior to be deterministic with custom seeds</li> <li>Resolved edge cases in streaming generation with custom seeds</li> </ul>"},{"location":"version_history/#requirements","title":"\ud83d\udccb Requirements","text":"<ul> <li>Python: 3.10+ (unchanged)</li> <li>PostgreSQL: 14+ (for pg_steadytext extension)</li> <li>Dependencies: All existing dependencies remain compatible</li> </ul>"},{"location":"api/","title":"API Reference","text":"<p>Complete documentation for SteadyText's Python API and command-line interface.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>SteadyText provides a simple, consistent API for deterministic AI operations:</p> <ul> <li><code>generate()</code> - Deterministic text generation with customizable seeds</li> <li><code>generate_iter()</code> - Streaming text generation with token-by-token output</li> <li><code>embed()</code> - Deterministic embeddings for semantic search and similarity</li> <li><code>preload_models()</code> - Pre-load models for better performance</li> <li>Daemon mode - Persistent model serving for 160x faster responses</li> <li>CLI tools - Command-line interface for all operations</li> </ul> <p>All functions are designed to never fail - they return deterministic fallbacks when models can't be loaded.</p>"},{"location":"api/#quick-reference","title":"Quick Reference","text":"<pre><code>import steadytext\n\n# Text generation with custom seed\ntext = steadytext.generate(\"your prompt\", seed=42)\ntext = steadytext.generate(\"your prompt\", seed=123)  # Different output\n\n# Return log probabilities\ntext, logprobs = steadytext.generate(\"prompt\", return_logprobs=True)\n\n# Streaming generation with custom seed\nfor token in steadytext.generate_iter(\"prompt\", seed=456):\n    print(token, end=\"\", flush=True)\n\n# Embeddings with custom seed\nvector = steadytext.embed(\"text to embed\", seed=789)\nvectors = steadytext.embed([\"multiple\", \"texts\"], seed=789)\n\n# Model management\nsteadytext.preload_models(verbose=True)\ncache_dir = steadytext.get_model_cache_dir()\n\n# Daemon usage (for better performance)\nfrom steadytext.daemon import use_daemon\nwith use_daemon():\n    text = steadytext.generate(\"fast generation\")\n    vec = steadytext.embed(\"fast embedding\")\n\n# Cache management\nfrom steadytext import get_cache_manager\ncache_manager = get_cache_manager()\nstats = cache_manager.get_cache_stats()\ncache_manager.clear_all_caches()\n</code></pre>"},{"location":"api/#detailed-documentation","title":"Detailed Documentation","text":""},{"location":"api/#core-apis","title":"Core APIs","text":"<ul> <li>Text Generation - Complete guide to <code>generate()</code> and <code>generate_iter()</code></li> <li>Basic usage and parameters</li> <li>Custom seed support for variations</li> <li>Streaming generation</li> <li>Advanced patterns and pipelines</li> <li>Error handling and edge cases</li> <li> <p>Integration examples</p> </li> <li> <p>Embeddings - Complete guide to <code>embed()</code> function</p> </li> <li>Creating embeddings with seeds</li> <li>Vector operations and similarity</li> <li>Batch processing</li> <li>Advanced use cases</li> <li>Performance optimization</li> </ul>"},{"location":"api/#command-line-interface","title":"Command Line Interface","text":"<ul> <li>CLI Reference - Complete command-line documentation</li> <li>Text generation commands</li> <li>Embedding operations</li> <li>Model management</li> <li>Daemon control</li> <li>Index operations</li> <li> <p>Real-world examples</p> </li> <li> <p>Vector Operations - Vector math and operations</p> </li> <li>Similarity calculations</li> <li>Distance metrics</li> <li>Vector arithmetic</li> <li>Search operations</li> </ul>"},{"location":"api/#api-signatures","title":"API Signatures","text":""},{"location":"api/#text-generation","title":"Text Generation","text":"<pre><code>def generate(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\",\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED,\n    schema: Optional[Union[Dict[str, Any], type, object]] = None,\n    regex: Optional[str] = None,\n    choices: Optional[List[str]] = None,\n    response_format: Optional[Dict[str, Any]] = None,\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre>"},{"location":"api/#streaming-generation","title":"Streaming Generation","text":"<pre><code>def generate_iter(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False,\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre>"},{"location":"api/#embeddings","title":"Embeddings","text":"<pre><code>def embed(\n    text_input: Union[str, List[str]], \n    seed: int = DEFAULT_SEED\n) -&gt; np.ndarray\n</code></pre>"},{"location":"api/#utilities","title":"Utilities","text":"<pre><code>def preload_models(verbose: bool = False) -&gt; None\ndef get_model_cache_dir() -&gt; Path\ndef get_cache_manager() -&gt; CacheManager\n</code></pre>"},{"location":"api/#constants","title":"Constants","text":""},{"location":"api/#core-constants","title":"Core Constants","text":"<pre><code>steadytext.DEFAULT_SEED = 42              # Default seed for all operations\nsteadytext.GENERATION_MAX_NEW_TOKENS = 512  # Default max tokens for generation\nsteadytext.EMBEDDING_DIMENSION = 1024      # Embedding vector dimensions\n</code></pre>"},{"location":"api/#model-constants","title":"Model Constants","text":"<pre><code># Current models (v2.0.0+)\nDEFAULT_GENERATION_MODEL = \"gemma-3n-2b\"\nDEFAULT_EMBEDDING_MODEL = \"qwen3-embedding\"\n\n# Model sizes\nMODEL_SIZES = {\n    \"small\": \"gemma-3n-2b\",  # 2.0GB\n    \"large\": \"gemma-3n-4b\"   # 4.2GB\n}\n</code></pre>"},{"location":"api/#environment-variables","title":"Environment Variables","text":""},{"location":"api/#cache-configuration","title":"Cache Configuration","text":"<pre><code># Cache backend selection (sqlite, d1, memory)\nSTEADYTEXT_CACHE_BACKEND=sqlite  # Default\n\n# Generation cache settings\nSTEADYTEXT_GENERATION_CACHE_CAPACITY=256      # Maximum cache entries\nSTEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50.0  # Maximum cache file size\n\n# Embedding cache settings\nSTEADYTEXT_EMBEDDING_CACHE_CAPACITY=512       # Maximum cache entries\nSTEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100.0  # Maximum cache file size\n\n# D1 backend configuration (when CACHE_BACKEND=d1)\nSTEADYTEXT_D1_API_URL=https://your-worker.workers.dev\nSTEADYTEXT_D1_API_KEY=your-api-key\nSTEADYTEXT_D1_BATCH_SIZE=50\n\n# Disable caching entirely (not recommended)\nSTEADYTEXT_DISABLE_CACHE=1\n</code></pre> <p>For detailed cache backend documentation, see Cache Backends Guide.</p>"},{"location":"api/#daemon-configuration","title":"Daemon Configuration","text":"<pre><code># Disable daemon usage globally\nSTEADYTEXT_DISABLE_DAEMON=1\n\n# Custom daemon settings\nSTEADYTEXT_DAEMON_HOST=127.0.0.1\nSTEADYTEXT_DAEMON_PORT=5557\n</code></pre>"},{"location":"api/#developmenttesting","title":"Development/Testing","text":"<pre><code># Allow model downloads (for testing)\nSTEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Use fallback models for compatibility\nSTEADYTEXT_USE_FALLBACK_MODEL=true\n\n# Set default seed globally\nSTEADYTEXT_DEFAULT_SEED=42\n\n# Python hash seed (for reproducibility)\nPYTHONHASHSEED=0\n</code></pre>"},{"location":"api/#model-paths","title":"Model Paths","text":"<pre><code># Custom model cache directory\nSTEADYTEXT_MODEL_DIR=/path/to/models\n\n# Skip model verification\nSTEADYTEXT_SKIP_MODEL_VERIFICATION=1\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>SteadyText uses a \"never fail\" design philosophy with v2.1.0+ updates:</p> <p>Deterministic Behavior</p> <ul> <li>Text generation: Returns <code>None</code> when models unavailable (v2.1.0+)</li> <li>Embeddings: Returns <code>None</code> when models unavailable (v2.1.0+)</li> <li>Streaming: Returns empty iterator when models unavailable</li> <li>No exceptions: Functions handle errors gracefully</li> <li>Seed support: All fallbacks respect custom seeds</li> </ul> <p>Breaking Changes in v2.1.0</p> <p>The deterministic fallback behavior has been disabled. Functions now return <code>None</code> instead of generating fallback text/embeddings when models are unavailable.</p>"},{"location":"api/#thread-safety","title":"Thread Safety","text":"<p>All functions are thread-safe and support concurrent usage:</p> <ul> <li>Singleton models: Models loaded once with thread-safe locks</li> <li>Thread-safe caches: All caches use proper locking mechanisms</li> <li>Concurrent calls: Multiple threads can call functions simultaneously</li> <li>Daemon mode: ZeroMQ handles concurrent requests automatically</li> </ul> <p>Example of concurrent usage:</p> <pre><code>import concurrent.futures\nimport steadytext\n\ndef process_prompt(prompt, seed):\n    return steadytext.generate(prompt, seed=seed)\n\n# Process multiple prompts concurrently\nwith concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n    prompts = [\"prompt1\", \"prompt2\", \"prompt3\", \"prompt4\"]\n    seeds = [100, 200, 300, 400]\n\n    futures = [executor.submit(process_prompt, p, s) \n               for p, s in zip(prompts, seeds)]\n\n    results = [f.result() for f in futures]\n</code></pre>"},{"location":"api/#performance-notes","title":"Performance Notes","text":""},{"location":"api/#startup-performance","title":"Startup Performance","text":"<ul> <li>First call: Downloads models if needed (~2.6GB total)</li> <li>Model loading: 2-3 seconds on first use</li> <li>Daemon mode: Eliminates model loading overhead</li> <li>Preloading: Use <code>preload_models()</code> to load at startup</li> </ul>"},{"location":"api/#runtime-performance","title":"Runtime Performance","text":"<ul> <li>Generation speed: ~50-100 tokens/second</li> <li>Embedding speed: ~100-500 embeddings/second</li> <li>Cache hits: &lt;0.01 seconds for cached results</li> <li>Memory usage: ~2.6GB for all models loaded</li> </ul>"},{"location":"api/#optimization-tips","title":"Optimization Tips","text":"<ol> <li>Use daemon mode for production deployments</li> <li>Preload models at application startup</li> <li>Warm up cache with common prompts</li> <li>Use consistent seeds for better cache efficiency</li> <li>Batch operations when possible</li> <li>Monitor cache stats to tune capacity</li> </ol>"},{"location":"api/#version-compatibility","title":"Version Compatibility","text":""},{"location":"api/#model-versions","title":"Model Versions","text":"<p>Each major version uses fixed models:</p> <ul> <li>v2.0.0+: Gemma-3n models (generation), Qwen3 (embeddings)</li> <li>v1.x: Older model versions (deprecated)</li> </ul>"},{"location":"api/#api-stability","title":"API Stability","text":"<ul> <li>Stable APIs: <code>generate()</code>, <code>embed()</code>, <code>generate_iter()</code></li> <li>Seed parameter: Added in all APIs for v2.0.0+</li> <li>Daemon mode: Stable since v1.3.0</li> <li>Cache system: Centralized since v1.3.3</li> </ul>"},{"location":"api/#best-practices","title":"Best Practices","text":"<p>Production Usage</p> <ol> <li>Always specify seeds for reproducible results</li> <li>Use daemon mode for better performance</li> <li>Configure caches based on usage patterns</li> <li>Handle None returns appropriately (v2.1.0+)</li> <li>Monitor performance with cache statistics</li> <li>Test with models unavailable to ensure robustness</li> <li>Use environment variables for configuration</li> <li>Implement proper error handling for production</li> <li>Batch similar operations for efficiency</li> <li>Document your seed choices for reproducibility</li> </ol>"},{"location":"api/cli/","title":"CLI Reference","text":"<p>Complete command-line interface documentation for SteadyText.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>The CLI is automatically installed with SteadyText:</p> <pre><code># Using UV (recommended)\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre> <p>Two commands are available: - <code>steadytext</code> - Full command name - <code>st</code> - Short alias</p>"},{"location":"api/cli/#global-options","title":"Global Options","text":"<pre><code>st --version     # Show version\nst --help        # Show help\n</code></pre>"},{"location":"api/cli/#generate","title":"generate","text":"<p>Generate deterministic text from a prompt.</p>"},{"location":"api/cli/#usage","title":"Usage","text":"<pre><code># New pipe syntax (recommended)\necho \"prompt\" | st [OPTIONS]\necho \"prompt\" | steadytext [OPTIONS]\n\n# Legacy syntax (still supported)\nst generate [OPTIONS] PROMPT\nsteadytext generate [OPTIONS] PROMPT\n</code></pre>"},{"location":"api/cli/#options","title":"Options","text":"Option Short Type Default Description <code>--wait</code> <code>-w</code> flag <code>false</code> Wait for complete output (disable streaming) <code>--json</code> <code>-j</code> flag <code>false</code> Output as JSON with metadata <code>--logprobs</code> <code>-l</code> flag <code>false</code> Include log probabilities <code>--eos-string</code> <code>-e</code> string <code>\"[EOS]\"</code> Custom end-of-sequence string <code>--max-new-tokens</code> int <code>512</code> Maximum number of tokens to generate <code>--seed</code> int <code>42</code> Random seed for deterministic generation <code>--size</code> choice Model size: small (2B, default), large (4B) <code>--model</code> string Model name from registry (e.g., \"qwen2.5-3b\") <code>--model-repo</code> string Custom model repository <code>--model-filename</code> string Custom model filename <code>--no-index</code> flag <code>false</code> Disable automatic index search <code>--index-file</code> path Use specific index file <code>--top-k</code> int <code>3</code> Number of context chunks to retrieve"},{"location":"api/cli/#examples","title":"Examples","text":"Basic GenerationWait for Complete OutputJSON OutputWith Log ProbabilitiesCustom Stop StringCustom Seed for ReproducibilityCustom LengthUsing Size ParameterModel Selection <pre><code># New pipe syntax\necho \"Write a Python function to calculate fibonacci\" | st\n\n# Legacy syntax\nst generate \"Write a Python function to calculate fibonacci\"\n</code></pre> <pre><code># Disable streaming\necho \"Explain machine learning\" | st --wait\n</code></pre> <pre><code>st generate \"Hello world\" --json\n# Output:\n# {\n#   \"text\": \"Hello! How can I help you today?...\",\n#   \"tokens\": 15,\n#   \"cached\": false\n# }\n</code></pre> <pre><code>st generate \"Explain AI\" --logprobs --json\n# Includes token probabilities in JSON output\n</code></pre> <pre><code>st generate \"List colors until STOP\" --eos-string \"STOP\"\n</code></pre> <pre><code># Generate with specific seed for reproducible results\necho \"Write a story\" | st --seed 123\n\n# Same seed always produces same output\nst generate \"Tell me a joke\" --seed 456\nst generate \"Tell me a joke\" --seed 456  # Identical result\n\n# Different seeds produce different outputs\nst generate \"Explain AI\" --seed 100\nst generate \"Explain AI\" --seed 200  # Different result\n</code></pre> <pre><code># Generate shorter responses\necho \"Quick summary of Python\" | st --max-new-tokens 50\n\n# Generate longer responses\necho \"Detailed explanation of ML\" | st --max-new-tokens 200\n</code></pre> <pre><code># Fast generation with small model\nst generate \"Quick response\" --size small\n\n# High quality with large model  \nst generate \"Complex analysis\" --size large\n\n# Combine size with custom seed\nst generate \"Technical explanation\" --size large --seed 789\n</code></pre> <pre><code># Use specific model size\nst generate \"Technical explanation\" --size large\n\n# Use custom model (advanced)\nst generate \"Write code\" --model-repo ggml-org/gemma-3n-E4B-it-GGUF \\\n    --model-filename gemma-3n-E4B-it-Q8_0.gguf\n\n# Custom model with seed and length control\nst generate \"Complex task\" --model-repo ggml-org/gemma-3n-E4B-it-GGUF \\\n    --model-filename gemma-3n-E4B-it-Q8_0.gguf \\\n    --seed 999 --max-new-tokens 100\n</code></pre>"},{"location":"api/cli/#stdin-support","title":"Stdin Support","text":"<p>Generate from stdin when no prompt provided:</p> <pre><code>echo \"Write a haiku\" | st generate\ncat prompts.txt | st generate --stream\n</code></pre>"},{"location":"api/cli/#embed","title":"embed","text":"<p>Create deterministic embeddings for text.</p>"},{"location":"api/cli/#usage_1","title":"Usage","text":"<pre><code>st embed [OPTIONS] TEXT\nsteadytext embed [OPTIONS] TEXT\n</code></pre>"},{"location":"api/cli/#options_1","title":"Options","text":"Option Short Type Default Description <code>--format</code> <code>-f</code> choice <code>json</code> Output format: <code>json</code>, <code>numpy</code>, <code>hex</code> <code>--output</code> <code>-o</code> path <code>-</code> Output file (default: stdout) <code>--seed</code> int <code>42</code> Random seed for deterministic embedding generation"},{"location":"api/cli/#examples_1","title":"Examples","text":"Basic EmbeddingCustom SeedNumpy FormatHex FormatSave to File <pre><code>st embed \"machine learning\"\n# Outputs JSON array with 1024 float values\n</code></pre> <pre><code># Generate reproducible embeddings\nst embed \"artificial intelligence\" --seed 123\nst embed \"artificial intelligence\" --seed 123  # Same result\nst embed \"artificial intelligence\" --seed 456  # Different result\n\n# Compare embeddings with different seeds\nst embed \"test text\" --seed 100 --format json &gt; embed1.json\nst embed \"test text\" --seed 200 --format json &gt; embed2.json\n</code></pre> <pre><code>st embed \"text to embed\" --format numpy\n# Outputs binary numpy array\n</code></pre> <pre><code>st embed \"hello world\" --format hex\n# Outputs hex-encoded float32 array\n</code></pre> <pre><code>st embed \"important text\" --output embedding.json\nst embed \"data\" --format numpy --output embedding.npy\n\n# Save with custom seed\nst embed \"research data\" --seed 42 --output research_embedding.json\nst embed \"experiment\" --seed 123 --format numpy --output exp_embed.npy\n</code></pre>"},{"location":"api/cli/#stdin-support_1","title":"Stdin Support","text":"<p>Embed text from stdin:</p> <pre><code>echo \"text to embed\" | st embed\ncat document.txt | st embed --format numpy --output doc_embedding.npy\n\n# Stdin with custom seed\necho \"text to embed\" | st embed --seed 789\ncat document.txt | st embed --seed 42 --format numpy --output doc_embed_s42.npy\n</code></pre>"},{"location":"api/cli/#models","title":"models","text":"<p>Manage SteadyText models.</p>"},{"location":"api/cli/#usage_2","title":"Usage","text":"<pre><code>st models [OPTIONS]\nsteadytext models [OPTIONS]\n</code></pre>"},{"location":"api/cli/#options_2","title":"Options","text":"Option Short Description <code>--list</code> <code>-l</code> List available models <code>--preload</code> <code>-p</code> Preload all models <code>--cache-dir</code> Show model cache directory <code>--json</code> flag <code>false</code> <code>--seed</code> int"},{"location":"api/cli/#commands","title":"Commands","text":"Command Description <code>status</code> Check model download status <code>list</code> List available models <code>download</code> Pre-download models <code>delete</code> Delete cached models <code>preload</code> Preload models into memory <code>path</code> Show model cache directory"},{"location":"api/cli/#examples_2","title":"Examples","text":"List ModelsDownload ModelsDelete ModelsPreload ModelsCache Information <pre><code>st models list\n# Output:\n# Size Shortcuts:\n#   small \u2192 gemma-3n-2b\n#   large \u2192 gemma-3n-4b\n#\n# Available Models:\n#   gemma-3n-2b\n#     Repository: ggml-org/gemma-3n-E2B-it-GGUF\n#     Filename: gemma-3n-E2B-it-Q8_0.gguf\n#   gemma-3n-4b\n#     Repository: ggml-org/gemma-3n-E4B-it-GGUF\n#     Filename: gemma-3n-E4B-it-Q8_0.gguf\n</code></pre> <pre><code># Download default models\nst models download\n\n# Download by size\nst models download --size small\n\n# Download by name\nst models download --model gemma-3n-4b\n\n# Download all models\nst models download --all\n</code></pre> <pre><code># Delete by size\nst models delete --size small\n\n# Delete by name\nst models delete --model gemma-3n-4b\n\n# Delete all models with confirmation\nst models delete --all\n\n# Force delete all models without confirmation\nst models delete --all --force\n</code></pre> <pre><code>st models preload\n# Downloads and loads all models\n\n# Preload with specific seed for deterministic initialization\nst models preload --seed 42\n</code></pre> <pre><code>st models path\n# /home/user/.cache/steadytext/models/\n\nst models status\n# {\n#   \"model_directory\": \"/home/user/.cache/steadytext/models\",\n#   \"models\": { ... }\n# }\n</code></pre>"},{"location":"api/cli/#vector","title":"vector","text":"<p>Perform vector operations on embeddings.</p>"},{"location":"api/cli/#usage_3","title":"Usage","text":"<pre><code>st vector COMMAND [OPTIONS]\nsteadytext vector COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_1","title":"Commands","text":"Command Description <code>similarity</code> Compute similarity between text embeddings <code>distance</code> Compute distance between text embeddings <code>search</code> Find most similar texts from candidates <code>average</code> Compute average of multiple embeddings <code>arithmetic</code> Perform vector arithmetic operations"},{"location":"api/cli/#global-vector-options","title":"Global Vector Options","text":"Option Type Default Description <code>--seed</code> int <code>42</code> Random seed for deterministic embeddings <code>--json</code> flag <code>false</code> Output as JSON with metadata"},{"location":"api/cli/#examples_3","title":"Examples","text":"SimilarityDistanceSearchAverageArithmetic <pre><code># Cosine similarity\nst vector similarity \"cat\" \"dog\"\n# 0.823456\n\n# With JSON output\nst vector similarity \"king\" \"queen\" --json\n\n# Reproducible similarity with custom seed\nst vector similarity \"king\" \"queen\" --seed 123\nst vector similarity \"king\" \"queen\" --seed 123  # Same result\nst vector similarity \"king\" \"queen\" --seed 456  # Different result\n</code></pre> <pre><code># Euclidean distance\nst vector distance \"hot\" \"cold\"\n\n# Manhattan distance\nst vector distance \"yes\" \"no\" --metric manhattan\n</code></pre> <pre><code># Find similar from stdin\necho -e \"apple\\norange\\ncar\" | st vector search \"fruit\" --stdin\n\n# From file, top 3\nst vector search \"python\" --candidates langs.txt --top 3\n\n# Reproducible search with custom seed\necho -e \"apple\\norange\\ncar\" | st vector search \"fruit\" --stdin --seed 789\nst vector search \"programming\" --candidates langs.txt --top 3 --seed 42\n</code></pre> <pre><code># Average embeddings\nst vector average \"cat\" \"dog\" \"hamster\"\n\n# With full embedding output\nst vector average \"red\" \"green\" \"blue\" --json\n\n# Reproducible averaging with custom seed\nst vector average \"cat\" \"dog\" \"hamster\" --seed 555\nst vector average \"colors\" \"shapes\" \"sizes\" --seed 666 --json\n</code></pre> <pre><code># Classic analogy: king + woman - man \u2248 queen\nst vector arithmetic \"king\" \"woman\" --subtract \"man\"\n\n# Location arithmetic\nst vector arithmetic \"paris\" \"italy\" --subtract \"france\"\n\n# Reproducible arithmetic with custom seed\nst vector arithmetic \"king\" \"woman\" --subtract \"man\" --seed 777\nst vector arithmetic \"tokyo\" \"italy\" --subtract \"japan\" --seed 888 --json\n</code></pre> <p>See Vector Operations for detailed usage.</p>"},{"location":"api/cli/#cache","title":"cache","text":"<p>Manage result caches.</p>"},{"location":"api/cli/#usage_4","title":"Usage","text":"<pre><code>st cache [OPTIONS]\nsteadytext cache [OPTIONS]\n</code></pre>"},{"location":"api/cli/#options_3","title":"Options","text":"Option Short Description <code>--clear</code> <code>-c</code> Clear all caches <code>--status</code> <code>-s</code> Show cache status <code>--generation-only</code> Target only generation cache <code>--embedding-only</code> Target only embedding cache"},{"location":"api/cli/#examples_4","title":"Examples","text":"Cache StatusClear Caches <pre><code>st cache --status\n# Generation Cache: 45 entries, 12.3MB\n# Embedding Cache: 128 entries, 34.7MB\n</code></pre> <pre><code>st cache --clear\n# Cleared all caches\n\nst cache --clear --generation-only\n# Cleared generation cache only\n</code></pre>"},{"location":"api/cli/#daemon","title":"daemon","text":"<p>Manage the SteadyText daemon for persistent model serving.</p>"},{"location":"api/cli/#usage_5","title":"Usage","text":"<pre><code>st daemon COMMAND [OPTIONS]\nsteadytext daemon COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_2","title":"Commands","text":"Command Description <code>start</code> Start the daemon server <code>stop</code> Stop the daemon server <code>status</code> Check daemon status <code>restart</code> Restart the daemon server"},{"location":"api/cli/#global-daemon-options","title":"Global Daemon Options","text":"Option Type Default Description <code>--seed</code> int <code>42</code> Default seed for daemon operations"},{"location":"api/cli/#options_4","title":"Options","text":""},{"location":"api/cli/#start","title":"start","text":"Option Type Default Description <code>--host</code> string <code>127.0.0.1</code> Bind address <code>--port</code> int <code>5557</code> Port number <code>--foreground</code> flag <code>false</code> Run in foreground"},{"location":"api/cli/#stop","title":"stop","text":"Option Type Default Description <code>--force</code> flag <code>false</code> Force kill if graceful shutdown fails"},{"location":"api/cli/#status","title":"status","text":"Option Type Default Description <code>--json</code> flag <code>false</code> Output as JSON"},{"location":"api/cli/#examples_5","title":"Examples","text":"Start DaemonCheck StatusStop/Restart <pre><code># Start in background (default)\nst daemon start\n\n# Start in foreground for debugging\nst daemon start --foreground\n\n# Custom host/port\nst daemon start --host 0.0.0.0 --port 5557\n\n# Start with custom default seed\nst daemon start --seed 123\n\n# Combined options\nst daemon start --host 0.0.0.0 --port 5557 --seed 456 --foreground\n</code></pre> <pre><code>st daemon status\n# Output: Daemon is running (PID: 12345)\n\n# JSON output\nst daemon status --json\n# {\"running\": true, \"pid\": 12345, \"host\": \"127.0.0.1\", \"port\": 5557}\n</code></pre> <pre><code># Graceful stop\nst daemon stop\n\n# Force stop\nst daemon stop --force\n\n# Restart\nst daemon restart\n</code></pre>"},{"location":"api/cli/#benefits","title":"Benefits","text":"<ul> <li>160x faster first request: No model loading overhead</li> <li>Persistent cache: Shared across all operations</li> <li>Automatic fallback: Operations work without daemon</li> <li>Zero configuration: Used by default when available</li> </ul>"},{"location":"api/cli/#index","title":"index","text":"<p>Manage FAISS vector indexes for retrieval-augmented generation.</p>"},{"location":"api/cli/#usage_6","title":"Usage","text":"<pre><code>st index COMMAND [OPTIONS]\nsteadytext index COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_3","title":"Commands","text":"Command Description <code>create</code> Create index from text files <code>search</code> Search index for similar chunks <code>info</code> Show index information"},{"location":"api/cli/#global-index-options","title":"Global Index Options","text":"Option Type Default Description <code>--seed</code> int <code>42</code> Random seed for embedding generation"},{"location":"api/cli/#options_5","title":"Options","text":""},{"location":"api/cli/#create","title":"create","text":"Option Type Default Description <code>--output</code> path required Output index file <code>--chunk-size</code> int <code>512</code> Chunk size in tokens <code>--glob</code> string File glob pattern"},{"location":"api/cli/#search","title":"search","text":"Option Type Default Description <code>--top-k</code> int <code>5</code> Number of results <code>--threshold</code> float Similarity threshold"},{"location":"api/cli/#examples_6","title":"Examples","text":"Create IndexSearch IndexIndex Info <pre><code># From specific files\nst index create doc1.txt doc2.txt --output docs.faiss\n\n# From glob pattern\nst index create --glob \"**/*.md\" --output project.faiss\n\n# Custom chunk size\nst index create *.txt --output custom.faiss --chunk-size 256\n\n# Reproducible index creation with custom seed\nst index create doc1.txt doc2.txt --output docs_s123.faiss --seed 123\nst index create --glob \"**/*.md\" --output project_s456.faiss --seed 456\n</code></pre> <pre><code># Basic search\nst index search docs.faiss \"query text\"\n\n# Top 10 results\nst index search docs.faiss \"error message\" --top-k 10\n\n# With threshold\nst index search docs.faiss \"specific term\" --threshold 0.8\n\n# Reproducible search with custom seed\nst index search docs.faiss \"query text\" --seed 789\nst index search docs.faiss \"error message\" --top-k 10 --seed 123\n</code></pre> <pre><code>st index info docs.faiss\n# Output:\n# Index: docs.faiss\n# Chunks: 1,234\n# Dimension: 1024\n# Size: 5.2MB\n</code></pre>"},{"location":"api/cli/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"<p>Set these before running CLI commands:</p> <pre><code># Cache configuration\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=512\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=100\n\n# Allow model downloads (for development)\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Set default seed for all operations\nexport STEADYTEXT_DEFAULT_SEED=42\n\n# Then run commands\nst generate \"test prompt\"\nst generate \"test prompt\" --seed 123  # Override default seed\n</code></pre>"},{"location":"api/cli/#pipeline-usage","title":"Pipeline Usage","text":"<p>Chain commands with other tools:</p> <pre><code># Batch processing\ncat prompts.txt | while read prompt; do\n  echo \"Prompt: $prompt\"\n  st generate \"$prompt\" --json | jq '.text'\n  echo \"---\"\ndone\n\n# Generate and embed\ntext=$(st generate \"explain AI\")\necho \"$text\" | st embed --format hex &gt; ai_explanation.hex\n</code></pre>"},{"location":"api/cli/#scripting-examples","title":"Scripting Examples","text":"Bash ScriptPython Integration <pre><code>#!/bin/bash\n# generate_docs.sh\n\nprompts=(\n  \"Explain machine learning\"\n  \"What is deep learning?\"\n  \"Define neural networks\"\n)\n\nfor prompt in \"${prompts[@]}\"; do\n  echo \"=== $prompt ===\"\n  st generate \"$prompt\" --stream\n  echo -e \"\\n---\\n\"\ndone\n</code></pre> <pre><code>import subprocess\nimport json\n\ndef cli_generate(prompt):\n    \"\"\"Use CLI from Python.\"\"\"\n    result = subprocess.run([\n        'st', 'generate', prompt, '--json'\n    ], capture_output=True, text=True)\n\n    return json.loads(result.stdout)\n\n# Usage\nresult = cli_generate(\"Hello world\")\nprint(result['text'])\n</code></pre>"},{"location":"api/cli/#performance-tips","title":"Performance Tips","text":"<p>CLI Optimization</p> <ul> <li>Preload models: Run <code>st models --preload</code> once at startup</li> <li>Use JSON output: Easier to parse in scripts with <code>--json</code></li> <li>Batch operations: Process multiple items in single session</li> <li>Cache warmup: Generate common prompts to populate cache</li> </ul>"},{"location":"api/cli/#real-world-examples","title":"Real-World Examples","text":""},{"location":"api/cli/#content-generation-pipeline","title":"Content Generation Pipeline","text":"<pre><code>#!/bin/bash\n# blog_generator.sh - Generate blog posts with consistent style\n\nSEED=12345  # Consistent seed for reproducible content\n\n# Function to generate blog post\ngenerate_post() {\n    local topic=\"$1\"\n    local style=\"$2\"\n\n    echo \"Generating post about: $topic\"\n\n    # Generate title\n    title=$(st generate \"Create an engaging blog title about $topic\" --seed $SEED --wait)\n\n    # Generate introduction\n    intro=$(st generate \"Write a compelling introduction for a blog post about $topic\" --seed $(($SEED + 1)) --wait)\n\n    # Generate main content\n    content=$(st generate \"Write the main content for a blog post about $topic in a $style style\" --seed $(($SEED + 2)) --max-new-tokens 800 --wait)\n\n    # Generate conclusion\n    conclusion=$(st generate \"Write a strong conclusion for a blog post about $topic\" --seed $(($SEED + 3)) --wait)\n\n    # Combine into final post\n    cat &lt;&lt;EOF\n# $title\n\n## Introduction\n$intro\n\n## Main Content\n$content\n\n## Conclusion\n$conclusion\n\n---\nGenerated with SteadyText (seed: $SEED)\nEOF\n}\n\n# Generate multiple posts\ntopics=(\"Machine Learning\" \"Web Development\" \"Data Science\")\nstyles=(\"technical\" \"beginner-friendly\" \"professional\")\n\nfor i in \"${!topics[@]}\"; do\n    generate_post \"${topics[$i]}\" \"${styles[$i]}\" &gt; \"blog_${i}.md\"\n    echo \"Created blog_${i}.md\"\ndone\n</code></pre>"},{"location":"api/cli/#semantic-search-cli-tool","title":"Semantic Search CLI Tool","text":"<pre><code>#!/bin/bash\n# semantic_search.sh - Search documents using embeddings\n\nINDEX_FILE=\"documents.faiss\"\nSEED=42\n\n# Function to build index\nbuild_index() {\n    echo \"Building search index...\"\n    st index create --glob \"**/*.md\" --output \"$INDEX_FILE\" --chunk-size 256 --seed $SEED\n    echo \"Index created: $INDEX_FILE\"\n}\n\n# Function to search\nsearch_docs() {\n    local query=\"$1\"\n    local num_results=\"${2:-5}\"\n\n    echo \"Searching for: $query\"\n    echo \"========================\"\n\n    # Search and format results\n    st index search \"$INDEX_FILE\" \"$query\" --top-k $num_results --seed $SEED | \\\n    while IFS= read -r line; do\n        if [[ $line =~ ^([0-9]+)\\.\\s+(.+):\\s+(.+)$ ]]; then\n            rank=\"${BASH_REMATCH[1]}\"\n            file=\"${BASH_REMATCH[2]}\"\n            snippet=\"${BASH_REMATCH[3]}\"\n\n            echo -e \"\\n[$rank] $file\"\n            echo \"   $snippet\"\n        fi\n    done\n}\n\n# Main menu\nwhile true; do\n    echo -e \"\\nSemantic Search Tool\"\n    echo \"1. Build/Rebuild index\"\n    echo \"2. Search documents\"\n    echo \"3. Exit\"\n    read -p \"Choose option: \" choice\n\n    case $choice in\n        1) build_index ;;\n        2) \n            read -p \"Enter search query: \" query\n            read -p \"Number of results (default 5): \" num\n            search_docs \"$query\" \"${num:-5}\"\n            ;;\n        3) exit 0 ;;\n        *) echo \"Invalid option\" ;;\n    esac\ndone\n</code></pre>"},{"location":"api/cli/#ai-powered-code-documentation-generator","title":"AI-Powered Code Documentation Generator","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\ndocgen.py - Generate documentation from code using SteadyText CLI\n\"\"\"\n\nimport subprocess\nimport json\nimport re\nfrom pathlib import Path\nimport argparse\n\ndef run_steadytext(prompt, seed=42, max_tokens=512):\n    \"\"\"Run SteadyText CLI and return result.\"\"\"\n    cmd = [\n        'st', 'generate', prompt,\n        '--json',\n        '--wait',\n        '--seed', str(seed),\n        '--max-new-tokens', str(max_tokens)\n    ]\n\n    result = subprocess.run(cmd, capture_output=True, text=True)\n    if result.returncode == 0:\n        data = json.loads(result.stdout)\n        return data['text']\n    else:\n        raise Exception(f\"SteadyText error: {result.stderr}\")\n\ndef extract_functions(code):\n    \"\"\"Extract function definitions from Python code.\"\"\"\n    pattern = r'def\\s+(\\w+)\\s*\\([^)]*\\):'\n    return re.findall(pattern, code)\n\ndef generate_function_docs(file_path, seed=42):\n    \"\"\"Generate documentation for a Python file.\"\"\"\n    with open(file_path, 'r') as f:\n        code = f.read()\n\n    functions = extract_functions(code)\n    docs = []\n\n    # Generate module overview\n    module_prompt = f\"Write a brief overview of a Python module containing these functions: {', '.join(functions)}\"\n    overview = run_steadytext(module_prompt, seed=seed)\n    docs.append(f\"# {file_path.name}\\n\\n{overview}\\n\")\n\n    # Generate documentation for each function\n    for i, func in enumerate(functions):\n        # Extract function code\n        func_pattern = rf'(def\\s+{func}\\s*\\([^)]*\\):.*?)(?=\\ndef|\\Z)'\n        match = re.search(func_pattern, code, re.DOTALL)\n\n        if match:\n            func_code = match.group(1)\n\n            # Generate documentation\n            doc_prompt = f\"Write clear documentation for this Python function:\\n\\n{func_code}\"\n            func_doc = run_steadytext(doc_prompt, seed=seed + i + 1, max_tokens=300)\n\n            docs.append(f\"\\n## `{func}()`\\n\\n{func_doc}\\n\")\n\n    return '\\n'.join(docs)\n\ndef main():\n    parser = argparse.ArgumentParser(description='Generate documentation from Python code')\n    parser.add_argument('files', nargs='+', help='Python files to document')\n    parser.add_argument('--output', '-o', help='Output directory', default='./docs')\n    parser.add_argument('--seed', '-s', type=int, default=42, help='Random seed')\n\n    args = parser.parse_args()\n\n    output_dir = Path(args.output)\n    output_dir.mkdir(exist_ok=True)\n\n    for file_path in args.files:\n        file_path = Path(file_path)\n        if file_path.suffix == '.py':\n            print(f\"Generating documentation for {file_path}...\")\n\n            try:\n                docs = generate_function_docs(file_path, seed=args.seed)\n\n                # Save documentation\n                doc_path = output_dir / f\"{file_path.stem}_docs.md\"\n                with open(doc_path, 'w') as f:\n                    f.write(docs)\n\n                print(f\"  \u2192 Saved to {doc_path}\")\n            except Exception as e:\n                print(f\"  \u2717 Error: {e}\")\n\nif __name__ == '__main__':\n    main()\n</code></pre>"},{"location":"api/cli/#batch-text-analysis-tool","title":"Batch Text Analysis Tool","text":"<pre><code>#!/bin/bash\n# analyze_texts.sh - Analyze multiple texts for sentiment, topics, etc.\n\nSEED=999\nOUTPUT_DIR=\"analysis_results\"\nmkdir -p \"$OUTPUT_DIR\"\n\n# Function to analyze single text\nanalyze_text() {\n    local file=\"$1\"\n    local filename=$(basename \"$file\" .txt)\n    local output_file=\"$OUTPUT_DIR/${filename}_analysis.json\"\n\n    echo \"Analyzing: $file\"\n\n    # Read content\n    content=$(cat \"$file\")\n\n    # Generate various analyses\n    sentiment=$(st generate \"Analyze the sentiment of this text and respond with only: POSITIVE, NEGATIVE, or NEUTRAL: $content\" --seed $SEED --wait --max-new-tokens 10)\n\n    summary=$(st generate \"Write a one-sentence summary of: $content\" --seed $(($SEED + 1)) --wait --max-new-tokens 50)\n\n    topics=$(st generate \"List the main topics in this text as comma-separated values: $content\" --seed $(($SEED + 2)) --wait --max-new-tokens 30)\n\n    # Create embedding\n    embedding=$(echo \"$content\" | st embed --seed $SEED --format json)\n\n    # Combine results\n    cat &gt; \"$output_file\" &lt;&lt;EOF\n{\n  \"file\": \"$file\",\n  \"sentiment\": \"$sentiment\",\n  \"summary\": \"$summary\",\n  \"topics\": \"$topics\",\n  \"embedding_sample\": $(echo \"$embedding\" | jq '.[0:5]'),\n  \"timestamp\": \"$(date -u +%Y-%m-%dT%H:%M:%SZ)\"\n}\nEOF\n\n    echo \"  \u2192 Saved to $output_file\"\n}\n\n# Process all text files\nfor file in *.txt; do\n    if [ -f \"$file\" ]; then\n        analyze_text \"$file\"\n    fi\ndone\n\n# Generate summary report\necho -e \"\\n\\nGenerating summary report...\"\n\nst generate \"Based on these analysis results, write a summary report: $(cat $OUTPUT_DIR/*.json | jq -s '.')\" \\\n    --seed $(($SEED + 100)) \\\n    --max-new-tokens 500 \\\n    --wait &gt; \"$OUTPUT_DIR/summary_report.md\"\n\necho \"Analysis complete! Results in $OUTPUT_DIR/\"\n</code></pre>"},{"location":"api/cli/#interactive-qa-system","title":"Interactive Q&amp;A System","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nqa_system.py - Interactive Q&amp;A using SteadyText with context\n\"\"\"\n\nimport subprocess\nimport json\nimport readline  # For better input handling\nfrom datetime import datetime\n\nclass QASystem:\n    def __init__(self, seed=42):\n        self.seed = seed\n        self.context = []\n        self.max_context = 5\n\n    def ask(self, question):\n        \"\"\"Ask a question with context.\"\"\"\n        # Build context prompt\n        if self.context:\n            context_str = \"Previous Q&amp;A:\\n\"\n            for qa in self.context[-self.max_context:]:\n                context_str += f\"Q: {qa['q']}\\nA: {qa['a'][:100]}...\\n\\n\"\n            full_prompt = f\"{context_str}\\nNow answer this question: {question}\"\n        else:\n            full_prompt = question\n\n        # Generate answer\n        cmd = [\n            'st', 'generate', full_prompt,\n            '--seed', str(self.seed + len(self.context)),\n            '--max-new-tokens', '300',\n            '--wait',\n            '--json'\n        ]\n\n        result = subprocess.run(cmd, capture_output=True, text=True)\n        if result.returncode == 0:\n            data = json.loads(result.stdout)\n            answer = data['text']\n\n            # Store in context\n            self.context.append({\n                'q': question,\n                'a': answer,\n                'timestamp': datetime.now().isoformat()\n            })\n\n            return answer\n        else:\n            return f\"Error: {result.stderr}\"\n\n    def save_session(self, filename):\n        \"\"\"Save Q&amp;A session to file.\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(self.context, f, indent=2)\n        print(f\"Session saved to {filename}\")\n\n    def run_interactive(self):\n        \"\"\"Run interactive Q&amp;A session.\"\"\"\n        print(\"SteadyText Q&amp;A System\")\n        print(\"Type 'quit' to exit, 'save' to save session\")\n        print(\"-\" * 50)\n\n        while True:\n            try:\n                question = input(\"\\nYour question: \").strip()\n\n                if question.lower() == 'quit':\n                    break\n                elif question.lower() == 'save':\n                    filename = input(\"Save as: \") or \"qa_session.json\"\n                    self.save_session(filename)\n                    continue\n                elif not question:\n                    continue\n\n                print(\"\\nThinking...\")\n                answer = self.ask(question)\n                print(f\"\\nAnswer: {answer}\")\n\n            except KeyboardInterrupt:\n                print(\"\\n\\nGoodbye!\")\n                break\n            except Exception as e:\n                print(f\"Error: {e}\")\n\nif __name__ == '__main__':\n    import argparse\n\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--seed', type=int, default=42)\n    parser.add_argument('--load', help='Load previous session')\n\n    args = parser.parse_args()\n\n    qa = QASystem(seed=args.seed)\n\n    if args.load:\n        with open(args.load, 'r') as f:\n            qa.context = json.load(f)\n        print(f\"Loaded {len(qa.context)} previous Q&amp;As\")\n\n    qa.run_interactive()\n</code></pre>"},{"location":"api/cli/#multi-language-code-generator","title":"Multi-Language Code Generator","text":"<pre><code>#!/bin/bash\n# polyglot_codegen.sh - Generate code in multiple languages\n\ngenerate_code() {\n    local task=\"$1\"\n    local lang=\"$2\"\n    local seed=\"$3\"\n\n    prompt=\"Write a $lang function that $task. Include only the code, no explanations.\"\n\n    echo \"=== $lang ===\"\n    st generate \"$prompt\" --seed $seed --max-new-tokens 200 --wait\n    echo -e \"\\n\"\n}\n\n# Main\necho \"Multi-Language Code Generator\"\necho \"============================\"\nread -p \"What should the function do? \" task\n\n# Generate in multiple languages with consistent seeds\nLANGUAGES=(\"Python\" \"JavaScript\" \"Go\" \"Rust\" \"Java\" \"C++\" \"Ruby\" \"PHP\")\nBASE_SEED=1000\n\nfor i in \"${!LANGUAGES[@]}\"; do\n    generate_code \"$task\" \"${LANGUAGES[$i]}\" $(($BASE_SEED + $i))\ndone\n\n# Generate comparison\necho \"=== Performance Comparison ===\"\nst generate \"Compare the performance characteristics of these languages for $task: ${LANGUAGES[*]}\" \\\n    --seed $(($BASE_SEED + 100)) \\\n    --max-new-tokens 300 \\\n    --wait\n</code></pre>"},{"location":"api/cli/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/cli/#common-issues","title":"Common Issues","text":"<p>Issue: Command not found <pre><code># Problem\n$ st generate \"test\"\nbash: st: command not found\n\n# Solution\n# Ensure SteadyText is installed\npip install steadytext\n\n# Or add to PATH if using local install\nexport PATH=\"$HOME/.local/bin:$PATH\"\n</code></pre></p> <p>Issue: Slow first generation <pre><code># Problem: First call takes 2-3 seconds\n\n# Solution 1: Preload models\nst models preload\n\n# Solution 2: Use daemon mode\nst daemon start\nst generate \"test\"  # Now fast!\n</code></pre></p> <p>Issue: Different results across runs <pre><code># Problem: Results vary between sessions\n\n# Solution: Use explicit seeds\nst generate \"test\" --seed 42  # Always same result\nst embed \"test\" --seed 42     # Always same embedding\n</code></pre></p> <p>Issue: JSON parsing errors <pre><code># Problem: Invalid JSON output\n\n# Solution: Use proper error handling\nresult=$(st generate \"test\" --json 2&gt;/dev/null)\nif [ $? -eq 0 ]; then\n    echo \"$result\" | jq '.text'\nelse\n    echo \"Error generating text\"\nfi\n</code></pre></p>"},{"location":"api/cli/#best-practices","title":"Best Practices","text":"<p>CLI Best Practices</p> <ol> <li>Always use seeds for reproducible results in production</li> <li>Start daemon for better performance in scripts</li> <li>Use JSON output for reliable parsing</li> <li>Handle errors properly in scripts</li> <li>Batch operations when possible</li> <li>Set environment variables for consistent configuration</li> <li>Use appropriate output formats (JSON for parsing, plain for display)</li> <li>Chain commands efficiently with pipes</li> <li>Cache warmup for frequently used prompts</li> <li>Monitor performance with timing commands</li> </ol>"},{"location":"api/embedding/","title":"Embeddings API","text":"<p>Functions for creating deterministic text embeddings.</p>"},{"location":"api/embedding/#embed","title":"embed()","text":"<p>Create deterministic embeddings for text input.</p> <pre><code>def embed(text_input: Union[str, List[str]], seed: int = DEFAULT_SEED) -&gt; np.ndarray\n</code></pre>"},{"location":"api/embedding/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>text_input</code> <code>Union[str, List[str]]</code> required Text string or list of strings to embed <code>seed</code> <code>int</code> <code>42</code> Random seed for deterministic embedding generation"},{"location":"api/embedding/#returns","title":"Returns","text":"<p>Returns: <code>np.ndarray</code> - 1024-dimensional L2-normalized float32 array</p>"},{"location":"api/embedding/#examples","title":"Examples","text":"Single TextCustom SeedMultiple TextsSimilarity Comparison <pre><code>import steadytext\nimport numpy as np\n\n# Embed single text\nvector = steadytext.embed(\"Hello world\")\n\nprint(f\"Shape: {vector.shape}\")        # (1024,)\nprint(f\"Type: {vector.dtype}\")         # float32\nprint(f\"Norm: {np.linalg.norm(vector):.6f}\")  # 1.000000 (L2 normalized)\n</code></pre> <pre><code># Generate different embeddings with different seeds\nvec1 = steadytext.embed(\"Hello world\", seed=123)\nvec2 = steadytext.embed(\"Hello world\", seed=123)  # Same as vec1\nvec3 = steadytext.embed(\"Hello world\", seed=456)  # Different from vec1\n\nprint(f\"Seed 123 vs 123 equal: {np.array_equal(vec1, vec2)}\")  # True\nprint(f\"Seed 123 vs 456 equal: {np.array_equal(vec1, vec3)}\")  # False\n\n# Calculate similarity between different seed embeddings\nsimilarity = np.dot(vec1, vec3)  # Cosine similarity (vectors are normalized)\nprint(f\"Similarity between seeds: {similarity:.3f}\")\n</code></pre> <pre><code># Embed multiple texts (returns a single, averaged embedding)\ntexts = [\"machine learning\", \"artificial intelligence\", \"deep learning\"]\nvector = steadytext.embed(texts)\n\nprint(f\"Combined embedding shape: {vector.shape}\")  # (1024,)\n# Result is averaged across all input texts\n</code></pre> <pre><code>import numpy as np\n\n# Create embeddings for comparison with consistent seed\nseed = 42\nvec1 = steadytext.embed(\"machine learning\", seed=seed)\nvec2 = steadytext.embed(\"artificial intelligence\", seed=seed) \nvec3 = steadytext.embed(\"cooking recipes\", seed=seed)\n\n# Calculate cosine similarity (vectors are already L2 normalized)\nsim_ml_ai = np.dot(vec1, vec2)\nsim_ml_cooking = np.dot(vec1, vec3)\n\nprint(f\"ML vs AI similarity: {sim_ml_ai:.3f}\")\nprint(f\"ML vs Cooking similarity: {sim_ml_cooking:.3f}\")\n# ML and AI should have higher similarity than ML and cooking\n\n# Compare same text with different seeds\nvec_seed1 = steadytext.embed(\"machine learning\", seed=100)\nvec_seed2 = steadytext.embed(\"machine learning\", seed=200)\nseed_similarity = np.dot(vec_seed1, vec_seed2)\nprint(f\"Same text, different seeds similarity: {seed_similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/embedding/#deterministic-behavior","title":"Deterministic Behavior","text":"<p>Embeddings are completely deterministic for the same input text and seed:</p> <pre><code># Same text, same seed - always identical\nvec1 = steadytext.embed(\"test text\")\nvec2 = steadytext.embed(\"test text\")\nassert np.array_equal(vec1, vec2)  # Always passes!\n\n# Same text, explicit same seed - always identical\nvec3 = steadytext.embed(\"test text\", seed=42)\nvec4 = steadytext.embed(\"test text\", seed=42)\nassert np.array_equal(vec3, vec4)  # Always passes!\n\n# Same text, different seeds - different results\nvec5 = steadytext.embed(\"test text\", seed=123)\nvec6 = steadytext.embed(\"test text\", seed=456)\nassert not np.array_equal(vec5, vec6)  # Different seeds produce different embeddings\n\n# But each seed is still deterministic\nvec7 = steadytext.embed(\"test text\", seed=123)\nassert np.array_equal(vec5, vec7)  # Same seed always produces same result\n</code></pre>"},{"location":"api/embedding/#seed-use-cases","title":"Seed Use Cases","text":"<pre><code># Experimental variations - try different embeddings for the same text\ntext = \"artificial intelligence\"\nbaseline_embedding = steadytext.embed(text, seed=42)\nvariation1 = steadytext.embed(text, seed=100)\nvariation2 = steadytext.embed(text, seed=200)\n\n# Compare variations\nprint(f\"Baseline vs Variation 1: {np.dot(baseline_embedding, variation1):.3f}\")\nprint(f\"Baseline vs Variation 2: {np.dot(baseline_embedding, variation2):.3f}\")\nprint(f\"Variation 1 vs Variation 2: {np.dot(variation1, variation2):.3f}\")\n\n# Reproducible research - document your seeds\nresearch_texts = [\"AI\", \"ML\", \"DL\"]\nresearch_seed = 42\nembeddings = []\nfor text in research_texts:\n    embedding = steadytext.embed(text, seed=research_seed)\n    embeddings.append(embedding)\n    print(f\"Text: {text}, Seed: {research_seed}\")\n</code></pre>"},{"location":"api/embedding/#preprocessing","title":"Preprocessing","text":"<p>Text is automatically preprocessed before embedding:</p> <pre><code># These produce different embeddings due to different text\nvec1 = steadytext.embed(\"Hello World\")\nvec2 = steadytext.embed(\"hello world\")\nvec3 = steadytext.embed(\"HELLO WORLD\")\n\n# Case sensitivity matters\nassert not np.array_equal(vec1, vec2)\n</code></pre>"},{"location":"api/embedding/#batch-processing","title":"Batch Processing","text":"<p>For multiple texts, pass as a list with consistent seeding:</p> <pre><code># Individual embeddings with consistent seed\nseed = 42\nvec1 = steadytext.embed(\"first text\", seed=seed)\nvec2 = steadytext.embed(\"second text\", seed=seed) \nvec3 = steadytext.embed(\"third text\", seed=seed)\n\n# Batch embedding (averaged) with same seed\nvec_batch = steadytext.embed([\"first text\", \"second text\", \"third text\"], seed=seed)\n\n# The batch result is the average of individual embeddings\nexpected = (vec1 + vec2 + vec3) / 3\nexpected = expected / np.linalg.norm(expected)  # Re-normalize after averaging\nassert np.allclose(vec_batch, expected, atol=1e-6)\n\n# Different seeds produce different batch results\nvec_batch_alt = steadytext.embed([\"first text\", \"second text\", \"third text\"], seed=123)\nassert not np.array_equal(vec_batch, vec_batch_alt)\n</code></pre>"},{"location":"api/embedding/#caching","title":"Caching","text":"<p>Embeddings are cached for performance, with seed as part of the cache key:</p> <pre><code># First call: computes and caches embedding for default seed\nvec1 = steadytext.embed(\"common text\")  # ~0.5 seconds\n\n# Second call with same seed: returns cached result\nvec2 = steadytext.embed(\"common text\")  # ~0.01 seconds\nassert np.array_equal(vec1, vec2)  # Same result, much faster\n\n# Different seed: computes and caches separately\nvec3 = steadytext.embed(\"common text\", seed=123)  # ~0.5 seconds (new cache entry)\nvec4 = steadytext.embed(\"common text\", seed=123)  # ~0.01 seconds (cached)\n\nassert np.array_equal(vec3, vec4)  # Same seed, same cached result\nassert not np.array_equal(vec1, vec3)  # Different seeds, different results\n\n# Each seed gets its own cache entry\nfor seed in [100, 200, 300]:\n    steadytext.embed(\"cache test\", seed=seed)  # Each gets cached separately\n</code></pre>"},{"location":"api/embedding/#fallback-behavior","title":"Fallback Behavior","text":"<p>When models can't be loaded, deterministic fallback vectors are generated using the seed:</p> <pre><code># Even without models, function never fails and respects seeds\nvector1 = steadytext.embed(\"any text\", seed=42)\nvector2 = steadytext.embed(\"any text\", seed=42)\nvector3 = steadytext.embed(\"any text\", seed=123)\n\nassert vector1.shape == (1024,)     # Correct shape\nassert vector1.dtype == np.float32  # Correct type\nassert np.array_equal(vector1, vector2)  # Same seed, same fallback\nassert not np.array_equal(vector1, vector3)  # Different seed, different fallback\n\n# Fallback vectors are normalized and deterministic\nassert abs(np.linalg.norm(vector1) - 1.0) &lt; 1e-6  # Properly normalized\n</code></pre>"},{"location":"api/embedding/#use-cases","title":"Use Cases","text":""},{"location":"api/embedding/#document-similarity","title":"Document Similarity","text":"<pre><code>import steadytext\nimport numpy as np\n\ndef document_similarity(doc1: str, doc2: str, seed: int = 42) -&gt; float:\n    \"\"\"Calculate similarity between two documents.\"\"\"\n    vec1 = steadytext.embed(doc1, seed=seed)\n    vec2 = steadytext.embed(doc2, seed=seed)\n    return np.dot(vec1, vec2)  # Already L2 normalized\n\n# Usage\nsimilarity = document_similarity(\n    \"Machine learning algorithms\",\n    \"AI and neural networks\"\n)\nprint(f\"Similarity: {similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#semantic-search","title":"Semantic Search","text":"<pre><code>def semantic_search(query: str, documents: List[str], top_k: int = 5, seed: int = 42):\n    \"\"\"Find most similar documents to query.\"\"\"\n    query_vec = steadytext.embed(query, seed=seed)\n    doc_vecs = [steadytext.embed(doc, seed=seed) for doc in documents]\n\n    similarities = [np.dot(query_vec, doc_vec) for doc_vec in doc_vecs]\n    top_indices = np.argsort(similarities)[-top_k:][::-1]\n\n    return [(documents[i], similarities[i]) for i in top_indices]\n\n# Usage  \ndocs = [\"AI research\", \"Machine learning\", \"Cooking recipes\", \"Data science\"]\nresults = semantic_search(\"artificial intelligence\", docs, top_k=2)\n\nfor doc, score in results:\n    print(f\"{doc}: {score:.3f}\")\n</code></pre>"},{"location":"api/embedding/#clustering","title":"Clustering","text":"<pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_texts(texts: List[str], n_clusters: int = 3, seed: int = 42):\n    \"\"\"Cluster texts using their embeddings.\"\"\"\n    embeddings = np.array([steadytext.embed(text, seed=seed) for text in texts])\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(embeddings)\n\n    return clusters\n\n# Usage\ntexts = [\n    \"machine learning\", \"deep learning\", \"neural networks\",  # AI cluster\n    \"pizza recipe\", \"pasta cooking\", \"italian food\",        # Food cluster  \n    \"stock market\", \"trading\", \"investment\"                 # Finance cluster\n]\n\nclusters = cluster_texts(texts, n_clusters=3)\nfor text, cluster in zip(texts, clusters):\n    print(f\"Cluster {cluster}: {text}\")\n</code></pre>"},{"location":"api/embedding/#performance-notes","title":"Performance Notes","text":"<p>Optimization Tips</p> <ul> <li>Preload models: Call <code>steadytext.preload_models()</code> at startup</li> <li>Batch similar texts: Group related texts together for cache efficiency  </li> <li>Memory usage: ~610MB for embedding model (loaded once)</li> <li>Speed: ~100-500 embeddings/second depending on text length</li> <li>Seed consistency: Use consistent seeds across related embeddings for comparable results</li> <li>Cache efficiency: Different seeds create separate cache entries, so choose seeds wisely</li> </ul>"},{"location":"api/embedding/#advanced-examples","title":"Advanced Examples","text":""},{"location":"api/embedding/#vector-database-integration","title":"Vector Database Integration","text":"<pre><code>import steadytext\nimport numpy as np\nimport faiss\n\nclass VectorDB:\n    \"\"\"Simple vector database using FAISS.\"\"\"\n\n    def __init__(self, dimension: int = 1024, seed: int = 42):\n        self.dimension = dimension\n        self.seed = seed\n        self.index = faiss.IndexFlatL2(dimension)\n        self.metadata = []\n\n    def add_documents(self, documents: list, ids: list = None):\n        \"\"\"Add documents to the vector database.\"\"\"\n        embeddings = []\n\n        for i, doc in enumerate(documents):\n            # Use consistent seed for all documents\n            vec = steadytext.embed(doc, seed=self.seed)\n            embeddings.append(vec)\n\n            # Store metadata\n            self.metadata.append({\n                'id': ids[i] if ids else i,\n                'text': doc,\n                'embedding': vec\n            })\n\n        # Add to FAISS index\n        embeddings_array = np.array(embeddings).astype('float32')\n        self.index.add(embeddings_array)\n\n    def search(self, query: str, k: int = 5):\n        \"\"\"Search for similar documents.\"\"\"\n        # Use same seed as documents\n        query_vec = steadytext.embed(query, seed=self.seed).reshape(1, -1)\n\n        # Search in FAISS\n        distances, indices = self.index.search(query_vec.astype('float32'), k)\n\n        # Return results with metadata\n        results = []\n        for i, idx in enumerate(indices[0]):\n            if idx != -1:\n                results.append({\n                    'id': self.metadata[idx]['id'],\n                    'text': self.metadata[idx]['text'],\n                    'distance': distances[0][i],\n                    'similarity': 1 / (1 + distances[0][i])  # Convert distance to similarity\n                })\n\n        return results\n\n# Example usage\ndb = VectorDB(seed=100)  # Custom seed for this database\n\n# Add documents\ndocuments = [\n    \"Introduction to machine learning algorithms\",\n    \"Deep learning with neural networks\",\n    \"Natural language processing basics\",\n    \"Computer vision applications\",\n    \"Reinforcement learning in robotics\"\n]\n\ndb.add_documents(documents, ids=['ML101', 'DL201', 'NLP301', 'CV401', 'RL501'])\n\n# Search\nresults = db.search(\"text processing and NLP\", k=3)\nfor result in results:\n    print(f\"ID: {result['id']}, Similarity: {result['similarity']:.3f}\")\n    print(f\"Text: {result['text']}\\n\")\n</code></pre>"},{"location":"api/embedding/#multi-modal-embeddings","title":"Multi-Modal Embeddings","text":"<pre><code>import steadytext\nimport numpy as np\nfrom typing import Dict, Any\n\nclass MultiModalEmbedder:\n    \"\"\"Create combined embeddings from multiple modalities.\"\"\"\n\n    def __init__(self, base_seed: int = 42):\n        self.base_seed = base_seed\n        self.modality_seeds = {\n            'text': base_seed,\n            'title': base_seed + 1000,\n            'tags': base_seed + 2000,\n            'category': base_seed + 3000\n        }\n\n    def embed_document(self, document: Dict[str, Any]) -&gt; np.ndarray:\n        \"\"\"Create a combined embedding from multiple fields.\"\"\"\n        embeddings = []\n        weights = []\n\n        # Embed each modality with its own seed\n        if 'text' in document and document['text']:\n            vec = steadytext.embed(document['text'], seed=self.modality_seeds['text'])\n            embeddings.append(vec)\n            weights.append(0.5)  # Main content gets highest weight\n\n        if 'title' in document and document['title']:\n            vec = steadytext.embed(document['title'], seed=self.modality_seeds['title'])\n            embeddings.append(vec)\n            weights.append(0.3)\n\n        if 'tags' in document and document['tags']:\n            # Combine tags into single text\n            tags_text = \" \".join(document['tags'])\n            vec = steadytext.embed(tags_text, seed=self.modality_seeds['tags'])\n            embeddings.append(vec)\n            weights.append(0.15)\n\n        if 'category' in document and document['category']:\n            vec = steadytext.embed(document['category'], seed=self.modality_seeds['category'])\n            embeddings.append(vec)\n            weights.append(0.05)\n\n        if not embeddings:\n            # Fallback to zero vector if no content\n            return np.zeros(1024, dtype=np.float32)\n\n        # Weighted average\n        weights = np.array(weights) / sum(weights)  # Normalize weights\n        combined = np.average(embeddings, axis=0, weights=weights)\n\n        # Re-normalize\n        norm = np.linalg.norm(combined)\n        if norm &gt; 0:\n            combined = combined / norm\n\n        return combined\n\n# Example usage\nembedder = MultiModalEmbedder(base_seed=200)\n\n# Document with multiple fields\ndoc1 = {\n    'title': 'Introduction to Machine Learning',\n    'text': 'Machine learning is a subset of artificial intelligence...',\n    'tags': ['ML', 'AI', 'tutorial', 'beginner'],\n    'category': 'Education'\n}\n\ndoc2 = {\n    'title': 'Advanced Deep Learning Techniques',\n    'text': 'Deep learning has revolutionized computer vision...',\n    'tags': ['DL', 'neural networks', 'advanced'],\n    'category': 'Research'\n}\n\n# Create multi-modal embeddings\nvec1 = embedder.embed_document(doc1)\nvec2 = embedder.embed_document(doc2)\n\n# Compare similarity\nsimilarity = np.dot(vec1, vec2)\nprint(f\"Document similarity: {similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#incremental-embedding-updates","title":"Incremental Embedding Updates","text":"<pre><code>import steadytext\nimport numpy as np\nfrom collections import deque\n\nclass IncrementalEmbedder:\n    \"\"\"Maintain running average embeddings for evolving content.\"\"\"\n\n    def __init__(self, window_size: int = 10, seed: int = 42):\n        self.window_size = window_size\n        self.seed = seed\n        self.history = deque(maxlen=window_size)\n        self.current_embedding = None\n\n    def add_text(self, text: str) -&gt; np.ndarray:\n        \"\"\"Add new text and update running embedding.\"\"\"\n        # Embed new text\n        new_embedding = steadytext.embed(text, seed=self.seed)\n        self.history.append(new_embedding)\n\n        # Calculate running average\n        if len(self.history) &gt; 0:\n            avg_embedding = np.mean(list(self.history), axis=0)\n            # Re-normalize\n            self.current_embedding = avg_embedding / np.linalg.norm(avg_embedding)\n\n        return self.current_embedding\n\n    def get_evolution(self) -&gt; list:\n        \"\"\"Get the evolution of embeddings over time.\"\"\"\n        evolution = []\n        temp_history = []\n\n        for emb in self.history:\n            temp_history.append(emb)\n            avg = np.mean(temp_history, axis=0)\n            avg = avg / np.linalg.norm(avg)\n            evolution.append(avg)\n\n        return evolution\n\n# Example: Track topic drift in conversation\nembedder = IncrementalEmbedder(window_size=5, seed=300)\n\nconversation = [\n    \"Let's talk about machine learning\",\n    \"Neural networks are fascinating\",\n    \"Deep learning has many applications\",\n    \"But what about traditional algorithms?\",\n    \"Random forests are still useful\",\n    \"Statistical methods have their place\",\n    \"Linear regression is fundamental\"\n]\n\nprint(\"Conversation evolution:\")\nfor i, text in enumerate(conversation):\n    embedding = embedder.add_text(text)\n\n    if i &gt; 0:\n        # Compare to previous state\n        evolution = embedder.get_evolution()\n        similarity = np.dot(evolution[-1], evolution[0])\n        print(f\"Step {i}: '{text[:30]}...' - Drift from start: {1-similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#embedding-dimensionality-reduction","title":"Embedding Dimensionality Reduction","text":"<pre><code>import steadytext\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nimport matplotlib.pyplot as plt\n\nclass EmbeddingVisualizer:\n    \"\"\"Visualize high-dimensional embeddings in 2D/3D.\"\"\"\n\n    def __init__(self, seed: int = 42):\n        self.seed = seed\n        self.embeddings = []\n        self.labels = []\n\n    def add_texts(self, texts: list, labels: list = None):\n        \"\"\"Add texts with optional labels.\"\"\"\n        for i, text in enumerate(texts):\n            emb = steadytext.embed(text, seed=self.seed)\n            self.embeddings.append(emb)\n            self.labels.append(labels[i] if labels else str(i))\n\n    def reduce_pca(self, n_components: int = 2) -&gt; np.ndarray:\n        \"\"\"Reduce dimensions using PCA.\"\"\"\n        if not self.embeddings:\n            return np.array([])\n\n        pca = PCA(n_components=n_components, random_state=42)\n        reduced = pca.fit_transform(np.array(self.embeddings))\n\n        print(f\"PCA explained variance ratio: {pca.explained_variance_ratio_}\")\n        return reduced\n\n    def reduce_tsne(self, n_components: int = 2) -&gt; np.ndarray:\n        \"\"\"Reduce dimensions using t-SNE.\"\"\"\n        if not self.embeddings:\n            return np.array([])\n\n        tsne = TSNE(n_components=n_components, random_state=42, perplexity=5)\n        reduced = tsne.fit_transform(np.array(self.embeddings))\n        return reduced\n\n    def plot_2d(self, method: str = 'pca'):\n        \"\"\"Create 2D visualization.\"\"\"\n        if method == 'pca':\n            reduced = self.reduce_pca(2)\n        else:\n            reduced = self.reduce_tsne(2)\n\n        plt.figure(figsize=(10, 8))\n        plt.scatter(reduced[:, 0], reduced[:, 1])\n\n        for i, label in enumerate(self.labels):\n            plt.annotate(label, (reduced[i, 0], reduced[i, 1]), \n                        xytext=(5, 5), textcoords='offset points')\n\n        plt.title(f'Embedding Visualization ({method.upper()})')\n        plt.xlabel('Component 1')\n        plt.ylabel('Component 2')\n        plt.grid(True, alpha=0.3)\n        return plt\n\n# Example usage\nviz = EmbeddingVisualizer(seed=400)\n\n# Add different categories of text\ncategories = {\n    'AI': [\"machine learning\", \"neural networks\", \"deep learning\"],\n    'Food': [\"pizza recipe\", \"pasta cooking\", \"italian cuisine\"],\n    'Finance': [\"stock market\", \"investment strategy\", \"trading\"]\n}\n\nfor category, texts in categories.items():\n    for text in texts:\n        viz.add_texts([text], labels=[f\"{category}: {text}\"])\n\n# Visualize (would display plot in Jupyter)\n# plot = viz.plot_2d('tsne')\n# plot.show()\n</code></pre>"},{"location":"api/embedding/#cross-lingual-embeddings","title":"Cross-Lingual Embeddings","text":"<pre><code>import steadytext\nimport numpy as np\n\nclass CrossLingualEmbedder:\n    \"\"\"Create aligned embeddings across languages using seed variations.\"\"\"\n\n    def __init__(self, base_seed: int = 42):\n        self.base_seed = base_seed\n        # Different seed offsets for different languages\n        self.language_seeds = {\n            'en': base_seed,\n            'es': base_seed + 10000,\n            'fr': base_seed + 20000,\n            'de': base_seed + 30000,\n            'zh': base_seed + 40000\n        }\n\n    def embed(self, text: str, language: str = 'en') -&gt; np.ndarray:\n        \"\"\"Embed text with language-specific seed.\"\"\"\n        if language not in self.language_seeds:\n            language = 'en'  # Fallback to English\n\n        seed = self.language_seeds[language]\n        return steadytext.embed(text, seed=seed)\n\n    def align_embeddings(self, source_texts: list, target_texts: list, \n                        source_lang: str, target_lang: str) -&gt; tuple:\n        \"\"\"Create aligned embeddings for parallel texts.\"\"\"\n        source_embeddings = [self.embed(text, source_lang) for text in source_texts]\n        target_embeddings = [self.embed(text, target_lang) for text in target_texts]\n\n        # Simple alignment: compute transformation matrix\n        # In practice, you'd use more sophisticated methods\n        S = np.array(source_embeddings)\n        T = np.array(target_embeddings)\n\n        # Compute pseudo-inverse for alignment\n        # W = T @ S.T @ np.linalg.inv(S @ S.T)\n        # For simplicity, we'll just return the embeddings\n\n        return source_embeddings, target_embeddings\n\n    def cross_lingual_similarity(self, text1: str, lang1: str, \n                               text2: str, lang2: str) -&gt; float:\n        \"\"\"Compute similarity across languages.\"\"\"\n        vec1 = self.embed(text1, lang1)\n        vec2 = self.embed(text2, lang2)\n\n        # Apply simple heuristic adjustment for cross-lingual comparison\n        # In practice, you'd use learned alignment\n        if lang1 != lang2:\n            # Reduce similarity slightly for different languages\n            adjustment = 0.9\n        else:\n            adjustment = 1.0\n\n        return np.dot(vec1, vec2) * adjustment\n\n# Example usage\nembedder = CrossLingualEmbedder(base_seed=500)\n\n# Embed in different languages\nen_vec = embedder.embed(\"Hello world\", \"en\")\nes_vec = embedder.embed(\"Hola mundo\", \"es\")\nfr_vec = embedder.embed(\"Bonjour le monde\", \"fr\")\n\n# Compare cross-lingual similarities\nprint(\"Cross-lingual similarities:\")\nprint(f\"EN-ES: {embedder.cross_lingual_similarity('Hello world', 'en', 'Hola mundo', 'es'):.3f}\")\nprint(f\"EN-FR: {embedder.cross_lingual_similarity('Hello world', 'en', 'Bonjour le monde', 'fr'):.3f}\")\nprint(f\"ES-FR: {embedder.cross_lingual_similarity('Hola mundo', 'es', 'Bonjour le monde', 'fr'):.3f}\")\n\n# Same language comparison\nen_sim = embedder.cross_lingual_similarity('Hello world', 'en', 'Hi earth', 'en')\nprint(f\"\\nSame language (EN-EN): {en_sim:.3f}\")\n</code></pre>"},{"location":"api/embedding/#real-time-embedding-stream","title":"Real-time Embedding Stream","text":"<pre><code>import steadytext\nimport numpy as np\nimport time\nfrom typing import Iterator, Tuple\n\nclass EmbeddingStream:\n    \"\"\"Process streaming text data with real-time embeddings.\"\"\"\n\n    def __init__(self, chunk_size: int = 100, overlap: int = 20, seed: int = 42):\n        self.chunk_size = chunk_size\n        self.overlap = overlap\n        self.seed = seed\n        self.buffer = \"\"\n        self.processed_count = 0\n\n    def process_stream(self, text_stream: Iterator[str]) -&gt; Iterator[Tuple[str, np.ndarray]]:\n        \"\"\"Process streaming text and yield embeddings.\"\"\"\n        for text in text_stream:\n            self.buffer += text\n\n            # Process complete chunks\n            while len(self.buffer) &gt;= self.chunk_size:\n                # Extract chunk\n                chunk = self.buffer[:self.chunk_size]\n\n                # Generate embedding with position-based seed\n                chunk_seed = self.seed + self.processed_count\n                embedding = steadytext.embed(chunk, seed=chunk_seed)\n\n                yield chunk, embedding\n\n                # Move buffer forward with overlap\n                self.buffer = self.buffer[self.chunk_size - self.overlap:]\n                self.processed_count += 1\n\n        # Process remaining buffer\n        if self.buffer:\n            final_seed = self.seed + self.processed_count\n            embedding = steadytext.embed(self.buffer, seed=final_seed)\n            yield self.buffer, embedding\n\n# Example: Simulate streaming text\ndef text_generator():\n    \"\"\"Simulate streaming text data.\"\"\"\n    texts = [\n        \"Machine learning is transforming how we process information. \",\n        \"Neural networks can learn complex patterns from data. \",\n        \"Deep learning models require large amounts of training data. \",\n        \"Transfer learning helps when data is limited. \",\n        \"Embeddings capture semantic meaning in vector space. \"\n    ]\n\n    for text in texts:\n        # Simulate streaming by yielding words\n        words = text.split()\n        for word in words:\n            yield word + \" \"\n            time.sleep(0.1)  # Simulate real-time stream\n\n# Process stream\nstream_processor = EmbeddingStream(chunk_size=50, overlap=10, seed=600)\n\nprint(\"Processing text stream...\")\nembeddings_collected = []\n\nfor chunk, embedding in stream_processor.process_stream(text_generator()):\n    print(f\"Processed chunk: '{chunk[:30]}...' -&gt; Embedding shape: {embedding.shape}\")\n    embeddings_collected.append(embedding)\n\n# Analyze progression\nif len(embeddings_collected) &gt; 1:\n    print(f\"\\nTotal chunks processed: {len(embeddings_collected)}\")\n\n    # Check similarity progression\n    for i in range(1, len(embeddings_collected)):\n        sim = np.dot(embeddings_collected[i-1], embeddings_collected[i])\n        print(f\"Similarity between chunk {i-1} and {i}: {sim:.3f}\")\n</code></pre>"},{"location":"api/embedding/#troubleshooting","title":"Troubleshooting","text":""},{"location":"api/embedding/#common-issues","title":"Common Issues","text":"<p>Issue: Embeddings not deterministic <pre><code># Problem: Different results each run\nvec1 = steadytext.embed(\"test\")\n# ... restart Python ...\nvec2 = steadytext.embed(\"test\")\n# vec1 != vec2\n\n# Solution: Ensure consistent seed and environment\nimport os\nos.environ['PYTHONHASHSEED'] = '0'  # Set before importing steadytext\nimport steadytext\n\nvec1 = steadytext.embed(\"test\", seed=42)\nvec2 = steadytext.embed(\"test\", seed=42)\nassert np.array_equal(vec1, vec2)  # Now deterministic\n</code></pre></p> <p>Issue: Out of memory with large batches <pre><code># Problem: OOM with large text list\ntexts = [\"text\"] * 10000\nvectors = [steadytext.embed(t) for t in texts]  # May OOM\n\n# Solution: Process in batches\ndef embed_in_batches(texts, batch_size=100, seed=42):\n    embeddings = []\n    for i in range(0, len(texts), batch_size):\n        batch = texts[i:i+batch_size]\n        for text in batch:\n            embeddings.append(steadytext.embed(text, seed=seed))\n    return np.array(embeddings)\n\nvectors = embed_in_batches(texts)\n</code></pre></p> <p>Issue: Slow embedding generation <pre><code># Problem: First embedding is slow\nimport time\n\nstart = time.time()\nvec1 = steadytext.embed(\"test\")  # ~2-3 seconds (model loading)\nprint(f\"First: {time.time() - start:.2f}s\")\n\nstart = time.time()\nvec2 = steadytext.embed(\"test\")  # ~0.01 seconds (cached)\nprint(f\"Second: {time.time() - start:.2f}s\")\n\n# Solution: Preload models\nsteadytext.preload_models()  # Load once at startup\n# Now all embeddings will be fast\n</code></pre></p>"},{"location":"api/generation/","title":"Text Generation API","text":"<p>Functions for deterministic text generation.</p>"},{"location":"api/generation/#generate","title":"generate()","text":"<p>Generate deterministic text from a prompt.</p> <pre><code>def generate(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\",\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED,\n    schema: Optional[Union[Dict[str, Any], type, object]] = None,\n    regex: Optional[str] = None,\n    choices: Optional[List[str]] = None,\n    response_format: Optional[Dict[str, Any]] = None,\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre>"},{"location":"api/generation/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>prompt</code> <code>str</code> required Input text to generate from <code>max_new_tokens</code> <code>int</code> <code>512</code> Maximum number of tokens to generate <code>return_logprobs</code> <code>bool</code> <code>False</code> Return log probabilities with text <code>eos_string</code> <code>str</code> <code>\"[EOS]\"</code> Custom end-of-sequence string <code>model</code> <code>str</code> <code>None</code> Model name from registry (deprecated) <code>model_repo</code> <code>str</code> <code>None</code> Custom Hugging Face repository ID <code>model_filename</code> <code>str</code> <code>None</code> Custom model filename <code>size</code> <code>str</code> <code>None</code> Size shortcut: \"small\" or \"large\" <code>seed</code> <code>int</code> <code>42</code> Random seed for deterministic generation"},{"location":"api/generation/#returns","title":"Returns","text":"Basic UsageWith Log Probabilities <p>Returns: <code>str</code> - Generated text (512 tokens max)</p> <p>Returns: <code>Tuple[str, Optional[Dict]]</code> - Generated text and log probabilities</p>"},{"location":"api/generation/#examples","title":"Examples","text":"Simple GenerationCustom SeedCustom LengthWith Log ProbabilitiesCustom Stop String <pre><code>import steadytext\n\ntext = steadytext.generate(\"Write a Python function\")\nprint(text)\n# Always returns the same 512-token completion\n</code></pre> <pre><code># Generate with different seeds for variation\ntext1 = steadytext.generate(\"Write a story\", seed=123)\ntext2 = steadytext.generate(\"Write a story\", seed=123)  # Same as text1\ntext3 = steadytext.generate(\"Write a story\", seed=456)  # Different result\n\nprint(f\"Seed 123: {text1[:50]}...\")\nprint(f\"Seed 456: {text3[:50]}...\")\n</code></pre> <pre><code># Generate shorter responses\nshort_text = steadytext.generate(\"Explain AI\", max_new_tokens=50)\nlong_text = steadytext.generate(\"Explain AI\", max_new_tokens=200)\n\nprint(f\"Short ({len(short_text.split())} words): {short_text}\")\nprint(f\"Long ({len(long_text.split())} words): {long_text}\")\n</code></pre> <pre><code>text, logprobs = steadytext.generate(\n    \"Explain machine learning\", \n    return_logprobs=True\n)\n\nprint(\"Generated text:\", text)\nprint(\"Log probabilities:\", logprobs)\n</code></pre> <pre><code># Stop generation at custom string\ntext = steadytext.generate(\n    \"List programming languages until STOP\",\n    eos_string=\"STOP\"\n)\nprint(text)\n</code></pre>"},{"location":"api/generation/#generate_iter","title":"generate_iter()","text":"<p>Generate text iteratively, yielding tokens as produced.</p> <pre><code>def generate_iter(\n    prompt: str,\n    max_new_tokens: Optional[int] = None,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False,\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None,\n    seed: int = DEFAULT_SEED\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre>"},{"location":"api/generation/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>prompt</code> <code>str</code> required Input text to generate from <code>max_new_tokens</code> <code>int</code> <code>512</code> Maximum number of tokens to generate <code>eos_string</code> <code>str</code> <code>\"[EOS]\"</code> Custom end-of-sequence string <code>include_logprobs</code> <code>bool</code> <code>False</code> Yield log probabilities with tokens <code>model</code> <code>str</code> <code>None</code> Model name from registry (deprecated) <code>model_repo</code> <code>str</code> <code>None</code> Custom Hugging Face repository ID <code>model_filename</code> <code>str</code> <code>None</code> Custom model filename <code>size</code> <code>str</code> <code>None</code> Size shortcut: \"small\" or \"large\" <code>seed</code> <code>int</code> <code>42</code> Random seed for deterministic generation"},{"location":"api/generation/#returns_1","title":"Returns","text":"Basic StreamingWith Log Probabilities <p>Yields: <code>str</code> - Individual tokens/words</p> <p>Yields: <code>Tuple[str, Optional[Dict]]</code> - Token and log probabilities</p>"},{"location":"api/generation/#examples_1","title":"Examples","text":"Basic StreamingCustom Seed StreamingControlled Length StreamingWith Progress TrackingCustom Stop StringWith Log Probabilities <pre><code>import steadytext\n\nfor token in steadytext.generate_iter(\"Tell me a story\"):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code># Reproducible streaming with custom seeds\nprint(\"Stream 1 (seed=123):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=123):\n    print(token, end=\"\", flush=True)\n\nprint(\"\\n\\nStream 2 (seed=123 - same result):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=123):\n    print(token, end=\"\", flush=True)\n\nprint(\"\\n\\nStream 3 (seed=456 - different result):\")\nfor token in steadytext.generate_iter(\"Tell me a joke\", seed=456):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code># Stream with limited tokens\ntoken_count = 0\nfor token in steadytext.generate_iter(\"Explain quantum physics\", max_new_tokens=30):\n    print(token, end=\"\", flush=True)\n    token_count += 1\nprint(f\"\\nGenerated {token_count} tokens\")\n</code></pre> <pre><code>prompt = \"Explain quantum computing\"\ntokens = []\n\nfor token in steadytext.generate_iter(prompt):\n    tokens.append(token)\n    print(f\"Generated {len(tokens)} tokens\", end=\"\\r\")\n\nprint(f\"\\nComplete! Generated {len(tokens)} tokens\")\nprint(\"Full text:\", \"\".join(tokens))\n</code></pre> <pre><code>for token in steadytext.generate_iter(\n    \"Count from 1 to 10 then say DONE\", \n    eos_string=\"DONE\"\n):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code>for token, logprobs in steadytext.generate_iter(\n    \"Explain AI\", \n    include_logprobs=True\n):\n    confidence = logprobs.get('confidence', 0) if logprobs else 0\n    print(f\"{token} (confidence: {confidence:.2f})\", end=\"\")\n</code></pre>"},{"location":"api/generation/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/generation/#deterministic-behavior","title":"Deterministic Behavior","text":"<p>Both functions return identical results for identical inputs and seeds:</p> <pre><code># Default seed (42) - always identical\nresult1 = steadytext.generate(\"hello world\")\nresult2 = steadytext.generate(\"hello world\") \nassert result1 == result2  # Always passes!\n\n# Custom seeds - identical for same seed\nresult1 = steadytext.generate(\"hello world\", seed=123)\nresult2 = steadytext.generate(\"hello world\", seed=123)\nassert result1 == result2  # Always passes!\n\n# Different seeds produce different results\nresult1 = steadytext.generate(\"hello world\", seed=123)\nresult2 = steadytext.generate(\"hello world\", seed=456)\nassert result1 != result2  # Different seeds, different results\n\n# Streaming produces same tokens in same order for same seed\ntokens1 = list(steadytext.generate_iter(\"hello world\", seed=789))\ntokens2 = list(steadytext.generate_iter(\"hello world\", seed=789))\nassert tokens1 == tokens2  # Always passes!\n</code></pre>"},{"location":"api/generation/#custom-seed-use-cases","title":"Custom Seed Use Cases","text":"<pre><code># Experimental variations - try different seeds for the same prompt\nbaseline = steadytext.generate(\"Write a haiku about programming\", seed=42)\nvariation1 = steadytext.generate(\"Write a haiku about programming\", seed=123)\nvariation2 = steadytext.generate(\"Write a haiku about programming\", seed=456)\n\nprint(\"Baseline:\", baseline)\nprint(\"Variation 1:\", variation1)\nprint(\"Variation 2:\", variation2)\n\n# A/B testing - consistent results for testing\ntest_prompt = \"Explain machine learning to a beginner\"\nversion_a = steadytext.generate(test_prompt, seed=100)  # Version A\nversion_b = steadytext.generate(test_prompt, seed=200)  # Version B\n\n# Reproducible research - document your seeds\nresearch_seed = 42\nresults = []\nfor prompt in research_prompts:\n    result = steadytext.generate(prompt, seed=research_seed)\n    results.append((prompt, result))\n    research_seed += 1  # Increment for each prompt\n</code></pre>"},{"location":"api/generation/#caching","title":"Caching","text":"<p>Results are automatically cached using a frecency cache (LRU + frequency), with seed as part of the cache key:</p> <pre><code># First call: generates and caches result for default seed\ntext1 = steadytext.generate(\"common prompt\")  # ~2 seconds\n\n# Second call with same seed: returns cached result  \ntext2 = steadytext.generate(\"common prompt\")  # ~0.1 seconds\nassert text1 == text2  # Same result, much faster\n\n# Different seed: generates new result and caches separately\ntext3 = steadytext.generate(\"common prompt\", seed=123)  # ~2 seconds (new cache entry)\ntext4 = steadytext.generate(\"common prompt\", seed=123)  # ~0.1 seconds (cached)\n\nassert text3 == text4  # Same seed, same cached result\nassert text1 != text3  # Different seeds, different results\n\n# Cache keys include seed, so each seed gets its own cache entry\nfor seed in [100, 200, 300]:\n    steadytext.generate(\"warm up cache\", seed=seed)  # Each gets cached separately\n</code></pre>"},{"location":"api/generation/#fallback-behavior","title":"Fallback Behavior","text":"<p>When models can't be loaded, deterministic fallbacks are used with seed support:</p> <pre><code># Even without models, these return deterministic results based on seed\ntext1 = steadytext.generate(\"test prompt\", seed=42)  # Hash-based fallback\ntext2 = steadytext.generate(\"test prompt\", seed=42)  # Same result\ntext3 = steadytext.generate(\"test prompt\", seed=123) # Different result\n\nassert len(text1) &gt; 0  # Always has content\nassert text1 == text2  # Same seed, same fallback\nassert text1 != text3  # Different seed, different fallback\n\n# Fallback respects custom seeds for variation\nfallback_texts = []\nfor seed in [100, 200, 300]:\n    text = steadytext.generate(\"fallback test\", seed=seed)\n    fallback_texts.append(text)\n\n# All different due to different seeds\nassert len(set(fallback_texts)) == 3\n</code></pre>"},{"location":"api/generation/#performance-tips","title":"Performance Tips","text":"<p>Optimization Strategies</p> <ul> <li>Preload models: Call <code>steadytext.preload_models()</code> at startup</li> <li>Batch processing: Use <code>generate()</code> for multiple prompts rather than streaming individual tokens</li> <li>Cache warmup: Pre-generate common prompts to populate cache</li> <li>Memory management: Models stay loaded once initialized (singleton pattern)</li> <li>Seed management: Use consistent seeds for reproducible results, different seeds for variation</li> <li>Length control: Use <code>max_new_tokens</code> to control response length and generation time</li> </ul>"},{"location":"api/generation/#error-handling-and-edge-cases","title":"Error Handling and Edge Cases","text":""},{"location":"api/generation/#handling-invalid-inputs","title":"Handling Invalid Inputs","text":"<pre><code>import steadytext\n\n# Empty prompt handling\nempty_result = steadytext.generate(\"\")\nprint(f\"Empty prompt result: {empty_result[:50]}...\")  # Still generates deterministic output\n\n# Very long prompt handling (truncated to model's context window)\nlong_prompt = \"Explain \" * 1000 + \"machine learning\"\nresult = steadytext.generate(long_prompt)\nprint(f\"Long prompt handled: {len(result)} chars generated\")\n\n# Special characters and Unicode\nunicode_result = steadytext.generate(\"Write about \ud83e\udd16 and \u4eba\u5de5\u667a\u80fd\")\nprint(f\"Unicode handled: {unicode_result[:100]}...\")\n\n# Newlines and formatting\nmultiline = steadytext.generate(\"\"\"Write a function that:\n1. Takes a list\n2. Sorts it\n3. Returns the result\"\"\")\nprint(f\"Multiline prompt: {multiline[:100]}...\")\n</code></pre>"},{"location":"api/generation/#memory-efficient-streaming","title":"Memory-Efficient Streaming","text":"<pre><code>import sys\n\ndef stream_large_generation(prompt: str, max_chunks: int = 100):\n    \"\"\"Stream generation with memory tracking.\"\"\"\n    chunks = []\n    total_tokens = 0\n\n    for i, token in enumerate(steadytext.generate_iter(prompt)):\n        chunks.append(token)\n        total_tokens += 1\n\n        # Process in batches to manage memory\n        if len(chunks) &gt;= max_chunks:\n            # Process chunk (e.g., write to file)\n            sys.stdout.write(\"\".join(chunks))\n            sys.stdout.flush()\n            chunks = []\n\n    # Process remaining\n    if chunks:\n        sys.stdout.write(\"\".join(chunks))\n\n    print(f\"\\nGenerated {total_tokens} tokens\")\n\n# Use for large generations\nstream_large_generation(\"Write a comprehensive guide to Python programming\")\n</code></pre>"},{"location":"api/generation/#concurrent-generation","title":"Concurrent Generation","text":"<pre><code>import concurrent.futures\nimport steadytext\n\ndef parallel_generation(prompts: list, max_workers: int = 4):\n    \"\"\"Generate text for multiple prompts in parallel.\"\"\"\n    results = {}\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        # Submit all tasks\n        future_to_prompt = {\n            executor.submit(steadytext.generate, prompt, seed=idx): (prompt, idx)\n            for idx, prompt in enumerate(prompts)\n        }\n\n        # Collect results as they complete\n        for future in concurrent.futures.as_completed(future_to_prompt):\n            prompt, idx = future_to_prompt[future]\n            try:\n                result = future.result()\n                results[prompt] = result\n                print(f\"\u2713 Completed prompt {idx+1}: {prompt[:30]}...\")\n            except Exception as e:\n                print(f\"\u2717 Failed prompt {idx+1}: {e}\")\n                results[prompt] = None\n\n    return results\n\n# Example usage\nprompts = [\n    \"Write a Python function for sorting\",\n    \"Explain machine learning\",\n    \"Create a REST API example\",\n    \"Describe quantum computing\"\n]\n\nresults = parallel_generation(prompts)\nfor prompt, result in results.items():\n    print(f\"\\n{prompt}:\\n{result[:100]}...\\n\")\n</code></pre>"},{"location":"api/generation/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"api/generation/#custom-generation-pipeline","title":"Custom Generation Pipeline","text":"<pre><code>import steadytext\nimport re\n\nclass TextGenerator:\n    \"\"\"Custom text generation pipeline with preprocessing and postprocessing.\"\"\"\n\n    def __init__(self, default_seed: int = 42):\n        self.default_seed = default_seed\n        self.generation_count = 0\n\n    def preprocess(self, prompt: str) -&gt; str:\n        \"\"\"Clean and prepare prompt.\"\"\"\n        # Remove extra whitespace\n        prompt = \" \".join(prompt.split())\n\n        # Add context if needed\n        if not prompt.endswith((\".\", \"?\", \"!\", \":\")):\n            prompt += \":\"\n\n        return prompt\n\n    def postprocess(self, text: str) -&gt; str:\n        \"\"\"Clean generated text.\"\"\"\n        # Remove any [EOS] markers\n        text = text.replace(\"[EOS]\", \"\")\n\n        # Clean up whitespace\n        text = re.sub(r'\\s+', ' ', text).strip()\n\n        return text\n\n    def generate(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate with pre/post processing.\"\"\"\n        # Use incremental seeds for variety\n        seed = kwargs.pop('seed', self.default_seed + self.generation_count)\n        self.generation_count += 1\n\n        # Process\n        cleaned_prompt = self.preprocess(prompt)\n        raw_output = steadytext.generate(cleaned_prompt, seed=seed, **kwargs)\n        final_output = self.postprocess(raw_output)\n\n        return final_output\n\n# Usage\ngenerator = TextGenerator()\n\n# Generates different outputs due to incremental seeding\nresponse1 = generator.generate(\"write a function\")\nresponse2 = generator.generate(\"write a function\")  # Different seed\nresponse3 = generator.generate(\"write a function\", seed=100)  # Custom seed\n\nprint(f\"Response 1: {response1[:50]}...\")\nprint(f\"Response 2: {response2[:50]}...\")\nprint(f\"Response 3: {response3[:50]}...\")\n</code></pre>"},{"location":"api/generation/#template-based-generation","title":"Template-Based Generation","text":"<pre><code>import steadytext\nfrom typing import Dict, Any\n\nclass TemplateGenerator:\n    \"\"\"Generate text using templates with variable substitution.\"\"\"\n\n    def __init__(self):\n        self.templates = {\n            \"function\": \"Write a Python function that {action} for {input_type} and returns {output_type}\",\n            \"explanation\": \"Explain {concept} in simple terms for {audience}\",\n            \"comparison\": \"Compare and contrast {item1} and {item2} in terms of {criteria}\",\n            \"tutorial\": \"Create a step-by-step tutorial on {topic} for {skill_level} programmers\"\n        }\n\n    def generate_from_template(self, template_name: str, variables: Dict[str, Any], \n                             seed: int = 42, **kwargs) -&gt; str:\n        \"\"\"Generate text from a template with variables.\"\"\"\n        if template_name not in self.templates:\n            raise ValueError(f\"Unknown template: {template_name}\")\n\n        # Fill template\n        template = self.templates[template_name]\n        prompt = template.format(**variables)\n\n        # Generate\n        return steadytext.generate(prompt, seed=seed, **kwargs)\n\n    def batch_generate(self, template_name: str, variable_sets: list, \n                      base_seed: int = 42) -&gt; list:\n        \"\"\"Generate multiple outputs from the same template.\"\"\"\n        results = []\n\n        for i, variables in enumerate(variable_sets):\n            # Use different seed for each to ensure variety\n            result = self.generate_from_template(\n                template_name, \n                variables, \n                seed=base_seed + i\n            )\n            results.append({\n                \"variables\": variables,\n                \"output\": result\n            })\n\n        return results\n\n# Usage examples\ngen = TemplateGenerator()\n\n# Single generation\nfunction_code = gen.generate_from_template(\n    \"function\",\n    {\n        \"action\": \"calculates factorial\",\n        \"input_type\": \"positive integer\",\n        \"output_type\": \"integer\"\n    }\n)\nprint(f\"Generated function:\\n{function_code[:200]}...\\n\")\n\n# Batch generation with variations\ntutorials = gen.batch_generate(\n    \"tutorial\",\n    [\n        {\"topic\": \"async programming\", \"skill_level\": \"beginner\"},\n        {\"topic\": \"decorators\", \"skill_level\": \"intermediate\"},\n        {\"topic\": \"metaclasses\", \"skill_level\": \"advanced\"}\n    ]\n)\n\nfor tutorial in tutorials:\n    print(f\"\\nTopic: {tutorial['variables']['topic']}\")\n    print(f\"Output: {tutorial['output'][:150]}...\")\n</code></pre>"},{"location":"api/generation/#context-aware-generation","title":"Context-Aware Generation","text":"<pre><code>import steadytext\nfrom collections import deque\n\nclass ContextualGenerator:\n    \"\"\"Maintain context across multiple generations.\"\"\"\n\n    def __init__(self, context_window: int = 5):\n        self.context = deque(maxlen=context_window)\n        self.base_seed = 42\n        self.generation_count = 0\n\n    def add_context(self, text: str):\n        \"\"\"Add text to context history.\"\"\"\n        self.context.append(text)\n\n    def generate_with_context(self, prompt: str, include_context: bool = True) -&gt; str:\n        \"\"\"Generate text considering previous context.\"\"\"\n        if include_context and self.context:\n            # Build context prompt\n            context_str = \"\\n\".join(f\"Previous: {ctx}\" for ctx in self.context)\n            full_prompt = f\"{context_str}\\n\\nNow: {prompt}\"\n        else:\n            full_prompt = prompt\n\n        # Generate with unique seed\n        result = steadytext.generate(\n            full_prompt, \n            seed=self.base_seed + self.generation_count\n        )\n        self.generation_count += 1\n\n        # Add to context for next generation\n        self.add_context(f\"{prompt} -&gt; {result[:100]}...\")\n\n        return result\n\n    def clear_context(self):\n        \"\"\"Reset context history.\"\"\"\n        self.context.clear()\n        self.generation_count = 0\n\n# Example: Story continuation\nstory_gen = ContextualGenerator()\n\n# Generate story parts with context\npart1 = story_gen.generate_with_context(\"Once upon a time in a digital kingdom\")\nprint(f\"Part 1: {part1[:150]}...\\n\")\n\npart2 = story_gen.generate_with_context(\"The hero discovered a mysterious artifact\")\nprint(f\"Part 2 (with context): {part2[:150]}...\\n\")\n\npart3 = story_gen.generate_with_context(\"Suddenly, the artifact began to glow\")\nprint(f\"Part 3 (with context): {part3[:150]}...\\n\")\n\n# Generate without context for comparison\nstory_gen.clear_context()\npart3_no_context = story_gen.generate_with_context(\n    \"Suddenly, the artifact began to glow\", \n    include_context=False\n)\nprint(f\"Part 3 (no context): {part3_no_context[:150]}...\")\n</code></pre>"},{"location":"api/generation/#debugging-and-monitoring","title":"Debugging and Monitoring","text":""},{"location":"api/generation/#generation-analytics","title":"Generation Analytics","text":"<pre><code>import steadytext\nimport time\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass GenerationMetrics:\n    prompt: str\n    seed: int\n    duration: float\n    token_count: int\n    cached: bool\n    output_preview: str\n\nclass GenerationMonitor:\n    \"\"\"Monitor and analyze generation patterns.\"\"\"\n\n    def __init__(self):\n        self.metrics: List[GenerationMetrics] = []\n\n    def generate_with_metrics(self, prompt: str, seed: int = 42, **kwargs) -&gt; str:\n        \"\"\"Generate text while collecting metrics.\"\"\"\n        start_time = time.time()\n\n        # Check if likely cached (by doing a duplicate call)\n        _ = steadytext.generate(prompt, seed=seed, **kwargs)\n        check_time = time.time() - start_time\n\n        # Actual generation\n        start_time = time.time()\n        result = steadytext.generate(prompt, seed=seed, **kwargs)\n        duration = time.time() - start_time\n\n        # Determine if it was cached\n        cached = duration &lt; check_time * 0.5  # Much faster = likely cached\n\n        # Count tokens (approximate)\n        token_count = len(result.split())\n\n        # Store metrics\n        metric = GenerationMetrics(\n            prompt=prompt,\n            seed=seed,\n            duration=duration,\n            token_count=token_count,\n            cached=cached,\n            output_preview=result[:50] + \"...\"\n        )\n        self.metrics.append(metric)\n\n        return result\n\n    def get_summary(self):\n        \"\"\"Get generation performance summary.\"\"\"\n        if not self.metrics:\n            return \"No generations recorded\"\n\n        total_time = sum(m.duration for m in self.metrics)\n        cached_count = sum(1 for m in self.metrics if m.cached)\n        avg_tokens = sum(m.token_count for m in self.metrics) / len(self.metrics)\n\n        return f\"\"\"\nGeneration Summary:\n- Total generations: {len(self.metrics)}\n- Total time: {total_time:.2f}s\n- Average time: {total_time/len(self.metrics):.3f}s\n- Cached hits: {cached_count} ({cached_count/len(self.metrics)*100:.1f}%)\n- Average tokens: {avg_tokens:.0f}\n\"\"\"\n\n# Example usage\nmonitor = GenerationMonitor()\n\n# Generate with monitoring\nprompts = [\n    \"Write a Python function\",\n    \"Write a Python function\",  # Duplicate - should be cached\n    \"Explain recursion\",\n    \"Write a Python function\",  # Another duplicate\n    \"Create a class example\"\n]\n\nfor prompt in prompts:\n    result = monitor.generate_with_metrics(prompt)\n    print(f\"Generated for '{prompt[:20]}...': {len(result)} chars\")\n\nprint(monitor.get_summary())\n\n# Show detailed metrics\nprint(\"\\nDetailed Metrics:\")\nfor i, metric in enumerate(monitor.metrics, 1):\n    print(f\"{i}. {metric.prompt[:30]}... - {metric.duration:.3f}s \"\n          f\"{'(cached)' if metric.cached else '(computed)'}\")\n</code></pre>"},{"location":"api/generation/#integration-examples","title":"Integration Examples","text":""},{"location":"api/generation/#flask-web-service","title":"Flask Web Service","text":"<pre><code>from flask import Flask, request, jsonify\nimport steadytext\n\napp = Flask(__name__)\n\n@app.route('/generate', methods=['POST'])\ndef generate_text():\n    \"\"\"API endpoint for text generation.\"\"\"\n    data = request.get_json()\n\n    # Extract parameters\n    prompt = data.get('prompt', '')\n    seed = data.get('seed', 42)\n    max_tokens = data.get('max_tokens', 512)\n\n    if not prompt:\n        return jsonify({'error': 'No prompt provided'}), 400\n\n    try:\n        # Generate text\n        result = steadytext.generate(\n            prompt, \n            seed=seed,\n            max_new_tokens=max_tokens\n        )\n\n        return jsonify({\n            'prompt': prompt,\n            'seed': seed,\n            'generated_text': result,\n            'token_count': len(result.split())\n        })\n\n    except Exception as e:\n        return jsonify({'error': str(e)}), 500\n\n@app.route('/generate/stream', methods=['POST'])\ndef stream_text():\n    \"\"\"SSE endpoint for streaming generation.\"\"\"\n    from flask import Response\n\n    data = request.get_json()\n    prompt = data.get('prompt', '')\n    seed = data.get('seed', 42)\n\n    def generate():\n        yield \"data: {\\\"status\\\": \\\"starting\\\"}\\n\\n\"\n\n        for token in steadytext.generate_iter(prompt, seed=seed):\n            # Escape token for JSON\n            escaped = token.replace('\"', '\\\\\"').replace('\\n', '\\\\n')\n            yield f\"data: {{\\\"token\\\": \\\"{escaped}\\\"}}\\n\\n\"\n\n        yield \"data: {\\\"status\\\": \\\"complete\\\"}\\n\\n\"\n\n    return Response(generate(), mimetype=\"text/event-stream\")\n\n# Run with: flask run\n</code></pre>"},{"location":"api/generation/#async-generation-with-asyncio","title":"Async Generation with asyncio","text":"<pre><code>import asyncio\nimport steadytext\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass AsyncGenerator:\n    \"\"\"Async wrapper for SteadyText generation.\"\"\"\n\n    def __init__(self, max_workers: int = 4):\n        self.executor = ThreadPoolExecutor(max_workers=max_workers)\n\n    async def generate_async(self, prompt: str, **kwargs) -&gt; str:\n        \"\"\"Generate text asynchronously.\"\"\"\n        loop = asyncio.get_event_loop()\n        result = await loop.run_in_executor(\n            self.executor,\n            steadytext.generate,\n            prompt,\n            *kwargs.values()\n        )\n        return result\n\n    async def generate_many(self, prompts: list, base_seed: int = 42) -&gt; list:\n        \"\"\"Generate multiple texts concurrently.\"\"\"\n        tasks = [\n            self.generate_async(prompt, seed=base_seed + i)\n            for i, prompt in enumerate(prompts)\n        ]\n        return await asyncio.gather(*tasks)\n\n    def cleanup(self):\n        \"\"\"Cleanup executor.\"\"\"\n        self.executor.shutdown(wait=True)\n\n# Example usage\nasync def main():\n    generator = AsyncGenerator()\n\n    # Single async generation\n    result = await generator.generate_async(\"Write async Python code\")\n    print(f\"Single result: {result[:100]}...\\n\")\n\n    # Batch async generation\n    prompts = [\n        \"Explain async/await\",\n        \"Write a coroutine example\",\n        \"Describe event loops\",\n        \"Create an async API client\"\n    ]\n\n    start = asyncio.get_event_loop().time()\n    results = await generator.generate_many(prompts)\n    duration = asyncio.get_event_loop().time() - start\n\n    print(f\"Generated {len(results)} texts in {duration:.2f}s\")\n    for i, (prompt, result) in enumerate(zip(prompts, results)):\n        print(f\"\\n{i+1}. {prompt}:\\n{result[:100]}...\")\n\n    generator.cleanup()\n\n# Run the async example\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre>"},{"location":"examples/","title":"Examples","text":"<p>Real-world usage patterns and code examples for SteadyText.</p>"},{"location":"examples/#overview","title":"Overview","text":"<p>This section demonstrates practical applications of SteadyText across different use cases:</p> <ul> <li>Testing with AI - Reliable AI tests that never flake</li> <li>CLI Tools - Building deterministic command-line tools</li> <li>Caching Guide - Configure and optimize caching</li> <li>Custom Seeds Guide - Use custom seeds for reproducible variations</li> <li>Daemon Usage Guide - Persistent model serving for faster responses</li> <li>Error Handling Guide - Handle errors gracefully</li> <li>Performance Tuning Guide - Optimize for speed and efficiency</li> <li>PostgreSQL Integration Examples - Integrate with PostgreSQL</li> </ul> <p>All examples showcase SteadyText's core principle: same input \u2192 same output, every time.</p>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#basic-usage","title":"Basic Usage","text":"<pre><code>import steadytext\n\n# Deterministic text generation\ncode = steadytext.generate(\"implement binary search in Python\")\nassert \"def binary_search\" in code  # Always passes!\n\n# Streaming generation\nfor token in steadytext.generate_iter(\"explain quantum computing\"):\n    print(token, end=\"\", flush=True)\n\n# Deterministic embeddings  \nvec = steadytext.embed(\"Hello world\")  # 1024-dim numpy array\nprint(f\"Shape: {vec.shape}, Norm: {np.linalg.norm(vec):.6f}\")\n</code></pre>"},{"location":"examples/#testing-applications","title":"Testing Applications","text":"<pre><code>def test_ai_code_generation():\n    \"\"\"Test that never flakes - same input, same output.\"\"\"\n    prompt = \"write a function to reverse a string\"\n    result = my_ai_function(prompt)\n    expected = steadytext.generate(prompt)\n    assert result == expected  # Deterministic comparison!\n\ndef test_embedding_similarity():\n    \"\"\"Reliable similarity testing.\"\"\"\n    vec1 = steadytext.embed(\"machine learning\")\n    vec2 = steadytext.embed(\"artificial intelligence\")\n    similarity = np.dot(vec1, vec2)  # Already normalized\n    assert similarity &gt; 0.7  # Always passes with same threshold\n</code></pre>"},{"location":"examples/#cli-tool-building","title":"CLI Tool Building","text":"<pre><code>import click\nimport steadytext\n\n@click.command()\n@click.argument('topic')\ndef motivate(topic):\n    \"\"\"Generate motivational quotes about any topic.\"\"\"\n    prompt = f\"Write an inspiring quote about {topic}\"\n    quote = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udcaa {quote}\")\n\n# Usage: python script.py \"programming\"\n# Always generates the same motivational quote for \"programming\"\n</code></pre>"},{"location":"examples/#use-case-categories","title":"Use Case Categories","text":""},{"location":"examples/#testing-quality-assurance","title":"\ud83e\uddea Testing &amp; Quality Assurance","text":"<p>Perfect for: - Unit tests with AI components - Integration testing with deterministic outputs - Regression testing for AI features - Mock AI services for development</p>"},{"location":"examples/#developer-tools","title":"\ud83d\udee0\ufe0f Developer Tools","text":"<p>Ideal for: - Code generation tools - Documentation generators - CLI utilities with AI features - Build system integration</p>"},{"location":"examples/#data-content-generation","title":"\ud83d\udcca Data &amp; Content Generation","text":"<p>Great for: - Synthetic data generation - Content templates - Data augmentation for testing - Reproducible research datasets</p>"},{"location":"examples/#search-similarity","title":"\ud83d\udd0d Search &amp; Similarity","text":"<p>Excellent for: - Semantic search systems - Document clustering - Content recommendation - Duplicate detection</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<ol> <li>Browse examples - Check out Testing and CLI Tools</li> <li>Run the code - All examples are fully executable</li> <li>Adapt for your use case - Copy and modify patterns that fit your needs</li> </ol>"},{"location":"examples/#example-repository","title":"Example Repository","text":"<p>All examples are available in the examples/ directory of the SteadyText repository:</p> <pre><code>git clone https://github.com/julep-ai/steadytext.git\ncd steadytext/examples\npython basic_usage.py\npython testing_with_ai.py  \npython cli_tools.py\n</code></pre> <p>Deterministic Outputs</p> <p>Remember: all examples produce identical outputs every time you run them. This predictability is SteadyText's core feature and what makes it perfect for testing and tooling applications.</p>"},{"location":"examples/caching/","title":"Caching Guide","text":"<p>Learn how to configure and optimize SteadyText's caching system for maximum performance.</p>"},{"location":"examples/caching/#overview","title":"Overview","text":"<p>SteadyText uses a sophisticated frecency cache (frequency + recency) that combines: - LRU (Least Recently Used): Recent items stay cached - Frequency counting: Popular items are retained longer - Disk persistence: Cache survives restarts - Thread safety: Safe for concurrent access</p>"},{"location":"examples/caching/#cache-architecture","title":"Cache Architecture","text":""},{"location":"examples/caching/#two-tier-cache-system","title":"Two-Tier Cache System","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502         Application Layer           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502     Generation Cache    \u2502 Embedding \u2502\n\u2502    (256 entries, 50MB) \u2502   Cache   \u2502\n\u2502                        \u2502(512, 100MB)\u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502      SQLite Backend (Thread-Safe)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/caching/#cache-files-location","title":"Cache Files Location","text":"<pre><code>import steadytext\nfrom pathlib import Path\n\n# Get cache directory\ncache_dir = Path.home() / \".cache\" / \"steadytext\" / \"caches\"\nprint(f\"Cache location: {cache_dir}\")\n\n# Cache files\ngeneration_cache = cache_dir / \"generation_cache.db\"\nembedding_cache = cache_dir / \"embedding_cache.db\"\n</code></pre>"},{"location":"examples/caching/#configuration","title":"Configuration","text":""},{"location":"examples/caching/#environment-variables","title":"Environment Variables","text":"<pre><code># Generation cache settings\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256      # Max entries\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50.0  # Max file size\n\n# Embedding cache settings  \nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512       # Max entries\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100.0  # Max file size\n\n# Disable cache entirely (not recommended)\nexport STEADYTEXT_DISABLE_CACHE=1\n</code></pre>"},{"location":"examples/caching/#python-configuration","title":"Python Configuration","text":"<pre><code>import os\nimport steadytext\n\n# Configure before importing/using steadytext\nos.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = '1024'\nos.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = '200.0'\n\n# Verify configuration\nfrom steadytext import get_cache_manager\ncache_manager = get_cache_manager()\nstats = cache_manager.get_cache_stats()\nprint(f\"Generation cache capacity: {stats['generation']['capacity']}\")\n</code></pre>"},{"location":"examples/caching/#cache-management","title":"Cache Management","text":""},{"location":"examples/caching/#monitoring-cache-performance","title":"Monitoring Cache Performance","text":"<pre><code>from steadytext import get_cache_manager\nimport time\n\nclass CacheMonitor:\n    \"\"\"Monitor cache performance and hit rates.\"\"\"\n\n    def __init__(self):\n        self.cache_manager = get_cache_manager()\n        self.initial_stats = self.cache_manager.get_cache_stats()\n\n    def get_hit_rate(self, cache_type='generation'):\n        \"\"\"Calculate cache hit rate.\"\"\"\n        stats = self.cache_manager.get_cache_stats()[cache_type]\n        hits = stats.get('hits', 0)\n        misses = stats.get('misses', 0)\n        total = hits + misses\n\n        if total == 0:\n            return 0.0\n\n        return hits / total * 100\n\n    def monitor_operation(self, operation, *args, **kwargs):\n        \"\"\"Monitor a single operation's cache behavior.\"\"\"\n        stats_before = self.cache_manager.get_cache_stats()\n        start_time = time.time()\n\n        result = operation(*args, **kwargs)\n\n        duration = time.time() - start_time\n        stats_after = self.cache_manager.get_cache_stats()\n\n        # Determine if it was a cache hit\n        gen_hits_diff = stats_after['generation']['hits'] - stats_before['generation']['hits']\n        emb_hits_diff = stats_after['embedding']['hits'] - stats_before['embedding']['hits']\n\n        cache_hit = gen_hits_diff &gt; 0 or emb_hits_diff &gt; 0\n\n        return {\n            'result': result,\n            'duration': duration,\n            'cache_hit': cache_hit,\n            'stats_delta': {\n                'generation_hits': gen_hits_diff,\n                'embedding_hits': emb_hits_diff\n            }\n        }\n\n    def print_summary(self):\n        \"\"\"Print cache performance summary.\"\"\"\n        stats = self.cache_manager.get_cache_stats()\n\n        print(\"=== Cache Performance Summary ===\")\n        for cache_type in ['generation', 'embedding']:\n            cache_stats = stats[cache_type]\n            hit_rate = self.get_hit_rate(cache_type)\n\n            print(f\"\\n{cache_type.title()} Cache:\")\n            print(f\"  Size: {cache_stats['size']} entries\")\n            print(f\"  Hit Rate: {hit_rate:.1f}%\")\n            print(f\"  Hits: {cache_stats.get('hits', 0)}\")\n            print(f\"  Misses: {cache_stats.get('misses', 0)}\")\n\n# Usage example\nmonitor = CacheMonitor()\n\n# Monitor text generation\nresult1 = monitor.monitor_operation(\n    steadytext.generate, \n    \"Write a haiku about caching\"\n)\nprint(f\"First call: {result1['duration']:.3f}s (cache hit: {result1['cache_hit']})\")\n\n# Same prompt - should be cached\nresult2 = monitor.monitor_operation(\n    steadytext.generate, \n    \"Write a haiku about caching\"\n)\nprint(f\"Second call: {result2['duration']:.3f}s (cache hit: {result2['cache_hit']})\")\n\nmonitor.print_summary()\n</code></pre>"},{"location":"examples/caching/#cache-warming","title":"Cache Warming","text":"<pre><code>import steadytext\nfrom typing import List\nimport concurrent.futures\n\ndef warm_cache_sequential(prompts: List[str], seeds: List[int] = None):\n    \"\"\"Warm cache with common prompts sequentially.\"\"\"\n    if seeds is None:\n        seeds = [42]  # Default seed only\n\n    warmed = 0\n    for prompt in prompts:\n        for seed in seeds:\n            _ = steadytext.generate(prompt, seed=seed, max_new_tokens=100)\n            warmed += 1\n\n    return warmed\n\ndef warm_cache_parallel(prompts: List[str], seeds: List[int] = None, max_workers: int = 4):\n    \"\"\"Warm cache with parallel generation.\"\"\"\n    if seeds is None:\n        seeds = [42]\n\n    tasks = [(prompt, seed) for prompt in prompts for seed in seeds]\n\n    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n        futures = [\n            executor.submit(steadytext.generate, prompt, seed=seed, max_new_tokens=100)\n            for prompt, seed in tasks\n        ]\n\n        # Wait for all to complete\n        completed = 0\n        for future in concurrent.futures.as_completed(futures):\n            future.result()  # Get result to ensure completion\n            completed += 1\n\n    return completed\n\n# Common prompts to cache\ncommon_prompts = [\n    \"Write a Python function\",\n    \"Explain this error\",\n    \"Generate test data\",\n    \"Create documentation\",\n    \"Write unit tests\",\n    \"Optimize this code\",\n    \"Review this pull request\",\n    \"Suggest improvements\"\n]\n\n# Common seeds if using multiple\ncommon_seeds = [42, 100, 200]  # Add your common seeds\n\n# Warm cache\nprint(\"Warming cache...\")\nwarmed = warm_cache_parallel(common_prompts, common_seeds)\nprint(f\"Cache warmed with {warmed} entries\")\n\n# Verify cache is warm\nfrom steadytext import get_cache_manager\nstats = get_cache_manager().get_cache_stats()\nprint(f\"Generation cache size: {stats['generation']['size']}\")\n</code></pre>"},{"location":"examples/caching/#cache-optimization-strategies","title":"Cache Optimization Strategies","text":"<pre><code>import steadytext\nfrom collections import defaultdict\nfrom datetime import datetime, timedelta\n\nclass CacheOptimizer:\n    \"\"\"Optimize cache usage patterns.\"\"\"\n\n    def __init__(self):\n        self.usage_patterns = defaultdict(lambda: {\n            'count': 0,\n            'last_used': None,\n            'avg_generation_time': 0\n        })\n\n    def track_usage(self, prompt: str, seed: int, generation_time: float):\n        \"\"\"Track prompt usage patterns.\"\"\"\n        key = f\"{prompt}:{seed}\"\n        pattern = self.usage_patterns[key]\n\n        pattern['count'] += 1\n        pattern['last_used'] = datetime.now()\n\n        # Update average generation time\n        avg = pattern['avg_generation_time']\n        count = pattern['count']\n        pattern['avg_generation_time'] = (avg * (count - 1) + generation_time) / count\n\n    def get_cache_priorities(self, top_n: int = 20):\n        \"\"\"Get prompts that should be prioritized for caching.\"\"\"\n        # Score based on frequency and recency\n        now = datetime.now()\n        scores = []\n\n        for key, pattern in self.usage_patterns.items():\n            # Frequency score\n            freq_score = pattern['count']\n\n            # Recency score (higher for more recent)\n            if pattern['last_used']:\n                age = (now - pattern['last_used']).total_seconds()\n                recency_score = 1 / (1 + age / 3600)  # Decay over hours\n            else:\n                recency_score = 0\n\n            # Generation time score (prioritize slow generations)\n            time_score = pattern['avg_generation_time']\n\n            # Combined score\n            score = freq_score * 0.5 + recency_score * 0.3 + time_score * 0.2\n\n            scores.append((score, key, pattern))\n\n        # Sort by score\n        scores.sort(reverse=True)\n\n        return scores[:top_n]\n\n    def recommend_cache_size(self):\n        \"\"\"Recommend optimal cache size based on usage.\"\"\"\n        total_unique = len(self.usage_patterns)\n        frequently_used = sum(1 for p in self.usage_patterns.values() if p['count'] &gt; 5)\n\n        # Recommend 1.5x frequently used items + buffer\n        recommended = int(frequently_used * 1.5 + 50)\n\n        return {\n            'total_unique_prompts': total_unique,\n            'frequently_used': frequently_used,\n            'recommended_size': recommended,\n            'current_default': 256\n        }\n\n# Example usage\noptimizer = CacheOptimizer()\n\n# Simulate usage tracking\nimport time\ntest_prompts = [\n    (\"Write a function to sort a list\", 42),\n    (\"Explain machine learning\", 42),\n    (\"Write a function to sort a list\", 42),  # Repeated\n    (\"Generate test cases\", 100),\n    (\"Write a function to sort a list\", 42),  # Popular\n]\n\nfor prompt, seed in test_prompts:\n    start = time.time()\n    _ = steadytext.generate(prompt, seed=seed)\n    duration = time.time() - start\n    optimizer.track_usage(prompt, seed, duration)\n\n# Get optimization recommendations\nprint(\"=== Cache Optimization Report ===\")\n\npriorities = optimizer.get_cache_priorities(5)\nprint(\"\\nTop prompts to keep cached:\")\nfor score, key, pattern in priorities:\n    prompt, seed = key.rsplit(':', 1)\n    print(f\"  Score: {score:.2f} - {prompt[:50]}... (seed: {seed})\")\n    print(f\"    Used: {pattern['count']}x, Avg time: {pattern['avg_generation_time']:.3f}s\")\n\nrecommendations = optimizer.recommend_cache_size()\nprint(f\"\\nCache size recommendations:\")\nprint(f\"  Total unique: {recommendations['total_unique_prompts']}\")\nprint(f\"  Frequently used: {recommendations['frequently_used']}\")\nprint(f\"  Recommended size: {recommendations['recommended_size']}\")\n</code></pre>"},{"location":"examples/caching/#advanced-cache-patterns","title":"Advanced Cache Patterns","text":""},{"location":"examples/caching/#hierarchical-caching","title":"Hierarchical Caching","text":"<pre><code>import steadytext\nfrom typing import Dict, Any, Optional\nimport json\nimport hashlib\n\nclass HierarchicalCache:\n    \"\"\"Implement hierarchical caching for complex workflows.\"\"\"\n\n    def __init__(self):\n        self.memory_cache = {}  # Fast in-memory cache\n        self.cache_manager = steadytext.get_cache_manager()\n\n    def _generate_cache_key(self, category: str, subcategory: str, \n                          prompt: str, seed: int) -&gt; str:\n        \"\"\"Generate hierarchical cache key.\"\"\"\n        components = [category, subcategory, prompt, str(seed)]\n        combined = \":\".join(components)\n\n        # Create hash for consistent key length\n        key_hash = hashlib.md5(combined.encode()).hexdigest()\n\n        return f\"{category}:{subcategory}:{key_hash}\"\n\n    def get_or_generate(self, category: str, subcategory: str, \n                       prompt: str, seed: int = 42, **kwargs) -&gt; str:\n        \"\"\"Get from cache or generate with hierarchical key.\"\"\"\n        cache_key = self._generate_cache_key(category, subcategory, prompt, seed)\n\n        # Check memory cache first\n        if cache_key in self.memory_cache:\n            return self.memory_cache[cache_key]\n\n        # Generate and cache\n        result = steadytext.generate(prompt, seed=seed, **kwargs)\n\n        # Store in memory cache\n        self.memory_cache[cache_key] = result\n\n        return result\n\n    def preload_category(self, category: str, items: List[Dict[str, Any]]):\n        \"\"\"Preload entire category into cache.\"\"\"\n        loaded = 0\n\n        for item in items:\n            result = self.get_or_generate(\n                category,\n                item.get('subcategory', 'default'),\n                item['prompt'],\n                item.get('seed', 42),\n                **item.get('kwargs', {})\n            )\n            loaded += 1\n\n        return loaded\n\n    def clear_category(self, category: str):\n        \"\"\"Clear all cache entries for a category.\"\"\"\n        keys_to_remove = [k for k in self.memory_cache if k.startswith(f\"{category}:\")]\n\n        for key in keys_to_remove:\n            del self.memory_cache[key]\n\n        return len(keys_to_remove)\n\n# Usage example\nh_cache = HierarchicalCache()\n\n# Generate with hierarchy\nemail_subject = h_cache.get_or_generate(\n    \"emails\", \n    \"marketing\",\n    \"Write a subject line for Black Friday sale\",\n    seed=100\n)\n\nemail_body = h_cache.get_or_generate(\n    \"emails\",\n    \"marketing\", \n    \"Write email body for Black Friday sale\",\n    seed=100\n)\n\n# Preload documentation category\ndocs_to_cache = [\n    {\n        'subcategory': 'api',\n        'prompt': 'Document a REST API endpoint',\n        'seed': 42,\n        'kwargs': {'max_new_tokens': 200}\n    },\n    {\n        'subcategory': 'functions',\n        'prompt': 'Document a Python function',\n        'seed': 42,\n        'kwargs': {'max_new_tokens': 150}\n    }\n]\n\nloaded = h_cache.preload_category('documentation', docs_to_cache)\nprint(f\"Preloaded {loaded} documentation templates\")\n</code></pre>"},{"location":"examples/caching/#cache-aware-generation","title":"Cache-Aware Generation","text":"<pre><code>import steadytext\nfrom typing import Optional, Tuple\nimport time\n\nclass CacheAwareGenerator:\n    \"\"\"Generator that adapts based on cache state.\"\"\"\n\n    def __init__(self):\n        self.cache_manager = steadytext.get_cache_manager()\n        self.performance_threshold = 0.1  # 100ms\n\n    def is_likely_cached(self, prompt: str, seed: int = 42) -&gt; bool:\n        \"\"\"Check if a prompt is likely cached without generating.\"\"\"\n        # This is a heuristic - actual implementation would need\n        # to check cache internals\n        stats = self.cache_manager.get_cache_stats()\n\n        # Simple heuristic: if we have items in cache and\n        # this is a common prompt pattern\n        if stats['generation']['size'] &gt; 0:\n            common_patterns = ['Write a', 'Explain', 'Create', 'Generate']\n            return any(prompt.startswith(p) for p in common_patterns)\n\n        return False\n\n    def generate_with_fallback(self, primary_prompt: str, \n                             fallback_prompt: Optional[str] = None,\n                             seed: int = 42, **kwargs) -&gt; Tuple[str, bool]:\n        \"\"\"Generate with fallback if primary isn't cached.\"\"\"\n        start_time = time.time()\n\n        # Try primary prompt\n        result = steadytext.generate(primary_prompt, seed=seed, **kwargs)\n        duration = time.time() - start_time\n\n        # If slow (not cached) and we have fallback\n        if duration &gt; self.performance_threshold and fallback_prompt:\n            # Check if fallback might be cached\n            if self.is_likely_cached(fallback_prompt, seed):\n                fallback_result = steadytext.generate(fallback_prompt, seed=seed, **kwargs)\n                return fallback_result, True\n\n        return result, False\n\n    def batch_generate_optimized(self, prompts: List[str], seed: int = 42, **kwargs):\n        \"\"\"Generate batch with cache-aware ordering.\"\"\"\n        results = {}\n        timings = {}\n\n        # First pass: try all prompts and measure timing\n        for prompt in prompts:\n            start = time.time()\n            result = steadytext.generate(prompt, seed=seed, **kwargs)\n            duration = time.time() - start\n\n            results[prompt] = result\n            timings[prompt] = duration\n\n        # Analyze cache performance\n        cached_prompts = [p for p, t in timings.items() if t &lt; self.performance_threshold]\n        uncached_prompts = [p for p, t in timings.items() if t &gt;= self.performance_threshold]\n\n        stats = {\n            'total': len(prompts),\n            'cached': len(cached_prompts),\n            'uncached': len(uncached_prompts),\n            'cache_rate': len(cached_prompts) / len(prompts) * 100,\n            'avg_cached_time': sum(timings[p] for p in cached_prompts) / len(cached_prompts) if cached_prompts else 0,\n            'avg_uncached_time': sum(timings[p] for p in uncached_prompts) / len(uncached_prompts) if uncached_prompts else 0\n        }\n\n        return results, stats\n\n# Usage\ncache_gen = CacheAwareGenerator()\n\n# Single generation with fallback\nprimary = \"Generate a complex analysis of quantum computing applications in cryptography\"\nfallback = \"Explain quantum computing\"  # Likely cached\n\nresult, used_fallback = cache_gen.generate_with_fallback(\n    primary, \n    fallback,\n    max_new_tokens=200\n)\n\nprint(f\"Used fallback: {used_fallback}\")\n\n# Batch generation with analysis\ntest_prompts = [\n    \"Write a Python function\",  # Likely cached\n    \"Explain machine learning\",  # Likely cached\n    \"Analyze the socioeconomic impact of automation on rural communities\",  # Unlikely\n    \"Generate test data\",  # Possibly cached\n    \"Describe the philosophical implications of consciousness in AI systems\"  # Unlikely\n]\n\nresults, stats = cache_gen.batch_generate_optimized(test_prompts, max_new_tokens=100)\n\nprint(\"\\n=== Batch Generation Cache Stats ===\")\nprint(f\"Total prompts: {stats['total']}\")\nprint(f\"Cached: {stats['cached']} ({stats['cache_rate']:.1f}%)\")\nprint(f\"Average cached time: {stats['avg_cached_time']:.3f}s\")\nprint(f\"Average uncached time: {stats['avg_uncached_time']:.3f}s\")\nprint(f\"Speed improvement: {stats['avg_uncached_time'] / stats['avg_cached_time']:.1f}x\")\n</code></pre>"},{"location":"examples/caching/#cache-persistence-patterns","title":"Cache Persistence Patterns","text":"<pre><code>import steadytext\nimport json\nfrom pathlib import Path\nfrom typing import Dict, List\nimport pickle\n\nclass CachePersistenceManager:\n    \"\"\"Manage cache persistence and restoration.\"\"\"\n\n    def __init__(self, backup_dir: str = \"./cache_backups\"):\n        self.backup_dir = Path(backup_dir)\n        self.backup_dir.mkdir(exist_ok=True)\n        self.cache_manager = steadytext.get_cache_manager()\n\n    def export_cache_metadata(self) -&gt; Dict:\n        \"\"\"Export cache metadata for analysis.\"\"\"\n        stats = self.cache_manager.get_cache_stats()\n\n        metadata = {\n            'timestamp': datetime.now().isoformat(),\n            'generation_cache': {\n                'size': stats['generation']['size'],\n                'capacity': stats['generation'].get('capacity', 256),\n                'hit_rate': self._calculate_hit_rate(stats['generation'])\n            },\n            'embedding_cache': {\n                'size': stats['embedding']['size'],\n                'capacity': stats['embedding'].get('capacity', 512),\n                'hit_rate': self._calculate_hit_rate(stats['embedding'])\n            }\n        }\n\n        return metadata\n\n    def _calculate_hit_rate(self, cache_stats: Dict) -&gt; float:\n        \"\"\"Calculate cache hit rate.\"\"\"\n        hits = cache_stats.get('hits', 0)\n        misses = cache_stats.get('misses', 0)\n        total = hits + misses\n\n        return (hits / total * 100) if total &gt; 0 else 0.0\n\n    def save_cache_state(self, name: str):\n        \"\"\"Save current cache state metadata.\"\"\"\n        metadata = self.export_cache_metadata()\n\n        filename = self.backup_dir / f\"cache_state_{name}.json\"\n        with open(filename, 'w') as f:\n            json.dump(metadata, f, indent=2)\n\n        print(f\"Cache state saved to {filename}\")\n        return filename\n\n    def analyze_cache_history(self) -&gt; Dict:\n        \"\"\"Analyze cache performance over time.\"\"\"\n        history_files = list(self.backup_dir.glob(\"cache_state_*.json\"))\n\n        if not history_files:\n            return {\"error\": \"No cache history found\"}\n\n        history = []\n        for file in sorted(history_files):\n            with open(file, 'r') as f:\n                data = json.load(f)\n                data['filename'] = file.name\n                history.append(data)\n\n        # Analyze trends\n        analysis = {\n            'total_snapshots': len(history),\n            'date_range': {\n                'start': history[0]['timestamp'],\n                'end': history[-1]['timestamp']\n            },\n            'generation_cache_trend': {\n                'min_size': min(h['generation_cache']['size'] for h in history),\n                'max_size': max(h['generation_cache']['size'] for h in history),\n                'avg_hit_rate': sum(h['generation_cache']['hit_rate'] for h in history) / len(history)\n            },\n            'embedding_cache_trend': {\n                'min_size': min(h['embedding_cache']['size'] for h in history),\n                'max_size': max(h['embedding_cache']['size'] for h in history),\n                'avg_hit_rate': sum(h['embedding_cache']['hit_rate'] for h in history) / len(history)\n            }\n        }\n\n        return analysis\n\n# Usage\npersistence = CachePersistenceManager()\n\n# Save current state\npersistence.save_cache_state(\"before_optimization\")\n\n# Do some work...\nfor i in range(10):\n    steadytext.generate(f\"Test prompt {i}\", seed=42)\n\n# Save after work\npersistence.save_cache_state(\"after_batch_generation\")\n\n# Analyze history\nanalysis = persistence.analyze_cache_history()\nprint(\"\\n=== Cache History Analysis ===\")\nprint(json.dumps(analysis, indent=2))\n</code></pre>"},{"location":"examples/caching/#cache-performance-tuning","title":"Cache Performance Tuning","text":""},{"location":"examples/caching/#benchmark-cache-impact","title":"Benchmark Cache Impact","text":"<pre><code>import steadytext\nimport time\nimport statistics\nfrom typing import List, Dict\n\nclass CacheBenchmark:\n    \"\"\"Benchmark cache performance impact.\"\"\"\n\n    def __init__(self):\n        self.cache_manager = steadytext.get_cache_manager()\n\n    def benchmark_single_prompt(self, prompt: str, seed: int = 42, \n                              iterations: int = 10) -&gt; Dict:\n        \"\"\"Benchmark a single prompt with cold and warm cache.\"\"\"\n        # Clear cache for cold start\n        self.cache_manager.clear_all_caches()\n\n        timings = {\n            'cold': [],\n            'warm': []\n        }\n\n        # Cold cache timing (first call)\n        start = time.time()\n        _ = steadytext.generate(prompt, seed=seed)\n        timings['cold'].append(time.time() - start)\n\n        # Warm cache timings\n        for _ in range(iterations - 1):\n            start = time.time()\n            _ = steadytext.generate(prompt, seed=seed)\n            timings['warm'].append(time.time() - start)\n\n        return {\n            'prompt': prompt[:50] + '...' if len(prompt) &gt; 50 else prompt,\n            'cold_time': timings['cold'][0],\n            'warm_avg': statistics.mean(timings['warm']),\n            'warm_std': statistics.stdev(timings['warm']) if len(timings['warm']) &gt; 1 else 0,\n            'speedup': timings['cold'][0] / statistics.mean(timings['warm'])\n        }\n\n    def benchmark_cache_sizes(self, test_prompts: List[str], \n                            cache_sizes: List[int]) -&gt; Dict:\n        \"\"\"Benchmark performance with different cache sizes.\"\"\"\n        results = {}\n        original_capacity = os.environ.get('STEADYTEXT_GENERATION_CACHE_CAPACITY', '256')\n\n        try:\n            for size in cache_sizes:\n                # Set cache size\n                os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = str(size)\n\n                # Restart cache with new size\n                # Note: In practice, this would require restarting the process\n                self.cache_manager.clear_all_caches()\n\n                # Benchmark with this cache size\n                hit_count = 0\n                total_time = 0\n\n                for i, prompt in enumerate(test_prompts):\n                    start = time.time()\n                    _ = steadytext.generate(prompt, seed=42)\n                    duration = time.time() - start\n                    total_time += duration\n\n                    # Simple hit detection (fast = hit)\n                    if duration &lt; 0.1:\n                        hit_count += 1\n\n                results[size] = {\n                    'hit_rate': hit_count / len(test_prompts) * 100,\n                    'avg_time': total_time / len(test_prompts),\n                    'total_time': total_time\n                }\n\n        finally:\n            # Restore original capacity\n            os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = original_capacity\n\n        return results\n\n    def find_optimal_cache_size(self, typical_prompts: List[str]) -&gt; int:\n        \"\"\"Find optimal cache size for typical usage.\"\"\"\n        unique_prompts = len(set(typical_prompts))\n        prompt_frequency = {}\n\n        for prompt in typical_prompts:\n            prompt_frequency[prompt] = prompt_frequency.get(prompt, 0) + 1\n\n        # Prompts that appear more than once\n        repeated_prompts = sum(1 for count in prompt_frequency.values() if count &gt; 1)\n\n        # Recommend size based on usage pattern\n        if repeated_prompts / unique_prompts &gt; 0.5:\n            # High repetition - smaller cache OK\n            optimal = int(unique_prompts * 0.7)\n        else:\n            # Low repetition - need larger cache\n            optimal = int(unique_prompts * 1.2)\n\n        # Ensure reasonable bounds\n        optimal = max(64, min(optimal, 1024))\n\n        return optimal\n\n# Run benchmarks\nbenchmark = CacheBenchmark()\n\n# Single prompt benchmark\nprompt = \"Write a comprehensive guide to Python decorators\"\nresult = benchmark.benchmark_single_prompt(prompt, iterations=20)\n\nprint(\"=== Single Prompt Benchmark ===\")\nprint(f\"Prompt: {result['prompt']}\")\nprint(f\"Cold cache: {result['cold_time']:.3f}s\")\nprint(f\"Warm cache: {result['warm_avg']:.3f}s \u00b1 {result['warm_std']:.3f}s\")\nprint(f\"Speedup: {result['speedup']:.1f}x\")\n\n# Typical usage pattern\ntypical_prompts = [\n    \"Write a function\",\n    \"Explain this error\",\n    \"Write a function\",  # Repeated\n    \"Generate test data\",\n    \"Write a function\",  # Popular\n    \"Create documentation\",\n    \"Explain this error\",  # Repeated\n    \"Optimize code\",\n    \"Write unit tests\",\n    \"Write a function\"   # Very popular\n]\n\noptimal = benchmark.find_optimal_cache_size(typical_prompts)\nprint(f\"\\nRecommended cache size for your usage: {optimal}\")\n</code></pre>"},{"location":"examples/caching/#cache-debugging","title":"Cache Debugging","text":""},{"location":"examples/caching/#cache-inspector","title":"Cache Inspector","text":"<pre><code>import steadytext\nfrom typing import Optional\nimport json\n\nclass CacheInspector:\n    \"\"\"Debug and inspect cache behavior.\"\"\"\n\n    def __init__(self):\n        self.cache_manager = steadytext.get_cache_manager()\n        self.generation_log = []\n\n    def trace_generation(self, prompt: str, seed: int = 42, **kwargs):\n        \"\"\"Trace a generation through the cache system.\"\"\"\n        # Get initial stats\n        stats_before = self.cache_manager.get_cache_stats()\n\n        # Time the generation\n        import time\n        start_time = time.time()\n        result = steadytext.generate(prompt, seed=seed, **kwargs)\n        duration = time.time() - start_time\n\n        # Get final stats\n        stats_after = self.cache_manager.get_cache_stats()\n\n        # Analyze what happened\n        gen_cache_before = stats_before['generation']\n        gen_cache_after = stats_after['generation']\n\n        cache_hit = gen_cache_after.get('hits', 0) &gt; gen_cache_before.get('hits', 0)\n\n        trace = {\n            'prompt': prompt,\n            'seed': seed,\n            'duration': duration,\n            'cache_hit': cache_hit,\n            'cache_size_before': gen_cache_before['size'],\n            'cache_size_after': gen_cache_after['size'],\n            'result_preview': result[:100] + '...' if len(result) &gt; 100 else result\n        }\n\n        self.generation_log.append(trace)\n\n        return trace\n\n    def analyze_cache_behavior(self):\n        \"\"\"Analyze patterns in cache behavior.\"\"\"\n        if not self.generation_log:\n            return \"No generation logs to analyze\"\n\n        total = len(self.generation_log)\n        hits = sum(1 for log in self.generation_log if log['cache_hit'])\n\n        hit_timings = [log['duration'] for log in self.generation_log if log['cache_hit']]\n        miss_timings = [log['duration'] for log in self.generation_log if not log['cache_hit']]\n\n        analysis = {\n            'total_generations': total,\n            'cache_hits': hits,\n            'cache_misses': total - hits,\n            'hit_rate': hits / total * 100 if total &gt; 0 else 0,\n            'avg_hit_time': sum(hit_timings) / len(hit_timings) if hit_timings else 0,\n            'avg_miss_time': sum(miss_timings) / len(miss_timings) if miss_timings else 0,\n            'time_saved': sum(miss_timings) - sum(hit_timings) if hit_timings else 0\n        }\n\n        return analysis\n\n    def export_trace_log(self, filename: str):\n        \"\"\"Export trace log for analysis.\"\"\"\n        with open(filename, 'w') as f:\n            json.dump(self.generation_log, f, indent=2)\n\n        print(f\"Trace log exported to {filename}\")\n\n# Debug cache behavior\ninspector = CacheInspector()\n\n# Trace various generations\ntest_cases = [\n    (\"Write a hello world program\", 42),\n    (\"Write a hello world program\", 42),  # Should hit\n    (\"Explain recursion\", 42),\n    (\"Write a hello world program\", 100),  # Different seed\n    (\"Explain recursion\", 42),  # Should hit\n]\n\nprint(\"=== Cache Trace Log ===\")\nfor prompt, seed in test_cases:\n    trace = inspector.trace_generation(prompt, seed)\n    print(f\"Prompt: {prompt[:30]}... | Seed: {seed}\")\n    print(f\"  Hit: {trace['cache_hit']} | Time: {trace['duration']:.3f}s\")\n    print(f\"  Cache size: {trace['cache_size_before']} -&gt; {trace['cache_size_after']}\")\n    print()\n\n# Analyze behavior\nanalysis = inspector.analyze_cache_behavior()\nprint(\"\\n=== Cache Behavior Analysis ===\")\nprint(f\"Hit rate: {analysis['hit_rate']:.1f}%\")\nprint(f\"Average hit time: {analysis['avg_hit_time']:.3f}s\")\nprint(f\"Average miss time: {analysis['avg_miss_time']:.3f}s\")\nprint(f\"Time saved by cache: {analysis['time_saved']:.3f}s\")\n\n# Export for further analysis\ninspector.export_trace_log(\"cache_trace.json\")\n</code></pre>"},{"location":"examples/caching/#best-practices","title":"Best Practices","text":""},{"location":"examples/caching/#1-cache-configuration","title":"1. Cache Configuration","text":"<pre><code># optimal_config.py - Optimal cache configuration\n\nimport os\n\ndef configure_cache_for_production():\n    \"\"\"Configure cache for production use.\"\"\"\n    # Larger cache for production\n    os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = '1024'\n    os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = '200.0'\n\n    # Even larger for embeddings (they're smaller)\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_CAPACITY'] = '2048'\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB'] = '500.0'\n\ndef configure_cache_for_development():\n    \"\"\"Configure cache for development.\"\"\"\n    # Smaller cache for development\n    os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = '128'\n    os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = '25.0'\n\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_CAPACITY'] = '256'\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB'] = '50.0'\n\ndef configure_cache_for_testing():\n    \"\"\"Configure cache for testing.\"\"\"\n    # Minimal cache for testing\n    os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = '32'\n    os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = '10.0'\n\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_CAPACITY'] = '64'\n    os.environ['STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB'] = '20.0'\n</code></pre>"},{"location":"examples/caching/#2-cache-warming-strategy","title":"2. Cache Warming Strategy","text":"<pre><code># cache_warmer.py - Strategic cache warming\n\nimport steadytext\nfrom typing import List, Dict\n\nclass StrategicCacheWarmer:\n    \"\"\"Warm cache based on usage patterns.\"\"\"\n\n    def __init__(self):\n        self.priority_prompts = {\n            'high': [],      # Always cache\n            'medium': [],    # Cache if space\n            'low': []        # Cache opportunistically\n        }\n\n    def add_prompts(self, prompts: List[str], priority: str = 'medium'):\n        \"\"\"Add prompts to warming queue.\"\"\"\n        self.priority_prompts[priority].extend(prompts)\n\n    def warm_cache(self, available_time: float = 30.0):\n        \"\"\"Warm cache within time budget.\"\"\"\n        import time\n        start_time = time.time()\n        warmed = {'high': 0, 'medium': 0, 'low': 0}\n\n        # Process by priority\n        for priority in ['high', 'medium', 'low']:\n            for prompt in self.priority_prompts[priority]:\n                if time.time() - start_time &gt; available_time:\n                    break\n\n                _ = steadytext.generate(prompt, max_new_tokens=100)\n                warmed[priority] += 1\n\n        return warmed\n\n# Configure warming\nwarmer = StrategicCacheWarmer()\n\n# High priority - critical paths\nwarmer.add_prompts([\n    \"Generate error message\",\n    \"Create validation response\",\n    \"Format API response\"\n], priority='high')\n\n# Medium priority - common operations\nwarmer.add_prompts([\n    \"Write documentation\",\n    \"Generate test data\",\n    \"Create example\"\n], priority='medium')\n\n# Low priority - nice to have\nwarmer.add_prompts([\n    \"Explain concept\",\n    \"Generate tutorial\"\n], priority='low')\n\n# Warm with 10 second budget\nwarmed = warmer.warm_cache(available_time=10.0)\nprint(f\"Cache warmed: {warmed}\")\n</code></pre>"},{"location":"examples/caching/#3-cache-monitoring","title":"3. Cache Monitoring","text":"<pre><code># monitor_cache.py - Production cache monitoring\n\nimport steadytext\nimport time\nimport logging\nfrom datetime import datetime\n\nclass ProductionCacheMonitor:\n    \"\"\"Monitor cache in production.\"\"\"\n\n    def __init__(self, alert_threshold: float = 50.0):\n        self.alert_threshold = alert_threshold\n        self.logger = logging.getLogger(__name__)\n\n    def check_cache_health(self) -&gt; Dict:\n        \"\"\"Check cache health metrics.\"\"\"\n        cache_manager = steadytext.get_cache_manager()\n        stats = cache_manager.get_cache_stats()\n\n        health = {\n            'timestamp': datetime.now().isoformat(),\n            'healthy': True,\n            'warnings': []\n        }\n\n        # Check generation cache\n        gen_stats = stats['generation']\n        gen_hit_rate = self._calculate_hit_rate(gen_stats)\n\n        if gen_hit_rate &lt; self.alert_threshold:\n            health['warnings'].append(\n                f\"Low generation cache hit rate: {gen_hit_rate:.1f}%\"\n            )\n            health['healthy'] = False\n\n        # Check embedding cache\n        emb_stats = stats['embedding']\n        emb_hit_rate = self._calculate_hit_rate(emb_stats)\n\n        if emb_hit_rate &lt; self.alert_threshold:\n            health['warnings'].append(\n                f\"Low embedding cache hit rate: {emb_hit_rate:.1f}%\"\n            )\n            health['healthy'] = False\n\n        # Check cache size\n        if gen_stats['size'] &gt;= gen_stats.get('capacity', 256) * 0.95:\n            health['warnings'].append(\"Generation cache near capacity\")\n\n        if emb_stats['size'] &gt;= emb_stats.get('capacity', 512) * 0.95:\n            health['warnings'].append(\"Embedding cache near capacity\")\n\n        return health\n\n    def _calculate_hit_rate(self, stats: Dict) -&gt; float:\n        \"\"\"Calculate hit rate from stats.\"\"\"\n        hits = stats.get('hits', 0)\n        misses = stats.get('misses', 0)\n        total = hits + misses\n\n        return (hits / total * 100) if total &gt; 0 else 0.0\n\n    def continuous_monitoring(self, interval: int = 300):\n        \"\"\"Monitor cache continuously.\"\"\"\n        while True:\n            health = self.check_cache_health()\n\n            if not health['healthy']:\n                self.logger.warning(f\"Cache health issues: {health['warnings']}\")\n            else:\n                self.logger.info(\"Cache healthy\")\n\n            time.sleep(interval)\n\n# Set up monitoring\nmonitor = ProductionCacheMonitor(alert_threshold=60.0)\nhealth = monitor.check_cache_health()\n\nprint(\"=== Cache Health Check ===\")\nprint(f\"Status: {'Healthy' if health['healthy'] else 'Issues Detected'}\")\nif health['warnings']:\n    print(\"Warnings:\")\n    for warning in health['warnings']:\n        print(f\"  - {warning}\")\n</code></pre>"},{"location":"examples/caching/#summary","title":"Summary","text":"<p>Effective cache management in SteadyText involves:</p> <ol> <li>Configuration: Size caches appropriately for your workload</li> <li>Warming: Pre-populate cache with common prompts</li> <li>Monitoring: Track hit rates and performance</li> <li>Optimization: Adjust based on usage patterns</li> <li>Debugging: Use tools to understand cache behavior</li> </ol> <p>Remember: A well-tuned cache can provide 10-100x speedup for repeated operations!</p>"},{"location":"examples/custom-seeds/","title":"Custom Seeds Guide","text":"<p>Learn how to use custom seeds in SteadyText for reproducible variations in text generation and embeddings.</p>"},{"location":"examples/custom-seeds/#overview","title":"Overview","text":"<p>SteadyText uses seeds to control randomness, allowing you to: - Generate different outputs for the same prompt - Ensure reproducible results across runs - Create variations while maintaining determinism - Control randomness in production systems</p>"},{"location":"examples/custom-seeds/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding Seeds</li> <li>Text Generation with Seeds</li> <li>Streaming with Seeds</li> <li>Embeddings with Seeds</li> <li>Seed Strategies</li> <li>CLI Seed Usage</li> <li>Best Practices</li> <li>Troubleshooting</li> </ul>"},{"location":"examples/custom-seeds/#understanding-seeds","title":"Understanding Seeds","text":""},{"location":"examples/custom-seeds/#what-is-a-seed","title":"What is a Seed?","text":"<p>A seed is an integer that initializes the random number generator. Same seed + same input = same output, always.</p> <pre><code>import steadytext\n\n# Default seed (42) - always same result\ntext1 = steadytext.generate(\"Hello world\")\ntext2 = steadytext.generate(\"Hello world\")\nassert text1 == text2  # Always true\n\n# Custom seeds - different results\ntext3 = steadytext.generate(\"Hello world\", seed=123)\ntext4 = steadytext.generate(\"Hello world\", seed=456)\nassert text3 != text4  # Different seeds, different outputs\n</code></pre>"},{"location":"examples/custom-seeds/#seed-behavior","title":"Seed Behavior","text":"<ul> <li>Deterministic: Same seed always produces same result</li> <li>Independent: Each operation uses its own seed</li> <li>Cascading: Seed affects all random choices in generation</li> <li>Cross-platform: Same seed works identically everywhere</li> </ul>"},{"location":"examples/custom-seeds/#basic-seed-usage","title":"Basic Seed Usage","text":""},{"location":"examples/custom-seeds/#simple-text-generation","title":"Simple Text Generation","text":"<pre><code>import steadytext\n\n# Default seed (42) - consistent across runs\ntext1 = steadytext.generate(\"Write a haiku about AI\")\ntext2 = steadytext.generate(\"Write a haiku about AI\")\nassert text1 == text2  # Always identical\n\n# Custom seed - reproducible but different from default\ntext3 = steadytext.generate(\"Write a haiku about AI\", seed=123)\ntext4 = steadytext.generate(\"Write a haiku about AI\", seed=123)\nassert text3 == text4  # Same seed, same result\nassert text1 != text3  # Different seeds, different results\n\nprint(\"Default seed result:\", text1)\nprint(\"Custom seed result:\", text3)\n</code></pre>"},{"location":"examples/custom-seeds/#embedding-generation","title":"Embedding Generation","text":"<pre><code>import numpy as np\n\n# Default seed embeddings\nemb1 = steadytext.embed(\"artificial intelligence\")\nemb2 = steadytext.embed(\"artificial intelligence\")\nassert np.array_equal(emb1, emb2)  # Identical\n\n# Custom seed embeddings\nemb3 = steadytext.embed(\"artificial intelligence\", seed=456)\nemb4 = steadytext.embed(\"artificial intelligence\", seed=456)\nassert np.array_equal(emb3, emb4)  # Same seed, same result\nassert not np.array_equal(emb1, emb3)  # Different seeds, different embeddings\n\n# Calculate similarity between different seed embeddings\nsimilarity = np.dot(emb1, emb3)  # Cosine similarity (vectors are normalized)\nprint(f\"Similarity between different seeds: {similarity:.3f}\")\n</code></pre>"},{"location":"examples/custom-seeds/#reproducible-research","title":"Reproducible Research","text":""},{"location":"examples/custom-seeds/#research-workflow-example","title":"Research Workflow Example","text":"<p>```python import steadytext import json from datetime import datetime</p> <p>class ReproducibleResearch:     def init(self, base_seed=42):         self.base_seed = base_seed         self.current_seed = base_seed         self.results = []         self.metadata = {             \"start_time\": datetime.now().isoformat(),             \"base_seed\": base_seed,             \"steadytext_version\": \"2.1.0+\",         }</p> <pre><code>def generate_with_logging(self, prompt, **kwargs):\n    \"\"\"Generate text and log the result with seed information.\"\"\"\n    result = steadytext.generate(prompt, seed=self.current_seed, **kwargs)\n\n    self.results.append({\n        \"seed\": self.current_seed,\n        \"prompt\": prompt,\n        \"result\": result,\n        \"kwargs\": kwargs,\n        \"timestamp\": datetime.now().isoformat()\n    })\n\n    self.current_seed += 1  # Increment for next generation\n    return result\n\ndef embed_with_logging(self, text, **kwargs):\n    \"\"\"Generate embedding and log the result with seed information.\"\"\"\n    embedding = steadytext.embed(text, seed=self.current_seed, **kwargs)\n\n    self.results.append({\n        \"seed\": self.current_seed,\n        \"text\": text,\n        \"embedding\": embedding.tolist(),  # Convert numpy array to list\n        \"kwargs\": kwargs,\n        \"timestamp\": datetime.now().isoformat()\n    })\n\n    self.current_seed += 1\n    return embedding\n\ndef save_results(self, filename):\n    \"\"\"Save all results to a JSON file for reproducibility.\"\"\"\n    with open(filename, 'w') as f:\n        json.dump({\n            \"metadata\": self.metadata,\n            \"results\": self.results\n        }, f, indent=2)\n\ndef load_and_verify(self, filename):\n    \"\"\"Load previous results and verify reproducibility.\"\"\"\n    with open(filename, 'r') as f:\n        data = json.load(f)\n\n    print(\"Verifying reproducibility...\")\n    for result in data[\"results\"]:\n        if \"prompt\" in result:  # Text generation\n            regenerated = steadytext.generate(\n                result[\"prompt\"], \n                seed=result[\"seed\"],\n                **result[\"kwargs\"]\n            )\n            if regenerated == result[\"result\"]:\n                print(f\"\u2713 Seed {result['seed']}: Text generation verified\")\n            else:\n                print(f\"\u2717 Seed {result['seed']}: Text generation FAILED\")\n\n        elif \"text\" in result:  # Embedding\n            regenerated = steadytext.embed(\n                result[\"text\"],\n                seed=result[\"seed\"],\n                **result[\"kwargs\"]\n            )\n            if np.allclose(regenerated, result[\"embedding\"], atol=1e-6):\\n                    print(f\"\u2713 Seed {result['seed']}: Embedding verified\")\\n                else:\\n                    print(f\"\u2717 Seed {result['seed']}: Embedding FAILED\")\\n\\n# Usage example\\nresearch = ReproducibleResearch(base_seed=100)\\n\\n# Conduct research with automatic seed management\\nresearch_prompts = [\\n    \"Explain the benefits of renewable energy\",\\n    \"Describe the future of artificial intelligence\",\\n    \"Summarize the importance of biodiversity\"\\n]\\n\\nfor prompt in research_prompts:\\n    result = research.generate_with_logging(prompt, max_new_tokens=200)\\n    print(f\"Generated {len(result)} characters for: {prompt[:50]}...\")\\n\\n# Generate embeddings for analysis\\nembedding_texts = [\"AI\", \"machine learning\", \"deep learning\"]\\nfor text in embedding_texts:\\n    embedding = research.embed_with_logging(text)\\n    print(f\"Generated embedding for: {text}\")\\n\\n# Save results for reproducibility\\nresearch.save_results(\"research_results.json\")\\nprint(\"Results saved to research_results.json\")\\n\\n# Later: verify reproducibility\\nresearch.load_and_verify(\"research_results.json\")\\n```\\n\\n## A/B Testing with Seeds\\n\\n### Content Comparison Framework\\n\\n```python\\nimport steadytext\\nfrom typing import List, Dict, Any\\n\\nclass ABTester:\\n    def __init__(self):\\n        self.variants = {}\\n    \\n    def create_variants(self, prompt: str, variant_seeds: List[int], **kwargs) -&gt; Dict[str, str]:\\n        \"\"\"Create multiple variants of the same prompt using different seeds.\"\"\"\\n        variants = {}\\n        for i, seed in enumerate(variant_seeds):\\n            variant_name = f\"variant_{chr(65 + i)}\"  # A, B, C, etc.\\n            variants[variant_name] = steadytext.generate(\\n                prompt, \\n                seed=seed, \\n                **kwargs\\n            )\\n        return variants\\n    \\n    def compare_variants(self, prompt: str, seeds: List[int], **kwargs) -&gt; Dict[str, Any]:\\n        \"\"\"Generate and compare multiple variants.\"\"\"\\n        variants = self.create_variants(prompt, seeds, **kwargs)\\n        \\n        analysis = {\\n            \"prompt\": prompt,\\n            \"seeds\": seeds,\\n            \"variants\": variants,\\n            \"stats\": {\\n                variant: {\\n                    \"length\": len(text),\\n                    \"word_count\": len(text.split()),\\n                    \"seed\": seeds[i]\\n                }\\n                for i, (variant, text) in enumerate(variants.items())\\n            }\\n        }\\n        \\n        return analysis\\n    \\n    def batch_compare(self, prompts: List[str], seeds: List[int], **kwargs) -&gt; List[Dict[str, Any]]:\\n        \"\"\"Compare variants for multiple prompts.\"\"\"\\n        return [self.compare_variants(prompt, seeds, **kwargs) for prompt in prompts]\\n\\n# Usage example\\ntester = ABTester()\\n\\n# Define test variants with specific seeds\\ntest_seeds = [100, 200, 300, 400, 500]\\n\\n# Single prompt A/B test\\nresult = tester.compare_variants(\\n    \"Write a compelling product description for a smartwatch\",\\n    seeds=test_seeds[:3],  # Test 3 variants\\n    max_new_tokens=150\\n)\\n\\nprint(\"=== A/B Test Results ===\")\\nfor variant, text in result[\"variants\"].items():\\n    stats = result[\"stats\"][variant]\\n    print(f\"\\\\n{variant.upper()} (seed {stats['seed']}):\")\\n    print(f\"Length: {stats['length']} chars, {stats['word_count']} words\")\\n    print(f\"Text: {text[:100]}...\")\\n\\n# Batch testing for multiple prompts\\nmarketing_prompts = [\\n    \"Create an email subject line for a summer sale\",\\n    \"Write a social media post about a new product launch\",\\n    \"Compose a customer testimonial request\"\\n]\\n\\nbatch_results = tester.batch_compare(\\n    marketing_prompts, \\n    seeds=[42, 123, 456],\\n    max_new_tokens=100\\n)\\n\\nprint(\"\\\\n=== Batch A/B Test Results ===\")\\nfor i, result in enumerate(batch_results):\\n    print(f\"\\\\nPrompt {i+1}: {result['prompt'][:50]}...\")\\n    for variant, text in result[\"variants\"].items():\\n        seed = result[\"stats\"][variant][\"seed\"]\\n        print(f\"  {variant} (seed {seed}): {text[:80]}...\")\\n```\\n\\n### Email Campaign Testing\\n\\n```python\\nimport steadytext\\nimport random\\n\\ndef generate_email_variants(subject_base: str, body_base: str, num_variants: int = 5):\\n    \"\"\"Generate email variants for A/B testing.\"\"\"\\n    # Use consistent seed ranges for reproducibility\\n    seeds = [1000 + i * 100 for i in range(num_variants)]\\n    \\n    variants = []\\n    for i, seed in enumerate(seeds):\\n        subject = steadytext.generate(\\n            f\"Create an engaging email subject line based on: {subject_base}\",\\n            seed=seed,\\n            max_new_tokens=20\\n        ).strip()\\n        \\n        body = steadytext.generate(\\n            f\"Write a compelling email body for: {body_base}\",\\n            seed=seed,\\n            max_new_tokens=200\\n        ).strip()\\n        \\n        variants.append({\\n            \"variant_id\": f\"V{i+1}\",\\n            \"seed\": seed,\\n            \"subject\": subject,\\n            \"body\": body\\n        })\\n    \\n    return variants\\n\\n# Generate email campaign variants\\nvariants = generate_email_variants(\\n    subject_base=\"New product launch announcement\",\\n    body_base=\"Introducing our revolutionary AI-powered smartwatch with health monitoring\"\\n)\\n\\nprint(\"=== Email Campaign Variants ===\")\\nfor variant in variants:\\n    print(f\"\\\\n{variant['variant_id']} (seed {variant['seed']}):\\\")\\n    print(f\"Subject: {variant['subject']}\")\\n    print(f\"Body: {variant['body'][:100]}...\")\\n```\\n\\n## Content Variations\\n\\n### Style and Tone Variations\\n\\n```python\\nimport steadytext\\n\\ndef generate_style_variations(base_content: str, styles: Dict[str, int]):\\n    \\\"\\\"\\\"Generate content in different styles using specific seeds.\\\"\\\"\\\"\\n    variations = {}\\n    \\n    for style_name, seed in styles.items():\\n        prompt = f\"Rewrite the following content in a {style_name} style: {base_content}\"\\n        variation = steadytext.generate(\\n            prompt,\\n            seed=seed,\\n            max_new_tokens=250\\n        )\\n        variations[style_name] = {\\n            \"seed\": seed,\\n            \"content\": variation\\n        }\\n    \\n    return variations\\n\\n# Define styles with consistent seeds\\nstyles = {\\n    \"professional\": 2000,\\n    \"casual\": 2100,\\n    \"technical\": 2200,\\n    \"creative\": 2300,\\n    \"humorous\": 2400\\n}\\n\\nbase_content = \"Our new software helps businesses manage their data more efficiently.\"\\n\\nvariations = generate_style_variations(base_content, styles)\\n\\nprint(\"=== Style Variations ===\")\\nfor style, data in variations.items():\\n    print(f\"\\\\n{style.upper()} (seed {data['seed']}):\")\\n    print(data['content'])\\n```\\n\\n### Multi-Language Content\\n\\n```python\\nimport steadytext\\n\\ndef generate_multilingual_content(english_content: str, languages: Dict[str, int]):\\n    \\\"\\\"\\\"Generate content adapted for different languages/cultures using seeds.\\\"\\\"\\\"\\n    adaptations = {}\\n    \\n    for language, seed in languages.items():\\n        prompt = f\\\"Adapt this content for {language} audience, keeping cultural context in mind: {english_content}\\\"\\n        adaptation = steadytext.generate(\\n            prompt,\\n            seed=seed,\\n            max_new_tokens=200\\n        )\\n        adaptations[language] = {\\n            \"seed\": seed,\\n            \"content\": adaptation\\n        }\\n    \\n    return adaptations\\n\\n# Define languages with seeds\\nlanguages = {\\n    \"Spanish\": 3000,\\n    \"French\": 3100,\\n    \"German\": 3200,\\n    \"Japanese\": 3300,\\n    \"Brazilian Portuguese\": 3400\\n}\\n\\nenglish_content = \"Join our community of innovators and discover cutting-edge technology solutions.\"\\n\\nadaptations = generate_multilingual_content(english_content, languages)\\n\\nprint(\"=== Multilingual Adaptations ===\")\\nfor language, data in adaptations.items():\\n    print(f\"\\\\n{language} (seed {data['seed']}):\")\\n    print(data['content'])\\n```\\n\\n## Embedding Experiments\\n\\n### Semantic Similarity Analysis\\n\\n```python\\nimport steadytext\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\nimport matplotlib.pyplot as plt\\n\\ndef analyze_embedding_variations(text: str, seeds: List[int]):\\n    \\\"\\\"\\\"Analyze how different seeds affect embeddings for the same text.\\\"\\\"\\\"\\n    embeddings = []\\n    for seed in seeds:\\n        emb = steadytext.embed(text, seed=seed)\\n        embeddings.append(emb)\\n    \\n    embeddings = np.array(embeddings)\\n    \\n    # Calculate pairwise similarities\\n    similarities = []\\n    for i in range(len(embeddings)):\\n        for j in range(i+1, len(embeddings)):\\n            sim = np.dot(embeddings[i], embeddings[j])\\n            similarities.append(sim)\\n    \\n    analysis = {\\n        \"text\": text,\\n        \"seeds\": seeds,\\n        \"embeddings\": embeddings,\\n        \"pairwise_similarities\": similarities,\\n        \"mean_similarity\": np.mean(similarities),\\n        \"std_similarity\": np.std(similarities),\\n        \"min_similarity\": np.min(similarities),\\n        \"max_similarity\": np.max(similarities)\\n    }\\n    \\n    return analysis\\n\\n# Analyze embedding variations\\ntest_text = \"artificial intelligence and machine learning\"\\ntest_seeds = [4000, 4100, 4200, 4300, 4400]\\n\\nanalysis = analyze_embedding_variations(test_text, test_seeds)\\n\\nprint(f\"=== Embedding Variation Analysis ===\")\\nprint(f\"Text: {analysis['text']}\")\\nprint(f\"Seeds tested: {analysis['seeds']}\")\\nprint(f\"Mean similarity: {analysis['mean_similarity']:.4f}\")\\nprint(f\"Std similarity: {analysis['std_similarity']:.4f}\")\\nprint(f\"Range: {analysis['min_similarity']:.4f} - {analysis['max_similarity']:.4f}\")\\n\\n# Detailed similarity matrix\\nprint(\"\\\\nSimilarity Matrix:\")\\nembeddings = analysis['embeddings']\\nfor i, seed_i in enumerate(test_seeds):\\n    row = []\\n    for j, seed_j in enumerate(test_seeds):\\n        if i == j:\\n            sim = 1.0\\n        else:\\n            sim = np.dot(embeddings[i], embeddings[j])\\n        row.append(f\"{sim:.3f}\")\\n    print(f\"Seed {seed_i}: {' '.join(row)}\")\\n```\\n\\n### Domain-Specific Embedding Clusters\\n\\n```python\\nimport steadytext\\nimport numpy as np\\nfrom sklearn.cluster import KMeans\\nfrom collections import defaultdict\\n\\ndef create_domain_embeddings(domains: Dict[str, List[str]], seed_base: int = 5000):\\n    \\\"\\\"\\\"Create embeddings for different domains using consistent seeding.\\\"\\\"\\\"\\n    domain_embeddings = defaultdict(list)\\n    \\n    for domain, texts in domains.items():\\n        domain_seed = seed_base + hash(domain) % 1000  # Consistent seed per domain\\n        \\n        for text in texts:\\n            embedding = steadytext.embed(text, seed=domain_seed)\\n            domain_embeddings[domain].append({\\n                \"text\": text,\\n                \"embedding\": embedding,\\n                \"seed\": domain_seed\\n            })\\n    \\n    return dict(domain_embeddings)\\n\\n# Define domain-specific texts\\ndomains = {\\n    \"technology\": [\\n        \"artificial intelligence\",\\n        \"machine learning\",\\n        \"deep learning\",\\n        \"neural networks\",\\n        \"computer vision\"\\n    ],\\n    \"healthcare\": [\\n        \"medical diagnosis\",\\n        \"patient care\",\\n        \"clinical trials\",\\n        \"pharmaceutical research\",\\n        \"telemedicine\"\\n    ],\\n    \"finance\": [\\n        \"investment strategy\",\\n        \"risk management\",\\n        \"financial planning\",\\n        \"market analysis\",\\n        \"portfolio optimization\"\\n    ]\\n}\\n\\n# Generate embeddings\\ndomain_embeddings = create_domain_embeddings(domains)\\n\\n# Analyze domain clustering\\nall_embeddings = []\\nall_labels = []\\nall_texts = []\\n\\nfor domain, items in domain_embeddings.items():\\n    for item in items:\\n        all_embeddings.append(item['embedding'])\\n        all_labels.append(domain)\\n        all_texts.append(item['text'])\\n\\nall_embeddings = np.array(all_embeddings)\\n\\n# Perform clustering\\nkmeans = KMeans(n_clusters=3, random_state=42)\\ncluster_labels = kmeans.fit_predict(all_embeddings)\\n\\nprint(\"=== Domain Clustering Results ===\")\\nfor i, (text, true_domain, predicted_cluster) in enumerate(zip(all_texts, all_labels, cluster_labels)):\\n    print(f\"{text:25} | True: {true_domain:10} | Cluster: {predicted_cluster}\")\\n\\n# Calculate clustering accuracy\\nfrom sklearn.metrics import adjusted_rand_score\\nlabel_to_int = {label: i for i, label in enumerate(set(all_labels))}\\ntrue_labels_int = [label_to_int[label] for label in all_labels]\\n\\naccuracy = adjusted_rand_score(true_labels_int, cluster_labels)\\nprint(f\"\\\\nClustering accuracy (ARI): {accuracy:.3f}\")\\n```\\n\\n## CLI Workflows\\n\\n### Batch Processing Scripts\\n\\n```bash\\n#!/bin/bash\\n# batch_generation.sh - Generate content variants using CLI\\n\\n# Define seeds for different variants\\nSEEDS=(1000 2000 3000 4000 5000)\\nPROMPT=\"Write a brief product description for a smartwatch\"\\n\\necho \"=== Batch Generation with Different Seeds ===\"\\n\\nfor i in \"${!SEEDS[@]}\"; do\\n    seed=${SEEDS[$i]}\\n    variant_name=\"variant_$(echo $i | tr '0-4' 'A-E')\"  # A, B, C, D, E\\n    \\n    echo \\\"\\\"\\n    echo \\\"$variant_name (seed $seed):\\\"\\n    echo \\\"$PROMPT\\\" | st --seed $seed --max-new-tokens 100\\n    echo \\\"---\\\"\\ndone\\n```\\n\\n```bash\\n#!/bin/bash\\n# embedding_comparison.sh - Compare embeddings with different seeds\\n\\nTEXT=\\\"artificial intelligence\\\"\\nSEEDS=(6000 6100 6200)\\n\\necho \\\"=== Embedding Comparison ===\\\"\\necho \\\"Text: $TEXT\\\"\\n\\nfor seed in \\\"${SEEDS[@]}\\\"; do\\n    echo \\\"\\\"\\n    echo \\\"Seed $seed:\\\"\\n    st embed \\\"$TEXT\\\" --seed $seed --format json | jq '.[:5]'  # Show first 5 dimensions\\ndone\\n```\\n\\n### Reproducible Research Pipeline\\n\\n```bash\\n#!/bin/bash\\n# research_pipeline.sh - Complete research workflow with seeds\\n\\nRESEARCH_DIR=\\\"./research_$(date +%Y%m%d_%H%M%S)\\\"\\nBASE_SEED=7000\\n\\nmkdir -p \\\"$RESEARCH_DIR\\\"\\ncd \\\"$RESEARCH_DIR\\\"\\n\\necho \\\"=== Research Pipeline Started ===\\\" | tee research.log\\necho \\\"Base seed: $BASE_SEED\\\" | tee -a research.log\\necho \\\"Directory: $RESEARCH_DIR\\\" | tee -a research.log\\n\\n# Generate research questions\\necho \\\"Generating research questions...\\\" | tee -a research.log\\necho \\\"Generate 5 research questions about AI ethics\\\" | \\\\\\n    st --seed $BASE_SEED --max-new-tokens 200 &gt; questions.txt\\n\\n# Generate detailed explanations\\necho \\\"Generating detailed explanations...\\\" | tee -a research.log\\ncounter=0\\nwhile IFS= read -r question; do\\n    if [[ -n \\\"$question\\\" &amp;&amp; \\\"$question\\\" != *\\\"Generate\\\"* ]]; then\\n        seed=$((BASE_SEED + 100 + counter * 10))\\n        echo \\\"Processing: $question (seed $seed)\\\" | tee -a research.log\\n        echo \\\"$question\\\" | st --seed $seed --max-new-tokens 300 &gt; \\\"explanation_$counter.txt\\\"\\n        counter=$((counter + 1))\\n    fi\\ndone &lt; questions.txt\\n\\n# Generate embeddings for analysis\\necho \\\"Generating embeddings...\\\" | tee -a research.log\\nfor file in explanation_*.txt; do\\n    if [[ -f \\\"$file\\\" ]]; then\\n        seed=$((BASE_SEED + 500))\\n        echo \\\"Creating embedding for $file (seed $seed)\\\" | tee -a research.log\\n        cat \\\"$file\\\" | st embed --seed $seed --format json &gt; \\\"${file%.txt}_embedding.json\\\"\\n    fi\\ndone\\n\\necho \\\"Research pipeline completed. Results in: $RESEARCH_DIR\\\" | tee -a research.log\\necho \\\"Files generated:\\\" | tee -a research.log\\nls -la | tee -a research.log\\n```\\n\\n## Advanced Patterns\\n\\n### Seed Scheduling and Management\\n\\n```python\\nimport steadytext\\nfrom typing import Iterator, List, Dict, Any\\nimport hashlib\\n\\nclass SeedManager:\\n    \\\"\\\"\\\"Advanced seed management for complex workflows.\\\"\\\"\\\"\\n    \\n    def __init__(self, base_seed: int = 42):\\n        self.base_seed = base_seed\\n        self.used_seeds = set()\\n        self.seed_history = []\\n    \\n    def get_deterministic_seed(self, context: str) -&gt; int:\\n        \\\"\\\"\\\"Generate deterministic seed based on context string.\\\"\\\"\\\"\\n        # Create reproducible seed from context\\n        context_hash = hashlib.md5(context.encode()).hexdigest()\\n        seed = self.base_seed + int(context_hash[:8], 16) % 10000\\n        \\n        self.used_seeds.add(seed)\\n        self.seed_history.append({\\n            \"context\": context,\\n            \"seed\": seed,\\n            \"method\": \"deterministic\"\\n        })\\n        \\n        return seed\\n    \\n    def get_sequential_seed(self, increment: int = 1) -&gt; int:\\n        \\\"\\\"\\\"Get next seed in sequence.\\\"\\\"\\\"\\n        seed = self.base_seed + len(self.seed_history) * increment\\n        \\n        self.used_seeds.add(seed)\\n        self.seed_history.append({\\n            \"context\": f\"sequential_{len(self.seed_history)}\\\",\\n            \"seed\": seed,\\n            \"method\": \"sequential\"\\n        })\\n        \\n        return seed\\n    \\n    def get_category_seed(self, category: str, item_id: int = 0) -&gt; int:\\n        \\\"\\\"\\\"Get seed for specific category and item.\\\"\\\"\\\"\\n        category_base = hash(category) % 1000\\n        seed = self.base_seed + category_base * 100 + item_id\\n        \\n        self.used_seeds.add(seed)\\n        self.seed_history.append({\\n            \"context\": f\"{category}_{item_id}\\\",\\n            \"seed\": seed,\\n            \"method\": \"category\\\",\\n            \\\"category\\\": category,\\n            \\\"item_id\\\": item_id\\n        })\\n        \\n        return seed\\n    \\n    def generate_with_context(self, prompt: str, context: str, **kwargs) -&gt; str:\\n        \\\"\\\"\\\"Generate text with context-based seed.\\\"\\\"\\\"\\n        seed = self.get_deterministic_seed(context)\\n        return steadytext.generate(prompt, seed=seed, **kwargs)\\n    \\n    def embed_with_context(self, text: str, context: str, **kwargs):\\n        \\\"\\\"\\\"Generate embedding with context-based seed.\\\"\\\"\\\"\\n        seed = self.get_deterministic_seed(context)\\n        return steadytext.embed(text, seed=seed, **kwargs)\\n    \\n    def export_seed_history(self) -&gt; List[Dict[str, Any]]:\\n        \\\"\\\"\\\"Export seed usage history for reproducibility.\\\"\\\"\\\"\\n        return self.seed_history.copy()\\n\\n# Usage example\\nmanager = SeedManager(base_seed=10000)\\n\\n# Context-based generation\\ncontents = [\\n    (\\\"Write a technical blog post about AI\\\", \\\"blog_technical_ai\\\"),\\n    (\\\"Create a social media post about innovation\\\", \\\"social_innovation\\\"),\\n    (\\\"Generate a product description\\\", \\\"product_smartwatch\\\")\\n]\\n\\nresults = []\\nfor prompt, context in contents:\\n    result = manager.generate_with_context(\\n        prompt, \\n        context, \\n        max_new_tokens=150\\n    )\\n    results.append({\\n        \\\"context\\\": context,\\n        \\\"prompt\\\": prompt,\\n        \\\"result\\\": result\\n    })\\n\\n# Category-based generation\\ncategories = [\\\"marketing\\\", \\\"technical\\\", \\\"creative\\\"]\\nfor category in categories:\\n    for i in range(3):  # 3 items per category\\n        seed = manager.get_category_seed(category, i)\\n        prompt = f\\\"Write a {category} message about our new product\\\"\\n        result = steadytext.generate(prompt, seed=seed, max_new_tokens=100)\\n        print(f\\\"{category}_{i} (seed {seed}): {result[:50]}...\\\")\\n\\n# Export history for reproducibility\\nhistory = manager.export_seed_history()\\nprint(f\\\"\\\\nGenerated {len(history)} items with managed seeds\\\")\\nfor entry in history[-5:]:  # Show last 5 entries\\n    print(f\\\"Context: {entry['context']}, Seed: {entry['seed']}, Method: {entry['method']}\\\")\\n```\\n\\n### Conditional Seed Strategies\\n\\n```python\\nimport steadytext\\nfrom enum import Enum\\nfrom typing import Optional, Callable\\n\\nclass SeedStrategy(Enum):\\n    DETERMINISTIC = \\\"deterministic\\\"  # Same input always gives same seed\\n    SEQUENTIAL = \\\"sequential\\\"        # Incrementing seed sequence\\n    RANDOM_BOUNDED = \\\"random_bounded\\\" # Random within bounds\\n    CONTENT_BASED = \\\"content_based\\\"   # Seed based on content analysis\\n\\nclass ConditionalSeedGenerator:\\n    \\\"\\\"\\\"Generate seeds based on content and context conditions.\\\"\\\"\\\"\\n    \\n    def __init__(self, base_seed: int = 42):\\n        self.base_seed = base_seed\\n        self.counters = {}\\n    \\n    def analyze_content(self, content: str) -&gt; Dict[str, Any]:\\n        \\\"\\\"\\\"Analyze content to determine appropriate seed strategy.\\\"\\\"\\\"\\n        word_count = len(content.split())\\n        has_technical_terms = any(term in content.lower() for term in \\n                                ['algorithm', 'neural', 'machine', 'ai', 'data'])\\n        has_creative_intent = any(term in content.lower() for term in \\n                             ['story', 'creative', 'imagine', 'artistic'])\\n        \\n        return {\\n            \\\"word_count\\\": word_count,\\n            \\\"is_technical\\\": has_technical_terms,\\n            \\\"is_creative\\\": has_creative_intent,\\n            \\\"is_short\\\": word_count &lt; 10,\\n            \\\"is_long\\\": word_count &gt; 50\\n        }\\n    \\n    def determine_strategy(self, content: str, context: Optional[str] = None) -&gt; SeedStrategy:\\n        \\\"\\\"\\\"Determine best seed strategy based on content analysis.\\\"\\\"\\\"\\n        analysis = self.analyze_content(content)\\n        \\n        if analysis[\\\"is_creative\\\"]:\\n            return SeedStrategy.RANDOM_BOUNDED  # More variation for creative content\\n        elif analysis[\\\"is_technical\\\"]:\\n            return SeedStrategy.DETERMINISTIC   # Consistency for technical content\\n        elif analysis[\\\"is_short\\\"]:\\n            return SeedStrategy.CONTENT_BASED   # Content-based for short prompts\\n        else:\\n            return SeedStrategy.SEQUENTIAL      # Sequential for general content\\n    \\n    def generate_seed(self, content: str, strategy: Optional[SeedStrategy] = None, \\n                     context: Optional[str] = None) -&gt; int:\\n        \\\"\\\"\\\"Generate seed using specified or determined strategy.\\\"\\\"\\\"\\n        if strategy is None:\\n            strategy = self.determine_strategy(content, context)\\n        \\n        if strategy == SeedStrategy.DETERMINISTIC:\\n            # Hash-based deterministic seed\\n            content_hash = hash(content) % 10000\\n            return self.base_seed + content_hash\\n        \\n        elif strategy == SeedStrategy.SEQUENTIAL:\\n            # Sequential counter per context\\n            key = context or \\\"default\\\"\\n            if key not in self.counters:\\n                self.counters[key] = 0\\n            self.counters[key] += 1\\n            return self.base_seed + self.counters[key] * 100\\n        \\n        elif strategy == SeedStrategy.RANDOM_BOUNDED:\\n            # Bounded random based on content\\n            content_hash = abs(hash(content))\\n            random_offset = content_hash % 1000\\n            return self.base_seed + 5000 + random_offset\\n        \\n        elif strategy == SeedStrategy.CONTENT_BASED:\\n            # Seed based on content characteristics\\n            analysis = self.analyze_content(content)\\n            seed_offset = (\\n                analysis[\\\"word_count\\\"] * 10 +\\n                (100 if analysis[\\\"is_technical\\\"] else 0) +\\n                (200 if analysis[\\\"is_creative\\\"] else 0)\\n            )\\n            return self.base_seed + seed_offset\\n        \\n        return self.base_seed\\n    \\n    def smart_generate(self, content: str, context: Optional[str] = None, **kwargs) -&gt; str:\\n        \\\"\\\"\\\"Generate text with automatically chosen seed strategy.\\\"\\\"\\\"\\n        strategy = self.determine_strategy(content, context)\\n        seed = self.generate_seed(content, strategy, context)\\n        \\n        print(f\\\"Strategy: {strategy.value}, Seed: {seed}\\\")\\n        return steadytext.generate(content, seed=seed, **kwargs)\\n\\n# Usage example\\ngenerator = ConditionalSeedGenerator(base_seed=20000)\\n\\n# Test different content types\\ntest_prompts = [\\n    \\\"Write a creative story about a robot\\\",  # Should use RANDOM_BOUNDED\\n    \\\"Explain the neural network algorithm\\\",  # Should use DETERMINISTIC\\n    \\\"Hello\\\",                                 # Should use CONTENT_BASED\\n    \\\"Generate a comprehensive technical report about machine learning applications in healthcare\\\"  # Should use SEQUENTIAL\\n]\\n\\nprint(\\\"=== Conditional Seed Strategy Results ===\\\")\\nfor prompt in test_prompts:\\n    print(f\\\"\\\\nPrompt: {prompt}\\\")\\n    result = generator.smart_generate(prompt, max_new_tokens=50)\\n    print(f\\\"Result: {result[:80]}...\\\")\\n```\\n\\n## Best Practices\\n\\n### 1. Documentation and Reproducibility\\n\\n```python\\n# Always document your seeds\\nCONSTANT_SEEDS = {\\n    \\\"BASELINE_RESEARCH\\\": 42,\\n    \\\"VARIATION_A\\\": 100,\\n    \\\"VARIATION_B\\\": 200,\\n    \\\"CREATIVE_CONTENT\\\": 300,\\n    \\\"TECHNICAL_CONTENT\\\": 400,\\n    \\\"PRODUCTION_DEFAULT\\\": 500\\n}\\n\\n# Use descriptive seed values\\ndef generate_with_purpose(prompt: str, purpose: str):\\n    seed = CONSTANT_SEEDS.get(purpose.upper(), CONSTANT_SEEDS[\\\"BASELINE_RESEARCH\\\"])\\n    return steadytext.generate(prompt, seed=seed)\\n```\\n\\n### 2. Seed Range Management\\n\\n```python\\n# Organize seeds by ranges to avoid conflicts\\nSEED_RANGES = {\\n    \\\"research\\\": (1000, 1999),\\n    \\\"production\\\": (2000, 2999),\\n    \\\"testing\\\": (3000, 3999),\\n    \\\"experiments\\\": (4000, 4999),\\n    \\\"benchmarks\\\": (5000, 5999)\\n}\\n\\ndef get_range_seed(category: str, offset: int = 0) -&gt; int:\\n    if category not in SEED_RANGES:\\n        raise ValueError(f\\\"Unknown category: {category}\\\")\\n    \\n    start, end = SEED_RANGES[category]\\n    seed = start + offset\\n    \\n    if seed &gt; end:\\n        raise ValueError(f\\\"Seed {seed} exceeds range for {category} ({start}-{end})\\\")\\n    \\n    return seed\\n```\\n\\n### 3. Testing and Validation\\n\\n```python\\ndef validate_reproducibility(prompt: str, seed: int, iterations: int = 5):\\n    \\\"\\\"\\\"Validate that a prompt+seed combination is truly reproducible.\\\"\\\"\\\"\\n    results = []\\n    for i in range(iterations):\\n        result = steadytext.generate(prompt, seed=seed)\\n        results.append(result)\\n    \\n    # Check if all results are identical\\n    is_reproducible = all(result == results[0] for result in results)\\n    \\n    print(f\\\"Reproducibility test for seed {seed}: {'PASS' if is_reproducible else 'FAIL'}\\\")\\n    if not is_reproducible:\\n        print(\\\"Different results found:\\\")\\n        for i, result in enumerate(results):\\n            print(f\\\"  Iteration {i+1}: {result[:50]}...\\\")\\n    \\n    return is_reproducible\\n\\n# Test key seeds\\nfor purpose, seed in CONSTANT_SEEDS.items():\\n    validate_reproducibility(\\\"Test prompt for validation\\\", seed)\\n```\\n\\nThis comprehensive guide demonstrates the power and flexibility of custom seeds in SteadyText. By using seeds strategically, you can achieve reproducible research, conduct effective A/B testing, generate controlled variations, and build robust content generation pipelines.\\n\n</code></pre>"},{"location":"examples/daemon-usage/","title":"Daemon Usage Guide","text":"<p>Learn how to use SteadyText's daemon mode for persistent model serving and 160x faster response times.</p>"},{"location":"examples/daemon-usage/#overview","title":"Overview","text":"<p>The SteadyText daemon is a background service that keeps models loaded in memory, eliminating the 2-3 second startup overhead for each operation. It provides:</p> <ul> <li>160x faster first response - No model loading delay</li> <li>Shared cache - All clients benefit from cached results</li> <li>Automatic fallback - Operations work without daemon</li> <li>Zero configuration - Used by default when available</li> <li>Thread-safe - Handles concurrent requests efficiently</li> </ul>"},{"location":"examples/daemon-usage/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Understanding the Daemon</li> <li>Starting and Stopping</li> <li>Configuration</li> <li>Python SDK Usage</li> <li>CLI Integration</li> <li>Production Deployment</li> <li>Monitoring and Debugging</li> <li>Performance Optimization</li> <li>Troubleshooting</li> <li>Best Practices</li> </ul>"},{"location":"examples/daemon-usage/#understanding-the-daemon","title":"Understanding the Daemon","text":""},{"location":"examples/daemon-usage/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Client Applications            \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  Python SDK  \u2502  CLI Tools  \u2502  Custom Apps   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           ZeroMQ Client Layer               \u2502\n\u2502         (Automatic Fallback)                \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502              ZeroMQ REP Server              \u2502\n\u2502            (TCP Port 5557)                  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           Daemon Server Process             \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Loaded Models  \u2502  Shared Cache     \u2502   \u2502\n\u2502  \u2502  - Gemma-3n     \u2502  - Generation     \u2502   \u2502\n\u2502  \u2502  - Qwen3        \u2502  - Embeddings     \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"examples/daemon-usage/#how-it-works","title":"How It Works","text":"<ol> <li>First Request: Client checks if daemon is running</li> <li>Daemon Available: Sends request via ZeroMQ</li> <li>Daemon Unavailable: Falls back to direct model loading</li> <li>Response: Client receives result (cached or generated)</li> </ol>"},{"location":"examples/daemon-usage/#starting-and-stopping","title":"Starting and Stopping","text":""},{"location":"examples/daemon-usage/#basic-commands","title":"Basic Commands","text":"<pre><code># Start daemon in background (default)\nst daemon start\n\n# Start with custom settings\nst daemon start --host 0.0.0.0 --port 5557 --seed 42\n\n# Check status\nst daemon status\n\n# Stop daemon\nst daemon stop\n\n# Restart daemon\nst daemon restart\n</code></pre>"},{"location":"examples/daemon-usage/#foreground-mode-debugging","title":"Foreground Mode (Debugging)","text":"<pre><code># Run in foreground to see logs\nst daemon start --foreground\n\n# Output:\n# SteadyText daemon starting...\n# Loading generation model...\n# Loading embedding model...\n# Daemon ready on tcp://127.0.0.1:5557\n# [2024-01-15 10:23:45] Request: generate (seed=42)\n# [2024-01-15 10:23:45] Cache hit for generation\n</code></pre>"},{"location":"examples/daemon-usage/#systemd-service-production","title":"Systemd Service (Production)","text":"<pre><code># /etc/systemd/system/steadytext.service\n[Unit]\nDescription=SteadyText Daemon\nAfter=network.target\n\n[Service]\nType=simple\nUser=steadytext\nGroup=steadytext\nWorkingDirectory=/var/lib/steadytext\nExecStart=/usr/local/bin/st daemon start --foreground\nExecStop=/usr/local/bin/st daemon stop\nRestart=always\nRestartSec=10\nStandardOutput=append:/var/log/steadytext/daemon.log\nStandardError=append:/var/log/steadytext/daemon.error.log\n\n# Environment\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_CAPACITY=1024\"\nEnvironment=\"STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=200\"\nEnvironment=\"PYTHONUNBUFFERED=1\"\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Enable and start: <pre><code>sudo systemctl enable steadytext\nsudo systemctl start steadytext\nsudo systemctl status steadytext\n</code></pre></p>"},{"location":"examples/daemon-usage/#configuration","title":"Configuration","text":""},{"location":"examples/daemon-usage/#environment-variables","title":"Environment Variables","text":"<pre><code># Daemon settings\nexport STEADYTEXT_DAEMON_HOST=0.0.0.0      # Bind address\nexport STEADYTEXT_DAEMON_PORT=5557         # Port number\nexport STEADYTEXT_DISABLE_DAEMON=1         # Disable daemon usage\n\n# Cache settings (shared by daemon)\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=1024\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=200\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=2048\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=400\n\n# Model settings\nexport STEADYTEXT_DEFAULT_SEED=42\nexport STEADYTEXT_MODEL_DIR=/path/to/models\n</code></pre>"},{"location":"examples/daemon-usage/#configuration-file","title":"Configuration File","text":"<pre><code># steadytext_config.py\nimport os\n\n# Daemon configuration\nDAEMON_CONFIG = {\n    \"host\": os.getenv(\"STEADYTEXT_DAEMON_HOST\", \"127.0.0.1\"),\n    \"port\": int(os.getenv(\"STEADYTEXT_DAEMON_PORT\", 5557)),\n    \"timeout\": 5000,  # milliseconds\n    \"max_retries\": 3,\n    \"retry_delay\": 0.1  # seconds\n}\n\n# Cache configuration\nCACHE_CONFIG = {\n    \"generation\": {\n        \"capacity\": int(os.getenv(\"STEADYTEXT_GENERATION_CACHE_CAPACITY\", 256)),\n        \"max_size_mb\": float(os.getenv(\"STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB\", 50.0))\n    },\n    \"embedding\": {\n        \"capacity\": int(os.getenv(\"STEADYTEXT_EMBEDDING_CACHE_CAPACITY\", 512)),\n        \"max_size_mb\": float(os.getenv(\"STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB\", 100.0))\n    }\n}\n\n# Apply configuration\nos.environ.update({\n    \"STEADYTEXT_DAEMON_HOST\": DAEMON_CONFIG[\"host\"],\n    \"STEADYTEXT_DAEMON_PORT\": str(DAEMON_CONFIG[\"port\"]),\n    \"STEADYTEXT_GENERATION_CACHE_CAPACITY\": str(CACHE_CONFIG[\"generation\"][\"capacity\"]),\n    \"STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB\": str(CACHE_CONFIG[\"generation\"][\"max_size_mb\"]),\n    \"STEADYTEXT_EMBEDDING_CACHE_CAPACITY\": str(CACHE_CONFIG[\"embedding\"][\"capacity\"]),\n    \"STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB\": str(CACHE_CONFIG[\"embedding\"][\"max_size_mb\"])\n})\n</code></pre>"},{"location":"examples/daemon-usage/#python-sdk-usage","title":"Python SDK Usage","text":""},{"location":"examples/daemon-usage/#automatic-daemon-usage","title":"Automatic Daemon Usage","text":"<pre><code>import steadytext\n\n# Daemon is used automatically if available\ntext = steadytext.generate(\"Hello world\", seed=42)  # Fast if daemon running\nembedding = steadytext.embed(\"test text\", seed=123)  # Uses daemon\n\n# Check if daemon was used\nfrom steadytext.daemon.client import is_daemon_running\nif is_daemon_running():\n    print(\"Using daemon for fast responses\")\nelse:\n    print(\"Daemon not available, using direct mode\")\n</code></pre>"},{"location":"examples/daemon-usage/#explicit-daemon-context","title":"Explicit Daemon Context","text":"<pre><code>from steadytext.daemon import use_daemon\nimport steadytext\n\n# Force daemon usage (raises error if not available)\nwith use_daemon():\n    text = steadytext.generate(\"Hello world\", seed=42)\n    embedding = steadytext.embed(\"test\", seed=123)\n\n    # All operations in this context use daemon\n    for i in range(100):\n        result = steadytext.generate(f\"Item {i}\", seed=i)\n</code></pre>"},{"location":"examples/daemon-usage/#connection-management","title":"Connection Management","text":"<pre><code>from steadytext.daemon.client import DaemonClient\nimport time\n\nclass ManagedDaemonClient:\n    \"\"\"Daemon client with connection pooling and retries.\"\"\"\n\n    def __init__(self, max_retries=3, timeout=5000):\n        self.max_retries = max_retries\n        self.timeout = timeout\n        self._client = None\n\n    def _get_client(self):\n        \"\"\"Get or create daemon client.\"\"\"\n        if self._client is None:\n            self._client = DaemonClient(timeout=self.timeout)\n        return self._client\n\n    def generate_with_retry(self, prompt, **kwargs):\n        \"\"\"Generate with automatic retry on failure.\"\"\"\n        for attempt in range(self.max_retries):\n            try:\n                client = self._get_client()\n                return client.generate(prompt, **kwargs)\n            except Exception as e:\n                if attempt == self.max_retries - 1:\n                    raise\n                print(f\"Retry {attempt + 1}/{self.max_retries} after error: {e}\")\n                time.sleep(0.1 * (attempt + 1))\n                self._client = None  # Reset connection\n\n    def close(self):\n        \"\"\"Close daemon connection.\"\"\"\n        if self._client:\n            self._client.close()\n            self._client = None\n\n# Usage\nclient = ManagedDaemonClient()\ntry:\n    text = client.generate_with_retry(\"Hello world\", seed=42)\n    print(text)\nfinally:\n    client.close()\n</code></pre>"},{"location":"examples/daemon-usage/#streaming-with-daemon","title":"Streaming with Daemon","text":"<pre><code>import steadytext\nfrom steadytext.daemon import use_daemon\n\n# Streaming works identically with daemon\nwith use_daemon():\n    print(\"Streaming with daemon:\")\n    for token in steadytext.generate_iter(\"Tell me a story\", seed=42):\n        print(token, end=\"\", flush=True)\n    print()\n\n# The daemon handles streaming efficiently:\n# 1. Client sends streaming request\n# 2. Daemon generates tokens\n# 3. Tokens sent with acknowledgment protocol\n# 4. Client controls flow with ACK messages\n</code></pre>"},{"location":"examples/daemon-usage/#batch-operations","title":"Batch Operations","text":"<pre><code>import concurrent.futures\nimport steadytext\nfrom steadytext.daemon import use_daemon\nimport time\n\ndef benchmark_daemon_performance():\n    \"\"\"Compare daemon vs direct performance.\"\"\"\n    prompts = [f\"Generate text for item {i}\" for i in range(20)]\n\n    # Test without daemon\n    start = time.time()\n    results_direct = []\n    for prompt in prompts:\n        # Force direct mode\n        import os\n        os.environ[\"STEADYTEXT_DISABLE_DAEMON\"] = \"1\"\n        result = steadytext.generate(prompt, seed=42)\n        results_direct.append(result)\n        del os.environ[\"STEADYTEXT_DISABLE_DAEMON\"]\n    direct_time = time.time() - start\n\n    # Test with daemon\n    start = time.time()\n    results_daemon = []\n    with use_daemon():\n        for prompt in prompts:\n            result = steadytext.generate(prompt, seed=42)\n            results_daemon.append(result)\n    daemon_time = time.time() - start\n\n    print(f\"Direct mode: {direct_time:.2f}s\")\n    print(f\"Daemon mode: {daemon_time:.2f}s\")\n    print(f\"Speedup: {direct_time/daemon_time:.1f}x\")\n\n# Parallel batch processing\ndef process_batch_parallel(prompts, max_workers=4):\n    \"\"\"Process prompts in parallel using daemon.\"\"\"\n    with use_daemon():\n        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n            # Submit all tasks\n            futures = {\n                executor.submit(steadytext.generate, prompt, seed=idx): (prompt, idx)\n                for idx, prompt in enumerate(prompts)\n            }\n\n            # Collect results\n            results = {}\n            for future in concurrent.futures.as_completed(futures):\n                prompt, idx = futures[future]\n                try:\n                    result = future.result()\n                    results[idx] = result\n                except Exception as e:\n                    print(f\"Error processing {prompt}: {e}\")\n                    results[idx] = None\n\n            # Return in order\n            return [results[i] for i in range(len(prompts))]\n\n# Usage\nprompts = [\"Task 1\", \"Task 2\", \"Task 3\", \"Task 4\"]\nresults = process_batch_parallel(prompts)\n</code></pre>"},{"location":"examples/daemon-usage/#cli-integration","title":"CLI Integration","text":""},{"location":"examples/daemon-usage/#automatic-daemon-usage_1","title":"Automatic Daemon Usage","text":"<pre><code># CLI automatically uses daemon if available\nst generate \"Hello world\" --seed 42\n\n# Check if daemon is being used\nst daemon status &amp;&amp; echo \"Daemon active\" || echo \"No daemon\"\n\n# Force direct mode (bypass daemon)\nSTEADYTEXT_DISABLE_DAEMON=1 st generate \"Hello world\"\n</code></pre>"},{"location":"examples/daemon-usage/#shell-script-integration","title":"Shell Script Integration","text":"<pre><code>#!/bin/bash\n# daemon_batch.sh - Batch processing with daemon\n\n# Ensure daemon is running\nensure_daemon() {\n    if ! st daemon status &gt;/dev/null 2&gt;&amp;1; then\n        echo \"Starting daemon...\"\n        st daemon start\n        sleep 2  # Wait for startup\n    fi\n}\n\n# Process files with daemon\nprocess_files() {\n    local files=(\"$@\")\n\n    ensure_daemon\n\n    for file in \"${files[@]}\"; do\n        echo \"Processing: $file\"\n\n        # Generate summary using daemon\n        summary=$(cat \"$file\" | st generate \"Summarize this text\" --wait --seed 42)\n\n        # Generate embedding using daemon  \n        embedding=$(cat \"$file\" | st embed --format json --seed 42)\n\n        # Save results\n        echo \"$summary\" &gt; \"${file%.txt}_summary.txt\"\n        echo \"$embedding\" &gt; \"${file%.txt}_embedding.json\"\n    done\n}\n\n# Main\nif [ $# -eq 0 ]; then\n    echo \"Usage: $0 file1.txt file2.txt ...\"\n    exit 1\nfi\n\nprocess_files \"$@\"\n\necho \"Processing complete!\"\n</code></pre>"},{"location":"examples/daemon-usage/#performance-monitoring","title":"Performance Monitoring","text":"<pre><code>#!/bin/bash\n# monitor_daemon.sh - Monitor daemon performance\n\n# Function to time operations\ntime_operation() {\n    local operation=\"$1\"\n    local start=$(date +%s.%N)\n    eval \"$operation\" &gt;/dev/null 2&gt;&amp;1\n    local end=$(date +%s.%N)\n    echo \"$(echo \"$end - $start\" | bc)\"\n}\n\n# Monitor daemon performance\nmonitor_daemon() {\n    echo \"Daemon Performance Monitor\"\n    echo \"=========================\"\n\n    # Check daemon status\n    if st daemon status --json | jq -e '.running' &gt;/dev/null; then\n        echo \"\u2713 Daemon is running\"\n    else\n        echo \"\u2717 Daemon is not running\"\n        return 1\n    fi\n\n    # Test generation speed\n    echo -e \"\\nGeneration Performance:\"\n    for i in {1..5}; do\n        time=$(time_operation \"echo 'test' | st --seed $i\")\n        echo \"  Request $i: ${time}s\"\n    done\n\n    # Test embedding speed\n    echo -e \"\\nEmbedding Performance:\"\n    for i in {1..5}; do\n        time=$(time_operation \"st embed 'test text' --seed $i\")\n        echo \"  Request $i: ${time}s\"\n    done\n\n    # Cache statistics\n    echo -e \"\\nCache Statistics:\"\n    st cache --status\n}\n\n# Run monitoring\nmonitor_daemon\n</code></pre>"},{"location":"examples/daemon-usage/#production-deployment","title":"Production Deployment","text":""},{"location":"examples/daemon-usage/#docker-deployment","title":"Docker Deployment","text":"<pre><code># Dockerfile\nFROM python:3.11-slim\n\n# Install dependencies\nRUN apt-get update &amp;&amp; apt-get install -y \\\n    build-essential \\\n    curl \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\n# Create app user\nRUN useradd -m -s /bin/bash steadytext\n\n# Install SteadyText\nRUN pip install steadytext\n\n# Create directories\nRUN mkdir -p /var/log/steadytext /var/lib/steadytext &amp;&amp; \\\n    chown -R steadytext:steadytext /var/log/steadytext /var/lib/steadytext\n\n# Switch to app user\nUSER steadytext\nWORKDIR /home/steadytext\n\n# Download models during build\nRUN st models download --all\n\n# Expose daemon port\nEXPOSE 5557\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s --start-period=10s --retries=3 \\\n    CMD st daemon status || exit 1\n\n# Start daemon\nCMD [\"st\", \"daemon\", \"start\", \"--foreground\", \"--host\", \"0.0.0.0\"]\n</code></pre> <pre><code># docker-compose.yml\nversion: '3.8'\n\nservices:\n  steadytext:\n    build: .\n    ports:\n      - \"5557:5557\"\n    volumes:\n      - steadytext-cache:/home/steadytext/.cache/steadytext\n      - ./logs:/var/log/steadytext\n    environment:\n      - STEADYTEXT_GENERATION_CACHE_CAPACITY=1024\n      - STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=200\n      - STEADYTEXT_EMBEDDING_CACHE_CAPACITY=2048\n      - STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=400\n      - STEADYTEXT_DEFAULT_SEED=42\n    restart: unless-stopped\n    healthcheck:\n      test: [\"CMD\", \"st\", \"daemon\", \"status\"]\n      interval: 30s\n      timeout: 3s\n      retries: 3\n\nvolumes:\n  steadytext-cache:\n</code></pre>"},{"location":"examples/daemon-usage/#kubernetes-deployment","title":"Kubernetes Deployment","text":"<pre><code># steadytext-deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: steadytext-daemon\n  labels:\n    app: steadytext\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: steadytext\n  template:\n    metadata:\n      labels:\n        app: steadytext\n    spec:\n      containers:\n      - name: steadytext\n        image: steadytext:latest\n        ports:\n        - containerPort: 5557\n        env:\n        - name: STEADYTEXT_GENERATION_CACHE_CAPACITY\n          value: \"2048\"\n        - name: STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB\n          value: \"500\"\n        resources:\n          requests:\n            memory: \"4Gi\"\n            cpu: \"2\"\n          limits:\n            memory: \"8Gi\"\n            cpu: \"4\"\n        livenessProbe:\n          exec:\n            command:\n            - st\n            - daemon\n            - status\n          initialDelaySeconds: 30\n          periodSeconds: 10\n        readinessProbe:\n          tcpSocket:\n            port: 5557\n          initialDelaySeconds: 15\n          periodSeconds: 5\n        volumeMounts:\n        - name: cache\n          mountPath: /home/steadytext/.cache/steadytext\n      volumes:\n      - name: cache\n        persistentVolumeClaim:\n          claimName: steadytext-cache-pvc\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: steadytext-service\nspec:\n  selector:\n    app: steadytext\n  ports:\n    - protocol: TCP\n      port: 5557\n      targetPort: 5557\n  type: LoadBalancer\n\n---\napiVersion: v1\nkind: PersistentVolumeClaim\nmetadata:\n  name: steadytext-cache-pvc\nspec:\n  accessModes:\n    - ReadWriteMany\n  resources:\n    requests:\n      storage: 50Gi\n</code></pre>"},{"location":"examples/daemon-usage/#high-availability-setup","title":"High Availability Setup","text":"<pre><code># ha_daemon_client.py - High availability daemon client\nimport random\nimport time\nfrom typing import List, Optional\nimport steadytext\nfrom steadytext.daemon.client import DaemonClient\n\nclass HADaemonClient:\n    \"\"\"High availability client with multiple daemon endpoints.\"\"\"\n\n    def __init__(self, endpoints: List[tuple]):\n        \"\"\"\n        Initialize with multiple endpoints.\n\n        Args:\n            endpoints: List of (host, port) tuples\n        \"\"\"\n        self.endpoints = endpoints\n        self.clients = {}\n        self.failed_endpoints = set()\n        self.last_health_check = 0\n        self.health_check_interval = 60  # seconds\n\n    def _get_client(self, endpoint: tuple) -&gt; Optional[DaemonClient]:\n        \"\"\"Get or create client for endpoint.\"\"\"\n        if endpoint not in self.clients:\n            try:\n                host, port = endpoint\n                client = DaemonClient(host=host, port=port, timeout=2000)\n                # Test connection\n                client._send_request({\"type\": \"ping\"})\n                self.clients[endpoint] = client\n            except Exception:\n                return None\n        return self.clients.get(endpoint)\n\n    def _health_check(self):\n        \"\"\"Periodic health check of failed endpoints.\"\"\"\n        if time.time() - self.last_health_check &gt; self.health_check_interval:\n            recovered = set()\n            for endpoint in self.failed_endpoints:\n                if self._get_client(endpoint):\n                    recovered.add(endpoint)\n            self.failed_endpoints -= recovered\n            self.last_health_check = time.time()\n\n    def _get_available_endpoint(self) -&gt; Optional[tuple]:\n        \"\"\"Get random available endpoint.\"\"\"\n        self._health_check()\n        available = [ep for ep in self.endpoints if ep not in self.failed_endpoints]\n        return random.choice(available) if available else None\n\n    def generate(self, prompt: str, **kwargs):\n        \"\"\"Generate with automatic failover.\"\"\"\n        attempts = 0\n        endpoints_tried = set()\n\n        while attempts &lt; len(self.endpoints):\n            endpoint = self._get_available_endpoint()\n            if not endpoint or endpoint in endpoints_tried:\n                break\n\n            endpoints_tried.add(endpoint)\n            client = self._get_client(endpoint)\n\n            if client:\n                try:\n                    return client.generate(prompt, **kwargs)\n                except Exception as e:\n                    print(f\"Failed on {endpoint}: {e}\")\n                    self.failed_endpoints.add(endpoint)\n                    if endpoint in self.clients:\n                        del self.clients[endpoint]\n\n            attempts += 1\n\n        # All endpoints failed, fall back to direct mode\n        print(\"All daemon endpoints failed, using direct mode\")\n        return steadytext.generate(prompt, **kwargs)\n\n    def embed(self, text: str, **kwargs):\n        \"\"\"Embed with automatic failover.\"\"\"\n        # Similar implementation to generate\n        pass\n\n# Usage\nha_client = HADaemonClient([\n    (\"daemon1.example.com\", 5557),\n    (\"daemon2.example.com\", 5557),\n    (\"daemon3.example.com\", 5557)\n])\n\n# Automatic failover\nresult = ha_client.generate(\"Hello world\", seed=42)\n</code></pre>"},{"location":"examples/daemon-usage/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"examples/daemon-usage/#logging-configuration","title":"Logging Configuration","text":"<pre><code># logging_config.py\nimport logging\nimport sys\nfrom pathlib import Path\n\ndef setup_daemon_logging(log_dir=\"/var/log/steadytext\"):\n    \"\"\"Configure comprehensive daemon logging.\"\"\"\n    log_dir = Path(log_dir)\n    log_dir.mkdir(parents=True, exist_ok=True)\n\n    # Configure formatters\n    detailed_formatter = logging.Formatter(\n        '%(asctime)s - %(name)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'\n    )\n    simple_formatter = logging.Formatter(\n        '%(asctime)s - %(levelname)s - %(message)s'\n    )\n\n    # File handler for all logs\n    all_handler = logging.FileHandler(log_dir / \"daemon.log\")\n    all_handler.setLevel(logging.DEBUG)\n    all_handler.setFormatter(detailed_formatter)\n\n    # File handler for errors only\n    error_handler = logging.FileHandler(log_dir / \"daemon.error.log\")\n    error_handler.setLevel(logging.ERROR)\n    error_handler.setFormatter(detailed_formatter)\n\n    # Console handler\n    console_handler = logging.StreamHandler(sys.stdout)\n    console_handler.setLevel(logging.INFO)\n    console_handler.setFormatter(simple_formatter)\n\n    # Configure root logger\n    root_logger = logging.getLogger()\n    root_logger.setLevel(logging.DEBUG)\n    root_logger.addHandler(all_handler)\n    root_logger.addHandler(error_handler)\n    root_logger.addHandler(console_handler)\n\n    # Configure specific loggers\n    logging.getLogger(\"steadytext.daemon\").setLevel(logging.DEBUG)\n    logging.getLogger(\"zmq\").setLevel(logging.WARNING)\n\n    return root_logger\n\n# Request logging middleware\nclass RequestLogger:\n    \"\"\"Log all daemon requests for debugging.\"\"\"\n\n    def __init__(self, daemon_server):\n        self.daemon_server = daemon_server\n        self.logger = logging.getLogger(\"steadytext.daemon.requests\")\n\n    def log_request(self, request_id, request_type, request_data):\n        \"\"\"Log incoming request.\"\"\"\n        self.logger.info(f\"Request {request_id}: {request_type}\", extra={\n            \"request_id\": request_id,\n            \"request_type\": request_type,\n            \"seed\": request_data.get(\"seed\"),\n            \"prompt_length\": len(request_data.get(\"prompt\", \"\")),\n            \"timestamp\": time.time()\n        })\n\n    def log_response(self, request_id, response_data, duration):\n        \"\"\"Log outgoing response.\"\"\"\n        self.logger.info(f\"Response {request_id}: {duration:.3f}s\", extra={\n            \"request_id\": request_id,\n            \"success\": response_data.get(\"success\"),\n            \"cached\": response_data.get(\"cached\", False),\n            \"duration\": duration,\n            \"timestamp\": time.time()\n        })\n</code></pre>"},{"location":"examples/daemon-usage/#performance-metrics","title":"Performance Metrics","text":"<pre><code># metrics.py - Daemon performance metrics\nimport time\nimport psutil\nimport json\nfrom collections import deque\nfrom datetime import datetime\nfrom typing import Dict, Any\n\nclass DaemonMetrics:\n    \"\"\"Collect and report daemon performance metrics.\"\"\"\n\n    def __init__(self, window_size=1000):\n        self.request_times = deque(maxlen=window_size)\n        self.cache_hits = 0\n        self.cache_misses = 0\n        self.total_requests = 0\n        self.errors = 0\n        self.start_time = time.time()\n        self.process = psutil.Process()\n\n    def record_request(self, duration: float, cached: bool, success: bool):\n        \"\"\"Record request metrics.\"\"\"\n        self.total_requests += 1\n        self.request_times.append(duration)\n\n        if cached:\n            self.cache_hits += 1\n        else:\n            self.cache_misses += 1\n\n        if not success:\n            self.errors += 1\n\n    def get_metrics(self) -&gt; Dict[str, Any]:\n        \"\"\"Get current metrics snapshot.\"\"\"\n        uptime = time.time() - self.start_time\n\n        # Calculate percentiles\n        if self.request_times:\n            sorted_times = sorted(self.request_times)\n            p50 = sorted_times[len(sorted_times) // 2]\n            p95 = sorted_times[int(len(sorted_times) * 0.95)]\n            p99 = sorted_times[int(len(sorted_times) * 0.99)]\n            avg_time = sum(sorted_times) / len(sorted_times)\n        else:\n            p50 = p95 = p99 = avg_time = 0\n\n        # System metrics\n        cpu_percent = self.process.cpu_percent()\n        memory_info = self.process.memory_info()\n\n        return {\n            \"timestamp\": datetime.now().isoformat(),\n            \"uptime_seconds\": uptime,\n            \"total_requests\": self.total_requests,\n            \"requests_per_second\": self.total_requests / uptime if uptime &gt; 0 else 0,\n            \"cache_hit_rate\": self.cache_hits / self.total_requests if self.total_requests &gt; 0 else 0,\n            \"error_rate\": self.errors / self.total_requests if self.total_requests &gt; 0 else 0,\n            \"response_times\": {\n                \"average\": avg_time,\n                \"p50\": p50,\n                \"p95\": p95,\n                \"p99\": p99\n            },\n            \"system\": {\n                \"cpu_percent\": cpu_percent,\n                \"memory_mb\": memory_info.rss / 1024 / 1024,\n                \"threads\": self.process.num_threads()\n            }\n        }\n\n    def export_prometheus(self) -&gt; str:\n        \"\"\"Export metrics in Prometheus format.\"\"\"\n        metrics = self.get_metrics()\n        lines = []\n\n        # Request metrics\n        lines.append(f'steadytext_requests_total {metrics[\"total_requests\"]}')\n        lines.append(f'steadytext_requests_per_second {metrics[\"requests_per_second\"]:.2f}')\n        lines.append(f'steadytext_cache_hit_rate {metrics[\"cache_hit_rate\"]:.4f}')\n        lines.append(f'steadytext_error_rate {metrics[\"error_rate\"]:.4f}')\n\n        # Response time metrics\n        lines.append(f'steadytext_response_time_seconds{{quantile=\"0.5\"}} {metrics[\"response_times\"][\"p50\"]:.4f}')\n        lines.append(f'steadytext_response_time_seconds{{quantile=\"0.95\"}} {metrics[\"response_times\"][\"p95\"]:.4f}')\n        lines.append(f'steadytext_response_time_seconds{{quantile=\"0.99\"}} {metrics[\"response_times\"][\"p99\"]:.4f}')\n\n        # System metrics\n        lines.append(f'steadytext_cpu_percent {metrics[\"system\"][\"cpu_percent\"]:.2f}')\n        lines.append(f'steadytext_memory_megabytes {metrics[\"system\"][\"memory_mb\"]:.2f}')\n        lines.append(f'steadytext_threads {metrics[\"system\"][\"threads\"]}')\n\n        return '\\n'.join(lines)\n\n# HTTP metrics endpoint\nfrom flask import Flask, Response\n\napp = Flask(__name__)\nmetrics = DaemonMetrics()\n\n@app.route('/metrics')\ndef prometheus_metrics():\n    return Response(metrics.export_prometheus(), mimetype='text/plain')\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=9090)\n</code></pre>"},{"location":"examples/daemon-usage/#debug-tools","title":"Debug Tools","text":"<pre><code>#!/bin/bash\n# debug_daemon.sh - Comprehensive daemon debugging\n\n# Function to trace daemon requests\ntrace_requests() {\n    echo \"Tracing daemon requests...\"\n\n    # Start tcpdump on daemon port\n    sudo tcpdump -i lo -w daemon_trace.pcap port 5557 &amp;\n    TCPDUMP_PID=$!\n\n    # Run test requests\n    for i in {1..10}; do\n        st generate \"Test $i\" --seed $i &amp;\n    done\n    wait\n\n    # Stop tcpdump\n    sudo kill $TCPDUMP_PID\n\n    echo \"Trace saved to daemon_trace.pcap\"\n}\n\n# Function to profile daemon\nprofile_daemon() {\n    echo \"Profiling daemon performance...\"\n\n    # Get daemon PID\n    DAEMON_PID=$(st daemon status --json | jq -r '.pid')\n\n    if [ -z \"$DAEMON_PID\" ]; then\n        echo \"Daemon not running\"\n        return 1\n    fi\n\n    # CPU profiling\n    echo \"CPU profiling for 30 seconds...\"\n    sudo perf record -F 99 -p $DAEMON_PID -g -- sleep 30\n    sudo perf report &gt; daemon_cpu_profile.txt\n\n    # Memory profiling\n    echo \"Memory snapshot...\"\n    sudo gcore -o daemon_memory $DAEMON_PID\n\n    # Strace\n    echo \"System call trace for 10 seconds...\"\n    sudo strace -p $DAEMON_PID -o daemon_strace.log -f -T &amp;\n    STRACE_PID=$!\n    sleep 10\n    sudo kill $STRACE_PID\n\n    echo \"Profiling complete\"\n}\n\n# Function to stress test daemon\nstress_test() {\n    local concurrent=${1:-10}\n    local requests=${2:-100}\n\n    echo \"Stress testing with $concurrent concurrent clients, $requests requests each\"\n\n    # Start monitoring\n    st daemon status --json &gt; stress_test_before.json\n\n    # Run concurrent requests\n    for i in $(seq 1 $concurrent); do\n        (\n            for j in $(seq 1 $requests); do\n                st generate \"Stress test $i-$j\" --seed $((i*1000+j)) &gt;/dev/null 2&gt;&amp;1\n            done\n            echo \"Client $i completed\"\n        ) &amp;\n    done\n\n    # Wait for completion\n    wait\n\n    # Get final status\n    st daemon status --json &gt; stress_test_after.json\n\n    echo \"Stress test complete\"\n}\n\n# Main menu\necho \"SteadyText Daemon Debug Tools\"\necho \"1. Trace requests\"\necho \"2. Profile daemon\"\necho \"3. Stress test\"\necho \"4. View logs\"\necho \"5. Export metrics\"\n\nread -p \"Select option: \" choice\n\ncase $choice in\n    1) trace_requests ;;\n    2) profile_daemon ;;\n    3) \n        read -p \"Concurrent clients (default 10): \" concurrent\n        read -p \"Requests per client (default 100): \" requests\n        stress_test ${concurrent:-10} ${requests:-100}\n        ;;\n    4) \n        tail -f /var/log/steadytext/daemon.log\n        ;;\n    5)\n        curl -s http://localhost:9090/metrics\n        ;;\n    *) echo \"Invalid option\" ;;\nesac\n</code></pre>"},{"location":"examples/daemon-usage/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/daemon-usage/#cache-warming","title":"Cache Warming","text":"<pre><code># cache_warmer.py - Pre-populate daemon cache\nimport steadytext\nfrom steadytext.daemon import use_daemon\nimport json\nfrom pathlib import Path\n\nclass DaemonCacheWarmer:\n    \"\"\"Warm up daemon cache with common requests.\"\"\"\n\n    def __init__(self, warmup_file=\"warmup_prompts.json\"):\n        self.warmup_file = Path(warmup_file)\n        self.load_prompts()\n\n    def load_prompts(self):\n        \"\"\"Load warmup prompts from file.\"\"\"\n        if self.warmup_file.exists():\n            with open(self.warmup_file) as f:\n                self.warmup_data = json.load(f)\n        else:\n            # Default warmup prompts\n            self.warmup_data = {\n                \"generation\": [\n                    {\"prompt\": \"Hello\", \"seed\": 42},\n                    {\"prompt\": \"Write a summary\", \"seed\": 42},\n                    {\"prompt\": \"Explain this concept\", \"seed\": 42},\n                    {\"prompt\": \"Generate code\", \"seed\": 42},\n                    {\"prompt\": \"Create documentation\", \"seed\": 42}\n                ],\n                \"embedding\": [\n                    {\"text\": \"search query\", \"seed\": 42},\n                    {\"text\": \"document text\", \"seed\": 42},\n                    {\"text\": \"user input\", \"seed\": 42}\n                ]\n            }\n\n    def warm_generation_cache(self):\n        \"\"\"Warm up generation cache.\"\"\"\n        print(\"Warming generation cache...\")\n\n        with use_daemon():\n            for item in self.warmup_data[\"generation\"]:\n                try:\n                    result = steadytext.generate(\n                        item[\"prompt\"],\n                        seed=item.get(\"seed\", 42),\n                        max_new_tokens=item.get(\"max_tokens\", 512)\n                    )\n                    print(f\"\u2713 Cached: {item['prompt'][:30]}...\")\n                except Exception as e:\n                    print(f\"\u2717 Failed: {item['prompt'][:30]}... - {e}\")\n\n    def warm_embedding_cache(self):\n        \"\"\"Warm up embedding cache.\"\"\"\n        print(\"\\nWarming embedding cache...\")\n\n        with use_daemon():\n            for item in self.warmup_data[\"embedding\"]:\n                try:\n                    result = steadytext.embed(\n                        item[\"text\"],\n                        seed=item.get(\"seed\", 42)\n                    )\n                    print(f\"\u2713 Cached: {item['text'][:30]}...\")\n                except Exception as e:\n                    print(f\"\u2717 Failed: {item['text'][:30]}... - {e}\")\n\n    def run(self):\n        \"\"\"Run complete cache warming.\"\"\"\n        print(\"Starting daemon cache warming...\")\n        self.warm_generation_cache()\n        self.warm_embedding_cache()\n        print(\"\\nCache warming complete!\")\n\n    def save_common_prompts(self, prompts_file=\"access.log\"):\n        \"\"\"Extract common prompts from access logs.\"\"\"\n        # Parse access logs to find common prompts\n        prompt_counts = {}\n\n        with open(prompts_file) as f:\n            for line in f:\n                # Extract prompt from log line\n                # Adjust parsing based on your log format\n                if \"prompt:\" in line:\n                    prompt = line.split(\"prompt:\")[1].strip()\n                    prompt_counts[prompt] = prompt_counts.get(prompt, 0) + 1\n\n        # Get top prompts\n        top_prompts = sorted(\n            prompt_counts.items(),\n            key=lambda x: x[1],\n            reverse=True\n        )[:50]\n\n        # Update warmup data\n        self.warmup_data[\"generation\"] = [\n            {\"prompt\": prompt, \"seed\": 42}\n            for prompt, _ in top_prompts\n        ]\n\n        # Save to file\n        with open(self.warmup_file, 'w') as f:\n            json.dump(self.warmup_data, f, indent=2)\n\n# Usage\nif __name__ == \"__main__\":\n    warmer = DaemonCacheWarmer()\n    warmer.run()\n</code></pre>"},{"location":"examples/daemon-usage/#connection-pooling","title":"Connection Pooling","text":"<pre><code># connection_pool.py - Daemon connection pooling\nimport queue\nimport threading\nfrom contextlib import contextmanager\nfrom steadytext.daemon.client import DaemonClient\n\nclass DaemonConnectionPool:\n    \"\"\"Thread-safe connection pool for daemon clients.\"\"\"\n\n    def __init__(self, host=\"127.0.0.1\", port=5557, pool_size=10, timeout=5000):\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self.pool_size = pool_size\n        self._pool = queue.Queue(maxsize=pool_size)\n        self._all_connections = []\n        self._lock = threading.Lock()\n        self._initialize_pool()\n\n    def _initialize_pool(self):\n        \"\"\"Create initial connections.\"\"\"\n        for _ in range(self.pool_size):\n            conn = self._create_connection()\n            if conn:\n                self._pool.put(conn)\n                self._all_connections.append(conn)\n\n    def _create_connection(self):\n        \"\"\"Create new daemon connection.\"\"\"\n        try:\n            return DaemonClient(\n                host=self.host,\n                port=self.port,\n                timeout=self.timeout\n            )\n        except Exception as e:\n            print(f\"Failed to create connection: {e}\")\n            return None\n\n    @contextmanager\n    def get_connection(self, timeout=None):\n        \"\"\"Get connection from pool.\"\"\"\n        connection = None\n        try:\n            connection = self._pool.get(timeout=timeout)\n            yield connection\n        finally:\n            if connection:\n                self._pool.put(connection)\n\n    def close_all(self):\n        \"\"\"Close all connections.\"\"\"\n        with self._lock:\n            while not self._pool.empty():\n                try:\n                    conn = self._pool.get_nowait()\n                    conn.close()\n                except:\n                    pass\n            self._all_connections.clear()\n\n# Global connection pool\n_connection_pool = None\n\ndef get_connection_pool():\n    \"\"\"Get or create global connection pool.\"\"\"\n    global _connection_pool\n    if _connection_pool is None:\n        _connection_pool = DaemonConnectionPool()\n    return _connection_pool\n\n# Usage example\ndef parallel_generate(prompts):\n    \"\"\"Generate text in parallel using connection pool.\"\"\"\n    pool = get_connection_pool()\n    results = {}\n\n    def process_prompt(idx, prompt):\n        with pool.get_connection() as conn:\n            if conn:\n                try:\n                    result = conn.generate(prompt, seed=idx)\n                    results[idx] = result\n                except Exception as e:\n                    results[idx] = f\"Error: {e}\"\n\n    threads = []\n    for idx, prompt in enumerate(prompts):\n        t = threading.Thread(target=process_prompt, args=(idx, prompt))\n        t.start()\n        threads.append(t)\n\n    for t in threads:\n        t.join()\n\n    return [results[i] for i in range(len(prompts))]\n</code></pre>"},{"location":"examples/daemon-usage/#memory-optimization","title":"Memory Optimization","text":"<pre><code># memory_optimization.py - Optimize daemon memory usage\nimport gc\nimport resource\nimport psutil\nfrom steadytext import get_cache_manager\n\nclass DaemonMemoryOptimizer:\n    \"\"\"Optimize memory usage for long-running daemons.\"\"\"\n\n    def __init__(self, max_memory_mb=4096):\n        self.max_memory_mb = max_memory_mb\n        self.process = psutil.Process()\n        self.cache_manager = get_cache_manager()\n\n    def set_memory_limits(self):\n        \"\"\"Set process memory limits.\"\"\"\n        # Convert MB to bytes\n        max_memory_bytes = self.max_memory_mb * 1024 * 1024\n\n        # Set soft and hard limits\n        resource.setrlimit(\n            resource.RLIMIT_AS,\n            (max_memory_bytes, max_memory_bytes)\n        )\n\n        print(f\"Memory limit set to {self.max_memory_mb}MB\")\n\n    def get_memory_usage(self):\n        \"\"\"Get current memory usage.\"\"\"\n        memory_info = self.process.memory_info()\n        return {\n            \"rss_mb\": memory_info.rss / 1024 / 1024,\n            \"vms_mb\": memory_info.vms / 1024 / 1024,\n            \"percent\": self.process.memory_percent()\n        }\n\n    def optimize_caches(self):\n        \"\"\"Optimize cache sizes based on memory usage.\"\"\"\n        usage = self.get_memory_usage()\n\n        if usage[\"percent\"] &gt; 80:\n            # Reduce cache sizes\n            print(\"High memory usage, reducing cache sizes...\")\n\n            # Get current stats\n            stats = self.cache_manager.get_cache_stats()\n\n            # Clear least recently used entries\n            self.cache_manager.clear_old_entries(keep_ratio=0.5)\n\n            # Force garbage collection\n            gc.collect()\n\n    def periodic_optimization(self, interval=300):\n        \"\"\"Run periodic memory optimization.\"\"\"\n        import time\n        import threading\n\n        def optimize():\n            while True:\n                try:\n                    self.optimize_caches()\n                    usage = self.get_memory_usage()\n                    print(f\"Memory: {usage['rss_mb']:.1f}MB ({usage['percent']:.1f}%)\")\n                except Exception as e:\n                    print(f\"Optimization error: {e}\")\n\n                time.sleep(interval)\n\n        thread = threading.Thread(target=optimize, daemon=True)\n        thread.start()\n\n# Apply optimizations at daemon startup\noptimizer = DaemonMemoryOptimizer(max_memory_mb=4096)\noptimizer.set_memory_limits()\noptimizer.periodic_optimization()\n</code></pre>"},{"location":"examples/daemon-usage/#troubleshooting","title":"Troubleshooting","text":""},{"location":"examples/daemon-usage/#common-issues-and-solutions","title":"Common Issues and Solutions","text":""},{"location":"examples/daemon-usage/#daemon-wont-start","title":"Daemon Won't Start","text":"<pre><code># Problem: Address already in use\n$ st daemon start\nError: Address already in use (127.0.0.1:5557)\n\n# Solution 1: Check for existing process\n$ lsof -i :5557\n$ kill -9 &lt;PID&gt;\n\n# Solution 2: Use different port\n$ st daemon start --port 5558\n</code></pre>"},{"location":"examples/daemon-usage/#connection-timeouts","title":"Connection Timeouts","text":"<pre><code># Problem: Timeout errors with daemon\n\n# Solution 1: Increase timeout\nfrom steadytext.daemon.client import DaemonClient\nclient = DaemonClient(timeout=10000)  # 10 seconds\n\n# Solution 2: Check daemon health\nimport requests\ntry:\n    response = requests.get(\"http://localhost:9090/metrics\", timeout=1)\n    print(\"Daemon healthy\")\nexcept:\n    print(\"Daemon unhealthy\")\n\n# Solution 3: Restart daemon\nimport subprocess\nsubprocess.run([\"st\", \"daemon\", \"restart\"])\n</code></pre>"},{"location":"examples/daemon-usage/#memory-issues","title":"Memory Issues","text":"<pre><code># Problem: Daemon using too much memory\n\n# Solution 1: Clear caches\n$ st cache --clear\n\n# Solution 2: Reduce cache sizes\n$ export STEADYTEXT_GENERATION_CACHE_CAPACITY=128\n$ export STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=25\n$ st daemon restart\n\n# Solution 3: Monitor memory usage\n$ watch -n 1 'ps aux | grep \"st daemon\" | grep -v grep'\n</code></pre>"},{"location":"examples/daemon-usage/#performance-degradation","title":"Performance Degradation","text":"<pre><code># diagnose_performance.py\nimport time\nimport statistics\nimport steadytext\nfrom steadytext.daemon import use_daemon\n\ndef diagnose_daemon_performance():\n    \"\"\"Diagnose daemon performance issues.\"\"\"\n\n    # Test direct mode\n    direct_times = []\n    for i in range(10):\n        start = time.time()\n        steadytext.generate(\"test\", seed=i)\n        direct_times.append(time.time() - start)\n\n    # Test daemon mode\n    daemon_times = []\n    with use_daemon():\n        for i in range(10):\n            start = time.time()\n            steadytext.generate(\"test\", seed=i+100)\n            daemon_times.append(time.time() - start)\n\n    print(\"Direct mode:\")\n    print(f\"  Mean: {statistics.mean(direct_times):.3f}s\")\n    print(f\"  Stdev: {statistics.stdev(direct_times):.3f}s\")\n\n    print(\"\\nDaemon mode:\")\n    print(f\"  Mean: {statistics.mean(daemon_times):.3f}s\")\n    print(f\"  Stdev: {statistics.stdev(daemon_times):.3f}s\")\n\n    if statistics.mean(daemon_times) &gt; statistics.mean(direct_times):\n        print(\"\\nWARNING: Daemon is slower than direct mode!\")\n        print(\"Possible causes:\")\n        print(\"- Network latency\")\n        print(\"- Daemon overloaded\")\n        print(\"- Cache thrashing\")\n\ndiagnose_daemon_performance()\n</code></pre>"},{"location":"examples/daemon-usage/#debug-checklist","title":"Debug Checklist","text":"<ol> <li> <p>Check daemon status <pre><code>st daemon status --json | jq .\n</code></pre></p> </li> <li> <p>Verify connectivity <pre><code>nc -zv 127.0.0.1 5557\n</code></pre></p> </li> <li> <p>Check logs <pre><code>tail -f /var/log/steadytext/daemon.log\ngrep ERROR /var/log/steadytext/daemon.error.log\n</code></pre></p> </li> <li> <p>Monitor resources <pre><code>htop -p $(pgrep -f \"st daemon\")\n</code></pre></p> </li> <li> <p>Test basic operations <pre><code>from steadytext.daemon.client import DaemonClient\nclient = DaemonClient()\nprint(client._send_request({\"type\": \"ping\"}))\n</code></pre></p> </li> </ol>"},{"location":"examples/daemon-usage/#best-practices","title":"Best Practices","text":""},{"location":"examples/daemon-usage/#1-production-configuration","title":"1. Production Configuration","text":"<pre><code># production.env\nSTEADYTEXT_DAEMON_HOST=0.0.0.0\nSTEADYTEXT_DAEMON_PORT=5557\nSTEADYTEXT_GENERATION_CACHE_CAPACITY=2048\nSTEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=500\nSTEADYTEXT_EMBEDDING_CACHE_CAPACITY=4096\nSTEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=1000\nSTEADYTEXT_DEFAULT_SEED=42\nPYTHONUNBUFFERED=1\n</code></pre>"},{"location":"examples/daemon-usage/#2-health-monitoring","title":"2. Health Monitoring","text":"<pre><code># health_check.py\ndef health_check():\n    \"\"\"Comprehensive daemon health check.\"\"\"\n    checks = {\n        \"daemon_running\": False,\n        \"response_time\": None,\n        \"cache_available\": False,\n        \"memory_ok\": False\n    }\n\n    # Check if daemon is running\n    try:\n        result = subprocess.run(\n            [\"st\", \"daemon\", \"status\", \"--json\"],\n            capture_output=True,\n            text=True\n        )\n        if result.returncode == 0:\n            status = json.loads(result.stdout)\n            checks[\"daemon_running\"] = status.get(\"running\", False)\n    except:\n        pass\n\n    # Check response time\n    if checks[\"daemon_running\"]:\n        start = time.time()\n        try:\n            steadytext.generate(\"health check\", seed=42)\n            checks[\"response_time\"] = time.time() - start\n        except:\n            pass\n\n    # Check cache\n    try:\n        cache_manager = get_cache_manager()\n        stats = cache_manager.get_cache_stats()\n        checks[\"cache_available\"] = True\n    except:\n        pass\n\n    # Check memory\n    if checks[\"daemon_running\"]:\n        memory = psutil.Process().memory_info().rss / 1024 / 1024\n        checks[\"memory_ok\"] = memory &lt; 4096  # 4GB limit\n\n    return checks\n</code></pre>"},{"location":"examples/daemon-usage/#3-graceful-degradation","title":"3. Graceful Degradation","text":"<pre><code># graceful_degradation.py\nclass ResilientClient:\n    \"\"\"Client with graceful degradation.\"\"\"\n\n    def __init__(self):\n        self.use_daemon = True\n        self.fallback_count = 0\n        self.max_fallbacks = 3\n\n    def generate(self, prompt, **kwargs):\n        \"\"\"Generate with automatic fallback.\"\"\"\n        if self.use_daemon and self.fallback_count &lt; self.max_fallbacks:\n            try:\n                with use_daemon():\n                    return steadytext.generate(prompt, **kwargs)\n            except Exception as e:\n                self.fallback_count += 1\n                print(f\"Daemon failed ({self.fallback_count}/{self.max_fallbacks}): {e}\")\n\n                if self.fallback_count &gt;= self.max_fallbacks:\n                    self.use_daemon = False\n                    print(\"Disabling daemon due to repeated failures\")\n\n        # Direct mode fallback\n        return steadytext.generate(prompt, **kwargs)\n</code></pre>"},{"location":"examples/daemon-usage/#4-security-considerations","title":"4. Security Considerations","text":"<pre><code># secure_daemon.py\nimport ssl\nimport secrets\n\nclass SecureDaemonConfig:\n    \"\"\"Secure daemon configuration.\"\"\"\n\n    @staticmethod\n    def generate_auth_token():\n        \"\"\"Generate secure auth token.\"\"\"\n        return secrets.token_urlsafe(32)\n\n    @staticmethod\n    def configure_tls(cert_path, key_path):\n        \"\"\"Configure TLS for daemon.\"\"\"\n        context = ssl.create_default_context(ssl.Purpose.CLIENT_AUTH)\n        context.load_cert_chain(cert_path, key_path)\n        return context\n\n    @staticmethod\n    def restrict_bind_address():\n        \"\"\"Restrict daemon to localhost only.\"\"\"\n        return {\n            \"host\": \"127.0.0.1\",  # Never use 0.0.0.0 in production\n            \"port\": 5557\n        }\n</code></pre> <p>This comprehensive guide covers all aspects of using SteadyText's daemon mode, from basic usage to advanced production deployments. The daemon provides significant performance benefits while maintaining the simplicity and reliability that SteadyText is known for.</p>"},{"location":"examples/error-handling/","title":"Error Handling Guide","text":"<p>Learn how to handle errors gracefully in SteadyText, implement robust fallback strategies, and build resilient applications.</p>"},{"location":"examples/error-handling/#overview","title":"Overview","text":"<p>SteadyText follows a \"never fail\" philosophy (with v2.1.0+ updates):</p> <ul> <li>Functions return <code>None</code> when models are unavailable (v2.1.0+)</li> <li>No exceptions are raised during normal operations</li> <li>Graceful degradation with predictable behavior</li> <li>Clear error indicators for proper handling</li> <li>Deterministic fallbacks respect seed values</li> </ul> <p>Breaking Change in v2.1.0</p> <p>The deterministic fallback behavior has been disabled. Functions now return <code>None</code> instead of generating fallback text/embeddings when models are unavailable.</p>"},{"location":"examples/error-handling/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Error Types and Handling</li> <li>Generation Error Handling</li> <li>Embedding Error Handling</li> <li>Streaming Error Handling</li> <li>Daemon Error Handling</li> <li>CLI Error Handling</li> <li>Production Patterns</li> <li>Monitoring and Alerting</li> <li>Recovery Strategies</li> <li>Best Practices</li> </ul>"},{"location":"examples/error-handling/#error-types-and-handling","title":"Error Types and Handling","text":""},{"location":"examples/error-handling/#common-error-scenarios","title":"Common Error Scenarios","text":"<pre><code>import steadytext\nimport logging\nfrom typing import Optional, Union\n\n# Configure logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef handle_generation_result(result: Optional[str], prompt: str) -&gt; str:\n    \"\"\"Handle generation result with proper error checking.\"\"\"\n    if result is None:\n        logger.error(f\"Generation failed for prompt: {prompt}\")\n        # Implement your fallback strategy\n        return f\"[Error: Unable to generate response for: {prompt}]\"\n\n    if not result.strip():\n        logger.warning(f\"Empty generation for prompt: {prompt}\")\n        return \"[Error: Empty response generated]\"\n\n    return result\n\n# Usage example\nprompt = \"Write a summary\"\nresult = steadytext.generate(prompt, seed=42)\nhandled_result = handle_generation_result(result, prompt)\nprint(handled_result)\n</code></pre>"},{"location":"examples/error-handling/#error-categories","title":"Error Categories","text":"<pre><code>from enum import Enum\nfrom dataclasses import dataclass\nfrom typing import Optional, Any\n\nclass ErrorType(Enum):\n    \"\"\"Types of errors in SteadyText operations.\"\"\"\n    MODEL_NOT_LOADED = \"model_not_loaded\"\n    INVALID_INPUT = \"invalid_input\"\n    DAEMON_UNAVAILABLE = \"daemon_unavailable\"\n    CACHE_ERROR = \"cache_error\"\n    TIMEOUT = \"timeout\"\n    MEMORY_ERROR = \"memory_error\"\n    UNKNOWN = \"unknown\"\n\n@dataclass\nclass SteadyTextError:\n    \"\"\"Structured error information.\"\"\"\n    error_type: ErrorType\n    message: str\n    context: dict\n    recoverable: bool\n    suggested_action: Optional[str] = None\n\nclass ErrorHandler:\n    \"\"\"Centralized error handling for SteadyText operations.\"\"\"\n\n    def __init__(self):\n        self.error_log = []\n        self.error_counts = {error_type: 0 for error_type in ErrorType}\n\n    def handle_error(self, error_type: ErrorType, message: str, \n                     context: dict = None, recoverable: bool = True) -&gt; SteadyTextError:\n        \"\"\"Handle and log an error.\"\"\"\n        error = SteadyTextError(\n            error_type=error_type,\n            message=message,\n            context=context or {},\n            recoverable=recoverable,\n            suggested_action=self._get_suggested_action(error_type)\n        )\n\n        self.error_log.append(error)\n        self.error_counts[error_type] += 1\n\n        logger.error(f\"{error_type.value}: {message}\", extra=context)\n\n        return error\n\n    def _get_suggested_action(self, error_type: ErrorType) -&gt; str:\n        \"\"\"Get suggested action for error type.\"\"\"\n        actions = {\n            ErrorType.MODEL_NOT_LOADED: \"Run 'st models download' to download models\",\n            ErrorType.INVALID_INPUT: \"Check input format and constraints\",\n            ErrorType.DAEMON_UNAVAILABLE: \"Start daemon with 'st daemon start'\",\n            ErrorType.CACHE_ERROR: \"Clear cache with 'st cache --clear'\",\n            ErrorType.TIMEOUT: \"Increase timeout or retry operation\",\n            ErrorType.MEMORY_ERROR: \"Reduce batch size or restart daemon\",\n            ErrorType.UNKNOWN: \"Check logs for more information\"\n        }\n        return actions.get(error_type, \"\")\n\n    def get_error_summary(self) -&gt; dict:\n        \"\"\"Get summary of all errors.\"\"\"\n        return {\n            \"total_errors\": len(self.error_log),\n            \"error_counts\": dict(self.error_counts),\n            \"recent_errors\": self.error_log[-10:],\n            \"most_common\": max(self.error_counts.items(), key=lambda x: x[1])\n        }\n\n# Global error handler\nerror_handler = ErrorHandler()\n</code></pre>"},{"location":"examples/error-handling/#generation-error-handling","title":"Generation Error Handling","text":""},{"location":"examples/error-handling/#basic-error-handling","title":"Basic Error Handling","text":"<pre><code>import steadytext\nfrom typing import Optional\n\ndef safe_generate(prompt: str, seed: int = 42, max_retries: int = 3) -&gt; Optional[str]:\n    \"\"\"Generate text with retry logic and error handling.\"\"\"\n    for attempt in range(max_retries):\n        try:\n            result = steadytext.generate(prompt, seed=seed)\n\n            if result is None:\n                logger.warning(f\"Generation returned None (attempt {attempt + 1}/{max_retries})\")\n                if attempt &lt; max_retries - 1:\n                    time.sleep(0.5 * (attempt + 1))  # Exponential backoff\n                    continue\n                else:\n                    error_handler.handle_error(\n                        ErrorType.MODEL_NOT_LOADED,\n                        \"Generation failed after all retries\",\n                        {\"prompt\": prompt, \"seed\": seed, \"attempts\": max_retries}\n                    )\n                    return None\n\n            return result\n\n        except Exception as e:\n            logger.exception(f\"Unexpected error in generation: {e}\")\n            error_handler.handle_error(\n                ErrorType.UNKNOWN,\n                str(e),\n                {\"prompt\": prompt, \"seed\": seed, \"attempt\": attempt + 1}\n            )\n\n            if attempt &lt; max_retries - 1:\n                time.sleep(0.5 * (attempt + 1))\n            else:\n                return None\n\n    return None\n\n# Usage\nresult = safe_generate(\"Write a poem\", seed=123)\nif result:\n    print(result)\nelse:\n    print(\"Failed to generate text. Please check the error log.\")\n</code></pre>"},{"location":"examples/error-handling/#advanced-generation-error-handling","title":"Advanced Generation Error Handling","text":"<pre><code>import steadytext\nfrom typing import Optional, Dict, Any, Callable\nimport time\nimport hashlib\n\nclass RobustGenerator:\n    \"\"\"Robust text generation with comprehensive error handling.\"\"\"\n\n    def __init__(self, \n                 fallback_strategy: str = \"template\",\n                 cache_fallbacks: bool = True,\n                 alert_threshold: int = 5):\n        self.fallback_strategy = fallback_strategy\n        self.cache_fallbacks = cache_fallbacks\n        self.alert_threshold = alert_threshold\n        self.fallback_cache = {}\n        self.consecutive_failures = 0\n        self.success_callbacks = []\n        self.failure_callbacks = []\n\n    def on_success(self, callback: Callable):\n        \"\"\"Register success callback.\"\"\"\n        self.success_callbacks.append(callback)\n\n    def on_failure(self, callback: Callable):\n        \"\"\"Register failure callback.\"\"\"\n        self.failure_callbacks.append(callback)\n\n    def generate(self, prompt: str, seed: int = 42, **kwargs) -&gt; str:\n        \"\"\"Generate text with comprehensive error handling.\"\"\"\n        start_time = time.time()\n\n        try:\n            # Attempt generation\n            result = steadytext.generate(prompt, seed=seed, **kwargs)\n\n            if result is not None:\n                # Success\n                self.consecutive_failures = 0\n                self._notify_success(prompt, result, time.time() - start_time)\n                return result\n\n            # Generation failed\n            self.consecutive_failures += 1\n            self._notify_failure(prompt, \"Generation returned None\")\n\n            # Check alert threshold\n            if self.consecutive_failures &gt;= self.alert_threshold:\n                self._trigger_alert(f\"Generation failures exceeded threshold: {self.consecutive_failures}\")\n\n            # Apply fallback strategy\n            return self._apply_fallback(prompt, seed)\n\n        except Exception as e:\n            self.consecutive_failures += 1\n            self._notify_failure(prompt, str(e))\n            return self._apply_fallback(prompt, seed)\n\n    def _apply_fallback(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Apply fallback strategy based on configuration.\"\"\"\n        # Check cache first\n        cache_key = self._get_cache_key(prompt, seed)\n        if self.cache_fallbacks and cache_key in self.fallback_cache:\n            return self.fallback_cache[cache_key]\n\n        # Generate fallback\n        if self.fallback_strategy == \"template\":\n            fallback = self._template_fallback(prompt)\n        elif self.fallback_strategy == \"hash\":\n            fallback = self._hash_fallback(prompt, seed)\n        elif self.fallback_strategy == \"empty\":\n            fallback = \"\"\n        elif self.fallback_strategy == \"error\":\n            fallback = f\"[Error: Unable to generate response for: {prompt[:50]}...]\"\n        else:\n            fallback = \"[Generation failed]\"\n\n        # Cache fallback\n        if self.cache_fallbacks:\n            self.fallback_cache[cache_key] = fallback\n\n        return fallback\n\n    def _template_fallback(self, prompt: str) -&gt; str:\n        \"\"\"Generate template-based fallback.\"\"\"\n        templates = {\n            \"summary\": \"This is a summary of the requested content.\",\n            \"explanation\": \"This explains the requested concept.\",\n            \"code\": \"# Code implementation would go here\",\n            \"story\": \"Once upon a time, there was a story to be told.\",\n            \"default\": \"Response generated for: {}\"\n        }\n\n        # Detect prompt type\n        prompt_lower = prompt.lower()\n        for key in templates:\n            if key in prompt_lower:\n                return templates[key].format(prompt[:30] + \"...\")\n\n        return templates[\"default\"].format(prompt[:30] + \"...\")\n\n    def _hash_fallback(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Generate deterministic hash-based fallback.\"\"\"\n        # Create deterministic hash\n        hash_input = f\"{prompt}:{seed}\"\n        hash_value = hashlib.sha256(hash_input.encode()).hexdigest()[:8]\n\n        return f\"[Fallback response {hash_value} for: {prompt[:30]}...]\"\n\n    def _get_cache_key(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Generate cache key for fallback.\"\"\"\n        return f\"{prompt}:{seed}\"\n\n    def _notify_success(self, prompt: str, result: str, duration: float):\n        \"\"\"Notify success callbacks.\"\"\"\n        for callback in self.success_callbacks:\n            try:\n                callback(prompt, result, duration)\n            except Exception as e:\n                logger.error(f\"Error in success callback: {e}\")\n\n    def _notify_failure(self, prompt: str, error: str):\n        \"\"\"Notify failure callbacks.\"\"\"\n        for callback in self.failure_callbacks:\n            try:\n                callback(prompt, error)\n            except Exception as e:\n                logger.error(f\"Error in failure callback: {e}\")\n\n    def _trigger_alert(self, message: str):\n        \"\"\"Trigger alert for critical errors.\"\"\"\n        logger.critical(f\"ALERT: {message}\")\n        # Implement your alerting mechanism here\n        # e.g., send email, Slack message, PagerDuty alert\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get generator statistics.\"\"\"\n        return {\n            \"consecutive_failures\": self.consecutive_failures,\n            \"fallback_cache_size\": len(self.fallback_cache),\n            \"fallback_strategy\": self.fallback_strategy\n        }\n\n# Usage example\ngenerator = RobustGenerator(fallback_strategy=\"template\")\n\n# Register callbacks\ngenerator.on_success(lambda p, r, d: logger.info(f\"Generated in {d:.2f}s\"))\ngenerator.on_failure(lambda p, e: logger.error(f\"Failed: {e}\"))\n\n# Generate with robust error handling\nresult = generator.generate(\"Write a technical blog post\", seed=42)\nprint(result)\n\n# Check stats\nstats = generator.get_stats()\nprint(f\"Generator stats: {stats}\")\n</code></pre>"},{"location":"examples/error-handling/#embedding-error-handling","title":"Embedding Error Handling","text":""},{"location":"examples/error-handling/#basic-embedding-error-handling","title":"Basic Embedding Error Handling","text":"<pre><code>import steadytext\nimport numpy as np\nfrom typing import Optional\n\ndef safe_embed(text: str, seed: int = 42) -&gt; Optional[np.ndarray]:\n    \"\"\"Safely generate embeddings with error handling.\"\"\"\n    try:\n        embedding = steadytext.embed(text, seed=seed)\n\n        if embedding is None:\n            logger.error(f\"Embedding returned None for text: {text[:50]}...\")\n            return None\n\n        # Validate embedding\n        if not isinstance(embedding, np.ndarray):\n            logger.error(f\"Invalid embedding type: {type(embedding)}\")\n            return None\n\n        if embedding.shape != (1024,):\n            logger.error(f\"Invalid embedding shape: {embedding.shape}\")\n            return None\n\n        # Check for NaN or Inf values\n        if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):\n            logger.error(\"Embedding contains NaN or Inf values\")\n            return None\n\n        return embedding\n\n    except Exception as e:\n        logger.exception(f\"Error generating embedding: {e}\")\n        return None\n\n# Usage with fallback\ndef get_embedding_with_fallback(text: str, seed: int = 42) -&gt; np.ndarray:\n    \"\"\"Get embedding with zero-vector fallback.\"\"\"\n    embedding = safe_embed(text, seed=seed)\n\n    if embedding is None:\n        logger.warning(\"Using zero-vector fallback for embedding\")\n        # Return zero vector with correct shape\n        return np.zeros(1024, dtype=np.float32)\n\n    return embedding\n</code></pre>"},{"location":"examples/error-handling/#advanced-embedding-error-handling","title":"Advanced Embedding Error Handling","text":"<pre><code>import steadytext\nimport numpy as np\nfrom typing import List, Optional, Dict, Tuple\nimport hashlib\n\nclass RobustEmbedder:\n    \"\"\"Robust embedding generation with comprehensive error handling.\"\"\"\n\n    def __init__(self, \n                 fallback_method: str = \"zero\",\n                 cache_embeddings: bool = True,\n                 similarity_threshold: float = 0.95):\n        self.fallback_method = fallback_method\n        self.cache_embeddings = cache_embeddings\n        self.similarity_threshold = similarity_threshold\n        self.embedding_cache = {}\n        self.error_count = 0\n        self.success_count = 0\n\n    def embed(self, text: str, seed: int = 42) -&gt; np.ndarray:\n        \"\"\"Generate embedding with error handling.\"\"\"\n        # Check cache\n        cache_key = self._get_cache_key(text, seed)\n        if self.cache_embeddings and cache_key in self.embedding_cache:\n            return self.embedding_cache[cache_key]\n\n        try:\n            # Attempt embedding\n            embedding = steadytext.embed(text, seed=seed)\n\n            if embedding is not None and self._validate_embedding(embedding):\n                self.success_count += 1\n\n                # Cache successful embedding\n                if self.cache_embeddings:\n                    self.embedding_cache[cache_key] = embedding\n\n                return embedding\n\n            # Embedding failed\n            self.error_count += 1\n            return self._generate_fallback(text, seed)\n\n        except Exception as e:\n            logger.exception(f\"Embedding error: {e}\")\n            self.error_count += 1\n            return self._generate_fallback(text, seed)\n\n    def embed_batch(self, texts: List[str], seed: int = 42) -&gt; List[np.ndarray]:\n        \"\"\"Generate embeddings for multiple texts with error handling.\"\"\"\n        embeddings = []\n        failed_indices = []\n\n        for i, text in enumerate(texts):\n            try:\n                # Use different seed for each text in batch\n                text_seed = seed + i\n                embedding = self.embed(text, seed=text_seed)\n                embeddings.append(embedding)\n            except Exception as e:\n                logger.error(f\"Failed to embed text {i}: {e}\")\n                failed_indices.append(i)\n                embeddings.append(self._generate_fallback(text, seed + i))\n\n        if failed_indices:\n            logger.warning(f\"Failed to embed {len(failed_indices)} texts: {failed_indices}\")\n\n        return embeddings\n\n    def find_similar_cached(self, embedding: np.ndarray, top_k: int = 5) -&gt; List[Tuple[str, float]]:\n        \"\"\"Find similar embeddings from cache.\"\"\"\n        if not self.embedding_cache:\n            return []\n\n        similarities = []\n        for cache_key, cached_embedding in self.embedding_cache.items():\n            similarity = np.dot(embedding, cached_embedding)\n            if similarity &gt;= self.similarity_threshold:\n                text = cache_key.split(\":\")[0]  # Extract text from cache key\n                similarities.append((text, similarity))\n\n        # Sort by similarity\n        similarities.sort(key=lambda x: x[1], reverse=True)\n        return similarities[:top_k]\n\n    def _validate_embedding(self, embedding: np.ndarray) -&gt; bool:\n        \"\"\"Validate embedding array.\"\"\"\n        if not isinstance(embedding, np.ndarray):\n            return False\n\n        if embedding.shape != (1024,):\n            return False\n\n        if np.any(np.isnan(embedding)) or np.any(np.isinf(embedding)):\n            return False\n\n        # Check if embedding is normalized\n        norm = np.linalg.norm(embedding)\n        if not np.isclose(norm, 1.0, atol=1e-6):\n            logger.warning(f\"Embedding not normalized: norm={norm}\")\n\n        return True\n\n    def _generate_fallback(self, text: str, seed: int) -&gt; np.ndarray:\n        \"\"\"Generate fallback embedding based on method.\"\"\"\n        if self.fallback_method == \"zero\":\n            return np.zeros(1024, dtype=np.float32)\n\n        elif self.fallback_method == \"random\":\n            # Deterministic random based on text and seed\n            np.random.seed(hash(f\"{text}:{seed}\") % (2**32))\n            embedding = np.random.randn(1024).astype(np.float32)\n            # Normalize\n            embedding = embedding / np.linalg.norm(embedding)\n            return embedding\n\n        elif self.fallback_method == \"hash\":\n            # Hash-based deterministic embedding\n            hash_input = f\"{text}:{seed}\".encode()\n            hash_bytes = hashlib.sha256(hash_input).digest()\n\n            # Convert hash to embedding\n            embedding = np.frombuffer(hash_bytes * 32, dtype=np.float32)[:1024]\n            # Normalize to [-1, 1] range\n            embedding = 2 * (embedding / 255.0) - 1\n            # L2 normalize\n            embedding = embedding / np.linalg.norm(embedding)\n            return embedding\n\n        else:\n            return np.zeros(1024, dtype=np.float32)\n\n    def _get_cache_key(self, text: str, seed: int) -&gt; str:\n        \"\"\"Generate cache key.\"\"\"\n        return f\"{text}:{seed}\"\n\n    def get_stats(self) -&gt; Dict[str, Any]:\n        \"\"\"Get embedder statistics.\"\"\"\n        total = self.success_count + self.error_count\n        return {\n            \"success_count\": self.success_count,\n            \"error_count\": self.error_count,\n            \"success_rate\": self.success_count / total if total &gt; 0 else 0,\n            \"cache_size\": len(self.embedding_cache),\n            \"fallback_method\": self.fallback_method\n        }\n\n# Usage example\nembedder = RobustEmbedder(fallback_method=\"hash\")\n\n# Single embedding\nembedding = embedder.embed(\"test text\", seed=42)\nprint(f\"Embedding shape: {embedding.shape}, norm: {np.linalg.norm(embedding):.4f}\")\n\n# Batch embedding\ntexts = [\"text 1\", \"text 2\", \"text 3\"]\nembeddings = embedder.embed_batch(texts, seed=100)\nprint(f\"Generated {len(embeddings)} embeddings\")\n\n# Find similar\nsimilar = embedder.find_similar_cached(embedding, top_k=3)\nprint(f\"Similar embeddings: {similar}\")\n\n# Stats\nprint(f\"Embedder stats: {embedder.get_stats()}\")\n</code></pre>"},{"location":"examples/error-handling/#streaming-error-handling","title":"Streaming Error Handling","text":""},{"location":"examples/error-handling/#basic-streaming-error-handling","title":"Basic Streaming Error Handling","text":"<pre><code>import steadytext\nfrom typing import Iterator, Optional\n\ndef safe_generate_iter(prompt: str, seed: int = 42) -&gt; Iterator[str]:\n    \"\"\"Safely generate streaming text with error handling.\"\"\"\n    try:\n        stream = steadytext.generate_iter(prompt, seed=seed)\n\n        # Check if stream is empty (indicates error)\n        first_token = None\n        try:\n            first_token = next(stream)\n        except StopIteration:\n            logger.error(\"Empty stream returned\")\n            yield \"[Error: No content generated]\"\n            return\n\n        # Yield first token\n        if first_token:\n            yield first_token\n\n        # Yield remaining tokens\n        for token in stream:\n            yield token\n\n    except Exception as e:\n        logger.exception(f\"Streaming error: {e}\")\n        yield f\"[Error: {str(e)}]\"\n\n# Usage with timeout\ndef generate_with_timeout(prompt: str, seed: int = 42, timeout: float = 30.0) -&gt; str:\n    \"\"\"Generate with streaming and timeout.\"\"\"\n    import signal\n\n    def timeout_handler(signum, frame):\n        raise TimeoutError(\"Generation timed out\")\n\n    # Set timeout\n    signal.signal(signal.SIGALRM, timeout_handler)\n    signal.alarm(int(timeout))\n\n    try:\n        result = []\n        for token in safe_generate_iter(prompt, seed=seed):\n            result.append(token)\n\n        # Cancel timeout\n        signal.alarm(0)\n        return \"\".join(result)\n\n    except TimeoutError:\n        logger.error(f\"Generation timed out after {timeout}s\")\n        return \"[Error: Generation timed out]\"\n    finally:\n        # Ensure timeout is cancelled\n        signal.alarm(0)\n</code></pre>"},{"location":"examples/error-handling/#advanced-streaming-error-handling","title":"Advanced Streaming Error Handling","text":"<pre><code>import steadytext\nfrom typing import Iterator, Optional, Callable\nimport time\nimport threading\nfrom queue import Queue, Empty\n\nclass RobustStreamer:\n    \"\"\"Robust streaming generation with comprehensive error handling.\"\"\"\n\n    def __init__(self,\n                 timeout: float = 30.0,\n                 max_tokens: int = 512,\n                 heartbeat_interval: float = 1.0):\n        self.timeout = timeout\n        self.max_tokens = max_tokens\n        self.heartbeat_interval = heartbeat_interval\n        self.error_handlers = []\n        self.token_validators = []\n\n    def on_error(self, handler: Callable):\n        \"\"\"Register error handler.\"\"\"\n        self.error_handlers.append(handler)\n\n    def add_token_validator(self, validator: Callable[[str], bool]):\n        \"\"\"Add token validator.\"\"\"\n        self.token_validators.append(validator)\n\n    def generate_stream(self, prompt: str, seed: int = 42) -&gt; Iterator[str]:\n        \"\"\"Generate streaming text with comprehensive error handling.\"\"\"\n        start_time = time.time()\n        tokens_generated = 0\n        last_token_time = start_time\n\n        try:\n            stream = steadytext.generate_iter(prompt, seed=seed)\n\n            for token in stream:\n                current_time = time.time()\n\n                # Check timeout\n                if current_time - start_time &gt; self.timeout:\n                    self._handle_error(\"Timeout\", prompt, tokens_generated)\n                    yield \"[Timeout]\"\n                    break\n\n                # Check token count\n                if tokens_generated &gt;= self.max_tokens:\n                    logger.warning(f\"Max tokens ({self.max_tokens}) reached\")\n                    break\n\n                # Validate token\n                if not self._validate_token(token):\n                    logger.warning(f\"Invalid token detected: {repr(token)}\")\n                    continue\n\n                # Check for stalled generation\n                if current_time - last_token_time &gt; self.heartbeat_interval * 10:\n                    self._handle_error(\"Stalled generation\", prompt, tokens_generated)\n                    yield \"[Stalled]\"\n                    break\n\n                # Yield valid token\n                yield token\n                tokens_generated += 1\n                last_token_time = current_time\n\n            # Check if generation completed successfully\n            if tokens_generated == 0:\n                self._handle_error(\"No tokens generated\", prompt, 0)\n                yield \"[No content]\"\n\n        except Exception as e:\n            self._handle_error(str(e), prompt, tokens_generated)\n            yield f\"[Error: {str(e)}]\"\n\n    def generate_async(self, prompt: str, seed: int = 42, \n                      callback: Optional[Callable] = None) -&gt; threading.Thread:\n        \"\"\"Generate asynchronously with error handling.\"\"\"\n        result_queue = Queue()\n\n        def worker():\n            try:\n                tokens = []\n                for token in self.generate_stream(prompt, seed):\n                    tokens.append(token)\n                    if callback:\n                        callback(token)\n\n                result_queue.put((\"success\", \"\".join(tokens)))\n\n            except Exception as e:\n                result_queue.put((\"error\", str(e)))\n\n        thread = threading.Thread(target=worker)\n        thread.start()\n\n        # Return thread and queue for monitoring\n        thread.result_queue = result_queue\n        return thread\n\n    def generate_with_fallback(self, prompt: str, seed: int = 42,\n                              fallback_prompts: Optional[List[str]] = None) -&gt; Iterator[str]:\n        \"\"\"Generate with fallback prompts on error.\"\"\"\n        fallback_prompts = fallback_prompts or [\n            f\"Please provide a response about: {prompt[:50]}...\",\n            \"Generate a helpful response.\",\n            \"Provide relevant information.\"\n        ]\n\n        # Try main prompt\n        tokens = list(self.generate_stream(prompt, seed))\n        if self._is_valid_generation(tokens):\n            for token in tokens:\n                yield token\n            return\n\n        # Try fallback prompts\n        for i, fallback in enumerate(fallback_prompts):\n            logger.info(f\"Trying fallback prompt {i+1}\")\n            tokens = list(self.generate_stream(fallback, seed + i + 1))\n            if self._is_valid_generation(tokens):\n                for token in tokens:\n                    yield token\n                return\n\n        # All attempts failed\n        yield \"[All generation attempts failed]\"\n\n    def _validate_token(self, token: str) -&gt; bool:\n        \"\"\"Validate a token.\"\"\"\n        # Basic validation\n        if not isinstance(token, str):\n            return False\n\n        # Custom validators\n        for validator in self.token_validators:\n            if not validator(token):\n                return False\n\n        return True\n\n    def _is_valid_generation(self, tokens: List[str]) -&gt; bool:\n        \"\"\"Check if generation is valid.\"\"\"\n        if not tokens:\n            return False\n\n        content = \"\".join(tokens)\n\n        # Check for error markers\n        if any(marker in content for marker in [\"[Error\", \"[Timeout\", \"[Stalled\", \"[No content\"]):\n            return False\n\n        # Check minimum length\n        if len(content.strip()) &lt; 10:\n            return False\n\n        return True\n\n    def _handle_error(self, error: str, prompt: str, tokens_generated: int):\n        \"\"\"Handle streaming error.\"\"\"\n        error_info = {\n            \"error\": error,\n            \"prompt\": prompt,\n            \"tokens_generated\": tokens_generated,\n            \"timestamp\": time.time()\n        }\n\n        logger.error(f\"Streaming error: {error_info}\")\n\n        for handler in self.error_handlers:\n            try:\n                handler(error_info)\n            except Exception as e:\n                logger.error(f\"Error in error handler: {e}\")\n\n# Usage example\nstreamer = RobustStreamer(timeout=20.0, max_tokens=300)\n\n# Add custom token validator\nstreamer.add_token_validator(lambda token: len(token) &lt; 100)\n\n# Add error handler\nstreamer.on_error(lambda info: print(f\"Error handled: {info['error']}\"))\n\n# Stream with error handling\nprint(\"Streaming with error handling:\")\nfor token in streamer.generate_stream(\"Write a story\", seed=42):\n    print(token, end=\"\", flush=True)\nprint()\n\n# Async generation\nprint(\"\\nAsync generation:\")\nthread = streamer.generate_async(\n    \"Explain quantum computing\",\n    seed=123,\n    callback=lambda token: print(token, end=\"\", flush=True)\n)\n\n# Wait for completion\nthread.join()\ntry:\n    status, result = thread.result_queue.get(timeout=1)\n    print(f\"\\nAsync result: {status}\")\nexcept Empty:\n    print(\"\\nAsync generation did not complete\")\n\n# Generation with fallbacks\nprint(\"\\nGeneration with fallbacks:\")\nfor token in streamer.generate_with_fallback(\"Complex prompt that might fail\", seed=456):\n    print(token, end=\"\", flush=True)\nprint()\n</code></pre>"},{"location":"examples/error-handling/#daemon-error-handling","title":"Daemon Error Handling","text":""},{"location":"examples/error-handling/#basic-daemon-error-handling","title":"Basic Daemon Error Handling","text":"<pre><code>import steadytext\nfrom steadytext.daemon import use_daemon\nfrom steadytext.daemon.client import is_daemon_running\nimport subprocess\nimport time\n\ndef ensure_daemon_running(max_retries: int = 3) -&gt; bool:\n    \"\"\"Ensure daemon is running with retries.\"\"\"\n    for attempt in range(max_retries):\n        if is_daemon_running():\n            return True\n\n        logger.info(f\"Daemon not running, attempting to start (attempt {attempt + 1}/{max_retries})\")\n\n        try:\n            # Start daemon\n            result = subprocess.run(\n                [\"st\", \"daemon\", \"start\"],\n                capture_output=True,\n                text=True,\n                timeout=10\n            )\n\n            if result.returncode == 0:\n                # Wait for daemon to be ready\n                time.sleep(2)\n                if is_daemon_running():\n                    logger.info(\"Daemon started successfully\")\n                    return True\n            else:\n                logger.error(f\"Failed to start daemon: {result.stderr}\")\n\n        except subprocess.TimeoutExpired:\n            logger.error(\"Daemon start timed out\")\n        except Exception as e:\n            logger.error(f\"Error starting daemon: {e}\")\n\n        time.sleep(1)\n\n    return False\n\ndef generate_with_daemon_fallback(prompt: str, seed: int = 42) -&gt; Optional[str]:\n    \"\"\"Generate with automatic daemon fallback.\"\"\"\n    try:\n        # Try with daemon first\n        with use_daemon():\n            return steadytext.generate(prompt, seed=seed)\n    except Exception as e:\n        logger.warning(f\"Daemon generation failed: {e}, falling back to direct mode\")\n\n        # Fall back to direct generation\n        try:\n            return steadytext.generate(prompt, seed=seed)\n        except Exception as e2:\n            logger.error(f\"Direct generation also failed: {e2}\")\n            return None\n\n# Usage\nif ensure_daemon_running():\n    result = generate_with_daemon_fallback(\"Hello world\", seed=42)\n    if result:\n        print(result)\n    else:\n        print(\"Generation failed\")\nelse:\n    print(\"Could not start daemon, using direct mode\")\n    result = steadytext.generate(\"Hello world\", seed=42)\n</code></pre>"},{"location":"examples/error-handling/#advanced-daemon-error-handling","title":"Advanced Daemon Error Handling","text":"<pre><code>import steadytext\nfrom steadytext.daemon import use_daemon\nfrom steadytext.daemon.client import DaemonClient, is_daemon_running\nimport time\nimport threading\nfrom typing import Optional, Dict, Any, Callable\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\n\n@dataclass\nclass DaemonHealth:\n    \"\"\"Daemon health status.\"\"\"\n    is_running: bool\n    response_time: Optional[float]\n    last_check: datetime\n    consecutive_failures: int\n    error_message: Optional[str] = None\n\nclass ResilientDaemonClient:\n    \"\"\"Resilient daemon client with comprehensive error handling.\"\"\"\n\n    def __init__(self,\n                 health_check_interval: int = 60,\n                 max_consecutive_failures: int = 5,\n                 auto_restart: bool = True):\n        self.health_check_interval = health_check_interval\n        self.max_consecutive_failures = max_consecutive_failures\n        self.auto_restart = auto_restart\n        self.health = DaemonHealth(\n            is_running=False,\n            response_time=None,\n            last_check=datetime.now(),\n            consecutive_failures=0\n        )\n        self._health_check_thread = None\n        self._stop_health_check = threading.Event()\n        self._fallback_mode = False\n        self._callbacks = {\n            \"daemon_down\": [],\n            \"daemon_up\": [],\n            \"daemon_slow\": [],\n            \"fallback_activated\": []\n        }\n\n    def on(self, event: str, callback: Callable):\n        \"\"\"Register event callback.\"\"\"\n        if event in self._callbacks:\n            self._callbacks[event].append(callback)\n\n    def start_monitoring(self):\n        \"\"\"Start health monitoring thread.\"\"\"\n        if self._health_check_thread is None or not self._health_check_thread.is_alive():\n            self._stop_health_check.clear()\n            self._health_check_thread = threading.Thread(\n                target=self._health_monitor_loop,\n                daemon=True\n            )\n            self._health_check_thread.start()\n\n    def stop_monitoring(self):\n        \"\"\"Stop health monitoring.\"\"\"\n        self._stop_health_check.set()\n        if self._health_check_thread:\n            self._health_check_thread.join(timeout=5)\n\n    def generate(self, prompt: str, seed: int = 42, **kwargs) -&gt; Optional[str]:\n        \"\"\"Generate with comprehensive error handling.\"\"\"\n        # Check if we should use fallback mode\n        if self._fallback_mode or not self.health.is_running:\n            return self._fallback_generate(prompt, seed, **kwargs)\n\n        try:\n            # Attempt daemon generation\n            start_time = time.time()\n\n            with use_daemon():\n                result = steadytext.generate(prompt, seed=seed, **kwargs)\n\n            # Update response time\n            response_time = time.time() - start_time\n            self._update_health(True, response_time)\n\n            # Check for slow responses\n            if response_time &gt; 5.0:\n                self._trigger_event(\"daemon_slow\", {\"response_time\": response_time})\n\n            return result\n\n        except Exception as e:\n            logger.error(f\"Daemon generation error: {e}\")\n            self._update_health(False, error=str(e))\n\n            # Check if we should switch to fallback mode\n            if self.health.consecutive_failures &gt;= self.max_consecutive_failures:\n                self._activate_fallback_mode()\n\n            # Try direct generation\n            return self._fallback_generate(prompt, seed, **kwargs)\n\n    def embed(self, text: str, seed: int = 42) -&gt; Optional[np.ndarray]:\n        \"\"\"Embed with comprehensive error handling.\"\"\"\n        # Similar implementation to generate\n        pass\n\n    def _health_monitor_loop(self):\n        \"\"\"Background health monitoring loop.\"\"\"\n        while not self._stop_health_check.is_set():\n            try:\n                self._perform_health_check()\n            except Exception as e:\n                logger.error(f\"Health check error: {e}\")\n\n            # Wait for next check\n            self._stop_health_check.wait(self.health_check_interval)\n\n    def _perform_health_check(self):\n        \"\"\"Perform daemon health check.\"\"\"\n        try:\n            start_time = time.time()\n\n            # Check if daemon is running\n            if not is_daemon_running():\n                self._update_health(False, error=\"Daemon not running\")\n\n                if self.auto_restart and self.health.consecutive_failures &gt; 0:\n                    self._attempt_restart()\n                return\n\n            # Test daemon responsiveness\n            client = DaemonClient()\n            response = client._send_request({\"type\": \"ping\"})\n\n            if response and response.get(\"success\"):\n                response_time = time.time() - start_time\n                self._update_health(True, response_time)\n\n                # Check if we can deactivate fallback mode\n                if self._fallback_mode and self.health.consecutive_failures == 0:\n                    self._deactivate_fallback_mode()\n            else:\n                self._update_health(False, error=\"Ping failed\")\n\n        except Exception as e:\n            self._update_health(False, error=str(e))\n\n    def _update_health(self, success: bool, response_time: Optional[float] = None,\n                      error: Optional[str] = None):\n        \"\"\"Update health status.\"\"\"\n        self.health.last_check = datetime.now()\n\n        if success:\n            self.health.is_running = True\n            self.health.response_time = response_time\n            self.health.error_message = None\n\n            if self.health.consecutive_failures &gt; 0:\n                # Daemon recovered\n                self.health.consecutive_failures = 0\n                self._trigger_event(\"daemon_up\", {\"response_time\": response_time})\n        else:\n            self.health.consecutive_failures += 1\n            self.health.error_message = error\n\n            if self.health.consecutive_failures == 1:\n                # Daemon just went down\n                self._trigger_event(\"daemon_down\", {\"error\": error})\n\n            if self.health.consecutive_failures &gt;= self.max_consecutive_failures:\n                self.health.is_running = False\n\n    def _activate_fallback_mode(self):\n        \"\"\"Activate fallback mode.\"\"\"\n        if not self._fallback_mode:\n            self._fallback_mode = True\n            logger.warning(\"Activating fallback mode due to daemon failures\")\n            self._trigger_event(\"fallback_activated\", {\n                \"consecutive_failures\": self.health.consecutive_failures\n            })\n\n    def _deactivate_fallback_mode(self):\n        \"\"\"Deactivate fallback mode.\"\"\"\n        if self._fallback_mode:\n            self._fallback_mode = False\n            logger.info(\"Deactivating fallback mode - daemon recovered\")\n\n    def _fallback_generate(self, prompt: str, seed: int = 42, **kwargs) -&gt; Optional[str]:\n        \"\"\"Fallback generation without daemon.\"\"\"\n        try:\n            logger.info(\"Using direct generation (fallback mode)\")\n            return steadytext.generate(prompt, seed=seed, **kwargs)\n        except Exception as e:\n            logger.error(f\"Fallback generation error: {e}\")\n            return None\n\n    def _attempt_restart(self):\n        \"\"\"Attempt to restart daemon.\"\"\"\n        logger.info(\"Attempting to restart daemon\")\n\n        try:\n            # Stop daemon if running\n            subprocess.run([\"st\", \"daemon\", \"stop\"], timeout=5)\n            time.sleep(1)\n\n            # Start daemon\n            result = subprocess.run(\n                [\"st\", \"daemon\", \"start\"],\n                capture_output=True,\n                text=True,\n                timeout=10\n            )\n\n            if result.returncode == 0:\n                logger.info(\"Daemon restart successful\")\n                time.sleep(2)  # Wait for startup\n            else:\n                logger.error(f\"Daemon restart failed: {result.stderr}\")\n\n        except Exception as e:\n            logger.error(f\"Error restarting daemon: {e}\")\n\n    def _trigger_event(self, event: str, data: Dict[str, Any]):\n        \"\"\"Trigger event callbacks.\"\"\"\n        for callback in self._callbacks.get(event, []):\n            try:\n                callback(data)\n            except Exception as e:\n                logger.error(f\"Error in {event} callback: {e}\")\n\n    def get_status(self) -&gt; Dict[str, Any]:\n        \"\"\"Get current status.\"\"\"\n        return {\n            \"health\": {\n                \"is_running\": self.health.is_running,\n                \"response_time\": self.health.response_time,\n                \"last_check\": self.health.last_check.isoformat(),\n                \"consecutive_failures\": self.health.consecutive_failures,\n                \"error_message\": self.health.error_message\n            },\n            \"fallback_mode\": self._fallback_mode,\n            \"monitoring\": self._health_check_thread.is_alive() if self._health_check_thread else False\n        }\n\n# Usage example\nclient = ResilientDaemonClient(auto_restart=True)\n\n# Register event handlers\nclient.on(\"daemon_down\", lambda data: print(f\"Daemon down: {data}\"))\nclient.on(\"daemon_up\", lambda data: print(f\"Daemon recovered: {data}\"))\nclient.on(\"daemon_slow\", lambda data: print(f\"Slow response: {data}\"))\nclient.on(\"fallback_activated\", lambda data: print(f\"Fallback mode: {data}\"))\n\n# Start monitoring\nclient.start_monitoring()\n\n# Use with automatic error handling\nresult = client.generate(\"Write a poem\", seed=42)\nif result:\n    print(result)\nelse:\n    print(\"Generation failed\")\n\n# Check status\nstatus = client.get_status()\nprint(f\"Client status: {status}\")\n\n# Stop monitoring when done\nclient.stop_monitoring()\n</code></pre>"},{"location":"examples/error-handling/#cli-error-handling","title":"CLI Error Handling","text":""},{"location":"examples/error-handling/#shell-script-error-handling","title":"Shell Script Error Handling","text":"<pre><code>#!/bin/bash\n# robust_cli.sh - Robust CLI usage with error handling\n\nset -euo pipefail  # Exit on error, undefined variable, pipe failure\n\n# Error handling function\nhandle_error() {\n    local exit_code=$?\n    local line_number=$1\n    echo \"Error on line $line_number: Command exited with status $exit_code\" &gt;&amp;2\n\n    # Log error\n    echo \"[$(date)] Error on line $line_number, exit code $exit_code\" &gt;&gt; steadytext_errors.log\n\n    # Cleanup if needed\n    cleanup\n\n    exit $exit_code\n}\n\n# Set error trap\ntrap 'handle_error ${LINENO}' ERR\n\n# Cleanup function\ncleanup() {\n    # Remove temporary files\n    rm -f /tmp/steadytext_temp_*\n}\n\n# Function to safely generate text\nsafe_generate() {\n    local prompt=\"$1\"\n    local seed=\"${2:-42}\"\n    local max_retries=3\n    local retry_count=0\n\n    while [ $retry_count -lt $max_retries ]; do\n        if result=$(st generate \"$prompt\" --seed \"$seed\" --json 2&gt;/dev/null); then\n            # Extract text from JSON\n            if text=$(echo \"$result\" | jq -r '.text' 2&gt;/dev/null); then\n                echo \"$text\"\n                return 0\n            else\n                echo \"Error: Invalid JSON response\" &gt;&amp;2\n            fi\n        else\n            echo \"Error: Generation failed (attempt $((retry_count + 1))/$max_retries)\" &gt;&amp;2\n        fi\n\n        retry_count=$((retry_count + 1))\n        sleep 1\n    done\n\n    return 1\n}\n\n# Function to check daemon status\ncheck_daemon() {\n    if st daemon status &gt;/dev/null 2&gt;&amp;1; then\n        echo \"Daemon is running\"\n        return 0\n    else\n        echo \"Daemon is not running\"\n        return 1\n    fi\n}\n\n# Function to ensure daemon is running\nensure_daemon() {\n    if ! check_daemon; then\n        echo \"Starting daemon...\"\n        if st daemon start; then\n            sleep 2\n            if check_daemon; then\n                echo \"Daemon started successfully\"\n                return 0\n            fi\n        fi\n        echo \"Failed to start daemon\" &gt;&amp;2\n        return 1\n    fi\n    return 0\n}\n\n# Main script\nmain() {\n    echo \"SteadyText Robust CLI Example\"\n    echo \"=============================\"\n\n    # Ensure daemon is running (optional)\n    if ensure_daemon; then\n        echo \"Using daemon mode\"\n    else\n        echo \"Using direct mode\"\n    fi\n\n    # Generate with error handling\n    echo -e \"\\nGenerating text...\"\n    if text=$(safe_generate \"Write a haiku about error handling\" 123); then\n        echo \"Generated text:\"\n        echo \"$text\"\n    else\n        echo \"Failed to generate text\"\n        exit 1\n    fi\n\n    # Batch processing with error handling\n    echo -e \"\\nBatch processing...\"\n    prompts=(\"Task 1\" \"Task 2\" \"Task 3\")\n\n    for i in \"${!prompts[@]}\"; do\n        prompt=\"${prompts[$i]}\"\n        echo -n \"Processing '$prompt': \"\n\n        if result=$(safe_generate \"$prompt\" $((100 + i))); then\n            echo \"Success\"\n            echo \"$result\" &gt; \"output_$i.txt\"\n        else\n            echo \"Failed\"\n            # Continue with next prompt instead of exiting\n        fi\n    done\n\n    echo -e \"\\nCompleted successfully\"\n}\n\n# Run main function\nmain \"$@\"\n</code></pre>"},{"location":"examples/error-handling/#python-cli-wrapper","title":"Python CLI Wrapper","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nrobust_cli_wrapper.py - Robust wrapper for SteadyText CLI\n\"\"\"\n\nimport subprocess\nimport json\nimport time\nimport sys\nimport logging\nfrom typing import Optional, Dict, Any, List\nfrom pathlib import Path\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nclass SteadyTextCLI:\n    \"\"\"Robust wrapper for SteadyText CLI with error handling.\"\"\"\n\n    def __init__(self, \n                 timeout: int = 30,\n                 max_retries: int = 3,\n                 retry_delay: float = 1.0):\n        self.timeout = timeout\n        self.max_retries = max_retries\n        self.retry_delay = retry_delay\n        self.daemon_checked = False\n        self.daemon_available = False\n\n    def _run_command(self, cmd: List[str], input_text: Optional[str] = None) -&gt; subprocess.CompletedProcess:\n        \"\"\"Run CLI command with timeout and error handling.\"\"\"\n        try:\n            result = subprocess.run(\n                cmd,\n                input=input_text,\n                capture_output=True,\n                text=True,\n                timeout=self.timeout\n            )\n            return result\n\n        except subprocess.TimeoutExpired as e:\n            logger.error(f\"Command timed out: {' '.join(cmd)}\")\n            raise\n        except Exception as e:\n            logger.error(f\"Command error: {e}\")\n            raise\n\n    def check_daemon(self) -&gt; bool:\n        \"\"\"Check if daemon is running.\"\"\"\n        if not self.daemon_checked:\n            try:\n                result = self._run_command([\"st\", \"daemon\", \"status\", \"--json\"])\n                if result.returncode == 0:\n                    status = json.loads(result.stdout)\n                    self.daemon_available = status.get(\"running\", False)\n            except:\n                self.daemon_available = False\n\n            self.daemon_checked = True\n            logger.info(f\"Daemon available: {self.daemon_available}\")\n\n        return self.daemon_available\n\n    def generate(self, prompt: str, seed: int = 42, **kwargs) -&gt; Optional[str]:\n        \"\"\"Generate text with error handling and retries.\"\"\"\n        cmd = [\"st\", \"generate\", prompt, \"--seed\", str(seed), \"--json\", \"--wait\"]\n\n        # Add additional options\n        if \"max_new_tokens\" in kwargs:\n            cmd.extend([\"--max-new-tokens\", str(kwargs[\"max_new_tokens\"])])\n\n        for attempt in range(self.max_retries):\n            try:\n                result = self._run_command(cmd)\n\n                if result.returncode == 0:\n                    # Parse JSON response\n                    try:\n                        data = json.loads(result.stdout)\n                        return data.get(\"text\")\n                    except json.JSONDecodeError:\n                        logger.error(f\"Invalid JSON response: {result.stdout}\")\n                else:\n                    logger.error(f\"Generation failed: {result.stderr}\")\n\n            except subprocess.TimeoutExpired:\n                logger.error(f\"Generation timed out (attempt {attempt + 1}/{self.max_retries})\")\n            except Exception as e:\n                logger.error(f\"Generation error: {e}\")\n\n            if attempt &lt; self.max_retries - 1:\n                time.sleep(self.retry_delay * (attempt + 1))\n\n        return None\n\n    def generate_stream(self, prompt: str, seed: int = 42, callback=None) -&gt; bool:\n        \"\"\"Stream generation with error handling.\"\"\"\n        cmd = [\"st\", \"generate\", prompt, \"--seed\", str(seed)]\n\n        try:\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                text=True,\n                bufsize=1\n            )\n\n            # Read output character by character\n            while True:\n                char = process.stdout.read(1)\n                if not char:\n                    break\n\n                if callback:\n                    callback(char)\n                else:\n                    print(char, end=\"\", flush=True)\n\n            # Wait for process to complete\n            process.wait(timeout=self.timeout)\n\n            return process.returncode == 0\n\n        except subprocess.TimeoutExpired:\n            logger.error(\"Streaming generation timed out\")\n            process.kill()\n            return False\n        except Exception as e:\n            logger.error(f\"Streaming error: {e}\")\n            return False\n\n    def embed(self, text: str, seed: int = 42) -&gt; Optional[List[float]]:\n        \"\"\"Generate embedding with error handling.\"\"\"\n        cmd = [\"st\", \"embed\", text, \"--seed\", str(seed), \"--format\", \"json\"]\n\n        for attempt in range(self.max_retries):\n            try:\n                result = self._run_command(cmd)\n\n                if result.returncode == 0:\n                    # Parse JSON array\n                    try:\n                        embedding = json.loads(result.stdout)\n                        if isinstance(embedding, list) and len(embedding) == 1024:\n                            return embedding\n                        else:\n                            logger.error(\"Invalid embedding format\")\n                    except json.JSONDecodeError:\n                        logger.error(\"Invalid JSON embedding\")\n                else:\n                    logger.error(f\"Embedding failed: {result.stderr}\")\n\n            except Exception as e:\n                logger.error(f\"Embedding error: {e}\")\n\n            if attempt &lt; self.max_retries - 1:\n                time.sleep(self.retry_delay)\n\n        return None\n\n    def batch_generate(self, prompts: List[str], seeds: Optional[List[int]] = None) -&gt; List[Optional[str]]:\n        \"\"\"Batch generate with parallel processing.\"\"\"\n        import concurrent.futures\n\n        if seeds is None:\n            seeds = [42 + i for i in range(len(prompts))]\n\n        results = []\n        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n            futures = [\n                executor.submit(self.generate, prompt, seed)\n                for prompt, seed in zip(prompts, seeds)\n            ]\n\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    result = future.result()\n                    results.append(result)\n                except Exception as e:\n                    logger.error(f\"Batch generation error: {e}\")\n                    results.append(None)\n\n        return results\n\ndef main():\n    \"\"\"Example usage of robust CLI wrapper.\"\"\"\n    cli = SteadyTextCLI()\n\n    # Check daemon\n    if cli.check_daemon():\n        print(\"\u2713 Daemon is running\")\n    else:\n        print(\"\u2717 Daemon not available, using direct mode\")\n\n    # Single generation\n    print(\"\\nGenerating text...\")\n    text = cli.generate(\"Write a short poem\", seed=123)\n    if text:\n        print(f\"Generated: {text}\")\n    else:\n        print(\"Generation failed\")\n\n    # Streaming generation\n    print(\"\\nStreaming generation...\")\n    success = cli.generate_stream(\"Tell me a story\", seed=456)\n    print(f\"\\nStreaming {'succeeded' if success else 'failed'}\")\n\n    # Batch generation\n    print(\"\\nBatch generation...\")\n    prompts = [\"Task 1\", \"Task 2\", \"Task 3\"]\n    results = cli.batch_generate(prompts)\n    for i, (prompt, result) in enumerate(zip(prompts, results)):\n        status = \"\u2713\" if result else \"\u2717\"\n        print(f\"{status} {prompt}: {result[:50] if result else 'Failed'}...\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"examples/error-handling/#production-patterns","title":"Production Patterns","text":""},{"location":"examples/error-handling/#circuit-breaker-pattern","title":"Circuit Breaker Pattern","text":"<pre><code>from enum import Enum\nfrom datetime import datetime, timedelta\nfrom typing import Optional, Callable, Any\nimport threading\n\nclass CircuitState(Enum):\n    CLOSED = \"closed\"\n    OPEN = \"open\"\n    HALF_OPEN = \"half_open\"\n\nclass CircuitBreaker:\n    \"\"\"Circuit breaker for SteadyText operations.\"\"\"\n\n    def __init__(self,\n                 failure_threshold: int = 5,\n                 recovery_timeout: int = 60,\n                 expected_exception: type = Exception):\n        self.failure_threshold = failure_threshold\n        self.recovery_timeout = recovery_timeout\n        self.expected_exception = expected_exception\n        self.failure_count = 0\n        self.last_failure_time = None\n        self.state = CircuitState.CLOSED\n        self._lock = threading.Lock()\n\n    def call(self, func: Callable, *args, **kwargs) -&gt; Any:\n        \"\"\"Execute function with circuit breaker protection.\"\"\"\n        with self._lock:\n            if self.state == CircuitState.OPEN:\n                if self._should_attempt_reset():\n                    self.state = CircuitState.HALF_OPEN\n                else:\n                    raise Exception(\"Circuit breaker is OPEN\")\n\n        try:\n            result = func(*args, **kwargs)\n            self._on_success()\n            return result\n        except self.expected_exception as e:\n            self._on_failure()\n            raise\n\n    def _should_attempt_reset(self) -&gt; bool:\n        \"\"\"Check if we should attempt to reset circuit.\"\"\"\n        return (self.last_failure_time and\n                datetime.now() - self.last_failure_time &gt; timedelta(seconds=self.recovery_timeout))\n\n    def _on_success(self):\n        \"\"\"Handle successful call.\"\"\"\n        with self._lock:\n            self.failure_count = 0\n            if self.state == CircuitState.HALF_OPEN:\n                self.state = CircuitState.CLOSED\n\n    def _on_failure(self):\n        \"\"\"Handle failed call.\"\"\"\n        with self._lock:\n            self.failure_count += 1\n            self.last_failure_time = datetime.now()\n\n            if self.failure_count &gt;= self.failure_threshold:\n                self.state = CircuitState.OPEN\n            elif self.state == CircuitState.HALF_OPEN:\n                self.state = CircuitState.OPEN\n\n    def get_state(self) -&gt; Dict[str, Any]:\n        \"\"\"Get circuit breaker state.\"\"\"\n        with self._lock:\n            return {\n                \"state\": self.state.value,\n                \"failure_count\": self.failure_count,\n                \"last_failure\": self.last_failure_time.isoformat() if self.last_failure_time else None\n            }\n\n# Usage with SteadyText\ncircuit_breaker = CircuitBreaker(failure_threshold=3, recovery_timeout=30)\n\ndef protected_generate(prompt: str, seed: int = 42) -&gt; Optional[str]:\n    \"\"\"Generate with circuit breaker protection.\"\"\"\n    try:\n        return circuit_breaker.call(steadytext.generate, prompt, seed=seed)\n    except Exception as e:\n        logger.error(f\"Circuit breaker triggered: {e}\")\n        return None\n</code></pre>"},{"location":"examples/error-handling/#retry-with-exponential-backoff","title":"Retry with Exponential Backoff","text":"<pre><code>import time\nimport random\nfrom typing import TypeVar, Callable, Optional, Any\n\nT = TypeVar('T')\n\ndef retry_with_backoff(\n    func: Callable[..., T],\n    max_retries: int = 3,\n    base_delay: float = 1.0,\n    max_delay: float = 60.0,\n    exponential_base: float = 2.0,\n    jitter: bool = True\n) -&gt; Callable[..., Optional[T]]:\n    \"\"\"Decorator for retry with exponential backoff.\"\"\"\n\n    def wrapper(*args, **kwargs) -&gt; Optional[T]:\n        last_exception = None\n\n        for attempt in range(max_retries):\n            try:\n                return func(*args, **kwargs)\n            except Exception as e:\n                last_exception = e\n\n                if attempt == max_retries - 1:\n                    logger.error(f\"All {max_retries} attempts failed: {e}\")\n                    break\n\n                # Calculate delay with exponential backoff\n                delay = min(base_delay * (exponential_base ** attempt), max_delay)\n\n                # Add jitter\n                if jitter:\n                    delay = delay * (0.5 + random.random())\n\n                logger.warning(f\"Attempt {attempt + 1} failed, retrying in {delay:.2f}s: {e}\")\n                time.sleep(delay)\n\n        return None\n\n    return wrapper\n\n# Apply to SteadyText functions\n@retry_with_backoff\ndef robust_generate(prompt: str, seed: int = 42) -&gt; Optional[str]:\n    \"\"\"Generate with automatic retry.\"\"\"\n    result = steadytext.generate(prompt, seed=seed)\n    if result is None:\n        raise Exception(\"Generation returned None\")\n    return result\n\n@retry_with_backoff\ndef robust_embed(text: str, seed: int = 42) -&gt; Optional[np.ndarray]:\n    \"\"\"Embed with automatic retry.\"\"\"\n    result = steadytext.embed(text, seed=seed)\n    if result is None:\n        raise Exception(\"Embedding returned None\")\n    return result\n</code></pre>"},{"location":"examples/error-handling/#monitoring-and-alerting","title":"Monitoring and Alerting","text":""},{"location":"examples/error-handling/#error-monitoring-system","title":"Error Monitoring System","text":"<pre><code>import time\nimport json\nfrom datetime import datetime, timedelta\nfrom collections import deque, defaultdict\nfrom typing import Dict, List, Any, Optional\nimport smtplib\nfrom email.mime.text import MIMEText\n\nclass ErrorMonitor:\n    \"\"\"Comprehensive error monitoring for SteadyText.\"\"\"\n\n    def __init__(self,\n                 window_size: int = 1000,\n                 alert_threshold: int = 10,\n                 alert_window: int = 300):  # 5 minutes\n        self.window_size = window_size\n        self.alert_threshold = alert_threshold\n        self.alert_window = alert_window\n        self.errors = deque(maxlen=window_size)\n        self.error_counts = defaultdict(int)\n        self.alert_sent = {}\n        self.metrics = {\n            \"total_errors\": 0,\n            \"errors_by_type\": defaultdict(int),\n            \"errors_by_hour\": defaultdict(int),\n            \"recent_error_rate\": 0.0\n        }\n\n    def record_error(self, error_type: str, error_message: str,\n                    context: Optional[Dict[str, Any]] = None):\n        \"\"\"Record an error occurrence.\"\"\"\n        error = {\n            \"timestamp\": datetime.now(),\n            \"type\": error_type,\n            \"message\": error_message,\n            \"context\": context or {}\n        }\n\n        self.errors.append(error)\n        self.error_counts[error_type] += 1\n        self.metrics[\"total_errors\"] += 1\n        self.metrics[\"errors_by_type\"][error_type] += 1\n\n        # Update hourly metrics\n        hour = datetime.now().strftime(\"%Y-%m-%d %H:00\")\n        self.metrics[\"errors_by_hour\"][hour] += 1\n\n        # Check if alert needed\n        self._check_alert_condition(error_type)\n\n    def _check_alert_condition(self, error_type: str):\n        \"\"\"Check if we should send an alert.\"\"\"\n        # Count recent errors of this type\n        cutoff_time = datetime.now() - timedelta(seconds=self.alert_window)\n        recent_errors = sum(\n            1 for error in self.errors\n            if error[\"type\"] == error_type and error[\"timestamp\"] &gt; cutoff_time\n        )\n\n        # Check threshold\n        if recent_errors &gt;= self.alert_threshold:\n            last_alert = self.alert_sent.get(error_type)\n            if not last_alert or datetime.now() - last_alert &gt; timedelta(seconds=self.alert_window):\n                self._send_alert(error_type, recent_errors)\n                self.alert_sent[error_type] = datetime.now()\n\n    def _send_alert(self, error_type: str, error_count: int):\n        \"\"\"Send alert notification.\"\"\"\n        message = f\"\"\"\n        SteadyText Error Alert\n\n        Error Type: {error_type}\n        Count: {error_count} errors in last {self.alert_window} seconds\n        Threshold: {self.alert_threshold}\n        Time: {datetime.now().isoformat()}\n\n        Recent errors:\n        \"\"\"\n\n        # Add recent errors\n        recent = [e for e in self.errors if e[\"type\"] == error_type][-5:]\n        for error in recent:\n            message += f\"\\n- {error['timestamp']}: {error['message']}\"\n\n        logger.critical(f\"ALERT: {message}\")\n\n        # Implement your alert mechanism here\n        # e.g., send email, Slack, PagerDuty, etc.\n\n    def get_error_rate(self, window_seconds: int = 60) -&gt; float:\n        \"\"\"Calculate error rate in errors per second.\"\"\"\n        cutoff_time = datetime.now() - timedelta(seconds=window_seconds)\n        recent_errors = sum(\n            1 for error in self.errors\n            if error[\"timestamp\"] &gt; cutoff_time\n        )\n        return recent_errors / window_seconds\n\n    def get_report(self) -&gt; Dict[str, Any]:\n        \"\"\"Generate error report.\"\"\"\n        return {\n            \"summary\": {\n                \"total_errors\": self.metrics[\"total_errors\"],\n                \"unique_error_types\": len(self.error_counts),\n                \"error_rate_per_minute\": self.get_error_rate(60) * 60,\n                \"most_common_error\": max(self.error_counts.items(), key=lambda x: x[1]) if self.error_counts else None\n            },\n            \"errors_by_type\": dict(self.metrics[\"errors_by_type\"]),\n            \"recent_errors\": [\n                {\n                    \"timestamp\": e[\"timestamp\"].isoformat(),\n                    \"type\": e[\"type\"],\n                    \"message\": e[\"message\"]\n                }\n                for e in list(self.errors)[-10:]\n            ],\n            \"alerts_sent\": {\n                error_type: timestamp.isoformat()\n                for error_type, timestamp in self.alert_sent.items()\n            }\n        }\n\n    def export_metrics(self, filepath: str):\n        \"\"\"Export metrics to file.\"\"\"\n        with open(filepath, 'w') as f:\n            json.dump(self.get_report(), f, indent=2)\n\n# Global error monitor\nerror_monitor = ErrorMonitor(alert_threshold=5, alert_window=300)\n\n# Integration with SteadyText operations\ndef monitored_generate(prompt: str, seed: int = 42) -&gt; Optional[str]:\n    \"\"\"Generate with error monitoring.\"\"\"\n    try:\n        result = steadytext.generate(prompt, seed=seed)\n\n        if result is None:\n            error_monitor.record_error(\n                \"generation_failed\",\n                \"Generation returned None\",\n                {\"prompt\": prompt[:50], \"seed\": seed}\n            )\n\n        return result\n\n    except Exception as e:\n        error_monitor.record_error(\n            \"generation_exception\",\n            str(e),\n            {\"prompt\": prompt[:50], \"seed\": seed}\n        )\n        return None\n</code></pre>"},{"location":"examples/error-handling/#recovery-strategies","title":"Recovery Strategies","text":""},{"location":"examples/error-handling/#graceful-degradation","title":"Graceful Degradation","text":"<pre><code>class GracefulDegradationManager:\n    \"\"\"Manage graceful degradation strategies.\"\"\"\n\n    def __init__(self):\n        self.degradation_levels = {\n            0: \"full_service\",\n            1: \"reduced_quality\",\n            2: \"cached_only\",\n            3: \"static_responses\",\n            4: \"maintenance_mode\"\n        }\n        self.current_level = 0\n        self.level_thresholds = {\n            \"error_rate\": [0.1, 0.3, 0.5, 0.7, 0.9],\n            \"response_time\": [2.0, 5.0, 10.0, 20.0, 30.0]\n        }\n\n    def evaluate_service_health(self, metrics: Dict[str, float]) -&gt; int:\n        \"\"\"Evaluate service health and determine degradation level.\"\"\"\n        error_rate = metrics.get(\"error_rate\", 0.0)\n        response_time = metrics.get(\"response_time\", 0.0)\n\n        # Determine level based on metrics\n        level = 0\n        for i, (error_threshold, time_threshold) in enumerate(\n            zip(self.level_thresholds[\"error_rate\"], \n                self.level_thresholds[\"response_time\"])\n        ):\n            if error_rate &gt; error_threshold or response_time &gt; time_threshold:\n                level = i + 1\n\n        return min(level, 4)\n\n    def apply_degradation_strategy(self, level: int, operation: str, **kwargs) -&gt; Any:\n        \"\"\"Apply appropriate degradation strategy.\"\"\"\n        self.current_level = level\n        strategy = self.degradation_levels[level]\n\n        logger.info(f\"Applying degradation strategy: {strategy}\")\n\n        if strategy == \"full_service\":\n            return self._full_service(operation, **kwargs)\n        elif strategy == \"reduced_quality\":\n            return self._reduced_quality(operation, **kwargs)\n        elif strategy == \"cached_only\":\n            return self._cached_only(operation, **kwargs)\n        elif strategy == \"static_responses\":\n            return self._static_responses(operation, **kwargs)\n        else:  # maintenance_mode\n            return self._maintenance_mode(operation, **kwargs)\n\n    def _full_service(self, operation: str, **kwargs):\n        \"\"\"Normal operation.\"\"\"\n        if operation == \"generate\":\n            return steadytext.generate(**kwargs)\n        elif operation == \"embed\":\n            return steadytext.embed(**kwargs)\n\n    def _reduced_quality(self, operation: str, **kwargs):\n        \"\"\"Reduced quality but faster.\"\"\"\n        if operation == \"generate\":\n            # Reduce token count\n            kwargs[\"max_new_tokens\"] = min(kwargs.get(\"max_new_tokens\", 512), 100)\n            return steadytext.generate(**kwargs)\n\n    def _cached_only(self, operation: str, **kwargs):\n        \"\"\"Return only cached responses.\"\"\"\n        # Check cache directly\n        cache_manager = get_cache_manager()\n        # Implement cache-only logic\n        return None\n\n    def _static_responses(self, operation: str, **kwargs):\n        \"\"\"Return static pre-defined responses.\"\"\"\n        static_responses = {\n            \"generate\": \"Service is currently limited. Please try again later.\",\n            \"embed\": np.zeros(1024, dtype=np.float32)\n        }\n        return static_responses.get(operation)\n\n    def _maintenance_mode(self, operation: str, **kwargs):\n        \"\"\"System in maintenance mode.\"\"\"\n        return None\n</code></pre>"},{"location":"examples/error-handling/#best-practices","title":"Best Practices","text":""},{"location":"examples/error-handling/#1-comprehensive-error-handler","title":"1. Comprehensive Error Handler","text":"<pre><code>class SteadyTextErrorHandler:\n    \"\"\"Comprehensive error handler for all SteadyText operations.\"\"\"\n\n    def __init__(self):\n        self.handlers = {\n            \"generation\": self._handle_generation_error,\n            \"embedding\": self._handle_embedding_error,\n            \"streaming\": self._handle_streaming_error,\n            \"daemon\": self._handle_daemon_error\n        }\n        self.fallback_strategies = {\n            \"generation\": self._generation_fallback,\n            \"embedding\": self._embedding_fallback\n        }\n\n    def handle(self, operation: str, error: Any, context: Dict[str, Any]) -&gt; Any:\n        \"\"\"Central error handling.\"\"\"\n        handler = self.handlers.get(operation, self._default_handler)\n        return handler(error, context)\n\n    def _handle_generation_error(self, error: Any, context: Dict[str, Any]):\n        \"\"\"Handle generation errors.\"\"\"\n        logger.error(f\"Generation error: {error}\", extra=context)\n\n        # Try fallback\n        fallback = self.fallback_strategies[\"generation\"]\n        return fallback(context)\n\n    def _generation_fallback(self, context: Dict[str, Any]) -&gt; str:\n        \"\"\"Generation fallback strategy.\"\"\"\n        prompt = context.get(\"prompt\", \"\")\n        seed = context.get(\"seed\", 42)\n\n        # Try different approaches\n        approaches = [\n            lambda: f\"[Unable to generate response for: {prompt[:50]}...]\",\n            lambda: \"[Service temporarily unavailable]\",\n            lambda: \"\"\n        ]\n\n        for approach in approaches:\n            try:\n                return approach()\n            except:\n                continue\n\n        return \"[Critical error]\"\n</code></pre>"},{"location":"examples/error-handling/#2-error-context-manager","title":"2. Error Context Manager","text":"<pre><code>from contextlib import contextmanager\n\n@contextmanager\ndef error_handling(operation: str, **context):\n    \"\"\"Context manager for consistent error handling.\"\"\"\n    start_time = time.time()\n    try:\n        yield\n    except Exception as e:\n        duration = time.time() - start_time\n\n        # Log error with context\n        logger.error(f\"{operation} failed after {duration:.2f}s\", extra={\n            \"operation\": operation,\n            \"error\": str(e),\n            \"error_type\": type(e).__name__,\n            \"duration\": duration,\n            **context\n        })\n\n        # Record in monitoring\n        error_monitor.record_error(\n            f\"{operation}_error\",\n            str(e),\n            context\n        )\n\n        # Re-raise or handle based on configuration\n        if should_reraise(e):\n            raise\n        else:\n            return handle_gracefully(operation, e, context)\n\n# Usage\nwith error_handling(\"generation\", prompt=\"test\", seed=42):\n    result = steadytext.generate(\"test\", seed=42)\n</code></pre>"},{"location":"examples/error-handling/#3-testing-error-scenarios","title":"3. Testing Error Scenarios","text":"<pre><code>import unittest\nfrom unittest.mock import patch, MagicMock\n\nclass TestErrorHandling(unittest.TestCase):\n    \"\"\"Test error handling scenarios.\"\"\"\n\n    def test_generation_returns_none(self):\n        \"\"\"Test handling when generation returns None.\"\"\"\n        handler = SteadyTextErrorHandler()\n\n        with patch('steadytext.generate', return_value=None):\n            result = monitored_generate(\"test prompt\", seed=42)\n            self.assertIsNone(result)\n\n            # Check error was recorded\n            report = error_monitor.get_report()\n            self.assertGreater(report[\"summary\"][\"total_errors\"], 0)\n\n    def test_daemon_failure_fallback(self):\n        \"\"\"Test daemon failure with fallback.\"\"\"\n        with patch('steadytext.daemon.client.is_daemon_running', return_value=False):\n            result = generate_with_daemon_fallback(\"test\", seed=42)\n            self.assertIsNotNone(result)  # Should use direct mode\n\n    def test_circuit_breaker_opens(self):\n        \"\"\"Test circuit breaker opening after failures.\"\"\"\n        breaker = CircuitBreaker(failure_threshold=2)\n\n        def failing_func():\n            raise Exception(\"Test failure\")\n\n        # First failures\n        for _ in range(2):\n            with self.assertRaises(Exception):\n                breaker.call(failing_func)\n\n        # Circuit should be open\n        self.assertEqual(breaker.state, CircuitState.OPEN)\n\n        # Further calls should fail immediately\n        with self.assertRaises(Exception) as ctx:\n            breaker.call(failing_func)\n        self.assertIn(\"Circuit breaker is OPEN\", str(ctx.exception))\n</code></pre> <p>This comprehensive guide covers all aspects of error handling in SteadyText, from basic None checks to advanced production patterns like circuit breakers and graceful degradation. The key principle is that SteadyText's \"never fail\" philosophy requires careful handling of None returns and proper fallback strategies.</p>"},{"location":"examples/performance-tuning/","title":"Performance Tuning Guide","text":"<p>Optimize SteadyText for maximum performance, reduced latency, and efficient resource usage.</p>"},{"location":"examples/performance-tuning/#overview","title":"Overview","text":"<p>SteadyText performance optimization focuses on:</p> <ul> <li>Daemon mode: 160x faster first response</li> <li>Cache optimization: Hit rates up to 95%+</li> <li>Batch processing: Amortize model loading costs</li> <li>Resource management: Memory and CPU optimization</li> <li>Concurrent operations: Thread-safe parallel processing</li> </ul>"},{"location":"examples/performance-tuning/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Performance Metrics</li> <li>Daemon Optimization</li> <li>Cache Tuning</li> <li>Model Performance</li> <li>Batch Processing</li> <li>Memory Management</li> <li>Concurrent Operations</li> <li>Monitoring and Profiling</li> <li>Production Optimization</li> <li>Benchmarking</li> </ul>"},{"location":"examples/performance-tuning/#performance-metrics","title":"Performance Metrics","text":""},{"location":"examples/performance-tuning/#key-metrics-to-monitor","title":"Key Metrics to Monitor","text":"<pre><code>import time\nimport psutil\nimport steadytext\nfrom steadytext import get_cache_manager\nfrom dataclasses import dataclass\nfrom typing import List, Dict, Any\nimport statistics\n\n@dataclass\nclass PerformanceMetrics:\n    \"\"\"Performance measurement results.\"\"\"\n    operation: str\n    latency_ms: float\n    throughput_tps: float\n    memory_mb: float\n    cache_hit: bool\n    cpu_percent: float\n\nclass PerformanceMonitor:\n    \"\"\"Monitor SteadyText performance metrics.\"\"\"\n\n    def __init__(self):\n        self.metrics: List[PerformanceMetrics] = []\n        self.cache_manager = get_cache_manager()\n        self.process = psutil.Process()\n\n    def measure_operation(self, func, *args, **kwargs):\n        \"\"\"Measure performance of a single operation.\"\"\"\n        # Get initial state\n        initial_mem = self.process.memory_info().rss / 1024 / 1024\n        initial_cache_stats = self.cache_manager.get_cache_stats()\n\n        # Measure CPU\n        self.process.cpu_percent()  # Initialize\n\n        # Time the operation\n        start_time = time.perf_counter()\n        result = func(*args, **kwargs)\n        end_time = time.perf_counter()\n\n        # Calculate metrics\n        latency_ms = (end_time - start_time) * 1000\n        throughput_tps = 1000 / latency_ms\n        final_mem = self.process.memory_info().rss / 1024 / 1024\n        memory_delta = final_mem - initial_mem\n        cpu_percent = self.process.cpu_percent()\n\n        # Check cache hit\n        final_cache_stats = self.cache_manager.get_cache_stats()\n        cache_hit = self._detect_cache_hit(initial_cache_stats, final_cache_stats)\n\n        metric = PerformanceMetrics(\n            operation=func.__name__,\n            latency_ms=latency_ms,\n            throughput_tps=throughput_tps,\n            memory_mb=memory_delta,\n            cache_hit=cache_hit,\n            cpu_percent=cpu_percent\n        )\n\n        self.metrics.append(metric)\n        return result, metric\n\n    def _detect_cache_hit(self, initial: dict, final: dict) -&gt; bool:\n        \"\"\"Detect if a cache hit occurred.\"\"\"\n        for cache_type in ['generation', 'embedding']:\n            initial_hits = initial.get(cache_type, {}).get('hits', 0)\n            final_hits = final.get(cache_type, {}).get('hits', 0)\n            if final_hits &gt; initial_hits:\n                return True\n        return False\n\n    def get_summary(self) -&gt; Dict[str, Any]:\n        \"\"\"Get performance summary statistics.\"\"\"\n        if not self.metrics:\n            return {}\n\n        latencies = [m.latency_ms for m in self.metrics]\n        throughputs = [m.throughput_tps for m in self.metrics]\n        memory_deltas = [m.memory_mb for m in self.metrics]\n        cpu_percents = [m.cpu_percent for m in self.metrics]\n        cache_hits = sum(1 for m in self.metrics if m.cache_hit)\n\n        return {\n            'operations': len(self.metrics),\n            'cache_hit_rate': cache_hits / len(self.metrics),\n            'latency': {\n                'mean': statistics.mean(latencies),\n                'median': statistics.median(latencies),\n                'p95': sorted(latencies)[int(len(latencies) * 0.95)],\n                'p99': sorted(latencies)[int(len(latencies) * 0.99)],\n            },\n            'throughput': {\n                'mean': statistics.mean(throughputs),\n                'total': sum(throughputs),\n            },\n            'memory': {\n                'total_mb': sum(memory_deltas),\n                'mean_mb': statistics.mean(memory_deltas),\n            },\n            'cpu': {\n                'mean_percent': statistics.mean(cpu_percents),\n                'max_percent': max(cpu_percents),\n            }\n        }\n\n# Usage example\nmonitor = PerformanceMonitor()\n\n# Measure generation performance\nfor i in range(100):\n    prompt = f\"Test prompt {i}\"\n    result, metric = monitor.measure_operation(\n        steadytext.generate, \n        prompt, \n        seed=42\n    )\n    print(f\"Latency: {metric.latency_ms:.2f}ms, Cache: {metric.cache_hit}\")\n\n# Get summary\nsummary = monitor.get_summary()\nprint(f\"\\nPerformance Summary:\")\nprint(f\"Cache hit rate: {summary['cache_hit_rate']:.2%}\")\nprint(f\"Mean latency: {summary['latency']['mean']:.2f}ms\")\nprint(f\"P95 latency: {summary['latency']['p95']:.2f}ms\")\n</code></pre>"},{"location":"examples/performance-tuning/#daemon-optimization","title":"Daemon Optimization","text":""},{"location":"examples/performance-tuning/#startup-performance","title":"Startup Performance","text":"<pre><code>import subprocess\nimport time\nfrom typing import Optional\n\nclass DaemonOptimizer:\n    \"\"\"Optimize daemon startup and performance.\"\"\"\n\n    @staticmethod\n    def start_daemon_with_profiling():\n        \"\"\"Start daemon with performance profiling.\"\"\"\n        start_time = time.time()\n\n        # Start daemon\n        result = subprocess.run([\n            'st', 'daemon', 'start',\n            '--seed', '42'\n        ], capture_output=True, text=True)\n\n        # Wait for daemon to be ready\n        ready = False\n        for _ in range(30):  # 30 second timeout\n            status = subprocess.run([\n                'st', 'daemon', 'status', '--json'\n            ], capture_output=True, text=True)\n\n            if status.returncode == 0:\n                import json\n                data = json.loads(status.stdout)\n                if data.get('running'):\n                    ready = True\n                    break\n\n            time.sleep(0.1)\n\n        startup_time = time.time() - start_time\n        print(f\"Daemon startup time: {startup_time:.2f}s\")\n\n        return ready\n\n    @staticmethod\n    def benchmark_daemon_vs_direct():\n        \"\"\"Compare daemon vs direct performance.\"\"\"\n        import steadytext\n        from steadytext.daemon import use_daemon\n\n        prompt = \"Benchmark test prompt\"\n        iterations = 50\n\n        # Benchmark direct access\n        print(\"Benchmarking direct access...\")\n        direct_times = []\n        for _ in range(iterations):\n            start = time.perf_counter()\n            _ = steadytext.generate(prompt, seed=42)\n            direct_times.append(time.perf_counter() - start)\n\n        # Benchmark daemon access\n        print(\"Benchmarking daemon access...\")\n        daemon_times = []\n        with use_daemon():\n            for _ in range(iterations):\n                start = time.perf_counter()\n                _ = steadytext.generate(prompt, seed=42)\n                daemon_times.append(time.perf_counter() - start)\n\n        # Calculate statistics\n        direct_avg = sum(direct_times) / len(direct_times) * 1000\n        daemon_avg = sum(daemon_times) / len(daemon_times) * 1000\n        speedup = direct_avg / daemon_avg\n\n        print(f\"\\nResults:\")\n        print(f\"Direct access: {direct_avg:.2f}ms average\")\n        print(f\"Daemon access: {daemon_avg:.2f}ms average\")\n        print(f\"Speedup: {speedup:.1f}x\")\n\n        # First response comparison\n        print(f\"\\nFirst response:\")\n        print(f\"Direct: {direct_times[0]*1000:.2f}ms\")\n        print(f\"Daemon: {daemon_times[0]*1000:.2f}ms\")\n        print(f\"First response speedup: {direct_times[0]/daemon_times[0]:.1f}x\")\n</code></pre>"},{"location":"examples/performance-tuning/#connection-pooling","title":"Connection Pooling","text":"<pre><code>import zmq\nfrom contextlib import contextmanager\nfrom threading import Lock\nfrom typing import Dict, Any\n\nclass DaemonConnectionPool:\n    \"\"\"Connection pool for daemon clients.\"\"\"\n\n    def __init__(self, host='127.0.0.1', port=5557, pool_size=10):\n        self.host = host\n        self.port = port\n        self.pool_size = pool_size\n        self.connections = []\n        self.available = []\n        self.lock = Lock()\n        self._initialize_pool()\n\n    def _initialize_pool(self):\n        \"\"\"Initialize connection pool.\"\"\"\n        context = zmq.Context()\n        for _ in range(self.pool_size):\n            socket = context.socket(zmq.REQ)\n            socket.connect(f\"tcp://{self.host}:{self.port}\")\n            socket.setsockopt(zmq.LINGER, 0)\n            socket.setsockopt(zmq.RCVTIMEO, 5000)  # 5 second timeout\n            self.connections.append(socket)\n            self.available.append(socket)\n\n    @contextmanager\n    def get_connection(self):\n        \"\"\"Get a connection from the pool.\"\"\"\n        socket = None\n        try:\n            with self.lock:\n                if self.available:\n                    socket = self.available.pop()\n\n            if socket is None:\n                raise RuntimeError(\"No connections available\")\n\n            yield socket\n\n        finally:\n            if socket:\n                with self.lock:\n                    self.available.append(socket)\n\n    def execute_request(self, request: Dict[str, Any]) -&gt; Dict[str, Any]:\n        \"\"\"Execute request using pooled connection.\"\"\"\n        import json\n\n        with self.get_connection() as socket:\n            socket.send_json(request)\n            response = socket.recv_json()\n            return response\n\n# Usage\npool = DaemonConnectionPool(pool_size=20)\n\n# Concurrent requests\nimport concurrent.futures\n\ndef make_request(i):\n    request = {\n        'type': 'generate',\n        'prompt': f'Test {i}',\n        'seed': 42\n    }\n    return pool.execute_request(request)\n\nwith concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n    futures = [executor.submit(make_request, i) for i in range(100)]\n    results = [f.result() for f in futures]\n</code></pre>"},{"location":"examples/performance-tuning/#cache-tuning","title":"Cache Tuning","text":""},{"location":"examples/performance-tuning/#optimal-cache-configuration","title":"Optimal Cache Configuration","text":"<pre><code>import os\nfrom typing import Dict, Tuple\n\nclass CacheTuner:\n    \"\"\"Tune cache settings for optimal performance.\"\"\"\n\n    @staticmethod\n    def calculate_optimal_settings(\n        available_memory_mb: float,\n        expected_qps: float,\n        avg_prompt_length: int,\n        cache_for_hours: float = 24\n    ) -&gt; Dict[str, Dict[str, float]]:\n        \"\"\"Calculate optimal cache settings based on workload.\"\"\"\n\n        # Estimate cache entry sizes\n        gen_entry_size_kb = 2 + (avg_prompt_length * 0.001)  # Rough estimate\n        embed_entry_size_kb = 4.2  # 1024 floats + metadata\n\n        # Calculate expected entries\n        expected_requests = expected_qps * 3600 * cache_for_hours\n        unique_ratio = 0.3  # Assume 30% unique requests\n        expected_unique = expected_requests * unique_ratio\n\n        # Allocate memory (70% for generation, 30% for embedding)\n        gen_memory_mb = available_memory_mb * 0.7\n        embed_memory_mb = available_memory_mb * 0.3\n\n        # Calculate capacities\n        gen_capacity = min(\n            int(gen_memory_mb * 1024 / gen_entry_size_kb),\n            int(expected_unique * 0.8)  # 80% of expected unique\n        )\n\n        embed_capacity = min(\n            int(embed_memory_mb * 1024 / embed_entry_size_kb),\n            int(expected_unique * 0.5)  # 50% of expected unique\n        )\n\n        return {\n            'generation': {\n                'capacity': gen_capacity,\n                'max_size_mb': gen_memory_mb\n            },\n            'embedding': {\n                'capacity': embed_capacity,\n                'max_size_mb': embed_memory_mb\n            }\n        }\n\n    @staticmethod\n    def apply_settings(settings: Dict[str, Dict[str, float]]):\n        \"\"\"Apply cache settings via environment variables.\"\"\"\n        os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = str(\n            int(settings['generation']['capacity'])\n        )\n        os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = str(\n            settings['generation']['max_size_mb']\n        )\n        os.environ['STEADYTEXT_EMBEDDING_CACHE_CAPACITY'] = str(\n            int(settings['embedding']['capacity'])\n        )\n        os.environ['STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB'] = str(\n            settings['embedding']['max_size_mb']\n        )\n\n        print(\"Applied cache settings:\")\n        print(f\"Generation: {settings['generation']['capacity']} entries, \"\n              f\"{settings['generation']['max_size_mb']:.1f}MB\")\n        print(f\"Embedding: {settings['embedding']['capacity']} entries, \"\n              f\"{settings['embedding']['max_size_mb']:.1f}MB\")\n\n# Example usage\ntuner = CacheTuner()\n\n# For a server with 1GB available for caching, expecting 10 QPS\nsettings = tuner.calculate_optimal_settings(\n    available_memory_mb=1024,\n    expected_qps=10,\n    avg_prompt_length=100,\n    cache_for_hours=24\n)\n\ntuner.apply_settings(settings)\n</code></pre>"},{"location":"examples/performance-tuning/#cache-warming","title":"Cache Warming","text":"<pre><code>import asyncio\nfrom typing import List, Tuple\nimport steadytext\n\nclass CacheWarmer:\n    \"\"\"Warm up caches with common queries.\"\"\"\n\n    def __init__(self, prompts: List[str], seeds: List[int] = None):\n        self.prompts = prompts\n        self.seeds = seeds or [42]\n\n    async def warm_generation_cache(self):\n        \"\"\"Warm generation cache asynchronously.\"\"\"\n        tasks = []\n\n        for prompt in self.prompts:\n            for seed in self.seeds:\n                task = asyncio.create_task(\n                    self._generate_async(prompt, seed)\n                )\n                tasks.append(task)\n\n        results = await asyncio.gather(*tasks)\n        successful = sum(1 for r in results if r is not None)\n        print(f\"Warmed generation cache: {successful}/{len(tasks)} entries\")\n\n    async def _generate_async(self, prompt: str, seed: int):\n        \"\"\"Generate text asynchronously.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None, \n            steadytext.generate, \n            prompt, \n            seed\n        )\n\n    def warm_embedding_cache(self):\n        \"\"\"Warm embedding cache.\"\"\"\n        successful = 0\n\n        for text in self.prompts:\n            for seed in self.seeds:\n                try:\n                    _ = steadytext.embed(text, seed=seed)\n                    successful += 1\n                except Exception as e:\n                    print(f\"Failed to warm embed cache: {e}\")\n\n        print(f\"Warmed embedding cache: {successful}/{len(self.prompts) * len(self.seeds)} entries\")\n\n# Common prompts for warming\nCOMMON_PROMPTS = [\n    \"Explain machine learning\",\n    \"Write a Python function\",\n    \"What is artificial intelligence?\",\n    \"How does deep learning work?\",\n    \"Summarize this text:\",\n    \"Translate to Spanish:\",\n    \"Generate documentation for\",\n    \"Create a test case\",\n    \"Explain the error:\",\n    \"Optimize this code:\"\n]\n\n# Warm caches on startup\nasync def warm_caches():\n    warmer = CacheWarmer(COMMON_PROMPTS, seeds=[42, 123, 456])\n    await warmer.warm_generation_cache()\n    warmer.warm_embedding_cache()\n\n# Run warming\nasyncio.run(warm_caches())\n</code></pre>"},{"location":"examples/performance-tuning/#model-performance","title":"Model Performance","text":""},{"location":"examples/performance-tuning/#model-size-selection","title":"Model Size Selection","text":"<pre><code>from typing import Dict, Any\nimport time\n\nclass ModelBenchmark:\n    \"\"\"Benchmark different model configurations.\"\"\"\n\n    @staticmethod\n    def compare_model_sizes():\n        \"\"\"Compare performance of different model sizes.\"\"\"\n        import subprocess\n        import json\n\n        test_prompts = [\n            \"Write a short function\",\n            \"Explain quantum computing in simple terms\",\n            \"Generate a creative story about AI\"\n        ]\n\n        results = {}\n\n        for size in ['small', 'large']:\n            print(f\"\\nBenchmarking {size} model...\")\n            size_results = {\n                'latencies': [],\n                'quality_scores': [],\n                'memory_usage': []\n            }\n\n            for prompt in test_prompts:\n                # Measure latency\n                start = time.perf_counter()\n                result = subprocess.run([\n                    'st', 'generate', prompt,\n                    '--size', size,\n                    '--json',\n                    '--wait'\n                ], capture_output=True, text=True)\n                latency = time.perf_counter() - start\n\n                if result.returncode == 0:\n                    data = json.loads(result.stdout)\n                    size_results['latencies'].append(latency)\n\n                    # Simple quality metric (length and vocabulary)\n                    text = data['text']\n                    quality = len(set(text.split())) / len(text.split())\n                    size_results['quality_scores'].append(quality)\n\n            results[size] = size_results\n\n        # Print comparison\n        print(\"\\nModel Size Comparison:\")\n        print(\"-\" * 50)\n        for size, data in results.items():\n            avg_latency = sum(data['latencies']) / len(data['latencies'])\n            avg_quality = sum(data['quality_scores']) / len(data['quality_scores'])\n\n            print(f\"{size.capitalize()} Model:\")\n            print(f\"  Average latency: {avg_latency:.2f}s\")\n            print(f\"  Quality score: {avg_quality:.3f}\")\n            print(f\"  Latency range: {min(data['latencies']):.2f}s - {max(data['latencies']):.2f}s\")\n\n        return results\n</code></pre>"},{"location":"examples/performance-tuning/#custom-model-configuration","title":"Custom Model Configuration","text":"<pre><code>class ModelOptimizer:\n    \"\"\"Optimize model loading and configuration.\"\"\"\n\n    @staticmethod\n    def get_optimal_config(use_case: str) -&gt; Dict[str, Any]:\n        \"\"\"Get optimal model configuration for use case.\"\"\"\n\n        configs = {\n            'realtime': {\n                'model': 'small',\n                'n_threads': 4,\n                'n_batch': 8,\n                'context_length': 512,\n                'use_mlock': True,\n                'use_mmap': True\n            },\n            'quality': {\n                'model': 'large',\n                'n_threads': 8,\n                'n_batch': 16,\n                'context_length': 2048,\n                'use_mlock': True,\n                'use_mmap': True\n            },\n            'batch': {\n                'model': 'large',\n                'n_threads': 16,\n                'n_batch': 32,\n                'context_length': 1024,\n                'use_mlock': False,\n                'use_mmap': True\n            }\n        }\n\n        return configs.get(use_case, configs['realtime'])\n\n    @staticmethod\n    def optimize_for_hardware():\n        \"\"\"Detect hardware and optimize configuration.\"\"\"\n        import psutil\n\n        # Get system info\n        cpu_count = psutil.cpu_count(logical=True)\n        memory_gb = psutil.virtual_memory().total / (1024**3)\n\n        # Determine optimal settings\n        if memory_gb &gt;= 32 and cpu_count &gt;= 16:\n            config = {\n                'profile': 'high-performance',\n                'model': 'large',\n                'n_threads': min(cpu_count - 2, 24),\n                'cache_size_mb': 2048\n            }\n        elif memory_gb &gt;= 16 and cpu_count &gt;= 8:\n            config = {\n                'profile': 'balanced',\n                'model': 'large',\n                'n_threads': min(cpu_count - 1, 12),\n                'cache_size_mb': 1024\n            }\n        else:\n            config = {\n                'profile': 'low-resource',\n                'model': 'small',\n                'n_threads': min(cpu_count, 4),\n                'cache_size_mb': 256\n            }\n\n        print(f\"Hardware profile: {config['profile']}\")\n        print(f\"Detected: {cpu_count} CPUs, {memory_gb:.1f}GB RAM\")\n        print(f\"Recommended: {config['model']} model, {config['n_threads']} threads\")\n\n        return config\n</code></pre>"},{"location":"examples/performance-tuning/#batch-processing","title":"Batch Processing","text":""},{"location":"examples/performance-tuning/#efficient-batch-operations","title":"Efficient Batch Operations","text":"<pre><code>from typing import List, Dict, Any\nimport concurrent.futures\nimport asyncio\n\nclass BatchProcessor:\n    \"\"\"Process multiple requests efficiently.\"\"\"\n\n    def __init__(self, max_workers: int = 4):\n        self.max_workers = max_workers\n\n    def process_batch_sync(\n        self, \n        prompts: List[str], \n        seeds: List[int] = None\n    ) -&gt; List[str]:\n        \"\"\"Process batch synchronously with thread pool.\"\"\"\n        if seeds is None:\n            seeds = [42] * len(prompts)\n\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            futures = []\n            for prompt, seed in zip(prompts, seeds):\n                future = executor.submit(steadytext.generate, prompt, seed)\n                futures.append(future)\n\n            results = []\n            for future in concurrent.futures.as_completed(futures):\n                try:\n                    result = future.result()\n                    results.append(result)\n                except Exception as e:\n                    results.append(f\"Error: {e}\")\n\n            return results\n\n    async def process_batch_async(\n        self, \n        prompts: List[str],\n        seeds: List[int] = None\n    ) -&gt; List[str]:\n        \"\"\"Process batch asynchronously.\"\"\"\n        if seeds is None:\n            seeds = [42] * len(prompts)\n\n        tasks = []\n        for prompt, seed in zip(prompts, seeds):\n            task = asyncio.create_task(\n                self._generate_async(prompt, seed)\n            )\n            tasks.append(task)\n\n        return await asyncio.gather(*tasks)\n\n    async def _generate_async(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Generate text asynchronously.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None,\n            steadytext.generate,\n            prompt,\n            seed\n        )\n\n    def process_streaming_batch(\n        self,\n        prompts: List[str],\n        callback: callable\n    ):\n        \"\"\"Process batch with streaming results.\"\"\"\n        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:\n            # Submit all tasks\n            future_to_prompt = {\n                executor.submit(steadytext.generate, prompt, 42): prompt\n                for prompt in prompts\n            }\n\n            # Process results as they complete\n            for future in concurrent.futures.as_completed(future_to_prompt):\n                prompt = future_to_prompt[future]\n                try:\n                    result = future.result()\n                    callback(prompt, result, None)\n                except Exception as e:\n                    callback(prompt, None, e)\n\n# Usage example\nprocessor = BatchProcessor(max_workers=8)\n\n# Sync batch processing\nprompts = [\"Explain \" + topic for topic in [\"AI\", \"ML\", \"DL\", \"NLP\"]]\nresults = processor.process_batch_sync(prompts)\n\n# Async batch processing\nasync def async_example():\n    results = await processor.process_batch_async(prompts)\n    for prompt, result in zip(prompts, results):\n        print(f\"{prompt}: {len(result)} chars\")\n\n# Streaming results\ndef handle_result(prompt, result, error):\n    if error:\n        print(f\"Error for '{prompt}': {error}\")\n    else:\n        print(f\"Completed '{prompt}': {len(result)} chars\")\n\nprocessor.process_streaming_batch(prompts, handle_result)\n</code></pre>"},{"location":"examples/performance-tuning/#pipeline-optimization","title":"Pipeline Optimization","text":"<pre><code>class Pipeline:\n    \"\"\"Optimized processing pipeline.\"\"\"\n\n    def __init__(self):\n        self.stages = []\n\n    def add_stage(self, func, name=None):\n        \"\"\"Add processing stage.\"\"\"\n        self.stages.append({\n            'func': func,\n            'name': name or func.__name__\n        })\n        return self\n\n    async def process(self, items: List[Any]) -&gt; List[Any]:\n        \"\"\"Process items through pipeline.\"\"\"\n        current = items\n\n        for stage in self.stages:\n            print(f\"Processing stage: {stage['name']}\")\n\n            # Process stage in parallel\n            tasks = []\n            for item in current:\n                task = asyncio.create_task(\n                    self._process_item(stage['func'], item)\n                )\n                tasks.append(task)\n\n            current = await asyncio.gather(*tasks)\n\n        return current\n\n    async def _process_item(self, func, item):\n        \"\"\"Process single item.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(None, func, item)\n\n# Example: Text processing pipeline\nasync def text_pipeline_example():\n    # Define stages\n    def clean_text(text):\n        return text.strip().lower()\n\n    def generate_summary(text):\n        prompt = f\"Summarize in one sentence: {text}\"\n        return steadytext.generate(prompt, seed=42)\n\n    def extract_keywords(summary):\n        prompt = f\"Extract 3 keywords from: {summary}\"\n        return steadytext.generate(prompt, seed=123)\n\n    # Build pipeline\n    pipeline = Pipeline()\n    pipeline.add_stage(clean_text, \"Clean\")\n    pipeline.add_stage(generate_summary, \"Summarize\")\n    pipeline.add_stage(extract_keywords, \"Keywords\")\n\n    # Process texts\n    texts = [\n        \"Machine learning is transforming industries...\",\n        \"Artificial intelligence enables computers...\",\n        \"Deep learning uses neural networks...\"\n    ]\n\n    results = await pipeline.process(texts)\n    return results\n</code></pre>"},{"location":"examples/performance-tuning/#memory-management","title":"Memory Management","text":""},{"location":"examples/performance-tuning/#memory-optimization-strategies","title":"Memory Optimization Strategies","text":"<pre><code>import gc\nimport tracemalloc\nfrom typing import List, Dict, Any\n\nclass MemoryOptimizer:\n    \"\"\"Optimize memory usage for SteadyText operations.\"\"\"\n\n    def __init__(self):\n        self.snapshots = []\n\n    def start_profiling(self):\n        \"\"\"Start memory profiling.\"\"\"\n        tracemalloc.start()\n        self.snapshots = []\n\n    def take_snapshot(self, label: str):\n        \"\"\"Take memory snapshot.\"\"\"\n        snapshot = tracemalloc.take_snapshot()\n        self.snapshots.append((label, snapshot))\n\n    def get_memory_report(self) -&gt; str:\n        \"\"\"Generate memory usage report.\"\"\"\n        if len(self.snapshots) &lt; 2:\n            return \"Not enough snapshots for comparison\"\n\n        report = []\n\n        for i in range(1, len(self.snapshots)):\n            label1, snap1 = self.snapshots[i-1]\n            label2, snap2 = self.snapshots[i]\n\n            diff = snap2.compare_to(snap1, 'lineno')\n            report.append(f\"\\n{label1} -&gt; {label2}:\")\n\n            for stat in diff[:10]:  # Top 10 differences\n                report.append(f\"  {stat}\")\n\n        return \"\\n\".join(report)\n\n    @staticmethod\n    def optimize_batch_memory(items: List[Any], batch_size: int = 100):\n        \"\"\"Process items in batches to control memory.\"\"\"\n        results = []\n\n        for i in range(0, len(items), batch_size):\n            batch = items[i:i + batch_size]\n\n            # Process batch\n            batch_results = [\n                steadytext.generate(item, seed=42)\n                for item in batch\n            ]\n\n            results.extend(batch_results)\n\n            # Force garbage collection after each batch\n            gc.collect()\n\n        return results\n\n    @staticmethod\n    def memory_efficient_streaming(prompts: List[str]):\n        \"\"\"Memory-efficient streaming generation.\"\"\"\n        for prompt in prompts:\n            # Generate and yield immediately\n            result = steadytext.generate(prompt, seed=42)\n            yield result\n\n            # Clear any references\n            del result\n\n            # Periodic garbage collection\n            if prompts.index(prompt) % 100 == 0:\n                gc.collect()\n\n# Example usage\noptimizer = MemoryOptimizer()\noptimizer.start_profiling()\n\n# Take initial snapshot\noptimizer.take_snapshot(\"Initial\")\n\n# Generate some text\ntexts = []\nfor i in range(1000):\n    text = steadytext.generate(f\"Test {i}\", seed=42)\n    texts.append(text)\n\noptimizer.take_snapshot(\"After 1000 generations\")\n\n# Clear and collect\ntexts.clear()\ngc.collect()\n\noptimizer.take_snapshot(\"After cleanup\")\n\n# Get report\nprint(optimizer.get_memory_report())\n</code></pre>"},{"location":"examples/performance-tuning/#resource-limits","title":"Resource Limits","text":"<pre><code>import resource\nimport signal\nfrom contextlib import contextmanager\n\nclass ResourceLimiter:\n    \"\"\"Set resource limits for operations.\"\"\"\n\n    @staticmethod\n    @contextmanager\n    def limit_memory(max_memory_mb: int):\n        \"\"\"Limit memory usage.\"\"\"\n        # Convert MB to bytes\n        max_memory = max_memory_mb * 1024 * 1024\n\n        # Set soft and hard limits\n        resource.setrlimit(\n            resource.RLIMIT_AS,\n            (max_memory, max_memory)\n        )\n\n        try:\n            yield\n        finally:\n            # Reset to unlimited\n            resource.setrlimit(\n                resource.RLIMIT_AS,\n                (resource.RLIM_INFINITY, resource.RLIM_INFINITY)\n            )\n\n    @staticmethod\n    @contextmanager\n    def timeout(seconds: int):\n        \"\"\"Set operation timeout.\"\"\"\n        def timeout_handler(signum, frame):\n            raise TimeoutError(f\"Operation timed out after {seconds} seconds\")\n\n        # Set handler\n        old_handler = signal.signal(signal.SIGALRM, timeout_handler)\n        signal.alarm(seconds)\n\n        try:\n            yield\n        finally:\n            signal.alarm(0)\n            signal.signal(signal.SIGALRM, old_handler)\n\n# Usage\nlimiter = ResourceLimiter()\n\n# Limit memory usage\ntry:\n    with limiter.limit_memory(1024):  # 1GB limit\n        # Memory-intensive operation\n        results = [\n            steadytext.generate(f\"Prompt {i}\", seed=42)\n            for i in range(10000)\n        ]\nexcept MemoryError:\n    print(\"Memory limit exceeded\")\n\n# Set timeout\ntry:\n    with limiter.timeout(5):  # 5 second timeout\n        result = steadytext.generate(\"Complex prompt\", seed=42)\nexcept TimeoutError:\n    print(\"Operation timed out\")\n</code></pre>"},{"location":"examples/performance-tuning/#concurrent-operations","title":"Concurrent Operations","text":""},{"location":"examples/performance-tuning/#thread-safe-operations","title":"Thread-Safe Operations","text":"<pre><code>import threading\nfrom queue import Queue\nfrom typing import List, Tuple, Any\n\nclass ConcurrentProcessor:\n    \"\"\"Thread-safe concurrent processing.\"\"\"\n\n    def __init__(self, num_workers: int = 4):\n        self.num_workers = num_workers\n        self.input_queue = Queue()\n        self.output_queue = Queue()\n        self.workers = []\n        self.running = False\n\n    def start(self):\n        \"\"\"Start worker threads.\"\"\"\n        self.running = True\n\n        for i in range(self.num_workers):\n            worker = threading.Thread(\n                target=self._worker,\n                name=f\"Worker-{i}\"\n            )\n            worker.daemon = True\n            worker.start()\n            self.workers.append(worker)\n\n    def stop(self):\n        \"\"\"Stop all workers.\"\"\"\n        self.running = False\n\n        # Add stop signals\n        for _ in range(self.num_workers):\n            self.input_queue.put(None)\n\n        # Wait for workers\n        for worker in self.workers:\n            worker.join()\n\n    def _worker(self):\n        \"\"\"Worker thread function.\"\"\"\n        while self.running:\n            item = self.input_queue.get()\n\n            if item is None:\n                break\n\n            prompt, seed, request_id = item\n\n            try:\n                result = steadytext.generate(prompt, seed=seed)\n                self.output_queue.put((request_id, result, None))\n            except Exception as e:\n                self.output_queue.put((request_id, None, e))\n\n            self.input_queue.task_done()\n\n    def process_concurrent(\n        self, \n        prompts: List[str], \n        seeds: List[int] = None\n    ) -&gt; List[Tuple[int, Any, Any]]:\n        \"\"\"Process prompts concurrently.\"\"\"\n        if seeds is None:\n            seeds = [42] * len(prompts)\n\n        # Add all items to queue\n        for i, (prompt, seed) in enumerate(zip(prompts, seeds)):\n            self.input_queue.put((prompt, seed, i))\n\n        # Collect results\n        results = []\n        for _ in range(len(prompts)):\n            result = self.output_queue.get()\n            results.append(result)\n\n        # Sort by request ID\n        results.sort(key=lambda x: x[0])\n\n        return results\n\n# Usage\nprocessor = ConcurrentProcessor(num_workers=8)\nprocessor.start()\n\n# Process requests\nprompts = [f\"Generate text about topic {i}\" for i in range(100)]\nresults = processor.process_concurrent(prompts)\n\n# Check results\nsuccessful = sum(1 for _, result, error in results if error is None)\nprint(f\"Processed {successful}/{len(prompts)} successfully\")\n\nprocessor.stop()\n</code></pre>"},{"location":"examples/performance-tuning/#async-concurrency","title":"Async Concurrency","text":"<pre><code>import asyncio\nfrom typing import List, Dict, Any\n\nclass AsyncConcurrentProcessor:\n    \"\"\"Asynchronous concurrent processing.\"\"\"\n\n    def __init__(self, max_concurrent: int = 10):\n        self.semaphore = asyncio.Semaphore(max_concurrent)\n        self.results = {}\n\n    async def process_with_limit(\n        self,\n        prompt: str,\n        seed: int,\n        request_id: int\n    ):\n        \"\"\"Process with concurrency limit.\"\"\"\n        async with self.semaphore:\n            result = await self._generate_async(prompt, seed)\n            self.results[request_id] = result\n            return result\n\n    async def _generate_async(self, prompt: str, seed: int) -&gt; str:\n        \"\"\"Async generation wrapper.\"\"\"\n        loop = asyncio.get_event_loop()\n        return await loop.run_in_executor(\n            None,\n            steadytext.generate,\n            prompt,\n            seed\n        )\n\n    async def process_batch_limited(\n        self,\n        prompts: List[str],\n        seeds: List[int] = None\n    ) -&gt; List[str]:\n        \"\"\"Process batch with concurrency limit.\"\"\"\n        if seeds is None:\n            seeds = [42] * len(prompts)\n\n        tasks = []\n        for i, (prompt, seed) in enumerate(zip(prompts, seeds)):\n            task = self.process_with_limit(prompt, seed, i)\n            tasks.append(task)\n\n        await asyncio.gather(*tasks)\n\n        # Return results in order\n        return [self.results[i] for i in range(len(prompts))]\n\n# Usage\nasync def concurrent_example():\n    processor = AsyncConcurrentProcessor(max_concurrent=20)\n\n    # Generate 100 prompts\n    prompts = [f\"Explain concept {i}\" for i in range(100)]\n\n    start_time = asyncio.get_event_loop().time()\n    results = await processor.process_batch_limited(prompts)\n    end_time = asyncio.get_event_loop().time()\n\n    print(f\"Processed {len(results)} prompts in {end_time - start_time:.2f}s\")\n    print(f\"Average: {(end_time - start_time) / len(results):.3f}s per prompt\")\n\n# Run\nasyncio.run(concurrent_example())\n</code></pre>"},{"location":"examples/performance-tuning/#monitoring-and-profiling","title":"Monitoring and Profiling","text":""},{"location":"examples/performance-tuning/#performance-dashboard","title":"Performance Dashboard","text":"<pre><code>import time\nfrom collections import deque\nfrom dataclasses import dataclass\nfrom typing import Deque, Dict, Any\nimport threading\n\n@dataclass\nclass MetricPoint:\n    \"\"\"Single metric data point.\"\"\"\n    timestamp: float\n    value: float\n    labels: Dict[str, str]\n\nclass PerformanceDashboard:\n    \"\"\"Real-time performance monitoring dashboard.\"\"\"\n\n    def __init__(self, window_size: int = 1000):\n        self.window_size = window_size\n        self.metrics: Dict[str, Deque[MetricPoint]] = {}\n        self.lock = threading.Lock()\n\n    def record_metric(self, name: str, value: float, labels: Dict[str, str] = None):\n        \"\"\"Record a metric value.\"\"\"\n        with self.lock:\n            if name not in self.metrics:\n                self.metrics[name] = deque(maxlen=self.window_size)\n\n            point = MetricPoint(\n                timestamp=time.time(),\n                value=value,\n                labels=labels or {}\n            )\n\n            self.metrics[name].append(point)\n\n    def get_stats(self, name: str, window_seconds: float = 60) -&gt; Dict[str, float]:\n        \"\"\"Get statistics for a metric.\"\"\"\n        with self.lock:\n            if name not in self.metrics:\n                return {}\n\n            current_time = time.time()\n            cutoff_time = current_time - window_seconds\n\n            # Filter points within window\n            points = [\n                p.value for p in self.metrics[name]\n                if p.timestamp &gt;= cutoff_time\n            ]\n\n            if not points:\n                return {}\n\n            return {\n                'count': len(points),\n                'mean': sum(points) / len(points),\n                'min': min(points),\n                'max': max(points),\n                'rate': len(points) / window_seconds\n            }\n\n    def print_dashboard(self):\n        \"\"\"Print performance dashboard.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"SteadyText Performance Dashboard\")\n        print(\"=\"*60)\n\n        for metric_name in sorted(self.metrics.keys()):\n            stats = self.get_stats(metric_name)\n            if stats:\n                print(f\"\\n{metric_name}:\")\n                print(f\"  Rate: {stats['rate']:.2f}/s\")\n                print(f\"  Mean: {stats['mean']:.2f}\")\n                print(f\"  Range: {stats['min']:.2f} - {stats['max']:.2f}\")\n\n# Global dashboard instance\ndashboard = PerformanceDashboard()\n\n# Instrumented generation function\ndef monitored_generate(prompt: str, seed: int = 42) -&gt; str:\n    \"\"\"Generate with monitoring.\"\"\"\n    start_time = time.perf_counter()\n\n    try:\n        result = steadytext.generate(prompt, seed=seed)\n        latency = (time.perf_counter() - start_time) * 1000\n\n        dashboard.record_metric('generation_latency_ms', latency)\n        dashboard.record_metric('generation_success', 1)\n\n        return result\n    except Exception as e:\n        dashboard.record_metric('generation_error', 1)\n        raise\n\n# Background monitoring thread\ndef monitor_system():\n    \"\"\"Monitor system metrics.\"\"\"\n    import psutil\n\n    while True:\n        # CPU usage\n        cpu_percent = psutil.cpu_percent(interval=1)\n        dashboard.record_metric('cpu_percent', cpu_percent)\n\n        # Memory usage\n        memory = psutil.virtual_memory()\n        dashboard.record_metric('memory_percent', memory.percent)\n        dashboard.record_metric('memory_mb', memory.used / 1024 / 1024)\n\n        # Print dashboard every 10 seconds\n        time.sleep(10)\n        dashboard.print_dashboard()\n\n# Start monitoring\nmonitor_thread = threading.Thread(target=monitor_system, daemon=True)\nmonitor_thread.start()\n</code></pre>"},{"location":"examples/performance-tuning/#production-optimization","title":"Production Optimization","text":""},{"location":"examples/performance-tuning/#production-configuration","title":"Production Configuration","text":"<pre><code>from typing import Dict, Any\nimport yaml\n\nclass ProductionConfig:\n    \"\"\"Production-optimized configuration.\"\"\"\n\n    @staticmethod\n    def generate_config(environment: str = 'production') -&gt; Dict[str, Any]:\n        \"\"\"Generate environment-specific configuration.\"\"\"\n\n        configs = {\n            'development': {\n                'daemon': {\n                    'enabled': False,\n                    'host': '127.0.0.1',\n                    'port': 5557\n                },\n                'cache': {\n                    'generation_capacity': 256,\n                    'generation_max_size_mb': 50,\n                    'embedding_capacity': 512,\n                    'embedding_max_size_mb': 100\n                },\n                'models': {\n                    'default_size': 'small',\n                    'preload': False\n                },\n                'monitoring': {\n                    'enabled': True,\n                    'verbose': True\n                }\n            },\n            'staging': {\n                'daemon': {\n                    'enabled': True,\n                    'host': '0.0.0.0',\n                    'port': 5557,\n                    'workers': 4\n                },\n                'cache': {\n                    'generation_capacity': 1024,\n                    'generation_max_size_mb': 200,\n                    'embedding_capacity': 2048,\n                    'embedding_max_size_mb': 400\n                },\n                'models': {\n                    'default_size': 'large',\n                    'preload': True\n                },\n                'monitoring': {\n                    'enabled': True,\n                    'verbose': False\n                }\n            },\n            'production': {\n                'daemon': {\n                    'enabled': True,\n                    'host': '0.0.0.0',\n                    'port': 5557,\n                    'workers': 16,\n                    'max_connections': 1000\n                },\n                'cache': {\n                    'generation_capacity': 4096,\n                    'generation_max_size_mb': 1024,\n                    'embedding_capacity': 8192,\n                    'embedding_max_size_mb': 2048\n                },\n                'models': {\n                    'default_size': 'large',\n                    'preload': True,\n                    'mlock': True\n                },\n                'monitoring': {\n                    'enabled': True,\n                    'verbose': False,\n                    'metrics_endpoint': '/metrics'\n                },\n                'security': {\n                    'rate_limiting': True,\n                    'max_requests_per_minute': 600,\n                    'require_auth': True\n                }\n            }\n        }\n\n        return configs.get(environment, configs['production'])\n\n    @staticmethod\n    def save_config(config: Dict[str, Any], filename: str):\n        \"\"\"Save configuration to file.\"\"\"\n        with open(filename, 'w') as f:\n            yaml.dump(config, f, default_flow_style=False)\n\n    @staticmethod\n    def apply_config(config: Dict[str, Any]):\n        \"\"\"Apply configuration to environment.\"\"\"\n        import os\n\n        # Apply cache settings\n        cache = config.get('cache', {})\n        os.environ['STEADYTEXT_GENERATION_CACHE_CAPACITY'] = str(\n            cache.get('generation_capacity', 256)\n        )\n        os.environ['STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB'] = str(\n            cache.get('generation_max_size_mb', 50)\n        )\n\n        # Apply model settings\n        models = config.get('models', {})\n        if models.get('preload'):\n            import subprocess\n            subprocess.run(['st', 'models', 'preload'], check=True)\n\n        print(f\"Applied configuration for environment\")\n\n# Generate and apply production config\nconfig = ProductionConfig.generate_config('production')\nProductionConfig.save_config(config, 'steadytext-prod.yaml')\nProductionConfig.apply_config(config)\n</code></pre>"},{"location":"examples/performance-tuning/#health-checks","title":"Health Checks","text":"<pre><code>import asyncio\nfrom enum import Enum\nfrom typing import Dict, Any, List\n\nclass HealthStatus(Enum):\n    \"\"\"Health check status.\"\"\"\n    HEALTHY = \"healthy\"\n    DEGRADED = \"degraded\"\n    UNHEALTHY = \"unhealthy\"\n\nclass HealthChecker:\n    \"\"\"Production health checking.\"\"\"\n\n    def __init__(self):\n        self.checks = {}\n\n    async def check_daemon_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check daemon health.\"\"\"\n        import subprocess\n        import json\n\n        try:\n            result = subprocess.run([\n                'st', 'daemon', 'status', '--json'\n            ], capture_output=True, text=True, timeout=5)\n\n            if result.returncode == 0:\n                data = json.loads(result.stdout)\n                return {\n                    'status': HealthStatus.HEALTHY if data.get('running') else HealthStatus.UNHEALTHY,\n                    'details': data\n                }\n            else:\n                return {\n                    'status': HealthStatus.UNHEALTHY,\n                    'error': 'Daemon not responding'\n                }\n        except Exception as e:\n            return {\n                'status': HealthStatus.UNHEALTHY,\n                'error': str(e)\n            }\n\n    async def check_model_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check model availability.\"\"\"\n        try:\n            # Quick generation test\n            start = time.time()\n            result = steadytext.generate(\"health check\", seed=42)\n            latency = time.time() - start\n\n            if result and latency &lt; 5.0:\n                status = HealthStatus.HEALTHY\n            elif result and latency &lt; 10.0:\n                status = HealthStatus.DEGRADED\n            else:\n                status = HealthStatus.UNHEALTHY\n\n            return {\n                'status': status,\n                'latency': latency,\n                'model_loaded': result is not None\n            }\n        except Exception as e:\n            return {\n                'status': HealthStatus.UNHEALTHY,\n                'error': str(e)\n            }\n\n    async def check_cache_health(self) -&gt; Dict[str, Any]:\n        \"\"\"Check cache health.\"\"\"\n        try:\n            cache_manager = get_cache_manager()\n            stats = cache_manager.get_cache_stats()\n\n            # Check if caches are responsive\n            gen_size = stats.get('generation', {}).get('size', 0)\n            embed_size = stats.get('embedding', {}).get('size', 0)\n\n            return {\n                'status': HealthStatus.HEALTHY,\n                'generation_cache_size': gen_size,\n                'embedding_cache_size': embed_size\n            }\n        except Exception as e:\n            return {\n                'status': HealthStatus.UNHEALTHY,\n                'error': str(e)\n            }\n\n    async def run_all_checks(self) -&gt; Dict[str, Any]:\n        \"\"\"Run all health checks.\"\"\"\n        checks = {\n            'daemon': self.check_daemon_health(),\n            'model': self.check_model_health(),\n            'cache': self.check_cache_health()\n        }\n\n        results = {}\n        for name, check in checks.items():\n            results[name] = await check\n\n        # Overall status\n        statuses = [r['status'] for r in results.values()]\n        if all(s == HealthStatus.HEALTHY for s in statuses):\n            overall = HealthStatus.HEALTHY\n        elif any(s == HealthStatus.UNHEALTHY for s in statuses):\n            overall = HealthStatus.UNHEALTHY\n        else:\n            overall = HealthStatus.DEGRADED\n\n        return {\n            'status': overall.value,\n            'checks': results,\n            'timestamp': time.time()\n        }\n\n# Health check endpoint\nasync def health_endpoint():\n    \"\"\"Health check endpoint for monitoring.\"\"\"\n    checker = HealthChecker()\n    result = await checker.run_all_checks()\n\n    # Return appropriate HTTP status\n    if result['status'] == 'healthy':\n        return result, 200\n    elif result['status'] == 'degraded':\n        return result, 200\n    else:\n        return result, 503\n</code></pre>"},{"location":"examples/performance-tuning/#benchmarking","title":"Benchmarking","text":""},{"location":"examples/performance-tuning/#comprehensive-benchmark-suite","title":"Comprehensive Benchmark Suite","text":"<pre><code>import json\nimport subprocess\nfrom pathlib import Path\nfrom typing import Dict, List, Any\n\nclass BenchmarkSuite:\n    \"\"\"Comprehensive performance benchmarking.\"\"\"\n\n    def __init__(self, output_dir: str = \"./benchmarks\"):\n        self.output_dir = Path(output_dir)\n        self.output_dir.mkdir(exist_ok=True)\n\n    def run_latency_benchmark(self) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark operation latencies.\"\"\"\n        results = {\n            'generation': [],\n            'embedding': []\n        }\n\n        # Test different prompt lengths\n        prompt_lengths = [10, 50, 100, 500, 1000]\n\n        for length in prompt_lengths:\n            prompt = \" \".join([\"word\"] * length)\n\n            # Generation latency\n            start = time.perf_counter()\n            _ = steadytext.generate(prompt, seed=42)\n            gen_latency = (time.perf_counter() - start) * 1000\n\n            # Embedding latency\n            start = time.perf_counter()\n            _ = steadytext.embed(prompt, seed=42)\n            embed_latency = (time.perf_counter() - start) * 1000\n\n            results['generation'].append({\n                'prompt_length': length,\n                'latency_ms': gen_latency\n            })\n\n            results['embedding'].append({\n                'text_length': length,\n                'latency_ms': embed_latency\n            })\n\n        return results\n\n    def run_throughput_benchmark(self, duration_seconds: int = 60) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark throughput over time.\"\"\"\n        results = {\n            'generation': {'requests': 0, 'duration': duration_seconds},\n            'embedding': {'requests': 0, 'duration': duration_seconds}\n        }\n\n        # Generation throughput\n        start_time = time.time()\n        gen_count = 0\n        while time.time() - start_time &lt; duration_seconds / 2:\n            _ = steadytext.generate(f\"Test {gen_count}\", seed=42)\n            gen_count += 1\n        results['generation']['requests'] = gen_count\n        results['generation']['rps'] = gen_count / (duration_seconds / 2)\n\n        # Embedding throughput\n        start_time = time.time()\n        embed_count = 0\n        while time.time() - start_time &lt; duration_seconds / 2:\n            _ = steadytext.embed(f\"Test {embed_count}\", seed=42)\n            embed_count += 1\n        results['embedding']['requests'] = embed_count\n        results['embedding']['rps'] = embed_count / (duration_seconds / 2)\n\n        return results\n\n    def run_cache_benchmark(self) -&gt; Dict[str, Any]:\n        \"\"\"Benchmark cache performance.\"\"\"\n        from steadytext import get_cache_manager\n\n        cache_manager = get_cache_manager()\n        results = {'before': {}, 'after': {}}\n\n        # Clear caches\n        cache_manager.clear_all_caches()\n\n        # Get initial stats\n        results['before'] = cache_manager.get_cache_stats()\n\n        # Generate cache misses\n        miss_times = []\n        for i in range(100):\n            start = time.perf_counter()\n            _ = steadytext.generate(f\"Unique prompt {i}\", seed=42)\n            miss_times.append((time.perf_counter() - start) * 1000)\n\n        # Generate cache hits\n        hit_times = []\n        for i in range(100):\n            start = time.perf_counter()\n            _ = steadytext.generate(f\"Unique prompt {i}\", seed=42)\n            hit_times.append((time.perf_counter() - start) * 1000)\n\n        # Get final stats\n        results['after'] = cache_manager.get_cache_stats()\n\n        results['performance'] = {\n            'miss_latency_avg': sum(miss_times) / len(miss_times),\n            'hit_latency_avg': sum(hit_times) / len(hit_times),\n            'speedup': sum(miss_times) / sum(hit_times)\n        }\n\n        return results\n\n    def run_full_benchmark(self) -&gt; Dict[str, Any]:\n        \"\"\"Run complete benchmark suite.\"\"\"\n        print(\"Running SteadyText Performance Benchmark Suite...\")\n\n        results = {\n            'timestamp': time.time(),\n            'latency': self.run_latency_benchmark(),\n            'throughput': self.run_throughput_benchmark(30),\n            'cache': self.run_cache_benchmark()\n        }\n\n        # Save results\n        output_file = self.output_dir / f\"benchmark_{int(time.time())}.json\"\n        with open(output_file, 'w') as f:\n            json.dump(results, f, indent=2)\n\n        print(f\"Benchmark complete. Results saved to {output_file}\")\n\n        # Print summary\n        self.print_summary(results)\n\n        return results\n\n    def print_summary(self, results: Dict[str, Any]):\n        \"\"\"Print benchmark summary.\"\"\"\n        print(\"\\n\" + \"=\"*60)\n        print(\"Benchmark Summary\")\n        print(\"=\"*60)\n\n        # Latency summary\n        gen_latencies = [r['latency_ms'] for r in results['latency']['generation']]\n        print(f\"\\nGeneration Latency:\")\n        print(f\"  Min: {min(gen_latencies):.2f}ms\")\n        print(f\"  Max: {max(gen_latencies):.2f}ms\")\n        print(f\"  Avg: {sum(gen_latencies)/len(gen_latencies):.2f}ms\")\n\n        # Throughput summary\n        print(f\"\\nThroughput:\")\n        print(f\"  Generation: {results['throughput']['generation']['rps']:.2f} req/s\")\n        print(f\"  Embedding: {results['throughput']['embedding']['rps']:.2f} req/s\")\n\n        # Cache summary\n        cache_perf = results['cache']['performance']\n        print(f\"\\nCache Performance:\")\n        print(f\"  Miss latency: {cache_perf['miss_latency_avg']:.2f}ms\")\n        print(f\"  Hit latency: {cache_perf['hit_latency_avg']:.2f}ms\")\n        print(f\"  Speedup: {cache_perf['speedup']:.1f}x\")\n\n# Run benchmarks\nif __name__ == \"__main__\":\n    suite = BenchmarkSuite()\n    suite.run_full_benchmark()\n</code></pre>"},{"location":"examples/performance-tuning/#best-practices","title":"Best Practices","text":""},{"location":"examples/performance-tuning/#performance-checklist","title":"Performance Checklist","text":"<ol> <li>Always use daemon mode for production deployments</li> <li>Configure caches based on workload and available memory</li> <li>Use appropriate model sizes - small for real-time, large for quality</li> <li>Batch operations when processing multiple items</li> <li>Monitor performance continuously in production</li> <li>Set resource limits to prevent runaway processes</li> <li>Use connection pooling for high-concurrency scenarios</li> <li>Implement health checks for production monitoring</li> <li>Profile regularly to identify bottlenecks</li> <li>Optimize for your hardware - use all available cores</li> </ol>"},{"location":"examples/performance-tuning/#quick-optimization-guide","title":"Quick Optimization Guide","text":"<pre><code># 1. Start daemon for 160x faster responses\nst daemon start\n\n# 2. Preload models to avoid first-request delay\nst models preload\n\n# 3. Configure optimal cache sizes\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=2048\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=500\n\n# 4. Use batch processing in your code\n# 5. Monitor with built-in tools\nst cache --status\n\n# 6. Run benchmarks to validate\npython benchmarks/run_all_benchmarks.py --quick\n</code></pre>"},{"location":"examples/performance-tuning/#common-pitfalls","title":"Common Pitfalls","text":"<p>Performance Pitfalls to Avoid</p> <ul> <li>Not using daemon mode - 160x slower first requests</li> <li>Cache thrashing - Set appropriate capacity limits</li> <li>Memory leaks - Use batch processing with cleanup</li> <li>Thread contention - Limit concurrent operations</li> <li>Inefficient prompts - Keep prompts concise</li> <li>Ignoring monitoring - Always track performance metrics</li> </ul>"},{"location":"examples/postgresql-integration/","title":"PostgreSQL Integration Examples","text":"<p>This guide provides comprehensive examples for integrating SteadyText with PostgreSQL using the <code>pg_steadytext</code> extension. Learn how to build powerful applications that combine structured data with AI-generated content and embeddings.</p>"},{"location":"examples/postgresql-integration/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Basic Setup</li> <li>Content Management</li> <li>Semantic Search</li> <li>Real-time Applications</li> <li>Advanced Workflows</li> <li>Performance Optimization</li> </ul>"},{"location":"examples/postgresql-integration/#basic-setup","title":"Basic Setup","text":""},{"location":"examples/postgresql-integration/#installation-and-configuration","title":"Installation and Configuration","text":"<pre><code>-- Enable required extensions\nCREATE EXTENSION IF NOT EXISTS plpython3u CASCADE;\nCREATE EXTENSION IF NOT EXISTS omni_python CASCADE;\nCREATE EXTENSION IF NOT EXISTS pgvector CASCADE;\nCREATE EXTENSION IF NOT EXISTS pg_steadytext CASCADE;\n\n-- Verify installation\nSELECT steadytext_version();\nSELECT * FROM steadytext_config;\n\n-- Configure for your environment\nSELECT steadytext_config_set('default_max_tokens', '256');\nSELECT steadytext_config_set('default_seed', '42');\nSELECT steadytext_config_set('cache_enabled', 'true');\n\n-- Start the daemon for better performance\nSELECT steadytext_daemon_start();\nSELECT * FROM steadytext_daemon_status();\n</code></pre>"},{"location":"examples/postgresql-integration/#basic-text-generation","title":"Basic Text Generation","text":"<pre><code>-- Simple text generation (uses default seed 42)\nSELECT steadytext_generate('Write a product description for a smartphone');\n\n-- With custom parameters and error handling\nSELECT \n    CASE \n        WHEN steadytext_generate(\n            'Explain machine learning',\n            max_tokens := 200,\n            seed := 123\n        ) IS NOT NULL \n        THEN steadytext_generate(\n            'Explain machine learning',\n            max_tokens := 200,\n            seed := 123\n        )\n        ELSE 'Error: Failed to generate content. Please check daemon status.'\n    END AS result;\n\n-- Batch generation with NULL handling\nWITH prompts AS (\n    SELECT unnest(ARRAY[\n        'Describe artificial intelligence',\n        'Explain quantum computing',\n        'What is blockchain technology'\n    ]) AS prompt\n)\nSELECT \n    prompt,\n    COALESCE(\n        steadytext_generate(prompt, max_tokens := 150, seed := 42),\n        '[Generation failed for this prompt]'\n    ) AS response,\n    CASE \n        WHEN steadytext_generate(prompt, max_tokens := 150, seed := 42) IS NOT NULL \n        THEN 'Success'\n        ELSE 'Failed'\n    END AS status\nFROM prompts;\n</code></pre>"},{"location":"examples/postgresql-integration/#basic-embeddings","title":"Basic Embeddings","text":"<pre><code>-- Generate embeddings (uses default seed 42)\nSELECT steadytext_embed('artificial intelligence');\n\n-- Generate with error checking\nSELECT \n    'artificial intelligence' AS text,\n    CASE \n        WHEN steadytext_embed('artificial intelligence') IS NOT NULL \n        THEN 'Embedding generated successfully'\n        ELSE 'Embedding generation failed'\n    END AS status;\n\n-- Create a table with embeddings\nCREATE TABLE concepts (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT,\n    embedding vector(1024),\n    embedding_status TEXT DEFAULT 'pending',\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Populate with embeddings (with error handling)\nWITH embedding_data AS (\n    SELECT \n        name,\n        description,\n        text,\n        steadytext_embed(text, seed := 42) AS embedding\n    FROM (VALUES \n        ('AI', 'Artificial Intelligence', 'artificial intelligence'),\n        ('ML', 'Machine Learning', 'machine learning'),\n        ('DL', 'Deep Learning', 'deep learning')\n    ) AS concepts(name, description, text)\n)\nINSERT INTO concepts (name, description, embedding, embedding_status)\nSELECT \n    name,\n    description,\n    embedding,\n    CASE \n        WHEN embedding IS NOT NULL THEN 'success'\n        ELSE 'failed'\n    END AS embedding_status\nFROM embedding_data;\n\n-- Find similar concepts (with NULL handling)\nWITH query_embedding AS (\n    SELECT steadytext_embed('neural networks', seed := 42) AS embedding\n)\nSELECT \n    c.name,\n    c.description,\n    CASE \n        WHEN c.embedding IS NOT NULL AND qe.embedding IS NOT NULL \n        THEN 1 - (c.embedding &lt;=&gt; qe.embedding)\n        ELSE NULL\n    END AS similarity,\n    CASE \n        WHEN c.embedding IS NULL THEN 'Missing concept embedding'\n        WHEN qe.embedding IS NULL THEN 'Query embedding failed'\n        ELSE 'OK'\n    END AS status\nFROM concepts c\nCROSS JOIN query_embedding qe\nWHERE c.embedding_status = 'success'\nORDER BY similarity DESC NULLS LAST;\n</code></pre>"},{"location":"examples/postgresql-integration/#content-management","title":"Content Management","text":""},{"location":"examples/postgresql-integration/#blog-platform","title":"Blog Platform","text":"<p>Build a complete blog platform with AI-generated content and semantic search.</p> <pre><code>-- Create blog schema\nCREATE TABLE authors (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    bio TEXT,\n    bio_embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE blog_posts (\n    id SERIAL PRIMARY KEY,\n    author_id INTEGER REFERENCES authors(id),\n    title TEXT NOT NULL,\n    content TEXT,\n    summary TEXT,\n    tags TEXT[],\n    title_embedding vector(1024),\n    content_embedding vector(1024),\n    published_at TIMESTAMP DEFAULT NOW(),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE comments (\n    id SERIAL PRIMARY KEY,\n    post_id INTEGER REFERENCES blog_posts(id),\n    author_name TEXT NOT NULL,\n    content TEXT NOT NULL,\n    sentiment_score REAL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Create indexes for vector similarity search\nCREATE INDEX ON blog_posts USING ivfflat (title_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON blog_posts USING ivfflat (content_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON authors USING ivfflat (bio_embedding vector_cosine_ops) WITH (lists = 100);\n\n-- Function to auto-generate blog content with error handling\nCREATE OR REPLACE FUNCTION generate_blog_post(\n    topic TEXT,\n    target_length INTEGER DEFAULT 500,\n    writing_style TEXT DEFAULT 'informative',\n    post_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(title TEXT, content TEXT, summary TEXT, tags TEXT[], generation_status TEXT) AS $$\nDECLARE\n    generated_title TEXT;\n    generated_content TEXT;\n    generated_summary TEXT;\n    generated_tags_text TEXT;\n    status TEXT := 'success';\nBEGIN\n    -- Generate title with error handling\n    generated_title := steadytext_generate(\n        format('Create an engaging blog post title about: %s', topic),\n        max_tokens := 20,\n        seed := post_seed\n    );\n\n    IF generated_title IS NULL THEN\n        generated_title := format('[Generated title for: %s]', topic);\n        status := 'partial_failure';\n    END IF;\n\n    -- Generate content with error handling\n    generated_content := steadytext_generate(\n        format('Write a %s word blog post about %s in a %s style', \n               target_length, topic, writing_style),\n        max_tokens := target_length,\n        seed := post_seed + 1\n    );\n\n    IF generated_content IS NULL THEN\n        generated_content := format('[Content about %s could not be generated]', topic);\n        status := 'partial_failure';\n    END IF;\n\n    -- Generate summary with error handling\n    IF generated_content IS NOT NULL AND generated_content NOT LIKE '[Content%' THEN\n        generated_summary := steadytext_generate(\n            format('Write a brief summary of this blog post: %s', generated_content),\n            max_tokens := 100,\n            seed := post_seed + 2\n        );\n    END IF;\n\n    IF generated_summary IS NULL THEN\n        generated_summary := format('Summary of %s blog post', topic);\n        status := 'partial_failure';\n    END IF;\n\n    -- Generate tags with error handling\n    generated_tags_text := steadytext_generate(\n        format('List 5 relevant tags for a blog post about: %s (comma-separated)', topic),\n        max_tokens := 30,\n        seed := post_seed + 3\n    );\n\n    IF generated_tags_text IS NOT NULL THEN\n        tags := string_to_array(generated_tags_text, ',');\n    ELSE\n        tags := ARRAY[topic, 'technology', 'blog'];\n        status := 'partial_failure';\n    END IF;\n\n    -- Return results\n    title := generated_title;\n    content := generated_content;\n    summary := generated_summary;\n    generation_status := status;\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Auto-populate blog with generated content (with error handling)\nWITH author_embeddings AS (\n    SELECT \n        name,\n        bio,\n        steadytext_embed(bio, seed := 100 + ROW_NUMBER() OVER()) AS bio_embedding\n    FROM (VALUES \n        ('AI Writer', 'An AI-powered content creator specializing in technology topics.'),\n        ('Tech Analyst', 'Expert in emerging technologies and digital transformation.')\n    ) AS authors(name, bio)\n)\nINSERT INTO authors (name, bio, bio_embedding)\nSELECT \n    name,\n    bio,\n    CASE \n        WHEN bio_embedding IS NOT NULL THEN bio_embedding\n        ELSE vector(array_fill(0::real, ARRAY[1024]))  -- Zero vector fallback\n    END\nFROM author_embeddings;\n\n-- Generate blog posts with comprehensive error handling\nWITH generated_posts AS (\n    SELECT \n        1 as author_id,\n        'Machine Learning' as topic,\n        generate_blog_post('Machine Learning', 400, 'technical', 200) as post_data\n    UNION ALL\n    SELECT \n        2 as author_id,\n        'Quantum Computing' as topic,\n        generate_blog_post('Quantum Computing', 350, 'educational', 300) as post_data\n    UNION ALL\n    SELECT \n        1 as author_id,\n        'Blockchain Technology' as topic,\n        generate_blog_post('Blockchain Technology', 450, 'analytical', 400) as post_data\n),\nembedded_posts AS (\n    SELECT \n        gp.author_id,\n        (gp.post_data).title,\n        (gp.post_data).content,\n        (gp.post_data).summary,\n        (gp.post_data).tags,\n        (gp.post_data).generation_status,\n        steadytext_embed((gp.post_data).title, seed := 500) AS title_embedding,\n        steadytext_embed((gp.post_data).content, seed := 600) AS content_embedding\n    FROM generated_posts gp\n)\nINSERT INTO blog_posts (author_id, title, content, summary, tags, title_embedding, content_embedding)\nSELECT \n    author_id,\n    title,\n    content,\n    summary,\n    tags,\n    COALESCE(title_embedding, vector(array_fill(0::real, ARRAY[1024]))),\n    COALESCE(content_embedding, vector(array_fill(0::real, ARRAY[1024])))\nFROM embedded_posts\nWHERE generation_status IS NOT NULL;  -- Only insert posts that were generated\n\n-- Semantic blog search function with NULL handling\nCREATE OR REPLACE FUNCTION search_blog_posts(\n    search_query TEXT,\n    max_results INTEGER DEFAULT 10,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    id INTEGER,\n    title TEXT,\n    summary TEXT,\n    author_name TEXT,\n    similarity_score REAL,\n    published_at TIMESTAMP,\n    search_status TEXT\n) AS $$\nDECLARE\n    query_embedding vector(1024);\nBEGIN\n    -- Generate query embedding with error handling\n    query_embedding := steadytext_embed(search_query, seed := search_seed);\n\n    -- Return empty result if embedding generation failed\n    IF query_embedding IS NULL THEN\n        RAISE WARNING 'Failed to generate embedding for search query: %', search_query;\n        RETURN;\n    END IF;\n\n    RETURN QUERY\n    SELECT \n        bp.id,\n        bp.title,\n        bp.summary,\n        a.name as author_name,\n        1 - (bp.content_embedding &lt;=&gt; query_embedding) AS similarity_score,\n        bp.published_at,\n        'success' AS search_status\n    FROM blog_posts bp\n    JOIN authors a ON bp.author_id = a.id\n    WHERE bp.content_embedding IS NOT NULL\n    ORDER BY bp.content_embedding &lt;=&gt; query_embedding\n    LIMIT max_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Search for posts with status checking\nSELECT \n    id,\n    title,\n    summary,\n    author_name,\n    similarity_score,\n    published_at,\n    search_status\nFROM search_blog_posts('artificial intelligence applications', 5);\n\n-- Handle failed searches\nWITH search_results AS (\n    SELECT * FROM search_blog_posts('future technology trends', 3)\n)\nSELECT \n    CASE \n        WHEN COUNT(*) &gt; 0 THEN 'Search completed successfully'\n        ELSE 'Search failed - no results returned'\n    END AS search_summary,\n    COUNT(*) AS result_count\nFROM search_results;\n\n-- Auto-generate related posts\nCREATE OR REPLACE FUNCTION suggest_related_posts(\n    post_id_input INTEGER,\n    max_suggestions INTEGER DEFAULT 3\n)\nRETURNS TABLE(\n    suggested_id INTEGER,\n    suggested_title TEXT,\n    similarity_score REAL\n) AS $$\nDECLARE\n    source_embedding vector(1024);\nBEGIN\n    -- Get the embedding of the source post\n    SELECT content_embedding INTO source_embedding\n    FROM blog_posts\n    WHERE id = post_id_input;\n\n    IF source_embedding IS NULL THEN\n        RAISE EXCEPTION 'Post not found or has no embedding';\n    END IF;\n\n    RETURN QUERY\n    SELECT \n        bp.id as suggested_id,\n        bp.title as suggested_title,\n        1 - (bp.content_embedding &lt;=&gt; source_embedding) AS similarity_score\n    FROM blog_posts bp\n    WHERE bp.id != post_id_input \n      AND bp.content_embedding IS NOT NULL\n    ORDER BY bp.content_embedding &lt;=&gt; source_embedding\n    LIMIT max_suggestions;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Find related posts\nSELECT * FROM suggest_related_posts(1, 3);\n</code></pre>"},{"location":"examples/postgresql-integration/#e-commerce-product-catalog","title":"E-commerce Product Catalog","text":"<p>Create an intelligent product catalog with AI-generated descriptions and semantic search.</p> <pre><code>-- E-commerce schema\nCREATE TABLE categories (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT,\n    description_embedding vector(1024)\n);\n\nCREATE TABLE products (\n    id SERIAL PRIMARY KEY,\n    category_id INTEGER REFERENCES categories(id),\n    name TEXT NOT NULL,\n    base_features JSONB,\n    ai_description TEXT,\n    ai_marketing_copy TEXT,\n    technical_specs TEXT,\n    price DECIMAL(10,2),\n    embedding vector(1024),\n    marketing_embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE product_reviews (\n    id SERIAL PRIMARY KEY,\n    product_id INTEGER REFERENCES products(id),\n    customer_name TEXT,\n    rating INTEGER CHECK (rating BETWEEN 1 AND 5),\n    review_text TEXT,\n    sentiment_analysis TEXT,\n    embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Indexes for vector search\nCREATE INDEX ON products USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON products USING ivfflat (marketing_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON product_reviews USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n\n-- Product content generation function\nCREATE OR REPLACE FUNCTION generate_product_content(\n    product_name TEXT,\n    features JSONB,\n    category_name TEXT,\n    price_value DECIMAL,\n    content_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    description TEXT,\n    marketing_copy TEXT,\n    technical_specs TEXT\n) AS $$\nDECLARE\n    features_text TEXT;\nBEGIN\n    -- Convert JSONB features to readable text\n    features_text := (\n        SELECT string_agg(key || ': ' || value, ', ')\n        FROM jsonb_each_text(features)\n    );\n\n    -- Generate product description\n    description := steadytext_generate(\n        format('Write a detailed product description for %s in the %s category. Features: %s. Price: $%s',\n               product_name, category_name, features_text, price_value),\n        max_tokens := 200,\n        seed := content_seed\n    );\n\n    -- Generate marketing copy\n    marketing_copy := steadytext_generate(\n        format('Create compelling marketing copy for %s. Highlight benefits and unique selling points. Features: %s',\n               product_name, features_text),\n        max_tokens := 150,\n        seed := content_seed + 100\n    );\n\n    -- Generate technical specifications\n    technical_specs := steadytext_generate(\n        format('Create detailed technical specifications for %s based on these features: %s',\n               product_name, features_text),\n        max_tokens := 250,\n        seed := content_seed + 200\n    );\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Populate categories\nINSERT INTO categories (name, description, description_embedding)\nVALUES \n    ('Smartphones', 'Mobile devices with advanced computing capabilities',\n     steadytext_embed('Mobile devices with advanced computing capabilities', seed := 1000)),\n    ('Laptops', 'Portable computers for professional and personal use',\n     steadytext_embed('Portable computers for professional and personal use', seed := 1001)),\n    ('Smartwatches', 'Wearable devices with health and connectivity features',\n     steadytext_embed('Wearable devices with health and connectivity features', seed := 1002));\n\n-- Generate products with AI content\nWITH product_data AS (\n    SELECT \n        'iPhone 15 Pro' as name,\n        1 as category_id,\n        '{\"screen\": \"6.1 inch OLED\", \"storage\": \"256GB\", \"camera\": \"48MP\", \"battery\": \"3274mAh\"}'::jsonb as features,\n        'Smartphones' as category_name,\n        999.99 as price,\n        2000 as seed\n    UNION ALL\n    SELECT \n        'MacBook Air M3' as name,\n        2 as category_id,\n        '{\"processor\": \"Apple M3\", \"memory\": \"16GB\", \"storage\": \"512GB SSD\", \"display\": \"13.6 inch Retina\"}'::jsonb as features,\n        'Laptops' as category_name,\n        1299.99 as price,\n        2100 as seed\n    UNION ALL\n    SELECT \n        'Apple Watch Series 9' as name,\n        3 as category_id,\n        '{\"display\": \"45mm Always-On Retina\", \"health\": \"Blood Oxygen, ECG\", \"battery\": \"18 hours\", \"connectivity\": \"GPS + Cellular\"}'::jsonb as features,\n        'Smartwatches' as category_name,\n        429.99 as price,\n        2200 as seed\n)\nINSERT INTO products (category_id, name, base_features, ai_description, ai_marketing_copy, technical_specs, price, embedding, marketing_embedding)\nSELECT \n    pd.category_id,\n    pd.name,\n    pd.features,\n    (generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).description,\n    (generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).marketing_copy,\n    (generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).technical_specs,\n    pd.price,\n    steadytext_embed(pd.name || ' ' || (generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).description, seed := 3000),\n    steadytext_embed((generate_product_content(pd.name, pd.features, pd.category_name, pd.price, pd.seed)).marketing_copy, seed := 3100)\nFROM product_data pd;\n\n-- Product search functions\nCREATE OR REPLACE FUNCTION search_products(\n    search_query TEXT,\n    search_type TEXT DEFAULT 'general', -- 'general', 'marketing', 'technical'\n    max_results INTEGER DEFAULT 10,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    id INTEGER,\n    name TEXT,\n    description TEXT,\n    price DECIMAL,\n    category_name TEXT,\n    similarity_score REAL\n) AS $$\nDECLARE\n    query_embedding vector(1024);\n    embedding_column TEXT;\nBEGIN\n    -- Generate embedding for search query\n    query_embedding := steadytext_embed(search_query, seed := search_seed);\n\n    -- Choose embedding column based on search type\n    embedding_column := CASE search_type\n        WHEN 'marketing' THEN 'marketing_embedding'\n        ELSE 'embedding'\n    END;\n\n    RETURN QUERY EXECUTE format('\n        SELECT \n            p.id,\n            p.name,\n            p.ai_description as description,\n            p.price,\n            c.name as category_name,\n            1 - (p.%I &lt;=&gt; $1) AS similarity_score\n        FROM products p\n        JOIN categories c ON p.category_id = c.id\n        WHERE p.%I IS NOT NULL\n        ORDER BY p.%I &lt;=&gt; $1\n        LIMIT $2\n    ', embedding_column, embedding_column, embedding_column)\n    USING query_embedding, max_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Search examples\nSELECT * FROM search_products('high-performance laptop for programming', 'general', 5);\nSELECT * FROM search_products('best smartphone camera', 'marketing', 3);\n\n-- Auto-generate product comparisons\nCREATE OR REPLACE FUNCTION compare_products(\n    product_ids INTEGER[],\n    comparison_seed INTEGER DEFAULT 42\n)\nRETURNS TEXT AS $$\nDECLARE\n    product_info TEXT;\n    comparison_result TEXT;\nBEGIN\n    -- Gather product information\n    SELECT string_agg(\n        format('%s: %s (Price: $%s)', name, ai_description, price),\n        E'\\n'\n    ) INTO product_info\n    FROM products\n    WHERE id = ANY(product_ids);\n\n    -- Generate comparison\n    comparison_result := steadytext_generate(\n        format('Compare these products and highlight their key differences and advantages:\\n%s', product_info),\n        max_tokens := 400,\n        seed := comparison_seed\n    );\n\n    RETURN comparison_result;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Compare products\nSELECT compare_products(ARRAY[1, 2, 3], 4000);\n</code></pre>"},{"location":"examples/postgresql-integration/#semantic-search","title":"Semantic Search","text":""},{"location":"examples/postgresql-integration/#document-management-system","title":"Document Management System","text":"<p>Build a comprehensive document search and analysis system.</p> <pre><code>-- Document management schema\nCREATE TABLE document_types (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT,\n    processing_instructions TEXT,\n    embedding vector(1024)\n);\n\nCREATE TABLE documents (\n    id SERIAL PRIMARY KEY,\n    type_id INTEGER REFERENCES document_types(id),\n    title TEXT NOT NULL,\n    content TEXT NOT NULL,\n    summary TEXT,\n    key_points TEXT[],\n    metadata JSONB,\n    content_embedding vector(1024),\n    summary_embedding vector(1024),\n    created_at TIMESTAMP DEFAULT NOW(),\n    processed_at TIMESTAMP\n);\n\nCREATE TABLE document_sections (\n    id SERIAL PRIMARY KEY,\n    document_id INTEGER REFERENCES documents(id),\n    section_title TEXT,\n    section_content TEXT NOT NULL,\n    section_order INTEGER,\n    embedding vector(1024)\n);\n\nCREATE TABLE document_relationships (\n    id SERIAL PRIMARY KEY,\n    source_doc_id INTEGER REFERENCES documents(id),\n    target_doc_id INTEGER REFERENCES documents(id),\n    relationship_type TEXT, -- 'similar', 'references', 'updated_version'\n    confidence_score REAL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Indexes for document search\nCREATE INDEX ON documents USING ivfflat (content_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON documents USING ivfflat (summary_embedding vector_cosine_ops) WITH (lists = 100);\nCREATE INDEX ON document_sections USING ivfflat (embedding vector_cosine_ops) WITH (lists = 100);\n\n-- Document processing function\nCREATE OR REPLACE FUNCTION process_document(\n    doc_id INTEGER,\n    processing_seed INTEGER DEFAULT 42\n)\nRETURNS BOOLEAN AS $$\nDECLARE\n    doc_record documents%ROWTYPE;\n    doc_summary TEXT;\n    doc_key_points TEXT[];\n    section_texts TEXT[];\n    section_text TEXT;\n    i INTEGER;\nBEGIN\n    -- Get document\n    SELECT * INTO doc_record FROM documents WHERE id = doc_id;\n\n    IF NOT FOUND THEN\n        RAISE EXCEPTION 'Document not found';\n    END IF;\n\n    -- Generate summary\n    doc_summary := steadytext_generate(\n        format('Summarize the following document in 2-3 sentences:\\n%s', \n               left(doc_record.content, 2000)),\n        max_tokens := 150,\n        seed := processing_seed\n    );\n\n    -- Extract key points\n    SELECT string_to_array(\n        steadytext_generate(\n            format('Extract 5 key points from this document (one per line):\\n%s',\n                   left(doc_record.content, 1500)),\n            max_tokens := 200,\n            seed := processing_seed + 100\n        ),\n        E'\\n'\n    ) INTO doc_key_points;\n\n    -- Update document with processed information\n    UPDATE documents SET\n        summary = doc_summary,\n        key_points = doc_key_points,\n        content_embedding = steadytext_embed(doc_record.content, seed := processing_seed + 200),\n        summary_embedding = steadytext_embed(doc_summary, seed := processing_seed + 300),\n        processed_at = NOW()\n    WHERE id = doc_id;\n\n    -- Create document sections (split content into chunks)\n    section_texts := string_to_array(doc_record.content, E'\\n\\n');\n\n    FOR i IN 1..array_length(section_texts, 1) LOOP\n        section_text := section_texts[i];\n        IF length(section_text) &gt; 50 THEN  -- Only process substantial sections\n            INSERT INTO document_sections (document_id, section_content, section_order, embedding)\n            VALUES (\n                doc_id,\n                section_text,\n                i,\n                steadytext_embed(section_text, seed := processing_seed + 400 + i)\n            );\n        END IF;\n    END LOOP;\n\n    RETURN TRUE;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Advanced search function\nCREATE OR REPLACE FUNCTION search_documents(\n    search_query TEXT,\n    search_mode TEXT DEFAULT 'content', -- 'content', 'summary', 'sections'\n    doc_type_filter INTEGER DEFAULT NULL,\n    max_results INTEGER DEFAULT 10,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    document_id INTEGER,\n    title TEXT,\n    summary TEXT,\n    similarity_score REAL,\n    doc_type TEXT,\n    section_match TEXT\n) AS $$\nDECLARE\n    query_embedding vector(1024);\nBEGIN\n    query_embedding := steadytext_embed(search_query, seed := search_seed);\n\n    IF search_mode = 'sections' THEN\n        -- Search within document sections\n        RETURN QUERY\n        SELECT \n            d.id as document_id,\n            d.title,\n            d.summary,\n            1 - (ds.embedding &lt;=&gt; query_embedding) AS similarity_score,\n            dt.name as doc_type,\n            left(ds.section_content, 200) as section_match\n        FROM document_sections ds\n        JOIN documents d ON ds.document_id = d.id\n        JOIN document_types dt ON d.type_id = dt.id\n        WHERE (doc_type_filter IS NULL OR d.type_id = doc_type_filter)\n          AND ds.embedding IS NOT NULL\n        ORDER BY ds.embedding &lt;=&gt; query_embedding\n        LIMIT max_results;\n\n    ELSIF search_mode = 'summary' THEN\n        -- Search within summaries\n        RETURN QUERY\n        SELECT \n            d.id as document_id,\n            d.title,\n            d.summary,\n            1 - (d.summary_embedding &lt;=&gt; query_embedding) AS similarity_score,\n            dt.name as doc_type,\n            NULL::TEXT as section_match\n        FROM documents d\n        JOIN document_types dt ON d.type_id = dt.id\n        WHERE (doc_type_filter IS NULL OR d.type_id = doc_type_filter)\n          AND d.summary_embedding IS NOT NULL\n        ORDER BY d.summary_embedding &lt;=&gt; query_embedding\n        LIMIT max_results;\n\n    ELSE\n        -- Default: search within full content\n        RETURN QUERY\n        SELECT \n            d.id as document_id,\n            d.title,\n            d.summary,\n            1 - (d.content_embedding &lt;=&gt; query_embedding) AS similarity_score,\n            dt.name as doc_type,\n            NULL::TEXT as section_match\n        FROM documents d\n        JOIN document_types dt ON d.type_id = dt.id\n        WHERE (doc_type_filter IS NULL OR d.type_id = doc_type_filter)\n          AND d.content_embedding IS NOT NULL\n        ORDER BY d.content_embedding &lt;=&gt; query_embedding\n        LIMIT max_results;\n    END IF;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Setup document types and sample data\nINSERT INTO document_types (name, description, processing_instructions, embedding)\nVALUES \n    ('Technical Manual', 'Technical documentation and user guides',\n     'Focus on technical details and step-by-step instructions',\n     steadytext_embed('Technical documentation and user guides', seed := 5000)),\n    ('Research Paper', 'Academic and research publications',\n     'Extract methodology, results, and conclusions',\n     steadytext_embed('Academic and research publications', seed := 5001)),\n    ('Legal Document', 'Contracts, agreements, and legal texts',\n     'Identify key terms, obligations, and legal implications',\n     steadytext_embed('Contracts, agreements, and legal texts', seed := 5002));\n\n-- Add sample documents\nINSERT INTO documents (type_id, title, content)\nVALUES \n    (1, 'API Integration Guide', \n     'This guide explains how to integrate our REST API into your application. The API provides endpoints for user authentication, data retrieval, and real-time updates. Authentication is handled via JWT tokens that must be included in request headers. Rate limiting is enforced at 1000 requests per hour per API key.'),\n    (2, 'Machine Learning in Healthcare',\n     'Recent advances in machine learning have shown significant promise in healthcare applications. This study examines the effectiveness of neural networks in medical diagnosis, analyzing data from 10,000 patient records. Results indicate 95% accuracy in early disease detection when compared to traditional diagnostic methods.'),\n    (3, 'Software License Agreement',\n     'This agreement governs the use of the software product. The licensee agrees to use the software solely for internal business purposes. Redistribution is prohibited without written consent. The license term is perpetual but may be terminated for breach of terms. Limitation of liability applies to all claims.');\n\n-- Process all documents\nSELECT process_document(id, 6000 + id) FROM documents;\n\n-- Search examples\nSELECT * FROM search_documents('API authentication methods', 'content', NULL, 5);\nSELECT * FROM search_documents('machine learning accuracy', 'summary', 2, 3);\nSELECT * FROM search_documents('license terms', 'sections', 3, 5);\n\n-- Find document relationships\nCREATE OR REPLACE FUNCTION find_related_documents(\n    source_doc_id INTEGER,\n    similarity_threshold REAL DEFAULT 0.7,\n    max_related INTEGER DEFAULT 5\n)\nRETURNS TABLE(\n    related_doc_id INTEGER,\n    related_title TEXT,\n    similarity_score REAL,\n    relationship_type TEXT\n) AS $$\nDECLARE\n    source_embedding vector(1024);\n    source_type_id INTEGER;\nBEGIN\n    -- Get source document embedding and type\n    SELECT content_embedding, type_id \n    INTO source_embedding, source_type_id\n    FROM documents \n    WHERE id = source_doc_id;\n\n    IF source_embedding IS NULL THEN\n        RAISE EXCEPTION 'Source document not found or not processed';\n    END IF;\n\n    RETURN QUERY\n    SELECT \n        d.id as related_doc_id,\n        d.title as related_title,\n        1 - (d.content_embedding &lt;=&gt; source_embedding) AS similarity_score,\n        CASE \n            WHEN d.type_id = source_type_id THEN 'same_type'\n            ELSE 'different_type'\n        END as relationship_type\n    FROM documents d\n    WHERE d.id != source_doc_id\n      AND d.content_embedding IS NOT NULL\n      AND 1 - (d.content_embedding &lt;=&gt; source_embedding) &gt;= similarity_threshold\n    ORDER BY d.content_embedding &lt;=&gt; source_embedding\n    LIMIT max_related;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Find related documents\nSELECT * FROM find_related_documents(1, 0.5, 3);\n</code></pre>"},{"location":"examples/postgresql-integration/#real-time-applications","title":"Real-time Applications","text":""},{"location":"examples/postgresql-integration/#chat-system-with-ai-assistance","title":"Chat System with AI Assistance","text":"<p>Build a real-time chat system with AI-powered features.</p> <pre><code>-- Chat system schema\nCREATE TABLE chat_rooms (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    description TEXT,\n    ai_assistant_enabled BOOLEAN DEFAULT false,\n    ai_personality TEXT,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE chat_participants (\n    id SERIAL PRIMARY KEY,\n    room_id INTEGER REFERENCES chat_rooms(id),\n    user_name TEXT NOT NULL,\n    joined_at TIMESTAMP DEFAULT NOW(),\n    is_ai_user BOOLEAN DEFAULT false\n);\n\nCREATE TABLE chat_messages (\n    id SERIAL PRIMARY KEY,\n    room_id INTEGER REFERENCES chat_rooms(id),\n    participant_id INTEGER REFERENCES chat_participants(id),\n    message_text TEXT NOT NULL,\n    ai_generated BOOLEAN DEFAULT false,\n    ai_confidence REAL,\n    message_embedding vector(1024),\n    reply_to_message_id INTEGER REFERENCES chat_messages(id),\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE chat_context (\n    id SERIAL PRIMARY KEY,\n    room_id INTEGER REFERENCES chat_rooms(id),\n    context_summary TEXT,\n    key_topics TEXT[],\n    context_embedding vector(1024),\n    updated_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Indexes for chat search and recommendations\nCREATE INDEX ON chat_messages USING ivfflat (message_embedding vector_cosine_ops) WITH (lists = 50);\nCREATE INDEX ON chat_context USING ivfflat (context_embedding vector_cosine_ops) WITH (lists = 20);\nCREATE INDEX ON chat_messages (room_id, created_at DESC);\n\n-- AI assistant function\nCREATE OR REPLACE FUNCTION generate_ai_response(\n    room_id_input INTEGER,\n    context_messages TEXT,\n    ai_personality_input TEXT DEFAULT 'helpful assistant',\n    response_seed INTEGER DEFAULT 42\n)\nRETURNS TEXT AS $$\nDECLARE\n    ai_response TEXT;\n    prompt_text TEXT;\nBEGIN\n    -- Build context-aware prompt\n    prompt_text := format(\n        'You are a %s in a chat room. Based on this recent conversation context, provide a helpful and relevant response:\\n\\n%s\\n\\nResponse:',\n        ai_personality_input,\n        context_messages\n    );\n\n    -- Generate AI response\n    ai_response := steadytext_generate(\n        prompt_text,\n        max_tokens := 200,\n        seed := response_seed\n    );\n\n    RETURN ai_response;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Context summarization function\nCREATE OR REPLACE FUNCTION update_chat_context(\n    room_id_input INTEGER,\n    context_seed INTEGER DEFAULT 42\n)\nRETURNS BOOLEAN AS $$\nDECLARE\n    recent_messages TEXT;\n    context_summary_new TEXT;\n    key_topics_new TEXT[];\nBEGIN\n    -- Get recent messages (last 20)\n    SELECT string_agg(\n        format('[%s] %s: %s', \n               to_char(cm.created_at, 'HH24:MI'), \n               cp.user_name, \n               cm.message_text),\n        E'\\n' ORDER BY cm.created_at DESC\n    )\n    INTO recent_messages\n    FROM chat_messages cm\n    JOIN chat_participants cp ON cm.participant_id = cp.id\n    WHERE cm.room_id = room_id_input\n      AND cm.created_at &gt; NOW() - INTERVAL '1 hour'\n    LIMIT 20;\n\n    IF recent_messages IS NULL THEN\n        RETURN FALSE;\n    END IF;\n\n    -- Generate context summary\n    context_summary_new := steadytext_generate(\n        format('Summarize the key points and current discussion topics from this chat conversation:\\n%s', recent_messages),\n        max_tokens := 150,\n        seed := context_seed\n    );\n\n    -- Extract key topics\n    SELECT string_to_array(\n        steadytext_generate(\n            format('Extract 5 main topics being discussed in this chat (comma-separated):\\n%s', recent_messages),\n            max_tokens := 50,\n            seed := context_seed + 100\n        ),\n        ','\n    ) INTO key_topics_new;\n\n    -- Update or insert context\n    INSERT INTO chat_context (room_id, context_summary, key_topics, context_embedding, updated_at)\n    VALUES (\n        room_id_input,\n        context_summary_new,\n        key_topics_new,\n        steadytext_embed(context_summary_new, seed := context_seed + 200),\n        NOW()\n    )\n    ON CONFLICT (room_id) DO UPDATE SET\n        context_summary = EXCLUDED.context_summary,\n        key_topics = EXCLUDED.key_topics,\n        context_embedding = EXCLUDED.context_embedding,\n        updated_at = EXCLUDED.updated_at;\n\n    RETURN TRUE;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Message processing trigger function\nCREATE OR REPLACE FUNCTION process_chat_message()\nRETURNS TRIGGER AS $$\nDECLARE\n    room_has_ai BOOLEAN;\n    ai_personality_val TEXT;\n    recent_context TEXT;\n    ai_participant_id INTEGER;\n    ai_response TEXT;\nBEGIN\n    -- Generate embedding for the new message\n    NEW.message_embedding := steadytext_embed(NEW.message_text, seed := 42);\n\n    -- Check if room has AI assistant enabled\n    SELECT ai_assistant_enabled, ai_personality \n    INTO room_has_ai, ai_personality_val\n    FROM chat_rooms \n    WHERE id = NEW.room_id;\n\n    -- If AI is enabled and this isn't an AI message, potentially generate response\n    IF room_has_ai AND NOT NEW.ai_generated THEN\n        -- Get recent conversation context\n        SELECT string_agg(\n            format('%s: %s', cp.user_name, cm.message_text),\n            E'\\n' ORDER BY cm.created_at DESC\n        )\n        INTO recent_context\n        FROM chat_messages cm\n        JOIN chat_participants cp ON cm.participant_id = cp.id\n        WHERE cm.room_id = NEW.room_id\n          AND cm.created_at &gt; NOW() - INTERVAL '10 minutes'\n        LIMIT 5;\n\n        -- Decide if AI should respond (simple logic - respond to questions or mentions)\n        IF NEW.message_text ILIKE '%?%' OR NEW.message_text ILIKE '%ai%' THEN\n            -- Get AI participant\n            SELECT id INTO ai_participant_id\n            FROM chat_participants\n            WHERE room_id = NEW.room_id AND is_ai_user = true\n            LIMIT 1;\n\n            IF ai_participant_id IS NOT NULL THEN\n                -- Generate AI response\n                ai_response := generate_ai_response(\n                    NEW.room_id,\n                    recent_context,\n                    ai_personality_val,\n                    extract(epoch from NOW())::integer % 10000\n                );\n\n                -- Insert AI response\n                INSERT INTO chat_messages (\n                    room_id, \n                    participant_id, \n                    message_text, \n                    ai_generated, \n                    ai_confidence,\n                    reply_to_message_id\n                )\n                VALUES (\n                    NEW.room_id,\n                    ai_participant_id,\n                    ai_response,\n                    true,\n                    0.8,\n                    NEW.id\n                );\n            END IF;\n        END IF;\n\n        -- Update chat context periodically\n        PERFORM update_chat_context(NEW.room_id);\n    END IF;\n\n    RETURN NEW;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Create trigger for message processing\nCREATE TRIGGER chat_message_processing\n    BEFORE INSERT ON chat_messages\n    FOR EACH ROW\n    EXECUTE FUNCTION process_chat_message();\n\n-- Message search function\nCREATE OR REPLACE FUNCTION search_chat_history(\n    room_id_input INTEGER,\n    search_query TEXT,\n    max_results INTEGER DEFAULT 10,\n    search_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    message_id INTEGER,\n    user_name TEXT,\n    message_text TEXT,\n    similarity_score REAL,\n    created_at TIMESTAMP\n) AS $$\nBEGIN\n    RETURN QUERY\n    SELECT \n        cm.id as message_id,\n        cp.user_name,\n        cm.message_text,\n        1 - (cm.message_embedding &lt;=&gt; steadytext_embed(search_query, seed := search_seed)) AS similarity_score,\n        cm.created_at\n    FROM chat_messages cm\n    JOIN chat_participants cp ON cm.participant_id = cp.id\n    WHERE cm.room_id = room_id_input\n      AND cm.message_embedding IS NOT NULL\n    ORDER BY cm.message_embedding &lt;=&gt; steadytext_embed(search_query, seed := search_seed)\n    LIMIT max_results;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Setup sample chat room\nINSERT INTO chat_rooms (name, description, ai_assistant_enabled, ai_personality)\nVALUES ('Tech Discussion', 'General technology discussion room', true, 'knowledgeable tech expert');\n\n-- Add participants\nINSERT INTO chat_participants (room_id, user_name, is_ai_user)\nVALUES \n    (1, 'Alice', false),\n    (1, 'Bob', false),\n    (1, 'TechBot', true);\n\n-- Simulate conversation\nINSERT INTO chat_messages (room_id, participant_id, message_text, ai_generated)\nVALUES \n    (1, 1, 'Hi everyone! What do you think about the latest AI developments?', false),\n    (1, 2, 'I think machine learning is advancing really fast. What are your thoughts on GPT models?', false);\n\n-- Search chat history\nSELECT * FROM search_chat_history(1, 'artificial intelligence developments', 5);\n</code></pre>"},{"location":"examples/postgresql-integration/#advanced-workflows","title":"Advanced Workflows","text":""},{"location":"examples/postgresql-integration/#content-pipeline-with-quality-control","title":"Content Pipeline with Quality Control","text":"<pre><code>-- Content pipeline schema\nCREATE TABLE content_templates (\n    id SERIAL PRIMARY KEY,\n    name TEXT NOT NULL,\n    template_text TEXT NOT NULL,\n    variables JSONB,\n    quality_criteria JSONB,\n    embedding vector(1024)\n);\n\nCREATE TABLE content_jobs (\n    id SERIAL PRIMARY KEY,\n    template_id INTEGER REFERENCES content_templates(id),\n    input_parameters JSONB,\n    status TEXT DEFAULT 'pending', -- pending, processing, completed, failed, review_needed\n    priority INTEGER DEFAULT 5,\n    created_at TIMESTAMP DEFAULT NOW(),\n    started_at TIMESTAMP,\n    completed_at TIMESTAMP\n);\n\nCREATE TABLE generated_content (\n    id SERIAL PRIMARY KEY,\n    job_id INTEGER REFERENCES content_jobs(id),\n    content_text TEXT NOT NULL,\n    quality_score REAL,\n    quality_issues TEXT[],\n    embedding vector(1024),\n    approved BOOLEAN DEFAULT false,\n    human_review_notes TEXT,\n    generation_seed INTEGER,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Quality assessment function\nCREATE OR REPLACE FUNCTION assess_content_quality(\n    content_text TEXT,\n    quality_criteria JSONB,\n    assessment_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    quality_score REAL,\n    quality_issues TEXT[]\n) AS $$\nDECLARE\n    criteria_text TEXT;\n    assessment_prompt TEXT;\n    assessment_result TEXT;\n    score_match TEXT;\n    issues_text TEXT;\nBEGIN\n    -- Convert criteria to readable format\n    SELECT string_agg(key || ': ' || value, ', ')\n    INTO criteria_text\n    FROM jsonb_each_text(quality_criteria);\n\n    -- Generate quality assessment\n    assessment_prompt := format(\n        'Assess the quality of this content against these criteria: %s\\n\\nContent to assess:\\n%s\\n\\nProvide a score from 0.0 to 1.0 and list any issues (format: Score: X.X, Issues: issue1, issue2)',\n        criteria_text,\n        content_text\n    );\n\n    assessment_result := steadytext_generate(\n        assessment_prompt,\n        max_tokens := 200,\n        seed := assessment_seed\n    );\n\n    -- Extract score (simple pattern matching)\n    score_match := substring(assessment_result from 'Score: ([0-9.]+)');\n    quality_score := COALESCE(score_match::real, 0.5);\n\n    -- Extract issues\n    issues_text := substring(assessment_result from 'Issues: (.+)');\n    IF issues_text IS NOT NULL THEN\n        quality_issues := string_to_array(trim(issues_text), ', ');\n    ELSE\n        quality_issues := ARRAY[]::TEXT[];\n    END IF;\n\n    RETURN NEXT;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Content generation worker function\nCREATE OR REPLACE FUNCTION process_content_job(\n    job_id_input INTEGER\n)\nRETURNS BOOLEAN AS $$\nDECLARE\n    job_record content_jobs%ROWTYPE;\n    template_record content_templates%ROWTYPE;\n    generation_prompt TEXT;\n    generated_text TEXT;\n    quality_result RECORD;\n    generation_seed INTEGER;\nBEGIN\n    -- Get job and template\n    SELECT * INTO job_record FROM content_jobs WHERE id = job_id_input;\n    SELECT * INTO template_record FROM content_templates WHERE id = job_record.template_id;\n\n    IF NOT FOUND THEN\n        RAISE EXCEPTION 'Job or template not found';\n    END IF;\n\n    -- Update job status\n    UPDATE content_jobs SET \n        status = 'processing',\n        started_at = NOW()\n    WHERE id = job_id_input;\n\n    -- Build generation prompt by substituting variables\n    generation_prompt := template_record.template_text;\n\n    -- Simple variable substitution (in practice, you'd want more sophisticated templating)\n    FOR variable_key, variable_value IN \n        SELECT key, value FROM jsonb_each_text(job_record.input_parameters)\n    LOOP\n        generation_prompt := replace(generation_prompt, '{{' || variable_key || '}}', variable_value);\n    END LOOP;\n\n    -- Generate unique seed for this job\n    generation_seed := 50000 + job_id_input;\n\n    -- Generate content\n    generated_text := steadytext_generate(\n        generation_prompt,\n        max_tokens := 500,\n        seed := generation_seed\n    );\n\n    -- Assess quality\n    SELECT * INTO quality_result FROM assess_content_quality(\n        generated_text,\n        template_record.quality_criteria,\n        generation_seed + 1000\n    );\n\n    -- Store generated content\n    INSERT INTO generated_content (\n        job_id,\n        content_text,\n        quality_score,\n        quality_issues,\n        embedding,\n        approved,\n        generation_seed\n    ) VALUES (\n        job_id_input,\n        generated_text,\n        quality_result.quality_score,\n        quality_result.quality_issues,\n        steadytext_embed(generated_text, seed := generation_seed + 2000),\n        quality_result.quality_score &gt;= 0.8, -- Auto-approve high quality content\n        generation_seed\n    );\n\n    -- Update job status\n    UPDATE content_jobs SET\n        status = CASE \n            WHEN quality_result.quality_score &gt;= 0.8 THEN 'completed'\n            ELSE 'review_needed'\n        END,\n        completed_at = NOW()\n    WHERE id = job_id_input;\n\n    RETURN TRUE;\nEXCEPTION\n    WHEN OTHERS THEN\n        UPDATE content_jobs SET status = 'failed' WHERE id = job_id_input;\n        RETURN FALSE;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Setup content templates\nINSERT INTO content_templates (name, template_text, variables, quality_criteria, embedding)\nVALUES \n    ('Product Description',\n     'Write a compelling product description for {{product_name}} in the {{category}} category. Key features: {{features}}. Target audience: {{audience}}.',\n     '{\"product_name\": \"string\", \"category\": \"string\", \"features\": \"string\", \"audience\": \"string\"}',\n     '{\"clarity\": \"high\", \"engagement\": \"high\", \"accuracy\": \"high\", \"length\": \"150-300 words\"}',\n     steadytext_embed('Product description template for e-commerce', seed := 60000)),\n\n    ('Blog Post Introduction',\n     'Write an engaging introduction for a blog post about {{topic}}. The post should appeal to {{audience}} and have a {{tone}} tone.',\n     '{\"topic\": \"string\", \"audience\": \"string\", \"tone\": \"string\"}',\n     '{\"hook\": \"strong\", \"clarity\": \"high\", \"engagement\": \"high\", \"length\": \"100-200 words\"}',\n     steadytext_embed('Blog post introduction template', seed := 60001));\n\n-- Create content jobs\nINSERT INTO content_jobs (template_id, input_parameters, priority)\nVALUES \n    (1, '{\"product_name\": \"Smart Fitness Tracker\", \"category\": \"wearables\", \"features\": \"heart rate monitoring, sleep tracking, waterproof\", \"audience\": \"fitness enthusiasts\"}', 1),\n    (2, '{\"topic\": \"sustainable technology\", \"audience\": \"environmentally conscious consumers\", \"tone\": \"informative yet inspiring\"}', 2);\n\n-- Process content jobs\nSELECT process_content_job(1);\nSELECT process_content_job(2);\n\n-- Review generated content\nSELECT \n    cj.id as job_id,\n    ct.name as template_name,\n    gc.content_text,\n    gc.quality_score,\n    gc.quality_issues,\n    gc.approved,\n    cj.status\nFROM content_jobs cj\nJOIN content_templates ct ON cj.template_id = ct.id\nJOIN generated_content gc ON cj.id = gc.job_id\nORDER BY cj.created_at DESC;\n</code></pre>"},{"location":"examples/postgresql-integration/#performance-optimization","title":"Performance Optimization","text":""},{"location":"examples/postgresql-integration/#monitoring-and-analytics","title":"Monitoring and Analytics","text":"<pre><code>-- Performance monitoring schema\nCREATE TABLE api_usage_logs (\n    id SERIAL PRIMARY KEY,\n    function_name TEXT NOT NULL,\n    input_text TEXT,\n    input_length INTEGER,\n    output_text TEXT,\n    output_length INTEGER,\n    processing_time_ms INTEGER,\n    cache_hit BOOLEAN,\n    seed_used INTEGER,\n    embedding_similarity REAL,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE TABLE cache_performance (\n    id SERIAL PRIMARY KEY,\n    cache_type TEXT NOT NULL, -- 'generation', 'embedding'\n    hit_rate REAL,\n    total_requests INTEGER,\n    cache_hits INTEGER,\n    cache_misses INTEGER,\n    avg_response_time_ms REAL,\n    measured_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Performance monitoring functions\nCREATE OR REPLACE FUNCTION log_steadytext_usage(\n    func_name TEXT,\n    input_text TEXT,\n    output_text TEXT,\n    start_time TIMESTAMP,\n    end_time TIMESTAMP,\n    was_cache_hit BOOLEAN DEFAULT false,\n    seed_value INTEGER DEFAULT 42\n)\nRETURNS VOID AS $$\nDECLARE\n    processing_ms INTEGER;\nBEGIN\n    processing_ms := EXTRACT(epoch FROM (end_time - start_time)) * 1000;\n\n    INSERT INTO api_usage_logs (\n        function_name,\n        input_text,\n        input_length,\n        output_text,\n        output_length,\n        processing_time_ms,\n        cache_hit,\n        seed_used\n    ) VALUES (\n        func_name,\n        input_text,\n        length(input_text),\n        output_text,\n        length(output_text),\n        processing_ms,\n        was_cache_hit,\n        seed_value\n    );\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Enhanced generation function with monitoring\nCREATE OR REPLACE FUNCTION monitored_generate(\n    prompt TEXT,\n    max_tokens INTEGER DEFAULT 512,\n    seed INTEGER DEFAULT 42\n)\nRETURNS TEXT AS $$\nDECLARE\n    start_time TIMESTAMP;\n    end_time TIMESTAMP;\n    result TEXT;\nBEGIN\n    start_time := clock_timestamp();\n\n    result := steadytext_generate(prompt, max_tokens, true, seed);\n\n    end_time := clock_timestamp();\n\n    -- Log the usage\n    PERFORM log_steadytext_usage(\n        'steadytext_generate',\n        prompt,\n        result,\n        start_time,\n        end_time,\n        false, -- We'd need to modify steadytext to return cache hit info\n        seed\n    );\n\n    RETURN result;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Performance analytics views\nCREATE VIEW performance_summary AS\nSELECT \n    function_name,\n    COUNT(*) as total_calls,\n    AVG(processing_time_ms) as avg_time_ms,\n    MIN(processing_time_ms) as min_time_ms,\n    MAX(processing_time_ms) as max_time_ms,\n    PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY processing_time_ms) as median_time_ms,\n    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY processing_time_ms) as p95_time_ms,\n    AVG(input_length) as avg_input_length,\n    AVG(output_length) as avg_output_length,\n    COUNT(*) FILTER (WHERE cache_hit) as cache_hits,\n    ROUND(100.0 * COUNT(*) FILTER (WHERE cache_hit) / COUNT(*), 2) as cache_hit_rate_pct\nFROM api_usage_logs\nWHERE created_at &gt; NOW() - INTERVAL '24 hours'\nGROUP BY function_name;\n\nCREATE VIEW hourly_usage AS\nSELECT \n    DATE_TRUNC('hour', created_at) as hour,\n    function_name,\n    COUNT(*) as calls,\n    AVG(processing_time_ms) as avg_time_ms,\n    COUNT(*) FILTER (WHERE cache_hit) as cache_hits\nFROM api_usage_logs\nWHERE created_at &gt; NOW() - INTERVAL '7 days'\nGROUP BY DATE_TRUNC('hour', created_at), function_name\nORDER BY hour DESC;\n\n-- Batch optimization function\nCREATE OR REPLACE FUNCTION optimize_batch_processing(\n    prompts TEXT[],\n    base_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    prompt_index INTEGER,\n    generated_text TEXT,\n    processing_time_ms INTEGER\n) AS $$\nDECLARE\n    prompt_text TEXT;\n    result TEXT;\n    start_time TIMESTAMP;\n    end_time TIMESTAMP;\n    i INTEGER := 1;\nBEGIN\n    FOREACH prompt_text IN ARRAY prompts LOOP\n        start_time := clock_timestamp();\n\n        result := steadytext_generate(\n            prompt_text,\n            max_tokens := 200,\n            seed := base_seed + i\n        );\n\n        end_time := clock_timestamp();\n\n        prompt_index := i;\n        generated_text := result;\n        processing_time_ms := EXTRACT(epoch FROM (end_time - start_time)) * 1000;\n\n        RETURN NEXT;\n        i := i + 1;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Usage examples and performance testing\nSELECT * FROM performance_summary;\nSELECT * FROM hourly_usage LIMIT 10;\n\n-- Test batch optimization\nSELECT * FROM optimize_batch_processing(ARRAY[\n    'Explain machine learning',\n    'Describe quantum computing',\n    'What is blockchain technology',\n    'How does AI work',\n    'Define neural networks'\n], 70000);\n\n-- Cache warming function\nCREATE OR REPLACE FUNCTION warm_cache(\n    common_prompts TEXT[],\n    base_seed INTEGER DEFAULT 42\n)\nRETURNS TABLE(\n    prompt TEXT,\n    cached BOOLEAN\n) AS $$\nDECLARE\n    prompt_text TEXT;\n    result TEXT;\nBEGIN\n    FOREACH prompt_text IN ARRAY common_prompts LOOP\n        -- Generate to populate cache\n        result := steadytext_generate(prompt_text, max_tokens := 100, seed := base_seed);\n\n        prompt := prompt_text;\n        cached := true;\n        RETURN NEXT;\n    END LOOP;\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Warm cache with common prompts\nSELECT * FROM warm_cache(ARRAY[\n    'Write a professional email',\n    'Create a product summary',\n    'Explain a technical concept',\n    'Generate a brief description'\n]);\n\n-- Performance monitoring dashboard query\nWITH recent_stats AS (\n    SELECT \n        function_name,\n        COUNT(*) as calls_last_hour,\n        AVG(processing_time_ms) as avg_time_last_hour,\n        COUNT(*) FILTER (WHERE cache_hit) as cache_hits_last_hour\n    FROM api_usage_logs \n    WHERE created_at &gt; NOW() - INTERVAL '1 hour'\n    GROUP BY function_name\n),\ndaily_stats AS (\n    SELECT \n        function_name,\n        COUNT(*) as calls_last_day,\n        AVG(processing_time_ms) as avg_time_last_day\n    FROM api_usage_logs \n    WHERE created_at &gt; NOW() - INTERVAL '24 hours'\n    GROUP BY function_name\n)\nSELECT \n    COALESCE(r.function_name, d.function_name) as function_name,\n    COALESCE(r.calls_last_hour, 0) as calls_last_hour,\n    COALESCE(d.calls_last_day, 0) as calls_last_day,\n    COALESCE(r.avg_time_last_hour, 0) as avg_time_last_hour,\n    COALESCE(d.avg_time_last_day, 0) as avg_time_last_day,\n    COALESCE(r.cache_hits_last_hour, 0) as cache_hits_last_hour,\n    CASE \n        WHEN r.calls_last_hour &gt; 0 THEN \n            ROUND(100.0 * r.cache_hits_last_hour / r.calls_last_hour, 2)\n        ELSE 0\n    END as cache_hit_rate_pct\nFROM recent_stats r\nFULL OUTER JOIN daily_stats d ON r.function_name = d.function_name\nORDER BY calls_last_day DESC;\n</code></pre> <p>This comprehensive guide demonstrates how to integrate SteadyText with PostgreSQL for building powerful AI-enhanced applications. The examples cover everything from basic setup to advanced workflows, providing a solid foundation for developing production-ready systems that combine structured data with AI-generated content and embeddings.</p>"},{"location":"examples/testing/","title":"Testing with AI","text":"<p>Learn how to use SteadyText to build reliable AI tests that never flake.</p>"},{"location":"examples/testing/#the-problem-with-ai-testing","title":"The Problem with AI Testing","text":"<p>Traditional AI testing is challenging because:</p> <ul> <li>Non-deterministic outputs: Same input produces different results</li> <li>Flaky tests: Tests pass sometimes, fail others  </li> <li>Hard to mock: AI services are complex to replicate</li> <li>Unpredictable behavior: Edge cases are difficult to reproduce</li> </ul> <p>SteadyText solves these by providing deterministic AI outputs - same input always produces the same result.</p>"},{"location":"examples/testing/#basic-test-patterns","title":"Basic Test Patterns","text":""},{"location":"examples/testing/#deterministic-assertions","title":"Deterministic Assertions","text":"<pre><code>import steadytext\n\ndef test_ai_code_generation():\n    \"\"\"Test that never flakes - same input, same output.\"\"\"\n\n    def my_ai_function(prompt):\n        # Your actual AI function (GPT-4, Claude, etc.)\n        # For testing, we compare against SteadyText\n        return call_real_ai_service(prompt)\n\n    prompt = \"write a function to reverse a string\"\n    result = my_ai_function(prompt)\n    expected = steadytext.generate(prompt)\n\n    # This assertion is deterministic and reliable\n    assert result.strip() == expected.strip()\n</code></pre>"},{"location":"examples/testing/#embedding-similarity-tests","title":"Embedding Similarity Tests","text":"<pre><code>import numpy as np\n\ndef test_document_similarity():\n    \"\"\"Test semantic similarity calculations.\"\"\"\n\n    def calculate_similarity(doc1, doc2):\n        vec1 = steadytext.embed(doc1)\n        vec2 = steadytext.embed(doc2)\n        return np.dot(vec1, vec2)  # Already normalized\n\n    # These similarities are always the same\n    similarity = calculate_similarity(\n        \"machine learning algorithms\",\n        \"artificial intelligence methods\"\n    )\n\n    assert similarity &gt; 0.7  # Reliable threshold\n    assert similarity &lt; 1.0  # Not identical documents\n</code></pre>"},{"location":"examples/testing/#mock-ai-services","title":"Mock AI Services","text":""},{"location":"examples/testing/#simple-mock","title":"Simple Mock","text":"<pre><code>class MockAI:\n    \"\"\"Deterministic AI mock for testing.\"\"\"\n\n    def complete(self, prompt: str) -&gt; str:\n        return steadytext.generate(prompt)\n\n    def embed(self, text: str) -&gt; np.ndarray:\n        return steadytext.embed(text)\n\n    def chat(self, messages: list) -&gt; str:\n        # Convert chat format to single prompt\n        prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" \n                           for msg in messages])\n        return steadytext.generate(f\"Chat response to: {prompt}\")\n\n# Usage in tests\ndef test_chat_functionality():\n    ai = MockAI()\n    response = ai.chat([\n        {\"role\": \"user\", \"content\": \"Hello\"}\n    ])\n\n    # Response is always the same\n    assert len(response) &gt; 0\n    assert \"hello\" in response.lower()\n</code></pre>"},{"location":"examples/testing/#advanced-mock-with-state","title":"Advanced Mock with State","text":"<pre><code>class StatefulMockAI:\n    \"\"\"Mock AI that maintains conversation state.\"\"\"\n\n    def __init__(self):\n        self.conversation_history = []\n\n    def chat(self, message: str) -&gt; str:\n        # Include history in prompt for context\n        history = \"\\n\".join(self.conversation_history[-5:])  # Last 5 messages\n        full_prompt = f\"History: {history}\\nNew message: {message}\"\n\n        response = steadytext.generate(full_prompt)\n\n        # Update history\n        self.conversation_history.append(f\"User: {message}\")\n        self.conversation_history.append(f\"AI: {response}\")\n\n        return response\n\ndef test_conversation_flow():\n    \"\"\"Test multi-turn conversations.\"\"\"\n    ai = StatefulMockAI()\n\n    response1 = ai.chat(\"What's the weather like?\")\n    response2 = ai.chat(\"What about tomorrow?\")\n\n    # Both responses are deterministic\n    assert len(response1) &gt; 0\n    assert len(response2) &gt; 0\n    # Tomorrow's response considers the context\n    assert response2 != response1\n</code></pre>"},{"location":"examples/testing/#test-data-generation","title":"Test Data Generation","text":""},{"location":"examples/testing/#reproducible-fixtures","title":"Reproducible Fixtures","text":"<pre><code>def generate_test_user(user_id: int) -&gt; dict:\n    \"\"\"Generate consistent test user data.\"\"\"\n    return {\n        \"id\": user_id,\n        \"name\": steadytext.generate(f\"Generate name for user {user_id}\"),\n        \"bio\": steadytext.generate(f\"Write bio for user {user_id}\"),\n        \"interests\": steadytext.generate(f\"List interests for user {user_id}\"),\n        \"embedding\": steadytext.embed(f\"user {user_id} profile\")\n    }\n\ndef test_user_recommendation():\n    \"\"\"Test user recommendation system.\"\"\"\n    # Generate consistent test users\n    users = [generate_test_user(i) for i in range(10)]\n\n    # Test similarity calculations\n    user1 = users[0]\n    user2 = users[1]\n\n    similarity = np.dot(user1[\"embedding\"], user2[\"embedding\"])\n\n    # Similarity is always the same for these users\n    assert isinstance(similarity, float)\n    assert -1.0 &lt;= similarity &lt;= 1.0\n</code></pre>"},{"location":"examples/testing/#fuzz-testing","title":"Fuzz Testing","text":"<pre><code>def generate_fuzz_input(test_name: str, iteration: int) -&gt; str:\n    \"\"\"Generate reproducible fuzz test inputs.\"\"\"\n    seed_prompt = f\"Generate test input for {test_name} iteration {iteration}\"\n    return steadytext.generate(seed_prompt)\n\ndef test_parser_robustness():\n    \"\"\"Fuzz test with reproducible inputs.\"\"\"\n\n    def parse_user_input(text):\n        # Your parsing function\n        return {\"words\": text.split(), \"length\": len(text)}\n\n    # Generate 100 consistent fuzz inputs\n    for i in range(100):\n        fuzz_input = generate_fuzz_input(\"parser_test\", i)\n\n        try:\n            result = parse_user_input(fuzz_input)\n            assert isinstance(result, dict)\n            assert \"words\" in result\n            assert \"length\" in result\n        except Exception as e:\n            # Reproducible error case\n            print(f\"Fuzz input {i} caused error: {e}\")\n            print(f\"Input was: {fuzz_input[:100]}...\")\n</code></pre>"},{"location":"examples/testing/#integration-testing","title":"Integration Testing","text":""},{"location":"examples/testing/#api-testing","title":"API Testing","text":"<pre><code>import requests_mock\n\ndef test_ai_api_integration():\n    \"\"\"Test integration with AI API using deterministic responses.\"\"\"\n\n    with requests_mock.Mocker() as m:\n        # Mock the AI API with deterministic responses\n        def generate_response(request, context):\n            prompt = request.json().get(\"prompt\", \"\")\n            return {\"response\": steadytext.generate(prompt)}\n\n        m.post(\"https://api.ai-service.com/generate\", json=generate_response)\n\n        # Your actual API client code\n        response = requests.post(\"https://api.ai-service.com/generate\", \n                               json={\"prompt\": \"Hello world\"})\n\n        # Response is always the same\n        expected_text = steadytext.generate(\"Hello world\")\n        assert response.json()[\"response\"] == expected_text\n</code></pre>"},{"location":"examples/testing/#database-testing","title":"Database Testing","text":"<pre><code>import sqlite3\n\ndef test_ai_content_storage():\n    \"\"\"Test storing AI-generated content in database.\"\"\"\n\n    # Create in-memory database\n    conn = sqlite3.connect(\":memory:\")\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n        CREATE TABLE content (\n            id INTEGER PRIMARY KEY,\n            prompt TEXT,\n            generated_text TEXT,\n            embedding BLOB\n        )\n    \"\"\")\n\n    # Generate deterministic content\n    prompt = \"Write a short story about AI\"\n    text = steadytext.generate(prompt)\n    embedding = steadytext.embed(text)\n\n    # Store in database\n    cursor.execute(\"\"\"\n        INSERT INTO content (prompt, generated_text, embedding) \n        VALUES (?, ?, ?)\n    \"\"\", (prompt, text, embedding.tobytes()))\n\n    # Verify storage\n    cursor.execute(\"SELECT * FROM content WHERE id = 1\")\n    row = cursor.fetchone()\n\n    assert row[1] == prompt\n    assert row[2] == text\n    assert len(row[3]) == 1024 * 4  # 1024 float32 values\n\n    conn.close()\n</code></pre>"},{"location":"examples/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"examples/testing/#consistency-benchmarks","title":"Consistency Benchmarks","text":"<pre><code>import time\n\ndef test_generation_performance():\n    \"\"\"Test that generation performance is consistent.\"\"\"\n\n    prompt = \"Explain machine learning in one paragraph\"\n    times = []\n\n    # Warm up cache\n    steadytext.generate(prompt)\n\n    # Measure cached performance\n    for _ in range(10):\n        start = time.time()\n        result = steadytext.generate(prompt)\n        end = time.time()\n        times.append(end - start)\n\n    avg_time = sum(times) / len(times)\n\n    # Cached calls should be very fast\n    assert avg_time &lt; 0.1  # Less than 100ms\n\n    # All results should be identical\n    results = [steadytext.generate(prompt) for _ in range(5)]\n    assert all(r == results[0] for r in results)\n</code></pre>"},{"location":"examples/testing/#best-practices","title":"Best Practices","text":"<p>Testing Guidelines</p> <ol> <li>Use deterministic prompts: Keep test prompts simple and specific</li> <li>Cache warmup: Call functions once before timing tests</li> <li>Mock external services: Use SteadyText to replace real AI APIs</li> <li>Test edge cases: Generate consistent edge case inputs</li> <li>Version pin: Keep SteadyText version fixed for test stability</li> </ol> <p>Limitations</p> <ul> <li>Model changes: Updates to SteadyText models will change outputs</li> <li>Creative tasks: SteadyText is optimized for consistency, not creativity</li> <li>Context length: Limited to model's context window</li> </ul>"},{"location":"examples/testing/#complete-example","title":"Complete Example","text":"<pre><code>import unittest\nimport numpy as np\nimport steadytext\n\nclass TestAIFeatures(unittest.TestCase):\n\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.mock_ai = MockAI()\n        self.test_prompts = [\n            \"Write a function to sort a list\",\n            \"Explain what is machine learning\",\n            \"Generate a product description\"\n        ]\n\n    def test_deterministic_generation(self):\n        \"\"\"Test that generation is deterministic.\"\"\"\n        for prompt in self.test_prompts:\n            result1 = steadytext.generate(prompt)\n            result2 = steadytext.generate(prompt)\n            self.assertEqual(result1, result2)\n\n    def test_embedding_consistency(self):\n        \"\"\"Test that embeddings are consistent.\"\"\"\n        text = \"test embedding consistency\"\n        vec1 = steadytext.embed(text)\n        vec2 = steadytext.embed(text)\n        np.testing.assert_array_equal(vec1, vec2)\n\n    def test_mock_ai_service(self):\n        \"\"\"Test mock AI service.\"\"\"\n        response = self.mock_ai.complete(\"Hello\")\n        self.assertIsInstance(response, str)\n        self.assertGreater(len(response), 0)\n\n        # Response should be deterministic\n        response2 = self.mock_ai.complete(\"Hello\")\n        self.assertEqual(response, response2)\n\nif __name__ == \"__main__\":\n    unittest.main()\n</code></pre> <p>This comprehensive testing approach ensures your AI features are reliable, reproducible, and maintainable.</p>"},{"location":"examples/tooling/","title":"CLI Tools &amp; Tooling","text":"<p>Build deterministic command-line tools and development utilities with SteadyText.</p>"},{"location":"examples/tooling/#why-steadytext-for-cli-tools","title":"Why SteadyText for CLI Tools?","text":"<p>Traditional AI-powered CLI tools have problems:</p> <ul> <li>Inconsistent outputs: Same command gives different results</li> <li>Unreliable automation: Scripts break due to changing responses  </li> <li>Hard to test: Non-deterministic behavior makes testing difficult</li> <li>User confusion: Users expect consistent behavior from tools</li> </ul> <p>SteadyText solves these with deterministic outputs - same input always produces the same result.</p>"},{"location":"examples/tooling/#basic-cli-patterns","title":"Basic CLI Patterns","text":""},{"location":"examples/tooling/#simple-command-tools","title":"Simple Command Tools","text":"<pre><code>import click\nimport steadytext\n\n@click.command()\n@click.argument('topic')\ndef motivate(topic):\n    \"\"\"Generate motivational quotes about any topic.\"\"\"\n    prompt = f\"Write an inspiring quote about {topic}\"\n    quote = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udcaa {quote}\")\n\n# Usage: python script.py programming\n# Always generates the same quote for \"programming\"\n</code></pre>"},{"location":"examples/tooling/#error-code-explainer","title":"Error Code Explainer","text":"<pre><code>@click.command()\n@click.argument('error_code')\ndef explain(error_code):\n    \"\"\"Convert error codes to friendly explanations.\"\"\"\n    prompt = f\"Explain error {error_code} in simple, user-friendly terms\"\n    explanation = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udd0d {error_code}: {explanation}\")\n\n# Usage: python script.py ECONNREFUSED\n# Always gives the same explanation for ECONNREFUSED\n</code></pre>"},{"location":"examples/tooling/#command-generator","title":"Command Generator","text":"<pre><code>@click.command()\n@click.argument('task')\ndef git_helper(task):\n    \"\"\"Generate git commands for common tasks.\"\"\"\n    prompt = f\"Git command to {task}. Return only the command.\"\n    command = steadytext.generate(prompt).strip()\n    click.echo(f\"\ud83d\udcbb {command}\")\n\n# Usage: python script.py \"undo last commit\"\n# Always suggests the same git command\n</code></pre>"},{"location":"examples/tooling/#development-tools","title":"Development Tools","text":""},{"location":"examples/tooling/#code-generation-helper","title":"Code Generation Helper","text":"<pre><code>import os\nimport click\n\n@click.group()\ndef codegen():\n    \"\"\"Code generation CLI tool.\"\"\"\n    pass\n\n@codegen.command()\n@click.argument('function_name')\n@click.argument('description')\n@click.option('--language', '-l', default='python', help='Programming language')\ndef function(function_name, description, language):\n    \"\"\"Generate a function from description.\"\"\"\n    prompt = f\"Write a {language} function named {function_name} that {description}\"\n    code = steadytext.generate(prompt)\n\n    # Save to file\n    ext = {'python': 'py', 'javascript': 'js', 'rust': 'rs'}.get(language, 'txt')\n    filename = f\"{function_name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(code)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n    click.echo(f\"\ud83d\udcc4 Preview:\\n{code[:200]}...\")\n\n# Usage: python codegen.py function binary_search \"search for item in sorted array\"\n</code></pre>"},{"location":"examples/tooling/#documentation-generator","title":"Documentation Generator","text":"<pre><code>@codegen.command()\n@click.argument('project_name')\ndef readme(project_name):\n    \"\"\"Generate README.md for a project.\"\"\"\n    prompt = f\"Write a comprehensive README.md for a project called {project_name}\"\n    readme_content = steadytext.generate(prompt)\n\n    with open('README.md', 'w') as f:\n        f.write(readme_content)\n\n    click.echo(\"\u2705 Generated README.md\")\n\n# Usage: python codegen.py readme \"my-awesome-project\"\n</code></pre>"},{"location":"examples/tooling/#testing-qa-tools","title":"Testing &amp; QA Tools","text":""},{"location":"examples/tooling/#test-case-generator","title":"Test Case Generator","text":"<pre><code>@click.command()\n@click.argument('function_description')\ndef test_cases(function_description):\n    \"\"\"Generate test cases for a function.\"\"\"\n    prompt = f\"Generate 5 test cases for a function that {function_description}\"\n    cases = steadytext.generate(prompt)\n\n    # Save to test file\n    with open('test_cases.py', 'w') as f:\n        f.write(f\"# Test cases for: {function_description}\\n\")\n        f.write(cases)\n\n    click.echo(\"\u2705 Generated test_cases.py\")\n    click.echo(f\"\ud83d\udccb Preview:\\n{cases[:300]}...\")\n\n# Usage: python tool.py \"calculates fibonacci numbers\"\n</code></pre>"},{"location":"examples/tooling/#mock-data-generator","title":"Mock Data Generator","text":"<pre><code>@click.command()\n@click.argument('data_type')\n@click.option('--count', '-c', default=10, help='Number of items to generate')\ndef mockdata(data_type, count):\n    \"\"\"Generate mock data for testing.\"\"\"\n    items = []\n\n    for i in range(count):\n        prompt = f\"Generate realistic {data_type} data item {i+1}\"\n        item = steadytext.generate(prompt)\n        items.append(item.strip())\n\n    # Output as JSON\n    import json\n    output = {data_type: items}\n\n    with open(f'mock_{data_type}.json', 'w') as f:\n        json.dump(output, f, indent=2)\n\n    click.echo(f\"\u2705 Generated mock_{data_type}.json with {count} items\")\n\n# Usage: python tool.py user_profiles --count 20\n</code></pre>"},{"location":"examples/tooling/#content-documentation-tools","title":"Content &amp; Documentation Tools","text":""},{"location":"examples/tooling/#commit-message-generator","title":"Commit Message Generator","text":"<pre><code>@click.command()\n@click.argument('changes', nargs=-1)\ndef commit_msg(changes):\n    \"\"\"Generate commit messages from change descriptions.\"\"\"\n    change_list = \" \".join(changes)\n    prompt = f\"Write a concise git commit message for: {change_list}\"\n    message = steadytext.generate(prompt).strip()\n\n    click.echo(f\"\ud83d\udcdd Suggested commit message:\")\n    click.echo(f\"   {message}\")\n\n    # Optionally copy to clipboard or commit directly\n    if click.confirm(\"Use this commit message?\"):\n        os.system(f'git commit -m \"{message}\"')\n        click.echo(\"\u2705 Committed!\")\n\n# Usage: python tool.py \"added user authentication\" \"fixed login bug\"\n</code></pre>"},{"location":"examples/tooling/#api-documentation-generator","title":"API Documentation Generator","text":"<pre><code>@click.command()\n@click.argument('api_endpoint')\n@click.argument('description')\ndef api_docs(api_endpoint, description):\n    \"\"\"Generate API documentation for an endpoint.\"\"\"\n    prompt = f\"\"\"Generate API documentation for endpoint {api_endpoint} that {description}.\n    Include: description, parameters, example request/response, error codes.\"\"\"\n\n    docs = steadytext.generate(prompt)\n\n    # Save to markdown file\n    safe_name = api_endpoint.replace('/', '_').replace('{', '').replace('}', '')\n    filename = f\"api_{safe_name}.md\"\n\n    with open(filename, 'w') as f:\n        f.write(f\"# {api_endpoint}\\n\\n\")\n        f.write(docs)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py \"/users/{id}\" \"returns user profile information\"\n</code></pre>"},{"location":"examples/tooling/#automation-scripting","title":"Automation &amp; Scripting","text":""},{"location":"examples/tooling/#configuration-generator","title":"Configuration Generator","text":"<pre><code>@click.command()\n@click.argument('service_name')\n@click.option('--format', '-f', default='yaml', help='Config format (yaml, json, toml)')\ndef config(service_name, format):\n    \"\"\"Generate configuration files for services.\"\"\"\n    prompt = f\"Generate a {format} configuration file for {service_name} service\"\n    config_content = steadytext.generate(prompt)\n\n    ext = {'yaml': 'yml', 'json': 'json', 'toml': 'toml'}.get(format, 'txt')\n    filename = f\"{service_name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(config_content)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py database --format yaml\n</code></pre>"},{"location":"examples/tooling/#script-template-generator","title":"Script Template Generator","text":"<pre><code>@click.command()\n@click.argument('script_type')\n@click.argument('purpose')\ndef script_template(script_type, purpose):\n    \"\"\"Generate script templates for common tasks.\"\"\"\n    prompt = f\"Generate a {script_type} script template for {purpose}\"\n    script = steadytext.generate(prompt)\n\n    ext = {'bash': 'sh', 'python': 'py', 'powershell': 'ps1'}.get(script_type, 'txt')\n    filename = f\"template.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(script)\n\n    # Make executable if shell script\n    if ext == 'sh':\n        os.chmod(filename, 0o755)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py bash \"automated deployment\"\n</code></pre>"},{"location":"examples/tooling/#complete-cli-tool-example","title":"Complete CLI Tool Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nDevHelper - A deterministic development tool powered by SteadyText\n\"\"\"\n\nimport os\nimport json\nimport click\nimport steadytext\n\n@click.group()\n@click.version_option()\ndef cli():\n    \"\"\"DevHelper - Deterministic development utilities.\"\"\"\n    pass\n\n@cli.group()\ndef generate():\n    \"\"\"Code and content generation commands.\"\"\"\n    pass\n\n@generate.command()\n@click.argument('name')\n@click.argument('description')\n@click.option('--lang', '-l', default='python', help='Programming language')\ndef function(name, description, lang):\n    \"\"\"Generate a function from description.\"\"\"\n    prompt = f\"Write a {lang} function named {name} that {description}\"\n    code = steadytext.generate(prompt)\n\n    ext = {'python': 'py', 'javascript': 'js', 'rust': 'rs'}.get(lang, 'txt')\n    filename = f\"{name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(code)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n@generate.command()\n@click.argument('count', type=int)\n@click.option('--type', '-t', default='user', help='Data type to generate')\ndef testdata(count, type):\n    \"\"\"Generate test data.\"\"\"\n    data = []\n\n    for i in range(count):\n        prompt = f\"Generate realistic {type} test data item {i+1} as JSON\"\n        item = steadytext.generate(prompt)\n        data.append(item.strip())\n\n    output_file = f\"test_{type}_data.json\"\n    with open(output_file, 'w') as f:\n        json.dump({f\"{type}_data\": data}, f, indent=2)\n\n    click.echo(f\"\u2705 Generated {output_file} with {count} items\")\n\n@cli.command()\n@click.argument('error_code')\ndef explain(error_code):\n    \"\"\"Explain error codes in friendly terms.\"\"\"\n    prompt = f\"Explain error {error_code} in simple, user-friendly terms\"\n    explanation = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udd0d {error_code}:\")\n    click.echo(f\"   {explanation}\")\n\n@cli.command()\n@click.argument('task')\ndef git(task):\n    \"\"\"Generate git commands for tasks.\"\"\"\n    prompt = f\"Git command to {task}. Return only the command.\"\n    command = steadytext.generate(prompt).strip()\n    click.echo(f\"\ud83d\udcbb {command}\")\n\n    if click.confirm(\"Execute this command?\"):\n        os.system(command)\n\nif __name__ == '__main__':\n    cli()\n</code></pre> <p>Save this as <code>devhelper.py</code> and use it:</p> <pre><code># Generate a function\npython devhelper.py generate function binary_search \"search sorted array\"\n\n# Generate test data  \npython devhelper.py generate testdata 10 --type user\n\n# Explain error codes\npython devhelper.py explain ECONNREFUSED\n\n# Get git commands\npython devhelper.py git \"undo last commit but keep changes\"\n</code></pre>"},{"location":"examples/tooling/#best-practices","title":"Best Practices","text":"<p>CLI Tool Guidelines</p> <ol> <li>Keep prompts specific: Clear, detailed prompts give better results</li> <li>Add confirmation prompts: For destructive operations, ask before executing</li> <li>Save outputs to files: Generate content to files for later use</li> <li>Use consistent formatting: Same input should always produce same output</li> <li>Add help text: Use Click's built-in help system</li> </ol> <p>Benefits of Deterministic CLI Tools</p> <ul> <li>Reliable automation: Scripts work consistently</li> <li>Easier testing: Predictable outputs make testing simple</li> <li>User trust: Users know what to expect</li> <li>Debugging: Reproducible behavior makes issues easier to track</li> <li>Documentation: Examples in docs always work</li> </ul> <p>Considerations</p> <ul> <li>Creative vs. Deterministic: SteadyText prioritizes consistency over creativity</li> <li>Context limits: Model has limited context window</li> <li>Update impacts: SteadyText updates may change outputs (major versions only)</li> </ul> <p>This approach creates reliable, testable CLI tools that users can depend on for consistent behavior.</p>"}]}