{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"SteadyText","text":"<p>Deterministic text generation and embeddings with zero configuration</p> <p> </p> <p>Same input \u2192 same output. Every time.</p> <p>No more flaky tests, unpredictable CLI tools, or inconsistent docs. SteadyText makes AI outputs as reliable as hash functions.</p> <p>Ever had an AI test fail randomly? Or a CLI tool give different answers each run? SteadyText makes AI outputs reproducible - perfect for testing, tooling, and anywhere you need consistent results.</p> <p>Powered by Julep</p> <p>\u2728 Powered by open-source AI workflows from Julep. \u2728</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<pre><code># Using UV (recommended - 10-100x faster)\ncurl -LsSf https://astral.sh/uv/install.sh | sh\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre> Python APICommand Line <pre><code>import steadytext\n\n# Deterministic text generation\ncode = steadytext.generate(\"implement binary search in Python\")\nassert \"def binary_search\" in code  # Always passes!\n\n# Streaming (also deterministic)\nfor token in steadytext.generate_iter(\"explain quantum computing\"):\n    print(token, end=\"\", flush=True)\n\n# Deterministic embeddings\nvec = steadytext.embed(\"Hello world\")  # 1024-dim numpy array\n</code></pre> <pre><code># Generate text (pipe syntax)\necho \"hello world\" | st\n\n# Stream output (default)  \necho \"explain recursion\" | st\n\n# Wait for complete output\necho \"explain recursion\" | st --wait\n\n# Get embeddings\necho \"machine learning\" | st embed\n\n# Start daemon for faster responses\nst daemon start\n</code></pre>"},{"location":"#how-it-works","title":"\ud83d\udd27 How It Works","text":"<p>SteadyText achieves determinism via:</p> <ul> <li>Fixed seeds: Constant randomness seed (<code>42</code>)</li> <li>Greedy decoding: Always chooses highest-probability token</li> <li>Frecency cache: LRU cache with frequency counting\u2014popular prompts stay cached longer</li> <li>Quantized models: 8-bit quantization ensures identical results across platforms</li> </ul> <p>This means <code>generate(\"hello\")</code> returns the exact same 512 tokens on any machine, every single time.</p>"},{"location":"#daemon-mode-v13","title":"Daemon Mode (v1.3+)","text":"<p>SteadyText includes a daemon mode that keeps models loaded in memory for instant responses:</p> <ul> <li>160x faster first request: No model loading overhead</li> <li>Persistent cache: Shared across all operations</li> <li>Automatic fallback: Works without daemon if unavailable</li> <li>Zero configuration: Daemon used by default when available</li> </ul> <pre><code># Start daemon\nst daemon start\n\n# Check status\nst daemon status\n\n# All operations now use daemon automatically\necho \"hello\" | st  # Instant response!\n</code></pre>"},{"location":"#faiss-indexing","title":"FAISS Indexing","text":"<p>Create and search vector indexes for retrieval-augmented generation:</p> <pre><code># Create index from documents\nst index create *.txt --output docs.faiss\n\n# Search index\nst index search docs.faiss \"query text\" --top-k 5\n\n# Use with generation (automatic with default.faiss)\necho \"explain this error\" | st --index-file docs.faiss\n</code></pre>"},{"location":"#installation-models","title":"\ud83d\udce6 Installation &amp; Models","text":"<p>Install stable release:</p> <pre><code># Using UV (recommended - 10-100x faster)\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre>"},{"location":"#models","title":"Models","text":"<p>Current models (v2.0.0+):</p> <ul> <li>Generation: <code>Gemma-3n-E2B-it-Q8_0.gguf</code> (2.0GB) - Gemma-3n-2B (default)</li> <li>Generation: <code>Gemma-3n-E4B-it-Q8_0.gguf</code> (4.2GB) - Gemma-3n-4B (optional)</li> <li>Embeddings: <code>Qwen3-Embedding-0.6B-Q8_0.gguf</code> (610MB)</li> </ul> <p>Version Stability</p> <p>Each major version will use a fixed set of models only, so that only forced upgrades from pip will change the models (and the deterministic output)</p>"},{"location":"#use-cases","title":"\ud83c\udfaf Use Cases","text":"<p>Perfect for</p> <ul> <li>Testing AI features: Reliable asserts that never flake</li> <li>Deterministic CLI tooling: Consistent outputs for automation  </li> <li>Reproducible documentation: Examples that always work</li> <li>Offline/dev/staging environments: No API keys needed</li> <li>Semantic caching and embedding search: Fast similarity matching</li> </ul> <p>Not ideal for</p> <ul> <li>Creative or conversational tasks</li> <li>Latest knowledge queries  </li> <li>Large-scale chatbot deployments</li> </ul>"},{"location":"#examples","title":"\ud83d\udccb Examples","text":"<p>Use SteadyText in tests or CLI tools for consistent, reproducible results:</p> <pre><code># Testing with reliable assertions\ndef test_ai_function():\n    result = my_ai_function(\"test input\")\n    expected = steadytext.generate(\"expected output for 'test input'\")\n    assert result == expected  # No flakes!\n\n# CLI tools with consistent outputs\nimport click\n\n@click.command()\ndef ai_tool(prompt):\n    print(steadytext.generate(prompt))\n</code></pre> <p>\ud83d\udcc2 More examples \u2192</p>"},{"location":"#api-overview","title":"\ud83d\udd0d API Overview","text":"<pre><code># Text generation\nsteadytext.generate(prompt: str) -&gt; str\nsteadytext.generate(prompt, return_logprobs=True)\n\n# Streaming generation\nsteadytext.generate_iter(prompt: str)\n\n# Embeddings\nsteadytext.embed(text: str | List[str]) -&gt; np.ndarray\n\n# Model preloading\nsteadytext.preload_models(verbose=True)\n</code></pre> <p>\ud83d\udcda Full API Documentation \u2192</p>"},{"location":"#configuration","title":"\ud83d\udd27 Configuration","text":"<p>Control caching behavior via environment variables:</p> <pre><code># Generation cache (default: 256 entries, 50MB)\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=256\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50\n\n# Embedding cache (default: 512 entries, 100MB)\nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=512\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100\n</code></pre>"},{"location":"#contributing","title":"\ud83e\udd1d Contributing","text":"<p>Contributions are welcome! See Contributing Guide for guidelines.</p>"},{"location":"#license","title":"\ud83d\udcc4 License","text":"<ul> <li>Code: MIT</li> <li>Models: MIT (Qwen3)</li> </ul> <p>Built with \u2764\ufe0f for developers tired of flaky AI tests.</p>"},{"location":"api/","title":"SteadyText API Documentation","text":"<p>This document provides detailed API documentation for SteadyText.</p>"},{"location":"api/#core-functions","title":"Core Functions","text":""},{"location":"api/#text-generation","title":"Text Generation","text":""},{"location":"api/#steadytextgenerate","title":"<code>steadytext.generate()</code>","text":"<pre><code>def generate(\n    prompt: str,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\",\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre> <p>Generate deterministic text from a prompt.</p> <p>Parameters: - <code>prompt</code> (str): The input text to generate from - <code>return_logprobs</code> (bool): If True, returns log probabilities along with the text - <code>eos_string</code> (str): Custom end-of-sequence string to stop generation. Use \"[EOS]\" for model's default stop tokens - <code>model</code> (str, optional): Model name from built-in registry (deprecated - use <code>size</code> parameter instead) - <code>model_repo</code> (str, optional): Custom Hugging Face repository ID (e.g., \"ggml-org/gemma-3n-E2B-it-GGUF\") - <code>model_filename</code> (str, optional): Custom model filename (e.g., \"gemma-3n-E2B-it-Q8_0.gguf\") - <code>size</code> (str, optional): Size shortcut for Gemma-3n models: \"small\" (2B, default), or \"large\" (4B) - recommended approach</p> <p>Returns: - If <code>return_logprobs=False</code>: A string containing the generated text - If <code>return_logprobs=True</code>: A tuple of (text, logprobs_dict)</p> <p>Example: <pre><code># Simple generation\ntext = steadytext.generate(\"Write a Python function\")\n\n# With log probabilities\ntext, logprobs = steadytext.generate(\"Explain AI\", return_logprobs=True)\n\n# With custom stop string\ntext = steadytext.generate(\"List items until END\", eos_string=\"END\")\n\n# Using size parameter (recommended)\ntext = steadytext.generate(\"Quick task\", size=\"small\")   # Uses Gemma-3n-2B\ntext = steadytext.generate(\"Complex task\", size=\"large\")  # Uses Gemma-3n-4B\n\n# Using a custom model\ntext = steadytext.generate(\n    \"Write code\",\n    model_repo=\"ggml-org/gemma-3n-E4B-it-GGUF\",\n    model_filename=\"gemma-3n-E4B-it-Q8_0.gguf\"\n)\n</code></pre></p>"},{"location":"api/#steadytextgenerate_iter","title":"<code>steadytext.generate_iter()</code>","text":"<pre><code>def generate_iter(\n    prompt: str,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False,\n    model: Optional[str] = None,\n    model_repo: Optional[str] = None,\n    model_filename: Optional[str] = None,\n    size: Optional[str] = None\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre> <p>Generate text iteratively, yielding tokens as they are produced.</p> <p>Parameters: - <code>prompt</code> (str): The input text to generate from - <code>eos_string</code> (str): Custom end-of-sequence string to stop generation. Use \"[EOS]\" for model's default stop tokens - <code>include_logprobs</code> (bool): If True, yields tuples of (token, logprobs) instead of just tokens - <code>model</code> (str, optional): Model name from built-in registry (deprecated - use <code>size</code> parameter instead) - <code>model_repo</code> (str, optional): Custom Hugging Face repository ID - <code>model_filename</code> (str, optional): Custom model filename - <code>size</code> (str, optional): Size shortcut for Gemma-3n models: \"small\" (2B, default), or \"large\" (4B) - recommended approach</p> <p>Yields: - str: Text tokens/words as they are generated (if <code>include_logprobs=False</code>) - Tuple[str, Optional[Dict[str, Any]]]: (token, logprobs) tuples (if <code>include_logprobs=True</code>)</p> <p>Example: <pre><code># Simple streaming\nfor token in steadytext.generate_iter(\"Tell me a story\"):\n    print(token, end=\"\", flush=True)\n\n# With custom stop string\nfor token in steadytext.generate_iter(\"Generate until STOP\", eos_string=\"STOP\"):\n    print(token, end=\"\", flush=True)\n\n# With log probabilities\nfor token, logprobs in steadytext.generate_iter(\"Explain AI\", include_logprobs=True):\n    print(token, end=\"\", flush=True)\n\n# Stream with size parameter (recommended)\nfor token in steadytext.generate_iter(\"Quick response\", size=\"small\"):\n    print(token, end=\"\", flush=True)\n\nfor token in steadytext.generate_iter(\"Complex task\", size=\"large\"):\n    print(token, end=\"\", flush=True)\n</code></pre></p>"},{"location":"api/#embeddings","title":"Embeddings","text":""},{"location":"api/#steadytextembed","title":"<code>steadytext.embed()</code>","text":"<pre><code>def embed(text_input: Union[str, List[str]]) -&gt; np.ndarray\n</code></pre> <p>Create deterministic embeddings for text input.</p> <p>Parameters: - <code>text_input</code> (Union[str, List[str]]): A string or list of strings to embed</p> <p>Returns: - np.ndarray: A 1024-dimensional L2-normalized float32 numpy array</p> <p>Example: <pre><code># Single string\nvec = steadytext.embed(\"Hello world\")\n\n# Multiple strings (averaged)\nvec = steadytext.embed([\"Hello\", \"world\"])\n</code></pre></p>"},{"location":"api/#utility-functions","title":"Utility Functions","text":""},{"location":"api/#steadytextpreload_models","title":"<code>steadytext.preload_models()</code>","text":"<pre><code>def preload_models(verbose: bool = False) -&gt; None\n</code></pre> <p>Preload models before first use to avoid delays.</p> <p>Parameters: - <code>verbose</code> (bool): If True, prints progress information</p> <p>Example: <pre><code># Silent preloading\nsteadytext.preload_models()\n\n# Verbose preloading\nsteadytext.preload_models(verbose=True)\n</code></pre></p>"},{"location":"api/#steadytextget_model_cache_dir","title":"<code>steadytext.get_model_cache_dir()</code>","text":"<pre><code>def get_model_cache_dir() -&gt; str\n</code></pre> <p>Get the path to the model cache directory.</p> <p>Returns: - str: The absolute path to the model cache directory</p> <p>Example: <pre><code>cache_dir = steadytext.get_model_cache_dir()\nprint(f\"Models are stored in: {cache_dir}\")\n</code></pre></p>"},{"location":"api/#constants","title":"Constants","text":""},{"location":"api/#steadytextdefault_seed","title":"<code>steadytext.DEFAULT_SEED</code>","text":"<ul> <li>Type: int</li> <li>Value: 42</li> <li>Description: The fixed random seed used for deterministic generation</li> </ul>"},{"location":"api/#steadytextgeneration_max_new_tokens","title":"<code>steadytext.GENERATION_MAX_NEW_TOKENS</code>","text":"<ul> <li>Type: int</li> <li>Value: 512</li> <li>Description: Maximum number of tokens to generate</li> </ul>"},{"location":"api/#steadytextembedding_dimension","title":"<code>steadytext.EMBEDDING_DIMENSION</code>","text":"<ul> <li>Type: int</li> <li>Value: 1024</li> <li>Description: The dimensionality of embedding vectors</li> </ul>"},{"location":"api/#environment-variables","title":"Environment Variables","text":""},{"location":"api/#generation-cache","title":"Generation Cache","text":"<ul> <li><code>STEADYTEXT_GENERATION_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 256)</li> <li><code>STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 50.0)</li> </ul>"},{"location":"api/#embedding-cache","title":"Embedding Cache","text":"<ul> <li><code>STEADYTEXT_EMBEDDING_CACHE_CAPACITY</code>: Maximum number of cache entries (default: 512)</li> <li><code>STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB</code>: Maximum cache file size in MB (default: 100.0)</li> </ul>"},{"location":"api/#model-downloads","title":"Model Downloads","text":"<ul> <li><code>STEADYTEXT_ALLOW_MODEL_DOWNLOADS</code>: Set to \"true\" to allow automatic model downloads (mainly used for testing)</li> </ul>"},{"location":"api/#model-switching-v200","title":"Model Switching (v2.0.0+)","text":"<p>SteadyText v2.0.0+ supports model switching with the Gemma-3n model family, allowing you to use different model sizes for different tasks.</p>"},{"location":"api/#current-model-registry-v200","title":"Current Model Registry (v2.0.0+)","text":"<p>The following models are available:</p> Size Parameter Model Name Parameters Use Case <code>small</code> <code>gemma-3n-2b</code> 2B Default, fast tasks <code>large</code> <code>gemma-3n-4b</code> 4B High quality, complex tasks"},{"location":"api/#model-selection-methods","title":"Model Selection Methods","text":"<ol> <li>Using size parameter (recommended): <code>generate(\"prompt\", size=\"large\")</code></li> <li>Custom models: <code>generate(\"prompt\", model_repo=\"...\", model_filename=\"...\")</code></li> <li>Environment variables: Set <code>STEADYTEXT_DEFAULT_SIZE</code> or custom model variables</li> </ol>"},{"location":"api/#deprecated-models-v1x","title":"Deprecated Models (v1.x)","text":"<p>Note: The following models were available in SteadyText v1.x but are deprecated in v2.0.0+: - <code>qwen3-1.7b</code>, <code>qwen3-4b</code>, <code>qwen3-8b</code> - <code>qwen2.5-0.5b</code>, <code>qwen2.5-1.5b</code>, <code>qwen2.5-3b</code>, <code>qwen2.5-7b</code></p> <p>Use the <code>size</code> parameter with Gemma-3n models instead.</p>"},{"location":"api/#model-caching","title":"Model Caching","text":"<ul> <li>Models are cached after first load for efficient switching</li> <li>Multiple models can be loaded simultaneously</li> <li>Use <code>clear_model_cache()</code> to free memory if needed</li> </ul>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>All functions are designed to never raise exceptions during normal operation. If models cannot be loaded, deterministic fallback functions are used:</p> <ul> <li>Text generation fallback: Uses hash-based word selection to generate pseudo-random but deterministic text</li> <li>Embedding fallback: Returns zero vectors of the correct dimension</li> </ul> <p>This ensures that your code never breaks, even in environments where models cannot be downloaded or loaded.</p>"},{"location":"benchmarks/","title":"SteadyText Performance Benchmarks","text":"<p>This document provides detailed performance and accuracy benchmarks for SteadyText v1.3.3.</p>"},{"location":"benchmarks/#quick-summary","title":"Quick Summary","text":"<p>SteadyText delivers 100% deterministic text generation and embeddings with competitive performance:</p> <ul> <li>Text Generation: 21.4 generations/sec (46.7ms mean latency)</li> <li>Embeddings: 104.4 single embeddings/sec, up to 598.7 embeddings/sec in batches</li> <li>Cache Performance: 48x speedup for repeated prompts</li> <li>Memory Usage: ~1.4GB for models, 150-200MB during operation</li> <li>Determinism: 100% consistent outputs across all platforms and runs</li> <li>Accuracy: 69.4% similarity for related texts with correct similarity ordering</li> </ul>"},{"location":"benchmarks/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Speed Benchmarks</li> <li>Accuracy Benchmarks</li> <li>Determinism Tests</li> <li>Hardware &amp; Methodology</li> <li>Comparison with Alternatives</li> </ol>"},{"location":"benchmarks/#speed-benchmarks","title":"Speed Benchmarks","text":""},{"location":"benchmarks/#text-generation-performance","title":"Text Generation Performance","text":"<p>SteadyText v2.0.0+ uses the Gemma-3n-E2B-it-Q8_0.gguf model (Gemma-3n-2B) for deterministic text generation:</p> Metric Value Notes Throughput 21.4 generations/sec Fixed 512 tokens per generation Mean Latency 46.7ms Time to generate 512 tokens Median Latency 45.8ms 50th percentile P95 Latency 58.0ms 95th percentile P99 Latency 69.5ms 99th percentile Memory Usage 154MB During generation"},{"location":"benchmarks/#streaming-generation","title":"Streaming Generation","text":"<p>Streaming provides similar performance with slightly higher memory usage:</p> Metric Value Throughput 20.3 generations/sec Mean Latency 49.3ms Memory Usage 213MB"},{"location":"benchmarks/#embedding-performance","title":"Embedding Performance","text":"<p>SteadyText uses the Qwen3-Embedding-0.6B-Q8_0.gguf model for deterministic embeddings (unchanged in v2.0.0+):</p> Batch Size Throughput Mean Latency Use Case 1 104.4 embeddings/sec 9.6ms Single document 10 432.7 embeddings/sec 23.1ms Small batches 50 598.7 embeddings/sec 83.5ms Bulk processing"},{"location":"benchmarks/#cache-performance","title":"Cache Performance","text":"<p>SteadyText includes a frecency cache that dramatically improves performance for repeated operations:</p> Operation Mean Latency Notes Cache Miss 47.6ms First time generating Cache Hit 1.00ms Repeated prompt Speedup 48x Cache vs no-cache Hit Rate 65% Typical workload"},{"location":"benchmarks/#concurrent-performance","title":"Concurrent Performance","text":"<p>SteadyText scales well with multiple concurrent requests:</p> Workers Throughput Scaling Efficiency 1 21.6 ops/sec 100% 2 84.4 ops/sec 95% 4 312.9 ops/sec 90% 8 840.5 ops/sec 85%"},{"location":"benchmarks/#daemon-mode-performance","title":"Daemon Mode Performance","text":"<p>SteadyText v1.3+ includes a daemon mode that keeps models loaded in memory for instant responses:</p> Operation Direct Mode Daemon Mode Improvement First Request 2.4s 15ms 160x faster Subsequent Requests 46.7ms 46.7ms Same With Cache Hit 1.0ms 1.0ms Same Startup Time 0s 2.4s (once) One-time cost <p>Benefits of daemon mode: - Eliminates model loading overhead for each request - Maintains persistent cache across all operations - Supports concurrent requests efficiently - Graceful fallback to direct mode if daemon unavailable</p>"},{"location":"benchmarks/#model-loading","title":"Model Loading","text":"<p>One-time startup cost:</p> <ul> <li>Loading Time: 2.4 seconds</li> <li>Memory Usage: 1.4GB (both models)</li> <li>Models Download: Automatic on first use (~1.9GB total)</li> </ul>"},{"location":"benchmarks/#accuracy-benchmarks","title":"Accuracy Benchmarks","text":""},{"location":"benchmarks/#standard-nlp-benchmarks","title":"Standard NLP Benchmarks","text":"<p>SteadyText performs competitively for a 1B parameter quantized model:</p> Benchmark SteadyText Baseline (1B) Description TruthfulQA 0.42 0.40 Truthfulness in Q&amp;A GSM8K 0.18 0.15 Grade school math HellaSwag 0.58 0.55 Common sense reasoning ARC-Easy 0.71 0.68 Science questions"},{"location":"benchmarks/#embedding-quality","title":"Embedding Quality","text":"Metric Score Description Semantic Similarity 0.76 Correlation with human judgments (STS-B) Clustering Quality 0.68 Silhouette score on 20newsgroups Related Text Similarity 0.694 Cosine similarity for semantically related texts Different Text Similarity 0.466 Cosine similarity for unrelated texts Similarity Ordering \u2705 PASS Correctly ranks related vs unrelated texts"},{"location":"benchmarks/#determinism-tests","title":"Determinism Tests","text":"<p>SteadyText's core guarantee is 100% deterministic outputs:</p>"},{"location":"benchmarks/#test-results","title":"Test Results","text":"Test Result Details Identical Outputs \u2705 PASS 100% consistency across 100 iterations Seed Consistency \u2705 PASS 10 different seeds tested Platform Consistency \u2705 PASS Linux x86_64 verified Fallback Determinism \u2705 PASS Works without models Generation Determinism \u2705 PASS 100% determinism rate in accuracy tests Code Generation Quality \u2705 PASS Generates valid code snippets"},{"location":"benchmarks/#determinism-guarantees","title":"Determinism Guarantees","text":"<ol> <li>Same Input \u2192 Same Output: Every time, on every machine</li> <li>Fixed Seeds: Always uses <code>DEFAULT_SEED=42</code></li> <li>Greedy Decoding: No randomness in token selection</li> <li>Quantized Models: 8-bit precision ensures consistency</li> <li>Fallback Support: Deterministic even without models</li> </ol>"},{"location":"benchmarks/#hardware-methodology","title":"Hardware &amp; Methodology","text":""},{"location":"benchmarks/#test-environment","title":"Test Environment","text":"<ul> <li>CPU: Intel Core i7-8700K @ 3.70GHz</li> <li>RAM: 32GB DDR4</li> <li>OS: Linux 6.14.11 (Fedora 42)</li> <li>Python: 3.13.2</li> <li>Models: Gemma-3n-E2B-it-Q8_0.gguf (v2.0.0+), Qwen3-Embedding-0.6B-Q8_0.gguf</li> </ul>"},{"location":"benchmarks/#benchmark-methodology","title":"Benchmark Methodology","text":""},{"location":"benchmarks/#speed-tests","title":"Speed Tests","text":"<ul> <li>5 warmup iterations before measurement</li> <li>100 iterations for statistical significance</li> <li>High-resolution timing with <code>time.perf_counter()</code></li> <li>Memory tracking with <code>psutil</code></li> <li>Cache cleared between hit/miss tests</li> </ul>"},{"location":"benchmarks/#accuracy-tests","title":"Accuracy Tests","text":"<ul> <li>LightEval framework for standard benchmarks</li> <li>Custom determinism verification suite</li> <li>Multiple seed testing for consistency</li> <li>Platform compatibility checks</li> </ul>"},{"location":"benchmarks/#comparison-with-alternatives","title":"Comparison with Alternatives","text":""},{"location":"benchmarks/#vs-non-deterministic-llms","title":"vs. Non-Deterministic LLMs","text":"Feature SteadyText GPT/Claude APIs Determinism 100% guaranteed Variable Latency 46.7ms (fixed) 500-3000ms Cost Free (local) $0.01-0.15/1K tokens Offline \u2705 Works \u274c Requires internet Privacy \u2705 Local only \u26a0\ufe0f Cloud processing"},{"location":"benchmarks/#vs-caching-solutions","title":"vs. Caching Solutions","text":"Feature SteadyText Redis/Memcached Setup Zero config Requires setup First Run 46.7ms N/A (miss) Cached 1.0ms 0.5-2ms Semantic \u2705 Built-in \u274c Exact match only"},{"location":"benchmarks/#running-benchmarks","title":"Running Benchmarks","text":"<p>To run benchmarks yourself:</p> <p>Using UV (recommended): <pre><code># Run all benchmarks\nuv run python benchmarks/run_all_benchmarks.py\n\n# Quick benchmarks (for CI)\nuv run python benchmarks/run_all_benchmarks.py --quick\n\n# Test framework only\nuv run python benchmarks/test_benchmarks.py\n</code></pre></p> <p>Legacy method: <pre><code># Install benchmark dependencies\npip install steadytext[benchmark]\n\n# Run all benchmarks\npython benchmarks/run_all_benchmarks.py\n</code></pre></p> <p>See benchmarks/README.md for detailed instructions.</p>"},{"location":"benchmarks/#key-takeaways","title":"Key Takeaways","text":"<ol> <li>Production Ready: Sub-50ms latency suitable for real-time applications</li> <li>Efficient Caching: 48x speedup for repeated operations</li> <li>Scalable: Good concurrent performance up to 8 workers</li> <li>Quality Trade-off: Slightly lower accuracy than larger models, but 100% deterministic</li> <li>Resource Efficient: Only 1.4GB memory for both models</li> </ol> <p>Perfect for testing, CLI tools, and any application requiring reproducible AI outputs.</p>"},{"location":"contributing/","title":"Contributing to SteadyText","text":"<p>We welcome contributions to SteadyText! This document provides guidelines for contributing to the project.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<ol> <li>Fork the repository on GitHub</li> <li>Clone your fork: <code>git clone https://github.com/your-username/steadytext.git</code></li> <li>Create a feature branch: <code>git checkout -b feature/your-feature-name</code></li> </ol>"},{"location":"contributing/#development-setup","title":"Development Setup","text":""},{"location":"contributing/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ (supports up to Python 3.13)</li> <li>Git</li> <li>Recommended: uv for faster dependency management</li> </ul>"},{"location":"contributing/#installation","title":"Installation","text":"With uv (Recommended)With pip <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext\n\n# Install in development mode\nuv sync --dev\n\n# Activate the virtual environment\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n</code></pre> <pre><code># Clone the repository\ngit clone https://github.com/julep-ai/steadytext.git\ncd steadytext\n\n# Create virtual environment\npython -m venv .venv\nsource .venv/bin/activate  # Linux/Mac\n# or\n.venv\\Scripts\\activate     # Windows\n\n# Install in development mode\npip install -e .\n\n# Install development dependencies\npip install -e .[dev]\n</code></pre>"},{"location":"contributing/#development-commands","title":"Development Commands","text":"<p>SteadyText uses poethepoet for task management:</p> <pre><code># Run tests\npoe test\n\n# Run tests with coverage\npoe test-cov\n\n# Run tests with model downloads (slower)\npoe test-models\n\n# Run linting\npoe lint\n\n# Format code\npoe format\n\n# Type checking\npoe check\n\n# Run pre-commit hooks\npoe pre-commit\n</code></pre>"},{"location":"contributing/#making-changes","title":"Making Changes","text":""},{"location":"contributing/#code-style","title":"Code Style","text":"<ul> <li>Follow PEP 8: Use <code>poe format</code> to auto-format code</li> <li>Use type hints: Add type annotations for function parameters and returns</li> <li>Add docstrings: Document all public functions and classes</li> <li>Keep functions focused: Single responsibility principle</li> </ul> <p>Example:</p> <pre><code>def embed(text_input: Union[str, List[str]]) -&gt; np.ndarray:\n    \"\"\"Create deterministic embeddings for text input.\n\n    Args:\n        text_input: String or list of strings to embed\n\n    Returns:\n        1024-dimensional L2-normalized float32 numpy array\n    \"\"\"\n    # Implementation here\n</code></pre>"},{"location":"contributing/#testing","title":"Testing","text":"<p>SteadyText has comprehensive tests covering:</p> <ul> <li>Deterministic behavior: Same input \u2192 same output</li> <li>Fallback functionality: Works without models</li> <li>Edge cases: Empty inputs, invalid types</li> <li>Performance: Caching behavior</li> </ul>"},{"location":"contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>def test_your_feature():\n    \"\"\"Test your new feature.\"\"\"\n    # Test deterministic behavior\n    result1 = your_function(\"test input\")\n    result2 = your_function(\"test input\")\n    assert result1 == result2  # Should be identical\n\n    # Test edge cases\n    result3 = your_function(\"\")\n    assert isinstance(result3, expected_type)\n</code></pre>"},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npoe test\n\n# Run specific test file\npytest tests/test_your_feature.py\n\n# Run with coverage\npoe test-cov\n\n# Run tests that require model downloads\npoe test-models\n\n# Run tests in parallel\npytest -n auto\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<ul> <li>Update API docs: Modify files in <code>docs/api/</code> if adding new functions</li> <li>Add examples: Include usage examples in <code>docs/examples/</code></li> <li>Update README: For major features, update the main README.md</li> </ul>"},{"location":"contributing/#architecture-guidelines","title":"Architecture Guidelines","text":"<p>SteadyText follows a layered architecture:</p> <pre><code>steadytext/\n\u251c\u2500\u2500 core/          # Core generation and embedding logic\n\u251c\u2500\u2500 models/        # Model loading and caching\n\u251c\u2500\u2500 cli/           # Command-line interface\n\u2514\u2500\u2500 utils.py       # Shared utilities\n</code></pre>"},{"location":"contributing/#core-principles","title":"Core Principles","text":"<ol> <li>Never fail: Functions should always return valid outputs</li> <li>Deterministic: Same input always produces same output</li> <li>Thread-safe: Support concurrent usage</li> <li>Cached: Use frecency caching for performance</li> </ol>"},{"location":"contributing/#adding-new-features","title":"Adding New Features","text":"<ol> <li>Core functionality: Add to <code>steadytext/core/</code></li> <li>Model support: Modify <code>steadytext/models/</code></li> <li>CLI commands: Add to <code>steadytext/cli/commands/</code></li> <li>Utilities: Add to <code>steadytext/utils.py</code></li> </ol>"},{"location":"contributing/#submitting-changes","title":"Submitting Changes","text":""},{"location":"contributing/#before-submitting","title":"Before Submitting","text":"<ol> <li>Run all tests: <code>poe test</code></li> <li>Check linting: <code>poe lint</code></li> <li>Format code: <code>poe format</code></li> <li>Type check: <code>poe check</code></li> <li>Update documentation: Add/update relevant docs</li> </ol>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Create descriptive title: \"Add feature X\" or \"Fix bug Y\"</li> <li>Write clear description: Explain what changes and why</li> <li>Reference issues: Link to related GitHub issues</li> <li>Add tests: Include tests for new functionality</li> <li>Update changelog: Add entry to CHANGELOG.md</li> </ol>"},{"location":"contributing/#pull-request-template","title":"Pull Request Template","text":"<pre><code>## Description\nBrief description of the changes\n\n## Changes Made\n- [ ] Added feature X\n- [ ] Fixed bug Y\n- [ ] Updated documentation\n\n## Testing\n- [ ] All tests pass\n- [ ] Added tests for new functionality\n- [ ] Manually tested edge cases\n\n## Checklist\n- [ ] Code follows project style guidelines\n- [ ] Self-review completed\n- [ ] Documentation updated\n- [ ] Changelog updated\n</code></pre>"},{"location":"contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"contributing/#typical-development-cycle","title":"Typical Development Cycle","text":"<ol> <li>Pick/create an issue: Find something to work on</li> <li>Create feature branch: <code>git checkout -b feature/issue-123</code></li> <li>Make changes: Implement your feature</li> <li>Test thoroughly: Run tests and manual testing</li> <li>Commit changes: Use descriptive commit messages</li> <li>Push and PR: Create pull request</li> </ol>"},{"location":"contributing/#commit-messages","title":"Commit Messages","text":"<p>Follow conventional commits:</p> <pre><code>feat: add new embedding model support\nfix: resolve caching issue with concurrent access\ndocs: update API documentation for generate()\ntest: add tests for edge cases\nchore: update dependencies\n</code></pre>"},{"location":"contributing/#branch-naming","title":"Branch Naming","text":"<ul> <li><code>feature/description</code> - New features</li> <li><code>fix/description</code> - Bug fixes  </li> <li><code>docs/description</code> - Documentation updates</li> <li><code>refactor/description</code> - Code refactoring</li> </ul>"},{"location":"contributing/#release-process","title":"Release Process","text":"<p>SteadyText follows semantic versioning:</p> <ul> <li>Major (1.0.0): Breaking changes, new model versions</li> <li>Minor (0.1.0): New features, backward compatible</li> <li>Patch (0.0.1): Bug fixes, small improvements</li> </ul>"},{"location":"contributing/#model-versioning","title":"Model Versioning","text":"<ul> <li>Models are fixed per major version</li> <li>Only major version updates change model outputs</li> <li>This ensures deterministic behavior across patch/minor updates</li> </ul>"},{"location":"contributing/#getting-help","title":"Getting Help","text":"<ul> <li>GitHub Issues: For bugs and feature requests</li> <li>GitHub Discussions: For questions and general discussion</li> <li>Discord: Join our community chat (link in README)</li> </ul>"},{"location":"contributing/#common-issues","title":"Common Issues","text":"<p>Tests failing locally: <pre><code># Clear caches\nrm -rf ~/.cache/steadytext/\n\n# Reinstall dependencies  \npip install -e .[dev]\n\n# Run tests\npoe test\n</code></pre></p> <p>Import errors: <pre><code># Make sure you're in the right directory\ncd steadytext/\n\n# Install in development mode\npip install -e .\n</code></pre></p> <p>Model download issues: <pre><code># Set environment variable\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Run tests\npoe test-models\n</code></pre></p>"},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Please be respectful and constructive in all interactions. We want SteadyText to be a welcoming project for everyone.</p>"},{"location":"contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - README.md: Major contributors listed - CHANGELOG.md: Contributions noted in releases - GitHub: Contributor graphs and statistics</p> <p>Thank you for contributing to SteadyText! \ud83d\ude80</p>"},{"location":"eos-string-implementation/","title":"EOS String Implementation Summary","text":"<p>This document summarizes the implementation of the custom <code>eos_string</code> parameter feature.</p>"},{"location":"eos-string-implementation/#changes-made","title":"Changes Made","text":""},{"location":"eos-string-implementation/#1-core-generation-module-steadytextcoregeneratorpy","title":"1. Core Generation Module (<code>steadytext/core/generator.py</code>)","text":"<ul> <li>Added <code>eos_string</code> parameter to <code>DeterministicGenerator.generate()</code> method</li> <li>Default value: <code>\"[EOS]\"</code> (special marker for model's default EOS token)</li> <li> <p>When custom value provided, it's added to the stop sequences</p> </li> <li> <p>Added <code>eos_string</code> parameter to <code>DeterministicGenerator.generate_iter()</code> method</p> </li> <li>Supports streaming generation with custom stop strings</li> <li> <p>Added <code>include_logprobs</code> parameter for compatibility with CLI</p> </li> <li> <p>Updated caching logic to include <code>eos_string</code> in cache key when not default</p> </li> <li>Ensures different eos_strings produce separately cached results</li> </ul>"},{"location":"eos-string-implementation/#2-public-api-steadytext__init__py","title":"2. Public API (<code>steadytext/__init__.py</code>)","text":"<ul> <li> <p>Updated <code>generate()</code> function signature:   <pre><code>def generate(prompt: str, return_logprobs: bool = False, eos_string: str = \"[EOS]\")\n</code></pre></p> </li> <li> <p>Updated <code>generate_iter()</code> function signature:   <pre><code>def generate_iter(prompt: str, eos_string: str = \"[EOS]\", include_logprobs: bool = False)\n</code></pre></p> </li> </ul>"},{"location":"eos-string-implementation/#3-cli-updates","title":"3. CLI Updates","text":""},{"location":"eos-string-implementation/#generate-command-steadytextclicommandsgeneratepy","title":"Generate Command (<code>steadytext/cli/commands/generate.py</code>)","text":"<ul> <li>Added <code>--eos-string</code> parameter (default: \"[EOS]\")</li> <li>Passes eos_string to both batch and streaming generation</li> </ul>"},{"location":"eos-string-implementation/#main-cli-steadytextclimainpy","title":"Main CLI (<code>steadytext/cli/main.py</code>)","text":"<ul> <li>Added <code>--quiet</code> / <code>-q</code> flag to silence log output</li> <li>Sets logging level to ERROR for both steadytext and llama_cpp loggers when quiet mode is enabled</li> </ul>"},{"location":"eos-string-implementation/#4-tests-teststest_steadytextpy","title":"4. Tests (<code>tests/test_steadytext.py</code>)","text":"<p>Added three new test methods: - <code>test_generate_with_custom_eos_string()</code> - Tests basic eos_string functionality - <code>test_generate_iter_with_eos_string()</code> - Tests streaming with custom eos_string - <code>test_generate_eos_string_with_logprobs()</code> - Tests combination of eos_string and logprobs</p>"},{"location":"eos-string-implementation/#5-test-scripts","title":"5. Test Scripts","text":"<p>Created two test scripts for manual verification: - <code>test_eos_string.py</code> - Python script testing various eos_string scenarios - <code>test_cli_eos.sh</code> - Bash script testing CLI functionality</p>"},{"location":"eos-string-implementation/#usage-examples","title":"Usage Examples","text":""},{"location":"eos-string-implementation/#python-api","title":"Python API","text":"<pre><code>import steadytext\n\n# Use model's default EOS token\ntext = steadytext.generate(\"Hello world\", eos_string=\"[EOS]\")\n\n# Stop at custom string\ntext = steadytext.generate(\"List items until END\", eos_string=\"END\")\n\n# Streaming with custom eos\nfor token in steadytext.generate_iter(\"Generate text\", eos_string=\"STOP\"):\n    print(token, end=\"\")\n</code></pre>"},{"location":"eos-string-implementation/#cli","title":"CLI","text":"<pre><code># Default behavior\nsteadytext \"Generate some text\"\n\n# Custom eos string\nsteadytext \"Generate until DONE\" --eos-string \"DONE\"\n\n# Quiet mode (no logs)\nsteadytext --quiet \"Generate without logs\"\n\n# Streaming with custom eos\nsteadytext \"Stream until END\" --stream --eos-string \"END\"\n</code></pre>"},{"location":"eos-string-implementation/#implementation-notes","title":"Implementation Notes","text":"<ol> <li> <p>The <code>\"[EOS]\"</code> string is a special marker that tells the system to use the model's default EOS token and stop sequences.</p> </li> <li> <p>When a custom eos_string is provided, it's added to the existing stop sequences rather than replacing them.</p> </li> <li> <p>Cache keys include the eos_string when it's not the default, ensuring proper caching behavior.</p> </li> <li> <p>The quiet flag affects all loggers in the steadytext namespace and llama_cpp if present.</p> </li> </ol>"},{"location":"model-switching/","title":"Model Switching in SteadyText","text":"<p>SteadyText v2.0.0+ supports dynamic model switching with the Gemma-3n model family, allowing you to use different model sizes without restarting your application.</p>"},{"location":"model-switching/#overview","title":"Overview","text":"<p>The model switching feature enables you to:</p> <ol> <li>Use different models for different tasks - Choose smaller models for speed or larger models for quality</li> <li>Switch models at runtime - No need to restart your application</li> <li>Maintain deterministic outputs - Each model produces consistent results</li> <li>Cache multiple models - Models are cached after first load for efficiency</li> </ol>"},{"location":"model-switching/#usage-methods","title":"Usage Methods","text":""},{"location":"model-switching/#1-using-size-parameter-new","title":"1. Using Size Parameter (New!)","text":"<p>The simplest way to choose a model based on your needs:</p> <pre><code>from steadytext import generate\n\n# Quick, lightweight tasks\ntext = generate(\"Simple task\", size=\"small\")   # Uses Gemma-3n-2B (default)\ntext = generate(\"Complex analysis\", size=\"large\")   # Uses Gemma-3n-4B\n</code></pre>"},{"location":"model-switching/#2-using-the-model-registry","title":"2. Using the Model Registry","text":"<p>For more specific model selection:</p> <pre><code>from steadytext import generate\n\n# Use a smaller, faster model\ntext = generate(\"Explain machine learning\", size=\"small\")   # Gemma-3n-2B\n\n# Use a larger, more capable model\ntext = generate(\"Write a detailed essay\", size=\"large\")    # Gemma-3n-4B\n</code></pre> <p>Available models in the registry (v2.0.0+):</p> Model Name Size Use Case Size Parameter <code>gemma-3n-2b</code> 2B Default, fast tasks <code>small</code> <code>gemma-3n-4b</code> 4B High quality, complex tasks <code>large</code> <p>Note: SteadyText v2.0.0+ focuses on the Gemma-3n model family. Previous versions (v1.x) supported Qwen models which are now deprecated.</p>"},{"location":"model-switching/#3-using-custom-models","title":"3. Using Custom Models","text":"<p>Specify any GGUF model from Hugging Face:</p> <pre><code>from steadytext import generate\n\n# Use a custom model\ntext = generate(\n    \"Create a Python function\",\n    model_repo=\"ggml-org/gemma-3n-E4B-it-GGUF\",\n    model_filename=\"gemma-3n-E4B-it-Q8_0.gguf\"\n)\n</code></pre>"},{"location":"model-switching/#4-using-environment-variables","title":"4. Using Environment Variables","text":"<p>Set default models via environment variables:</p> <pre><code># Use small model by default\nexport STEADYTEXT_DEFAULT_SIZE=\"small\"\n\n# Or specify custom model (advanced)\nexport STEADYTEXT_GENERATION_MODEL_REPO=\"ggml-org/gemma-3n-E2B-it-GGUF\"\nexport STEADYTEXT_GENERATION_MODEL_FILENAME=\"gemma-3n-E2B-it-Q8_0.gguf\"\n</code></pre>"},{"location":"model-switching/#streaming-generation","title":"Streaming Generation","text":"<p>Model switching works with streaming generation too:</p> <pre><code>from steadytext import generate_iter\n\n# Stream with a specific model size\nfor token in generate_iter(\"Tell me a story\", size=\"large\"):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"model-switching/#model-selection-guide","title":"Model Selection Guide","text":""},{"location":"model-switching/#for-speed-2b-model","title":"For Speed (2B model)","text":"<ul> <li>Use cases: Chat responses, simple completions, real-time applications</li> <li>Recommended: <code>gemma-3n-2b</code> (size=\"small\")</li> <li>Trade-off: Faster generation, simpler outputs</li> </ul>"},{"location":"model-switching/#for-quality-4b-model","title":"For Quality (4B model)","text":"<ul> <li>Use cases: Complex reasoning, detailed content, creative writing</li> <li>Recommended: <code>gemma-3n-4b</code> (size=\"large\")</li> <li>Trade-off: Best quality, slower generation</li> </ul>"},{"location":"model-switching/#performance-considerations","title":"Performance Considerations","text":"<ol> <li>First Load: The first use of a model downloads it (if not cached) and loads it into memory</li> <li>Model Caching: Once loaded, models remain in memory for fast switching</li> <li>Memory Usage: Each loaded model uses RAM - consider your available resources</li> <li>Determinism: All models maintain deterministic outputs with the same seed</li> </ol>"},{"location":"model-switching/#examples","title":"Examples","text":""},{"location":"model-switching/#adaptive-model-selection","title":"Adaptive Model Selection","text":"<pre><code>from steadytext import generate\n\ndef smart_generate(prompt, complexity=\"medium\"):\n    \"\"\"Use different models based on task complexity.\"\"\"\n    if complexity == \"low\":\n        # Use fast model for simple tasks\n        return generate(prompt, size=\"small\")\n    else:\n        # Use high-quality model for complex tasks\n        return generate(prompt, size=\"large\")\n</code></pre>"},{"location":"model-switching/#ab-testing-models","title":"A/B Testing Models","text":"<pre><code>from steadytext import generate\n\nprompts = [\"Explain quantum computing\", \"Write a haiku\", \"Solve 2+2\"]\n\nfor prompt in prompts:\n    print(f\"\\nPrompt: {prompt}\")\n\n    # Test with small model\n    small = generate(prompt, size=\"small\")\n    print(f\"Small model: {small[:100]}...\")\n\n    # Test with large model\n    large = generate(prompt, size=\"large\")\n    print(f\"Large model: {large[:100]}...\")\n</code></pre>"},{"location":"model-switching/#troubleshooting","title":"Troubleshooting","text":""},{"location":"model-switching/#model-not-found","title":"Model Not Found","text":"<p>If a model download fails, you'll get deterministic fallback text. Check: - Internet connection - Hugging Face availability - Model name spelling</p>"},{"location":"model-switching/#out-of-memory","title":"Out of Memory","text":"<p>Large models require significant RAM. Solutions: - Use smaller quantized models - Clear model cache with <code>clear_model_cache()</code> - Use one model at a time</p>"},{"location":"model-switching/#slow-first-load","title":"Slow First Load","text":"<p>Initial model loading takes time due to: - Downloading (first time only) - Loading into memory - Model initialization</p> <p>Subsequent uses are much faster as models are cached.</p>"},{"location":"quick-start/","title":"Quick Start Guide","text":"<p>Get started with SteadyText in minutes.</p>"},{"location":"quick-start/#installation","title":"Installation","text":"pipuvPoetry <pre><code>pip install steadytext\n</code></pre> <pre><code>uv add steadytext\n</code></pre> <pre><code>poetry add steadytext\n</code></pre>"},{"location":"quick-start/#first-steps","title":"First Steps","text":""},{"location":"quick-start/#1-basic-text-generation","title":"1. Basic Text Generation","text":"<pre><code>import steadytext\n\n# Generate deterministic text\ntext = steadytext.generate(\"Write a Python function to calculate fibonacci\")\nprint(text)\n</code></pre>"},{"location":"quick-start/#2-streaming-generation","title":"2. Streaming Generation","text":"<p>For real-time output:</p> <pre><code>for token in steadytext.generate_iter(\"Explain machine learning\"):\n    print(token, end=\"\", flush=True)\n</code></pre>"},{"location":"quick-start/#3-create-embeddings","title":"3. Create Embeddings","text":"<pre><code># Single text\nvector = steadytext.embed(\"Hello world\")\nprint(f\"Embedding shape: {vector.shape}\")  # (1024,)\n\n# Multiple texts (averaged)\nvector = steadytext.embed([\"Hello\", \"world\", \"AI\"])\n</code></pre>"},{"location":"quick-start/#command-line-usage","title":"Command Line Usage","text":"<p>SteadyText includes both <code>steadytext</code> and <code>st</code> commands:</p> <pre><code># Generate text\nst generate \"write a haiku about programming\"\n\n# Stream generation\nst generate \"explain quantum computing\" --stream\n\n# Create embeddings  \nst embed \"machine learning concepts\"\n\n# JSON output\nst generate \"list 3 colors\" --json\n\n# Preload models (optional)\nst models --preload\n</code></pre>"},{"location":"quick-start/#model-management","title":"Model Management","text":"<p>Models are automatically downloaded on first use to:</p> <ul> <li>Linux/Mac: <code>~/.cache/steadytext/models/</code></li> <li>Windows: <code>%LOCALAPPDATA%\\steadytext\\steadytext\\models\\</code></li> </ul> <pre><code># Check where models are stored\ncache_dir = steadytext.get_model_cache_dir()\nprint(f\"Models stored at: {cache_dir}\")\n\n# Preload models manually (optional)\nsteadytext.preload_models(verbose=True)\n</code></pre>"},{"location":"quick-start/#configuration","title":"Configuration","text":"<p>Control caching via environment variables:</p> <pre><code># Generation cache settings\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=512\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=100\n\n# Embedding cache settings  \nexport STEADYTEXT_EMBEDDING_CACHE_CAPACITY=1024\nexport STEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=200\n</code></pre>"},{"location":"quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>API Reference - Complete function documentation</li> <li>Examples - Real-world usage patterns</li> <li>CLI Reference - Command-line interface details</li> </ul>"},{"location":"quick-start/#need-help","title":"Need Help?","text":"<ul> <li>Issues: GitHub Issues</li> <li>Discussions: GitHub Discussions</li> </ul>"},{"location":"version_history/","title":"Version History","text":"<p>This document outlines the major versions of SteadyText and the key features introduced in each.</p> Version Key Features Default Generation Model Default Embedding Model Python Versions 2.x - Daemon Mode: Persistent model serving with ZeroMQ.- Gemma-3n Models: Switched to <code>gemma-3n</code> for generation.- Thinking Mode Deprecated: Removed thinking mode. <code>ggml-org/gemma-3n-E2B-it-GGUF</code> (gemma-3n-E2B-it-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) <code>&gt;=3.10, &lt;3.14</code> 1.x - Model Switching: Added support for switching models via environment variables and a model registry.- Qwen3 Models: Switched to <code>qwen3-1.7b</code> for generation.- Indexing: Added support for FAISS indexing. <code>Qwen/Qwen3-1.7B-GGUF</code> (Qwen3-1.7B-Q8_0.gguf) <code>Qwen/Qwen3-Embedding-0.6B-GGUF</code> (Qwen3-Embedding-0.6B-Q8_0.gguf) <code>&gt;=3.10, &lt;3.14</code> 0.x - Initial Release: Deterministic text generation and embedding. <code>Qwen/Qwen1.5-0.5B-Chat-GGUF</code> (qwen1_5-0_5b-chat-q4_k_m.gguf) <code>Qwen/Qwen1.5-0.5B-Chat-GGUF</code> (qwen1_5-0_5b-chat-q8_0.gguf) <code>&gt;=3.10</code>"},{"location":"api/","title":"API Reference","text":"<p>Complete documentation for SteadyText's Python API.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>SteadyText provides a simple, consistent API with two main functions:</p> <ul> <li><code>generate()</code> - Deterministic text generation</li> <li><code>embed()</code> - Deterministic embeddings</li> </ul> <p>All functions are designed to never fail - they return deterministic fallbacks when models can't be loaded.</p>"},{"location":"api/#quick-reference","title":"Quick Reference","text":"<pre><code>import steadytext\n\n# Text generation\ntext = steadytext.generate(\"your prompt\")\ntext, logprobs = steadytext.generate(\"prompt\", return_logprobs=True)\n\n# Streaming generation  \nfor token in steadytext.generate_iter(\"prompt\"):\n    print(token, end=\"\")\n\n# Embeddings\nvector = steadytext.embed(\"text to embed\")\nvectors = steadytext.embed([\"multiple\", \"texts\"])\n\n# Utilities\nsteadytext.preload_models()\ncache_dir = steadytext.get_model_cache_dir()\n</code></pre>"},{"location":"api/#detailed-documentation","title":"Detailed Documentation","text":"<ul> <li>Text Generation - <code>generate()</code> and <code>generate_iter()</code></li> <li>Embeddings - <code>embed()</code> function  </li> <li>CLI Reference - Command-line interface</li> <li>Vector Operations - CLI vector operations on embeddings</li> </ul>"},{"location":"api/#constants","title":"Constants","text":""},{"location":"api/#core-constants","title":"Core Constants","text":"<pre><code>steadytext.DEFAULT_SEED = 42\nsteadytext.GENERATION_MAX_NEW_TOKENS = 512  \nsteadytext.EMBEDDING_DIMENSION = 1024\n</code></pre>"},{"location":"api/#environment-variables","title":"Environment Variables","text":""},{"location":"api/#cache-configuration","title":"Cache Configuration","text":"<pre><code># Generation cache\nSTEADYTEXT_GENERATION_CACHE_CAPACITY=256      # max entries\nSTEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=50.0  # max file size\n\n# Embedding cache  \nSTEADYTEXT_EMBEDDING_CACHE_CAPACITY=512       # max entries\nSTEADYTEXT_EMBEDDING_CACHE_MAX_SIZE_MB=100.0  # max file size\n</code></pre>"},{"location":"api/#developmenttesting","title":"Development/Testing","text":"<pre><code># Allow model downloads (for testing)\nSTEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n</code></pre>"},{"location":"api/#error-handling","title":"Error Handling","text":"<p>SteadyText uses a \"never fail\" design philosophy:</p> <p>Deterministic Fallbacks</p> <ul> <li>Text generation: Uses hash-based word selection when models unavailable</li> <li>Embeddings: Returns zero vectors of correct dimensions</li> <li>No exceptions raised: Functions always return valid outputs</li> </ul> <p>This ensures your code works consistently across all environments, whether models are available or not.</p>"},{"location":"api/#thread-safety","title":"Thread Safety","text":"<p>All functions are thread-safe:</p> <ul> <li>Model loading uses singleton pattern with locks</li> <li>Caches are thread-safe with proper locking</li> <li>Multiple concurrent calls are supported</li> </ul>"},{"location":"api/#performance-notes","title":"Performance Notes","text":"<ul> <li>First call: May download models (~2GB total)</li> <li>Subsequent calls: Cached results when possible</li> <li>Memory usage: Models loaded once, cached in memory</li> <li>Disk cache: Frecency cache stores popular results</li> </ul>"},{"location":"api/cli/","title":"CLI Reference","text":"<p>Complete command-line interface documentation for SteadyText.</p>"},{"location":"api/cli/#installation","title":"Installation","text":"<p>The CLI is automatically installed with SteadyText:</p> <pre><code># Using UV (recommended)\nuv add steadytext\n\n# Or using pip\npip install steadytext\n</code></pre> <p>Two commands are available: - <code>steadytext</code> - Full command name - <code>st</code> - Short alias</p>"},{"location":"api/cli/#global-options","title":"Global Options","text":"<pre><code>st --version     # Show version\nst --help        # Show help\n</code></pre>"},{"location":"api/cli/#generate","title":"generate","text":"<p>Generate deterministic text from a prompt.</p>"},{"location":"api/cli/#usage","title":"Usage","text":"<pre><code># New pipe syntax (recommended)\necho \"prompt\" | st [OPTIONS]\necho \"prompt\" | steadytext [OPTIONS]\n\n# Legacy syntax (still supported)\nst generate [OPTIONS] PROMPT\nsteadytext generate [OPTIONS] PROMPT\n</code></pre>"},{"location":"api/cli/#options","title":"Options","text":"Option Short Type Default Description <code>--wait</code> <code>-w</code> flag <code>false</code> Wait for complete output (disable streaming) <code>--json</code> <code>-j</code> flag <code>false</code> Output as JSON with metadata <code>--logprobs</code> <code>-l</code> flag <code>false</code> Include log probabilities <code>--eos-string</code> <code>-e</code> string <code>\"[EOS]\"</code> Custom end-of-sequence string <code>--size</code> choice Model size: small (2B, default), large (4B) <code>--model</code> string Model name from registry (e.g., \"qwen2.5-3b\") <code>--model-repo</code> string Custom model repository <code>--model-filename</code> string Custom model filename <code>--no-index</code> flag <code>false</code> Disable automatic index search <code>--index-file</code> path Use specific index file <code>--top-k</code> int <code>3</code> Number of context chunks to retrieve"},{"location":"api/cli/#examples","title":"Examples","text":"Basic GenerationWait for Complete OutputJSON OutputWith Log ProbabilitiesCustom Stop StringUsing Size ParameterModel Selection <pre><code># New pipe syntax\necho \"Write a Python function to calculate fibonacci\" | st\n\n# Legacy syntax\nst generate \"Write a Python function to calculate fibonacci\"\n</code></pre> <pre><code># Disable streaming\necho \"Explain machine learning\" | st --wait\n</code></pre> <pre><code>st generate \"Hello world\" --json\n# Output:\n# {\n#   \"text\": \"Hello! How can I help you today?...\",\n#   \"tokens\": 15,\n#   \"cached\": false\n# }\n</code></pre> <pre><code>st generate \"Explain AI\" --logprobs --json\n# Includes token probabilities in JSON output\n</code></pre> <pre><code>st generate \"List colors until STOP\" --eos-string \"STOP\"\n</code></pre> <pre><code># Fast generation with small model\nst generate \"Quick response\" --size small\n\n# High quality with large model  \nst generate \"Complex analysis\" --size large\n</code></pre> <pre><code># Use specific model size\nst generate \"Technical explanation\" --size large\n\n# Use custom model (advanced)\nst generate \"Write code\" --model-repo ggml-org/gemma-3n-E4B-it-GGUF \\\n    --model-filename gemma-3n-E4B-it-Q8_0.gguf\n</code></pre>"},{"location":"api/cli/#stdin-support","title":"Stdin Support","text":"<p>Generate from stdin when no prompt provided:</p> <pre><code>echo \"Write a haiku\" | st generate\ncat prompts.txt | st generate --stream\n</code></pre>"},{"location":"api/cli/#embed","title":"embed","text":"<p>Create deterministic embeddings for text.</p>"},{"location":"api/cli/#usage_1","title":"Usage","text":"<pre><code>st embed [OPTIONS] TEXT\nsteadytext embed [OPTIONS] TEXT\n</code></pre>"},{"location":"api/cli/#options_1","title":"Options","text":"Option Short Type Default Description <code>--format</code> <code>-f</code> choice <code>json</code> Output format: <code>json</code>, <code>numpy</code>, <code>hex</code> <code>--output</code> <code>-o</code> path <code>-</code> Output file (default: stdout)"},{"location":"api/cli/#examples_1","title":"Examples","text":"Basic EmbeddingNumpy FormatHex FormatSave to File <pre><code>st embed \"machine learning\"\n# Outputs JSON array with 1024 float values\n</code></pre> <pre><code>st embed \"text to embed\" --format numpy\n# Outputs binary numpy array\n</code></pre> <pre><code>st embed \"hello world\" --format hex\n# Outputs hex-encoded float32 array\n</code></pre> <pre><code>st embed \"important text\" --output embedding.json\nst embed \"data\" --format numpy --output embedding.npy\n</code></pre>"},{"location":"api/cli/#stdin-support_1","title":"Stdin Support","text":"<p>Embed text from stdin:</p> <pre><code>echo \"text to embed\" | st embed\ncat document.txt | st embed --format numpy --output doc_embedding.npy\n</code></pre>"},{"location":"api/cli/#models","title":"models","text":"<p>Manage SteadyText models.</p>"},{"location":"api/cli/#usage_2","title":"Usage","text":"<pre><code>st models [OPTIONS]\nsteadytext models [OPTIONS]\n</code></pre>"},{"location":"api/cli/#options_2","title":"Options","text":"Option Short Description <code>--list</code> <code>-l</code> List available models <code>--preload</code> <code>-p</code> Preload all models <code>--cache-dir</code> Show model cache directory <code>--json</code> flag <code>false</code>"},{"location":"api/cli/#commands","title":"Commands","text":"Command Description <code>status</code> Check model download status <code>list</code> List available models <code>download</code> Pre-download models <code>delete</code> Delete cached models <code>preload</code> Preload models into memory <code>path</code> Show model cache directory"},{"location":"api/cli/#examples_2","title":"Examples","text":"List ModelsDownload ModelsDelete ModelsPreload ModelsCache Information <pre><code>st models list\n# Output:\n# Size Shortcuts:\n#   small \u2192 gemma-3n-2b\n#   large \u2192 gemma-3n-4b\n#\n# Available Models:\n#   gemma-3n-2b\n#     Repository: ggml-org/gemma-3n-E2B-it-GGUF\n#     Filename: gemma-3n-E2B-it-Q8_0.gguf\n#   gemma-3n-4b\n#     Repository: ggml-org/gemma-3n-E4B-it-GGUF\n#     Filename: gemma-3n-E4B-it-Q8_0.gguf\n</code></pre> <pre><code># Download default models\nst models download\n\n# Download by size\nst models download --size small\n\n# Download by name\nst models download --model gemma-3n-4b\n\n# Download all models\nst models download --all\n</code></pre> <pre><code># Delete by size\nst models delete --size small\n\n# Delete by name\nst models delete --model gemma-3n-4b\n\n# Delete all models with confirmation\nst models delete --all\n\n# Force delete all models without confirmation\nst models delete --all --force\n</code></pre> <pre><code>st models preload\n# Downloads and loads all models\n</code></pre> <pre><code>st models path\n# /home/user/.cache/steadytext/models/\n\nst models status\n# {\n#   \"model_directory\": \"/home/user/.cache/steadytext/models\",\n#   \"models\": { ... }\n# }\n</code></pre>"},{"location":"api/cli/#vector","title":"vector","text":"<p>Perform vector operations on embeddings.</p>"},{"location":"api/cli/#usage_3","title":"Usage","text":"<pre><code>st vector COMMAND [OPTIONS]\nsteadytext vector COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_1","title":"Commands","text":"Command Description <code>similarity</code> Compute similarity between text embeddings <code>distance</code> Compute distance between text embeddings <code>search</code> Find most similar texts from candidates <code>average</code> Compute average of multiple embeddings <code>arithmetic</code> Perform vector arithmetic operations"},{"location":"api/cli/#examples_3","title":"Examples","text":"SimilarityDistanceSearchAverageArithmetic <pre><code># Cosine similarity\nst vector similarity \"cat\" \"dog\"\n# 0.823456\n\n# With JSON output\nst vector similarity \"king\" \"queen\" --json\n</code></pre> <pre><code># Euclidean distance\nst vector distance \"hot\" \"cold\"\n\n# Manhattan distance\nst vector distance \"yes\" \"no\" --metric manhattan\n</code></pre> <pre><code># Find similar from stdin\necho -e \"apple\\norange\\ncar\" | st vector search \"fruit\" --stdin\n\n# From file, top 3\nst vector search \"python\" --candidates langs.txt --top 3\n</code></pre> <pre><code># Average embeddings\nst vector average \"cat\" \"dog\" \"hamster\"\n\n# With full embedding output\nst vector average \"red\" \"green\" \"blue\" --json\n</code></pre> <pre><code># Classic analogy: king + woman - man \u2248 queen\nst vector arithmetic \"king\" \"woman\" --subtract \"man\"\n\n# Location arithmetic\nst vector arithmetic \"paris\" \"italy\" --subtract \"france\"\n</code></pre> <p>See Vector Operations Documentation for detailed usage.</p>"},{"location":"api/cli/#cache","title":"cache","text":"<p>Manage result caches.</p>"},{"location":"api/cli/#usage_4","title":"Usage","text":"<pre><code>st cache [OPTIONS]\nsteadytext cache [OPTIONS]\n</code></pre>"},{"location":"api/cli/#options_3","title":"Options","text":"Option Short Description <code>--clear</code> <code>-c</code> Clear all caches <code>--status</code> <code>-s</code> Show cache status <code>--generation-only</code> Target only generation cache <code>--embedding-only</code> Target only embedding cache"},{"location":"api/cli/#examples_4","title":"Examples","text":"Cache StatusClear Caches <pre><code>st cache --status\n# Generation Cache: 45 entries, 12.3MB\n# Embedding Cache: 128 entries, 34.7MB\n</code></pre> <pre><code>st cache --clear\n# Cleared all caches\n\nst cache --clear --generation-only\n# Cleared generation cache only\n</code></pre>"},{"location":"api/cli/#daemon","title":"daemon","text":"<p>Manage the SteadyText daemon for persistent model serving.</p>"},{"location":"api/cli/#usage_5","title":"Usage","text":"<pre><code>st daemon COMMAND [OPTIONS]\nsteadytext daemon COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_2","title":"Commands","text":"Command Description <code>start</code> Start the daemon server <code>stop</code> Stop the daemon server <code>status</code> Check daemon status <code>restart</code> Restart the daemon server"},{"location":"api/cli/#options_4","title":"Options","text":""},{"location":"api/cli/#start","title":"start","text":"Option Type Default Description <code>--host</code> string <code>127.0.0.1</code> Bind address <code>--port</code> int <code>5557</code> Port number <code>--foreground</code> flag <code>false</code> Run in foreground"},{"location":"api/cli/#stop","title":"stop","text":"Option Type Default Description <code>--force</code> flag <code>false</code> Force kill if graceful shutdown fails"},{"location":"api/cli/#status","title":"status","text":"Option Type Default Description <code>--json</code> flag <code>false</code> Output as JSON"},{"location":"api/cli/#examples_5","title":"Examples","text":"Start DaemonCheck StatusStop/Restart <pre><code># Start in background (default)\nst daemon start\n\n# Start in foreground for debugging\nst daemon start --foreground\n\n# Custom host/port\nst daemon start --host 0.0.0.0 --port 8080\n</code></pre> <pre><code>st daemon status\n# Output: Daemon is running (PID: 12345)\n\n# JSON output\nst daemon status --json\n# {\"running\": true, \"pid\": 12345, \"host\": \"127.0.0.1\", \"port\": 5557}\n</code></pre> <pre><code># Graceful stop\nst daemon stop\n\n# Force stop\nst daemon stop --force\n\n# Restart\nst daemon restart\n</code></pre>"},{"location":"api/cli/#benefits","title":"Benefits","text":"<ul> <li>160x faster first request: No model loading overhead</li> <li>Persistent cache: Shared across all operations</li> <li>Automatic fallback: Operations work without daemon</li> <li>Zero configuration: Used by default when available</li> </ul>"},{"location":"api/cli/#index","title":"index","text":"<p>Manage FAISS vector indexes for retrieval-augmented generation.</p>"},{"location":"api/cli/#usage_6","title":"Usage","text":"<pre><code>st index COMMAND [OPTIONS]\nsteadytext index COMMAND [OPTIONS]\n</code></pre>"},{"location":"api/cli/#commands_3","title":"Commands","text":"Command Description <code>create</code> Create index from text files <code>search</code> Search index for similar chunks <code>info</code> Show index information"},{"location":"api/cli/#options_5","title":"Options","text":""},{"location":"api/cli/#create","title":"create","text":"Option Type Default Description <code>--output</code> path required Output index file <code>--chunk-size</code> int <code>512</code> Chunk size in tokens <code>--glob</code> string File glob pattern"},{"location":"api/cli/#search","title":"search","text":"Option Type Default Description <code>--top-k</code> int <code>5</code> Number of results <code>--threshold</code> float Similarity threshold"},{"location":"api/cli/#examples_6","title":"Examples","text":"Create IndexSearch IndexIndex Info <pre><code># From specific files\nst index create doc1.txt doc2.txt --output docs.faiss\n\n# From glob pattern\nst index create --glob \"**/*.md\" --output project.faiss\n\n# Custom chunk size\nst index create *.txt --output custom.faiss --chunk-size 256\n</code></pre> <pre><code># Basic search\nst index search docs.faiss \"query text\"\n\n# Top 10 results\nst index search docs.faiss \"error message\" --top-k 10\n\n# With threshold\nst index search docs.faiss \"specific term\" --threshold 0.8\n</code></pre> <pre><code>st index info docs.faiss\n# Output:\n# Index: docs.faiss\n# Chunks: 1,234\n# Dimension: 1024\n# Size: 5.2MB\n</code></pre>"},{"location":"api/cli/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/cli/#environment-variables","title":"Environment Variables","text":"<p>Set these before running CLI commands:</p> <pre><code># Cache configuration\nexport STEADYTEXT_GENERATION_CACHE_CAPACITY=512\nexport STEADYTEXT_GENERATION_CACHE_MAX_SIZE_MB=100\n\n# Allow model downloads (for development)\nexport STEADYTEXT_ALLOW_MODEL_DOWNLOADS=true\n\n# Then run commands\nst generate \"test prompt\"\n</code></pre>"},{"location":"api/cli/#pipeline-usage","title":"Pipeline Usage","text":"<p>Chain commands with other tools:</p> <pre><code># Batch processing\ncat prompts.txt | while read prompt; do\n  echo \"Prompt: $prompt\"\n  st generate \"$prompt\" --json | jq '.text'\n  echo \"---\"\ndone\n\n# Generate and embed\ntext=$(st generate \"explain AI\")\necho \"$text\" | st embed --format hex &gt; ai_explanation.hex\n</code></pre>"},{"location":"api/cli/#scripting-examples","title":"Scripting Examples","text":"Bash ScriptPython Integration <pre><code>#!/bin/bash\n# generate_docs.sh\n\nprompts=(\n  \"Explain machine learning\"\n  \"What is deep learning?\"\n  \"Define neural networks\"\n)\n\nfor prompt in \"${prompts[@]}\"; do\n  echo \"=== $prompt ===\"\n  st generate \"$prompt\" --stream\n  echo -e \"\\n---\\n\"\ndone\n</code></pre> <pre><code>import subprocess\nimport json\n\ndef cli_generate(prompt):\n    \"\"\"Use CLI from Python.\"\"\"\n    result = subprocess.run([\n        'st', 'generate', prompt, '--json'\n    ], capture_output=True, text=True)\n\n    return json.loads(result.stdout)\n\n# Usage\nresult = cli_generate(\"Hello world\")\nprint(result['text'])\n</code></pre>"},{"location":"api/cli/#performance-tips","title":"Performance Tips","text":"<p>CLI Optimization</p> <ul> <li>Preload models: Run <code>st models --preload</code> once at startup</li> <li>Use JSON output: Easier to parse in scripts with <code>--json</code></li> <li>Batch operations: Process multiple items in single session</li> <li>Cache warmup: Generate common prompts to populate cache</li> </ul>"},{"location":"api/embedding/","title":"Embeddings API","text":"<p>Functions for creating deterministic text embeddings.</p>"},{"location":"api/embedding/#embed","title":"embed()","text":"<p>Create deterministic embeddings for text input.</p> <pre><code>def embed(text_input: Union[str, List[str]]) -&gt; np.ndarray\n</code></pre>"},{"location":"api/embedding/#parameters","title":"Parameters","text":"Parameter Type Description <code>text_input</code> <code>Union[str, List[str]]</code> Text string or list of strings to embed"},{"location":"api/embedding/#returns","title":"Returns","text":"<p>Returns: <code>np.ndarray</code> - 1024-dimensional L2-normalized float32 array</p>"},{"location":"api/embedding/#examples","title":"Examples","text":"Single TextMultiple TextsSimilarity Comparison <pre><code>import steadytext\nimport numpy as np\n\n# Embed single text\nvector = steadytext.embed(\"Hello world\")\n\nprint(f\"Shape: {vector.shape}\")        # (1024,)\nprint(f\"Type: {vector.dtype}\")         # float32\nprint(f\"Norm: {np.linalg.norm(vector):.6f}\")  # 1.000000 (L2 normalized)\n</code></pre> <pre><code># Embed multiple texts (averaged)\ntexts = [\"machine learning\", \"artificial intelligence\", \"deep learning\"]\nvector = steadytext.embed(texts)\n\nprint(f\"Combined embedding shape: {vector.shape}\")  # (1024,)\n# Result is averaged across all input texts\n</code></pre> <pre><code>import numpy as np\n\n# Create embeddings for comparison\nvec1 = steadytext.embed(\"machine learning\")\nvec2 = steadytext.embed(\"artificial intelligence\") \nvec3 = steadytext.embed(\"cooking recipes\")\n\n# Calculate cosine similarity\ndef cosine_similarity(a, b):\n    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n\nsim_ml_ai = cosine_similarity(vec1, vec2)\nsim_ml_cooking = cosine_similarity(vec1, vec3)\n\nprint(f\"ML vs AI similarity: {sim_ml_ai:.3f}\")\nprint(f\"ML vs Cooking similarity: {sim_ml_cooking:.3f}\")\n# ML and AI should have higher similarity than ML and cooking\n</code></pre>"},{"location":"api/embedding/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/embedding/#deterministic-behavior","title":"Deterministic Behavior","text":"<p>Embeddings are completely deterministic:</p> <pre><code># These will always be identical\nvec1 = steadytext.embed(\"test text\")\nvec2 = steadytext.embed(\"test text\")\n\nassert np.array_equal(vec1, vec2)  # Always passes!\nassert np.allclose(vec1, vec2)     # Always passes!\n</code></pre>"},{"location":"api/embedding/#preprocessing","title":"Preprocessing","text":"<p>Text is automatically preprocessed before embedding:</p> <pre><code># These produce different embeddings due to different text\nvec1 = steadytext.embed(\"Hello World\")\nvec2 = steadytext.embed(\"hello world\")\nvec3 = steadytext.embed(\"HELLO WORLD\")\n\n# Case sensitivity matters\nassert not np.array_equal(vec1, vec2)\n</code></pre>"},{"location":"api/embedding/#batch-processing","title":"Batch Processing","text":"<p>For multiple texts, pass as a list:</p> <pre><code># Individual embeddings\nvec1 = steadytext.embed(\"first text\")\nvec2 = steadytext.embed(\"second text\") \nvec3 = steadytext.embed(\"third text\")\n\n# Batch embedding (averaged)\nvec_batch = steadytext.embed([\"first text\", \"second text\", \"third text\"])\n\n# The batch result is the average of individual embeddings\nexpected = (vec1 + vec2 + vec3) / 3\nassert np.allclose(vec_batch, expected, atol=1e-6)\n</code></pre>"},{"location":"api/embedding/#caching","title":"Caching","text":"<p>Embeddings are cached for performance:</p> <pre><code># First call: computes and caches embedding\nvec1 = steadytext.embed(\"common text\")  # ~0.5 seconds\n\n# Second call: returns cached result\nvec2 = steadytext.embed(\"common text\")  # ~0.01 seconds\n\nassert np.array_equal(vec1, vec2)  # Same result, much faster\n</code></pre>"},{"location":"api/embedding/#fallback-behavior","title":"Fallback Behavior","text":"<p>When models can't be loaded, zero vectors are returned:</p> <pre><code># Even without models, function never fails\nvector = steadytext.embed(\"any text\")\n\nassert vector.shape == (1024,)     # Correct shape\nassert vector.dtype == np.float32  # Correct type\nassert np.linalg.norm(vector) == 0 # Zero vector fallback\n</code></pre>"},{"location":"api/embedding/#use-cases","title":"Use Cases","text":""},{"location":"api/embedding/#document-similarity","title":"Document Similarity","text":"<pre><code>import steadytext\nimport numpy as np\n\ndef document_similarity(doc1: str, doc2: str) -&gt; float:\n    \"\"\"Calculate similarity between two documents.\"\"\"\n    vec1 = steadytext.embed(doc1)\n    vec2 = steadytext.embed(doc2)\n    return np.dot(vec1, vec2)  # Already L2 normalized\n\n# Usage\nsimilarity = document_similarity(\n    \"Machine learning algorithms\",\n    \"AI and neural networks\"\n)\nprint(f\"Similarity: {similarity:.3f}\")\n</code></pre>"},{"location":"api/embedding/#semantic-search","title":"Semantic Search","text":"<pre><code>def semantic_search(query: str, documents: List[str], top_k: int = 5):\n    \"\"\"Find most similar documents to query.\"\"\"\n    query_vec = steadytext.embed(query)\n    doc_vecs = [steadytext.embed(doc) for doc in documents]\n\n    similarities = [np.dot(query_vec, doc_vec) for doc_vec in doc_vecs]\n    top_indices = np.argsort(similarities)[-top_k:][::-1]\n\n    return [(documents[i], similarities[i]) for i in top_indices]\n\n# Usage  \ndocs = [\"AI research\", \"Machine learning\", \"Cooking recipes\", \"Data science\"]\nresults = semantic_search(\"artificial intelligence\", docs, top_k=2)\n\nfor doc, score in results:\n    print(f\"{doc}: {score:.3f}\")\n</code></pre>"},{"location":"api/embedding/#clustering","title":"Clustering","text":"<pre><code>from sklearn.cluster import KMeans\nimport numpy as np\n\ndef cluster_texts(texts: List[str], n_clusters: int = 3):\n    \"\"\"Cluster texts using their embeddings.\"\"\"\n    embeddings = np.array([steadytext.embed(text) for text in texts])\n\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n    clusters = kmeans.fit_predict(embeddings)\n\n    return clusters\n\n# Usage\ntexts = [\n    \"machine learning\", \"deep learning\", \"neural networks\",  # AI cluster\n    \"pizza recipe\", \"pasta cooking\", \"italian food\",        # Food cluster  \n    \"stock market\", \"trading\", \"investment\"                 # Finance cluster\n]\n\nclusters = cluster_texts(texts, n_clusters=3)\nfor text, cluster in zip(texts, clusters):\n    print(f\"Cluster {cluster}: {text}\")\n</code></pre>"},{"location":"api/embedding/#performance-notes","title":"Performance Notes","text":"<p>Optimization Tips</p> <ul> <li>Preload models: Call <code>steadytext.preload_models()</code> at startup</li> <li>Batch similar texts: Group related texts together for cache efficiency  </li> <li>Memory usage: ~610MB for embedding model (loaded once)</li> <li>Speed: ~100-500 embeddings/second depending on text length</li> </ul>"},{"location":"api/generation/","title":"Text Generation API","text":"<p>Functions for deterministic text generation.</p>"},{"location":"api/generation/#generate","title":"generate()","text":"<p>Generate deterministic text from a prompt.</p> <pre><code>def generate(\n    prompt: str,\n    return_logprobs: bool = False,\n    eos_string: str = \"[EOS]\"\n) -&gt; Union[str, Tuple[str, Optional[Dict[str, Any]]]]\n</code></pre>"},{"location":"api/generation/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>prompt</code> <code>str</code> required Input text to generate from <code>return_logprobs</code> <code>bool</code> <code>False</code> Return log probabilities with text <code>eos_string</code> <code>str</code> <code>\"[EOS]\"</code> Custom end-of-sequence string"},{"location":"api/generation/#returns","title":"Returns","text":"Basic UsageWith Log Probabilities <p>Returns: <code>str</code> - Generated text (512 tokens max)</p> <p>Returns: <code>Tuple[str, Optional[Dict]]</code> - Generated text and log probabilities</p>"},{"location":"api/generation/#examples","title":"Examples","text":"Simple GenerationWith Log ProbabilitiesCustom Stop String <pre><code>import steadytext\n\ntext = steadytext.generate(\"Write a Python function\")\nprint(text)\n# Always returns the same 512-token completion\n</code></pre> <pre><code>text, logprobs = steadytext.generate(\n    \"Explain machine learning\", \n    return_logprobs=True\n)\n\nprint(\"Generated text:\", text)\nprint(\"Log probabilities:\", logprobs)\n</code></pre> <pre><code># Stop generation at custom string\ntext = steadytext.generate(\n    \"List programming languages until STOP\",\n    eos_string=\"STOP\"\n)\nprint(text)\n</code></pre>"},{"location":"api/generation/#generate_iter","title":"generate_iter()","text":"<p>Generate text iteratively, yielding tokens as produced.</p> <pre><code>def generate_iter(\n    prompt: str,\n    eos_string: str = \"[EOS]\",\n    include_logprobs: bool = False\n) -&gt; Iterator[Union[str, Tuple[str, Optional[Dict[str, Any]]]]]\n</code></pre>"},{"location":"api/generation/#parameters_1","title":"Parameters","text":"Parameter Type Default Description <code>prompt</code> <code>str</code> required Input text to generate from <code>eos_string</code> <code>str</code> <code>\"[EOS]\"</code> Custom end-of-sequence string <code>include_logprobs</code> <code>bool</code> <code>False</code> Yield log probabilities with tokens"},{"location":"api/generation/#returns_1","title":"Returns","text":"Basic StreamingWith Log Probabilities <p>Yields: <code>str</code> - Individual tokens/words</p> <p>Yields: <code>Tuple[str, Optional[Dict]]</code> - Token and log probabilities</p>"},{"location":"api/generation/#examples_1","title":"Examples","text":"Basic StreamingWith Progress TrackingCustom Stop StringWith Log Probabilities <pre><code>import steadytext\n\nfor token in steadytext.generate_iter(\"Tell me a story\"):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code>prompt = \"Explain quantum computing\"\ntokens = []\n\nfor token in steadytext.generate_iter(prompt):\n    tokens.append(token)\n    print(f\"Generated {len(tokens)} tokens\", end=\"\\r\")\n\nprint(f\"\\nComplete! Generated {len(tokens)} tokens\")\nprint(\"Full text:\", \"\".join(tokens))\n</code></pre> <pre><code>for token in steadytext.generate_iter(\n    \"Count from 1 to 10 then say DONE\", \n    eos_string=\"DONE\"\n):\n    print(token, end=\"\", flush=True)\n</code></pre> <pre><code>for token, logprobs in steadytext.generate_iter(\n    \"Explain AI\", \n    include_logprobs=True\n):\n    confidence = logprobs.get('confidence', 0) if logprobs else 0\n    print(f\"{token} (confidence: {confidence:.2f})\", end=\"\")\n</code></pre>"},{"location":"api/generation/#advanced-usage","title":"Advanced Usage","text":""},{"location":"api/generation/#deterministic-behavior","title":"Deterministic Behavior","text":"<p>Both functions return identical results for identical inputs:</p> <pre><code># These will always be identical\nresult1 = steadytext.generate(\"hello world\")\nresult2 = steadytext.generate(\"hello world\") \nassert result1 == result2  # Always passes!\n\n# Streaming produces same tokens in same order\ntokens1 = list(steadytext.generate_iter(\"hello world\"))\ntokens2 = list(steadytext.generate_iter(\"hello world\"))\nassert tokens1 == tokens2  # Always passes!\n</code></pre>"},{"location":"api/generation/#caching","title":"Caching","text":"<p>Results are automatically cached using a frecency cache (LRU + frequency):</p> <pre><code># First call: generates and caches result\ntext1 = steadytext.generate(\"common prompt\")  # ~2 seconds\n\n# Second call: returns cached result  \ntext2 = steadytext.generate(\"common prompt\")  # ~0.1 seconds\n\nassert text1 == text2  # Same result, much faster\n</code></pre>"},{"location":"api/generation/#fallback-behavior","title":"Fallback Behavior","text":"<p>When models can't be loaded, deterministic fallbacks are used:</p> <pre><code># Even without models, these always return the same results\ntext = steadytext.generate(\"test prompt\")  # Hash-based fallback\nassert len(text) &gt; 0  # Always has content\n\n# Fallback is also deterministic\ntext1 = steadytext.generate(\"fallback test\")\ntext2 = steadytext.generate(\"fallback test\") \nassert text1 == text2  # Same fallback result\n</code></pre>"},{"location":"api/generation/#performance-tips","title":"Performance Tips","text":"<p>Optimization Strategies</p> <ul> <li>Preload models: Call <code>steadytext.preload_models()</code> at startup</li> <li>Batch processing: Use <code>generate()</code> for multiple prompts rather than streaming individual tokens</li> <li>Cache warmup: Pre-generate common prompts to populate cache</li> <li>Memory management: Models stay loaded once initialized (singleton pattern)</li> </ul>"},{"location":"api/vector/","title":"Vector Operations CLI","text":"<p>The <code>vector</code> command group provides various operations on text embeddings, enabling similarity comparisons, searches, and arithmetic operations.</p>"},{"location":"api/vector/#overview","title":"Overview","text":"<p>All vector operations work with SteadyText's 1024-dimensional L2-normalized embeddings. These embeddings are deterministic - the same text always produces the same embedding vector.</p>"},{"location":"api/vector/#commands","title":"Commands","text":""},{"location":"api/vector/#similarity","title":"similarity","text":"<p>Compute similarity between two text embeddings.</p> <pre><code>st vector similarity TEXT1 TEXT2 [OPTIONS]\n</code></pre> <p>Options: - <code>--metric [cosine|dot]</code>: Similarity metric to use (default: cosine) - <code>--json</code>: Output as JSON</p> <p>Examples: <pre><code># Basic cosine similarity\nst vector similarity \"apple\" \"orange\"\n# Output: 0.875432\n\n# Dot product similarity\nst vector similarity \"king\" \"queen\" --metric dot\n\n# JSON output\nst vector similarity \"hello\" \"world\" --json\n</code></pre></p>"},{"location":"api/vector/#distance","title":"distance","text":"<p>Compute distance between two text embeddings.</p> <pre><code>st vector distance TEXT1 TEXT2 [OPTIONS]\n</code></pre> <p>Options: - <code>--metric [euclidean|manhattan|cosine]</code>: Distance metric (default: euclidean) - <code>--json</code>: Output as JSON</p> <p>Examples: <pre><code># Euclidean distance\nst vector distance \"cat\" \"dog\"\n\n# Manhattan (L1) distance\nst vector distance \"yes\" \"no\" --metric manhattan\n\n# Cosine distance (1 - cosine_similarity)\nst vector distance \"hot\" \"cold\" --metric cosine --json\n</code></pre></p>"},{"location":"api/vector/#search","title":"search","text":"<p>Find the most similar texts from a list of candidates.</p> <pre><code>st vector search QUERY [OPTIONS]\n</code></pre> <p>Options: - <code>--candidates PATH</code>: File containing candidate texts (one per line) - <code>--stdin</code>: Read candidates from stdin - <code>--top N</code>: Number of top results to return (default: 1) - <code>--metric [cosine|euclidean]</code>: Similarity/distance metric (default: cosine) - <code>--json</code>: Output as JSON</p> <p>Examples: <pre><code># Search from stdin\necho -e \"apple\\norange\\ncar\\ntruck\" | st vector search \"fruit\" --stdin\n\n# Search from file, get top 3\nst vector search \"python\" --candidates languages.txt --top 3\n\n# Search with euclidean distance\nst vector search \"query\" --stdin --metric euclidean\n</code></pre></p>"},{"location":"api/vector/#average","title":"average","text":"<p>Compute the average of multiple text embeddings.</p> <pre><code>st vector average TEXT1 TEXT2 [TEXT3...] [OPTIONS]\n</code></pre> <p>Options: - <code>--json</code>: Output as JSON with full embedding</p> <p>Examples: <pre><code># Average animal embeddings\nst vector average \"cat\" \"dog\" \"hamster\"\n\n# Average programming languages\nst vector average \"python\" \"javascript\" \"rust\" --json\n</code></pre></p>"},{"location":"api/vector/#arithmetic","title":"arithmetic","text":"<p>Perform vector arithmetic operations on embeddings.</p> <pre><code>st vector arithmetic BASE [ADD_TERMS...] [OPTIONS]\n</code></pre> <p>Options: - <code>--subtract TEXT</code>: Terms to subtract (can be used multiple times) - <code>--normalize/--no-normalize</code>: Whether to L2 normalize result (default: normalize) - <code>--json</code>: Output as JSON</p> <p>Examples: <pre><code># Classic word analogy: king + woman - man \u2248 queen\nst vector arithmetic \"king\" \"woman\" --subtract \"man\"\n\n# Location arithmetic: paris - france + italy \u2248 rome\nst vector arithmetic \"paris\" \"italy\" --subtract \"france\"\n\n# Multiple additions\nst vector arithmetic \"good\" \"better\" \"best\"\n\n# Without normalization\nst vector arithmetic \"hot\" --subtract \"cold\" --no-normalize\n</code></pre></p>"},{"location":"api/vector/#output-formats","title":"Output Formats","text":""},{"location":"api/vector/#default-output","title":"Default Output","text":"<ul> <li>similarity/distance: Single floating-point number</li> <li>search: Tab-separated lines of text and score</li> <li>average/arithmetic: First 50 values of resulting vector</li> </ul>"},{"location":"api/vector/#json-output","title":"JSON Output","text":"<p>All commands support <code>--json</code> flag for structured output suitable for programmatic use.</p>"},{"location":"api/vector/#technical-details","title":"Technical Details","text":"<ul> <li>Embedding Dimension: 1024</li> <li>Normalization: All embeddings are L2-normalized (unit vectors)</li> <li>Determinism: Same input always produces same output</li> <li>Precision: Float32 for all calculations</li> <li>Cosine Similarity: Since vectors are normalized, computed as simple dot product</li> </ul>"},{"location":"api/vector/#use-cases","title":"Use Cases","text":"<ol> <li>Semantic Search: Find related concepts from a database</li> <li>Document Similarity: Compare similarity between texts</li> <li>Clustering: Group similar items using distance metrics</li> <li>Analogies: Explore semantic relationships with vector arithmetic</li> <li>Concept Interpolation: Create blended concepts with averaging</li> </ol>"},{"location":"examples/","title":"Examples","text":"<p>Real-world usage patterns and code examples for SteadyText.</p>"},{"location":"examples/#overview","title":"Overview","text":"<p>This section demonstrates practical applications of SteadyText across different use cases:</p> <ul> <li>Testing with AI - Reliable AI tests that never flake</li> <li>CLI Tools - Building deterministic command-line tools</li> </ul> <p>All examples showcase SteadyText's core principle: same input \u2192 same output, every time.</p>"},{"location":"examples/#quick-examples","title":"Quick Examples","text":""},{"location":"examples/#basic-usage","title":"Basic Usage","text":"<pre><code>import steadytext\n\n# Deterministic text generation\ncode = steadytext.generate(\"implement binary search in Python\")\nassert \"def binary_search\" in code  # Always passes!\n\n# Streaming generation\nfor token in steadytext.generate_iter(\"explain quantum computing\"):\n    print(token, end=\"\", flush=True)\n\n# Deterministic embeddings  \nvec = steadytext.embed(\"Hello world\")  # 1024-dim numpy array\nprint(f\"Shape: {vec.shape}, Norm: {np.linalg.norm(vec):.6f}\")\n</code></pre>"},{"location":"examples/#testing-applications","title":"Testing Applications","text":"<pre><code>def test_ai_code_generation():\n    \"\"\"Test that never flakes - same input, same output.\"\"\"\n    prompt = \"write a function to reverse a string\"\n    result = my_ai_function(prompt)\n    expected = steadytext.generate(prompt)\n    assert result == expected  # Deterministic comparison!\n\ndef test_embedding_similarity():\n    \"\"\"Reliable similarity testing.\"\"\"\n    vec1 = steadytext.embed(\"machine learning\")\n    vec2 = steadytext.embed(\"artificial intelligence\")\n    similarity = np.dot(vec1, vec2)  # Already normalized\n    assert similarity &gt; 0.7  # Always passes with same threshold\n</code></pre>"},{"location":"examples/#cli-tool-building","title":"CLI Tool Building","text":"<pre><code>import click\nimport steadytext\n\n@click.command()\n@click.argument('topic')\ndef motivate(topic):\n    \"\"\"Generate motivational quotes about any topic.\"\"\"\n    prompt = f\"Write an inspiring quote about {topic}\"\n    quote = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udcaa {quote}\")\n\n# Usage: python script.py \"programming\"\n# Always generates the same motivational quote for \"programming\"\n</code></pre>"},{"location":"examples/#use-case-categories","title":"Use Case Categories","text":""},{"location":"examples/#testing-quality-assurance","title":"\ud83e\uddea Testing &amp; Quality Assurance","text":"<p>Perfect for: - Unit tests with AI components - Integration testing with deterministic outputs - Regression testing for AI features - Mock AI services for development</p>"},{"location":"examples/#developer-tools","title":"\ud83d\udee0\ufe0f Developer Tools","text":"<p>Ideal for: - Code generation tools - Documentation generators - CLI utilities with AI features - Build system integration</p>"},{"location":"examples/#data-content-generation","title":"\ud83d\udcca Data &amp; Content Generation","text":"<p>Great for: - Synthetic data generation - Content templates - Data augmentation for testing - Reproducible research datasets</p>"},{"location":"examples/#search-similarity","title":"\ud83d\udd0d Search &amp; Similarity","text":"<p>Excellent for: - Semantic search systems - Document clustering - Content recommendation - Duplicate detection</p>"},{"location":"examples/#getting-started","title":"Getting Started","text":"<ol> <li>Browse examples - Check out Testing and CLI Tools</li> <li>Run the code - All examples are fully executable</li> <li>Adapt for your use case - Copy and modify patterns that fit your needs</li> </ol>"},{"location":"examples/#example-repository","title":"Example Repository","text":"<p>All examples are available in the examples/ directory of the SteadyText repository:</p> <pre><code>git clone https://github.com/julep-ai/steadytext.git\ncd steadytext/examples\npython basic_usage.py\npython testing_with_ai.py  \npython cli_tools.py\n</code></pre> <p>Deterministic Outputs</p> <p>Remember: all examples produce identical outputs every time you run them. This predictability is SteadyText's core feature and what makes it perfect for testing and tooling applications.</p>"},{"location":"examples/testing/","title":"Testing with AI","text":"<p>Learn how to use SteadyText to build reliable AI tests that never flake.</p>"},{"location":"examples/testing/#the-problem-with-ai-testing","title":"The Problem with AI Testing","text":"<p>Traditional AI testing is challenging because:</p> <ul> <li>Non-deterministic outputs: Same input produces different results</li> <li>Flaky tests: Tests pass sometimes, fail others  </li> <li>Hard to mock: AI services are complex to replicate</li> <li>Unpredictable behavior: Edge cases are difficult to reproduce</li> </ul> <p>SteadyText solves these by providing deterministic AI outputs - same input always produces the same result.</p>"},{"location":"examples/testing/#basic-test-patterns","title":"Basic Test Patterns","text":""},{"location":"examples/testing/#deterministic-assertions","title":"Deterministic Assertions","text":"<pre><code>import steadytext\n\ndef test_ai_code_generation():\n    \"\"\"Test that never flakes - same input, same output.\"\"\"\n\n    def my_ai_function(prompt):\n        # Your actual AI function (GPT-4, Claude, etc.)\n        # For testing, we compare against SteadyText\n        return call_real_ai_service(prompt)\n\n    prompt = \"write a function to reverse a string\"\n    result = my_ai_function(prompt)\n    expected = steadytext.generate(prompt)\n\n    # This assertion is deterministic and reliable\n    assert result.strip() == expected.strip()\n</code></pre>"},{"location":"examples/testing/#embedding-similarity-tests","title":"Embedding Similarity Tests","text":"<pre><code>import numpy as np\n\ndef test_document_similarity():\n    \"\"\"Test semantic similarity calculations.\"\"\"\n\n    def calculate_similarity(doc1, doc2):\n        vec1 = steadytext.embed(doc1)\n        vec2 = steadytext.embed(doc2)\n        return np.dot(vec1, vec2)  # Already normalized\n\n    # These similarities are always the same\n    similarity = calculate_similarity(\n        \"machine learning algorithms\",\n        \"artificial intelligence methods\"\n    )\n\n    assert similarity &gt; 0.7  # Reliable threshold\n    assert similarity &lt; 1.0  # Not identical documents\n</code></pre>"},{"location":"examples/testing/#mock-ai-services","title":"Mock AI Services","text":""},{"location":"examples/testing/#simple-mock","title":"Simple Mock","text":"<pre><code>class MockAI:\n    \"\"\"Deterministic AI mock for testing.\"\"\"\n\n    def complete(self, prompt: str) -&gt; str:\n        return steadytext.generate(prompt)\n\n    def embed(self, text: str) -&gt; np.ndarray:\n        return steadytext.embed(text)\n\n    def chat(self, messages: list) -&gt; str:\n        # Convert chat format to single prompt\n        prompt = \"\\n\".join([f\"{msg['role']}: {msg['content']}\" \n                           for msg in messages])\n        return steadytext.generate(f\"Chat response to: {prompt}\")\n\n# Usage in tests\ndef test_chat_functionality():\n    ai = MockAI()\n    response = ai.chat([\n        {\"role\": \"user\", \"content\": \"Hello\"}\n    ])\n\n    # Response is always the same\n    assert len(response) &gt; 0\n    assert \"hello\" in response.lower()\n</code></pre>"},{"location":"examples/testing/#advanced-mock-with-state","title":"Advanced Mock with State","text":"<pre><code>class StatefulMockAI:\n    \"\"\"Mock AI that maintains conversation state.\"\"\"\n\n    def __init__(self):\n        self.conversation_history = []\n\n    def chat(self, message: str) -&gt; str:\n        # Include history in prompt for context\n        history = \"\\n\".join(self.conversation_history[-5:])  # Last 5 messages\n        full_prompt = f\"History: {history}\\nNew message: {message}\"\n\n        response = steadytext.generate(full_prompt)\n\n        # Update history\n        self.conversation_history.append(f\"User: {message}\")\n        self.conversation_history.append(f\"AI: {response}\")\n\n        return response\n\ndef test_conversation_flow():\n    \"\"\"Test multi-turn conversations.\"\"\"\n    ai = StatefulMockAI()\n\n    response1 = ai.chat(\"What's the weather like?\")\n    response2 = ai.chat(\"What about tomorrow?\")\n\n    # Both responses are deterministic\n    assert len(response1) &gt; 0\n    assert len(response2) &gt; 0\n    # Tomorrow's response considers the context\n    assert response2 != response1\n</code></pre>"},{"location":"examples/testing/#test-data-generation","title":"Test Data Generation","text":""},{"location":"examples/testing/#reproducible-fixtures","title":"Reproducible Fixtures","text":"<pre><code>def generate_test_user(user_id: int) -&gt; dict:\n    \"\"\"Generate consistent test user data.\"\"\"\n    return {\n        \"id\": user_id,\n        \"name\": steadytext.generate(f\"Generate name for user {user_id}\"),\n        \"bio\": steadytext.generate(f\"Write bio for user {user_id}\"),\n        \"interests\": steadytext.generate(f\"List interests for user {user_id}\"),\n        \"embedding\": steadytext.embed(f\"user {user_id} profile\")\n    }\n\ndef test_user_recommendation():\n    \"\"\"Test user recommendation system.\"\"\"\n    # Generate consistent test users\n    users = [generate_test_user(i) for i in range(10)]\n\n    # Test similarity calculations\n    user1 = users[0]\n    user2 = users[1]\n\n    similarity = np.dot(user1[\"embedding\"], user2[\"embedding\"])\n\n    # Similarity is always the same for these users\n    assert isinstance(similarity, float)\n    assert -1.0 &lt;= similarity &lt;= 1.0\n</code></pre>"},{"location":"examples/testing/#fuzz-testing","title":"Fuzz Testing","text":"<pre><code>def generate_fuzz_input(test_name: str, iteration: int) -&gt; str:\n    \"\"\"Generate reproducible fuzz test inputs.\"\"\"\n    seed_prompt = f\"Generate test input for {test_name} iteration {iteration}\"\n    return steadytext.generate(seed_prompt)\n\ndef test_parser_robustness():\n    \"\"\"Fuzz test with reproducible inputs.\"\"\"\n\n    def parse_user_input(text):\n        # Your parsing function\n        return {\"words\": text.split(), \"length\": len(text)}\n\n    # Generate 100 consistent fuzz inputs\n    for i in range(100):\n        fuzz_input = generate_fuzz_input(\"parser_test\", i)\n\n        try:\n            result = parse_user_input(fuzz_input)\n            assert isinstance(result, dict)\n            assert \"words\" in result\n            assert \"length\" in result\n        except Exception as e:\n            # Reproducible error case\n            print(f\"Fuzz input {i} caused error: {e}\")\n            print(f\"Input was: {fuzz_input[:100]}...\")\n</code></pre>"},{"location":"examples/testing/#integration-testing","title":"Integration Testing","text":""},{"location":"examples/testing/#api-testing","title":"API Testing","text":"<pre><code>import requests_mock\n\ndef test_ai_api_integration():\n    \"\"\"Test integration with AI API using deterministic responses.\"\"\"\n\n    with requests_mock.Mocker() as m:\n        # Mock the AI API with deterministic responses\n        def generate_response(request, context):\n            prompt = request.json().get(\"prompt\", \"\")\n            return {\"response\": steadytext.generate(prompt)}\n\n        m.post(\"https://api.ai-service.com/generate\", json=generate_response)\n\n        # Your actual API client code\n        response = requests.post(\"https://api.ai-service.com/generate\", \n                               json={\"prompt\": \"Hello world\"})\n\n        # Response is always the same\n        expected_text = steadytext.generate(\"Hello world\")\n        assert response.json()[\"response\"] == expected_text\n</code></pre>"},{"location":"examples/testing/#database-testing","title":"Database Testing","text":"<pre><code>import sqlite3\n\ndef test_ai_content_storage():\n    \"\"\"Test storing AI-generated content in database.\"\"\"\n\n    # Create in-memory database\n    conn = sqlite3.connect(\":memory:\")\n    cursor = conn.cursor()\n\n    cursor.execute(\"\"\"\n        CREATE TABLE content (\n            id INTEGER PRIMARY KEY,\n            prompt TEXT,\n            generated_text TEXT,\n            embedding BLOB\n        )\n    \"\"\")\n\n    # Generate deterministic content\n    prompt = \"Write a short story about AI\"\n    text = steadytext.generate(prompt)\n    embedding = steadytext.embed(text)\n\n    # Store in database\n    cursor.execute(\"\"\"\n        INSERT INTO content (prompt, generated_text, embedding) \n        VALUES (?, ?, ?)\n    \"\"\", (prompt, text, embedding.tobytes()))\n\n    # Verify storage\n    cursor.execute(\"SELECT * FROM content WHERE id = 1\")\n    row = cursor.fetchone()\n\n    assert row[1] == prompt\n    assert row[2] == text\n    assert len(row[3]) == 1024 * 4  # 1024 float32 values\n\n    conn.close()\n</code></pre>"},{"location":"examples/testing/#performance-testing","title":"Performance Testing","text":""},{"location":"examples/testing/#consistency-benchmarks","title":"Consistency Benchmarks","text":"<pre><code>import time\n\ndef test_generation_performance():\n    \"\"\"Test that generation performance is consistent.\"\"\"\n\n    prompt = \"Explain machine learning in one paragraph\"\n    times = []\n\n    # Warm up cache\n    steadytext.generate(prompt)\n\n    # Measure cached performance\n    for _ in range(10):\n        start = time.time()\n        result = steadytext.generate(prompt)\n        end = time.time()\n        times.append(end - start)\n\n    avg_time = sum(times) / len(times)\n\n    # Cached calls should be very fast\n    assert avg_time &lt; 0.1  # Less than 100ms\n\n    # All results should be identical\n    results = [steadytext.generate(prompt) for _ in range(5)]\n    assert all(r == results[0] for r in results)\n</code></pre>"},{"location":"examples/testing/#best-practices","title":"Best Practices","text":"<p>Testing Guidelines</p> <ol> <li>Use deterministic prompts: Keep test prompts simple and specific</li> <li>Cache warmup: Call functions once before timing tests</li> <li>Mock external services: Use SteadyText to replace real AI APIs</li> <li>Test edge cases: Generate consistent edge case inputs</li> <li>Version pin: Keep SteadyText version fixed for test stability</li> </ol> <p>Limitations</p> <ul> <li>Model changes: Updates to SteadyText models will change outputs</li> <li>Creative tasks: SteadyText is optimized for consistency, not creativity</li> <li>Context length: Limited to model's context window</li> </ul>"},{"location":"examples/testing/#complete-example","title":"Complete Example","text":"<pre><code>import unittest\nimport numpy as np\nimport steadytext\n\nclass TestAIFeatures(unittest.TestCase):\n\n    def setUp(self):\n        \"\"\"Set up test fixtures.\"\"\"\n        self.mock_ai = MockAI()\n        self.test_prompts = [\n            \"Write a function to sort a list\",\n            \"Explain what is machine learning\",\n            \"Generate a product description\"\n        ]\n\n    def test_deterministic_generation(self):\n        \"\"\"Test that generation is deterministic.\"\"\"\n        for prompt in self.test_prompts:\n            result1 = steadytext.generate(prompt)\n            result2 = steadytext.generate(prompt)\n            self.assertEqual(result1, result2)\n\n    def test_embedding_consistency(self):\n        \"\"\"Test that embeddings are consistent.\"\"\"\n        text = \"test embedding consistency\"\n        vec1 = steadytext.embed(text)\n        vec2 = steadytext.embed(text)\n        np.testing.assert_array_equal(vec1, vec2)\n\n    def test_mock_ai_service(self):\n        \"\"\"Test mock AI service.\"\"\"\n        response = self.mock_ai.complete(\"Hello\")\n        self.assertIsInstance(response, str)\n        self.assertGreater(len(response), 0)\n\n        # Response should be deterministic\n        response2 = self.mock_ai.complete(\"Hello\")\n        self.assertEqual(response, response2)\n\nif __name__ == \"__main__\":\n    unittest.main()\n</code></pre> <p>This comprehensive testing approach ensures your AI features are reliable, reproducible, and maintainable.</p>"},{"location":"examples/tooling/","title":"CLI Tools &amp; Tooling","text":"<p>Build deterministic command-line tools and development utilities with SteadyText.</p>"},{"location":"examples/tooling/#why-steadytext-for-cli-tools","title":"Why SteadyText for CLI Tools?","text":"<p>Traditional AI-powered CLI tools have problems:</p> <ul> <li>Inconsistent outputs: Same command gives different results</li> <li>Unreliable automation: Scripts break due to changing responses  </li> <li>Hard to test: Non-deterministic behavior makes testing difficult</li> <li>User confusion: Users expect consistent behavior from tools</li> </ul> <p>SteadyText solves these with deterministic outputs - same input always produces the same result.</p>"},{"location":"examples/tooling/#basic-cli-patterns","title":"Basic CLI Patterns","text":""},{"location":"examples/tooling/#simple-command-tools","title":"Simple Command Tools","text":"<pre><code>import click\nimport steadytext\n\n@click.command()\n@click.argument('topic')\ndef motivate(topic):\n    \"\"\"Generate motivational quotes about any topic.\"\"\"\n    prompt = f\"Write an inspiring quote about {topic}\"\n    quote = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udcaa {quote}\")\n\n# Usage: python script.py programming\n# Always generates the same quote for \"programming\"\n</code></pre>"},{"location":"examples/tooling/#error-code-explainer","title":"Error Code Explainer","text":"<pre><code>@click.command()\n@click.argument('error_code')\ndef explain(error_code):\n    \"\"\"Convert error codes to friendly explanations.\"\"\"\n    prompt = f\"Explain error {error_code} in simple, user-friendly terms\"\n    explanation = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udd0d {error_code}: {explanation}\")\n\n# Usage: python script.py ECONNREFUSED\n# Always gives the same explanation for ECONNREFUSED\n</code></pre>"},{"location":"examples/tooling/#command-generator","title":"Command Generator","text":"<pre><code>@click.command()\n@click.argument('task')\ndef git_helper(task):\n    \"\"\"Generate git commands for common tasks.\"\"\"\n    prompt = f\"Git command to {task}. Return only the command.\"\n    command = steadytext.generate(prompt).strip()\n    click.echo(f\"\ud83d\udcbb {command}\")\n\n# Usage: python script.py \"undo last commit\"\n# Always suggests the same git command\n</code></pre>"},{"location":"examples/tooling/#development-tools","title":"Development Tools","text":""},{"location":"examples/tooling/#code-generation-helper","title":"Code Generation Helper","text":"<pre><code>import os\nimport click\n\n@click.group()\ndef codegen():\n    \"\"\"Code generation CLI tool.\"\"\"\n    pass\n\n@codegen.command()\n@click.argument('function_name')\n@click.argument('description')\n@click.option('--language', '-l', default='python', help='Programming language')\ndef function(function_name, description, language):\n    \"\"\"Generate a function from description.\"\"\"\n    prompt = f\"Write a {language} function named {function_name} that {description}\"\n    code = steadytext.generate(prompt)\n\n    # Save to file\n    ext = {'python': 'py', 'javascript': 'js', 'rust': 'rs'}.get(language, 'txt')\n    filename = f\"{function_name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(code)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n    click.echo(f\"\ud83d\udcc4 Preview:\\n{code[:200]}...\")\n\n# Usage: python codegen.py function binary_search \"search for item in sorted array\"\n</code></pre>"},{"location":"examples/tooling/#documentation-generator","title":"Documentation Generator","text":"<pre><code>@codegen.command()\n@click.argument('project_name')\ndef readme(project_name):\n    \"\"\"Generate README.md for a project.\"\"\"\n    prompt = f\"Write a comprehensive README.md for a project called {project_name}\"\n    readme_content = steadytext.generate(prompt)\n\n    with open('README.md', 'w') as f:\n        f.write(readme_content)\n\n    click.echo(\"\u2705 Generated README.md\")\n\n# Usage: python codegen.py readme \"my-awesome-project\"\n</code></pre>"},{"location":"examples/tooling/#testing-qa-tools","title":"Testing &amp; QA Tools","text":""},{"location":"examples/tooling/#test-case-generator","title":"Test Case Generator","text":"<pre><code>@click.command()\n@click.argument('function_description')\ndef test_cases(function_description):\n    \"\"\"Generate test cases for a function.\"\"\"\n    prompt = f\"Generate 5 test cases for a function that {function_description}\"\n    cases = steadytext.generate(prompt)\n\n    # Save to test file\n    with open('test_cases.py', 'w') as f:\n        f.write(f\"# Test cases for: {function_description}\\n\")\n        f.write(cases)\n\n    click.echo(\"\u2705 Generated test_cases.py\")\n    click.echo(f\"\ud83d\udccb Preview:\\n{cases[:300]}...\")\n\n# Usage: python tool.py \"calculates fibonacci numbers\"\n</code></pre>"},{"location":"examples/tooling/#mock-data-generator","title":"Mock Data Generator","text":"<pre><code>@click.command()\n@click.argument('data_type')\n@click.option('--count', '-c', default=10, help='Number of items to generate')\ndef mockdata(data_type, count):\n    \"\"\"Generate mock data for testing.\"\"\"\n    items = []\n\n    for i in range(count):\n        prompt = f\"Generate realistic {data_type} data item {i+1}\"\n        item = steadytext.generate(prompt)\n        items.append(item.strip())\n\n    # Output as JSON\n    import json\n    output = {data_type: items}\n\n    with open(f'mock_{data_type}.json', 'w') as f:\n        json.dump(output, f, indent=2)\n\n    click.echo(f\"\u2705 Generated mock_{data_type}.json with {count} items\")\n\n# Usage: python tool.py user_profiles --count 20\n</code></pre>"},{"location":"examples/tooling/#content-documentation-tools","title":"Content &amp; Documentation Tools","text":""},{"location":"examples/tooling/#commit-message-generator","title":"Commit Message Generator","text":"<pre><code>@click.command()\n@click.argument('changes', nargs=-1)\ndef commit_msg(changes):\n    \"\"\"Generate commit messages from change descriptions.\"\"\"\n    change_list = \" \".join(changes)\n    prompt = f\"Write a concise git commit message for: {change_list}\"\n    message = steadytext.generate(prompt).strip()\n\n    click.echo(f\"\ud83d\udcdd Suggested commit message:\")\n    click.echo(f\"   {message}\")\n\n    # Optionally copy to clipboard or commit directly\n    if click.confirm(\"Use this commit message?\"):\n        os.system(f'git commit -m \"{message}\"')\n        click.echo(\"\u2705 Committed!\")\n\n# Usage: python tool.py \"added user authentication\" \"fixed login bug\"\n</code></pre>"},{"location":"examples/tooling/#api-documentation-generator","title":"API Documentation Generator","text":"<pre><code>@click.command()\n@click.argument('api_endpoint')\n@click.argument('description')\ndef api_docs(api_endpoint, description):\n    \"\"\"Generate API documentation for an endpoint.\"\"\"\n    prompt = f\"\"\"Generate API documentation for endpoint {api_endpoint} that {description}.\n    Include: description, parameters, example request/response, error codes.\"\"\"\n\n    docs = steadytext.generate(prompt)\n\n    # Save to markdown file\n    safe_name = api_endpoint.replace('/', '_').replace('{', '').replace('}', '')\n    filename = f\"api_{safe_name}.md\"\n\n    with open(filename, 'w') as f:\n        f.write(f\"# {api_endpoint}\\n\\n\")\n        f.write(docs)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py \"/users/{id}\" \"returns user profile information\"\n</code></pre>"},{"location":"examples/tooling/#automation-scripting","title":"Automation &amp; Scripting","text":""},{"location":"examples/tooling/#configuration-generator","title":"Configuration Generator","text":"<pre><code>@click.command()\n@click.argument('service_name')\n@click.option('--format', '-f', default='yaml', help='Config format (yaml, json, toml)')\ndef config(service_name, format):\n    \"\"\"Generate configuration files for services.\"\"\"\n    prompt = f\"Generate a {format} configuration file for {service_name} service\"\n    config_content = steadytext.generate(prompt)\n\n    ext = {'yaml': 'yml', 'json': 'json', 'toml': 'toml'}.get(format, 'txt')\n    filename = f\"{service_name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(config_content)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py database --format yaml\n</code></pre>"},{"location":"examples/tooling/#script-template-generator","title":"Script Template Generator","text":"<pre><code>@click.command()\n@click.argument('script_type')\n@click.argument('purpose')\ndef script_template(script_type, purpose):\n    \"\"\"Generate script templates for common tasks.\"\"\"\n    prompt = f\"Generate a {script_type} script template for {purpose}\"\n    script = steadytext.generate(prompt)\n\n    ext = {'bash': 'sh', 'python': 'py', 'powershell': 'ps1'}.get(script_type, 'txt')\n    filename = f\"template.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(script)\n\n    # Make executable if shell script\n    if ext == 'sh':\n        os.chmod(filename, 0o755)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n# Usage: python tool.py bash \"automated deployment\"\n</code></pre>"},{"location":"examples/tooling/#complete-cli-tool-example","title":"Complete CLI Tool Example","text":"<pre><code>#!/usr/bin/env python3\n\"\"\"\nDevHelper - A deterministic development tool powered by SteadyText\n\"\"\"\n\nimport os\nimport json\nimport click\nimport steadytext\n\n@click.group()\n@click.version_option()\ndef cli():\n    \"\"\"DevHelper - Deterministic development utilities.\"\"\"\n    pass\n\n@cli.group()\ndef generate():\n    \"\"\"Code and content generation commands.\"\"\"\n    pass\n\n@generate.command()\n@click.argument('name')\n@click.argument('description')\n@click.option('--lang', '-l', default='python', help='Programming language')\ndef function(name, description, lang):\n    \"\"\"Generate a function from description.\"\"\"\n    prompt = f\"Write a {lang} function named {name} that {description}\"\n    code = steadytext.generate(prompt)\n\n    ext = {'python': 'py', 'javascript': 'js', 'rust': 'rs'}.get(lang, 'txt')\n    filename = f\"{name}.{ext}\"\n\n    with open(filename, 'w') as f:\n        f.write(code)\n\n    click.echo(f\"\u2705 Generated {filename}\")\n\n@generate.command()\n@click.argument('count', type=int)\n@click.option('--type', '-t', default='user', help='Data type to generate')\ndef testdata(count, type):\n    \"\"\"Generate test data.\"\"\"\n    data = []\n\n    for i in range(count):\n        prompt = f\"Generate realistic {type} test data item {i+1} as JSON\"\n        item = steadytext.generate(prompt)\n        data.append(item.strip())\n\n    output_file = f\"test_{type}_data.json\"\n    with open(output_file, 'w') as f:\n        json.dump({f\"{type}_data\": data}, f, indent=2)\n\n    click.echo(f\"\u2705 Generated {output_file} with {count} items\")\n\n@cli.command()\n@click.argument('error_code')\ndef explain(error_code):\n    \"\"\"Explain error codes in friendly terms.\"\"\"\n    prompt = f\"Explain error {error_code} in simple, user-friendly terms\"\n    explanation = steadytext.generate(prompt)\n    click.echo(f\"\ud83d\udd0d {error_code}:\")\n    click.echo(f\"   {explanation}\")\n\n@cli.command()\n@click.argument('task')\ndef git(task):\n    \"\"\"Generate git commands for tasks.\"\"\"\n    prompt = f\"Git command to {task}. Return only the command.\"\n    command = steadytext.generate(prompt).strip()\n    click.echo(f\"\ud83d\udcbb {command}\")\n\n    if click.confirm(\"Execute this command?\"):\n        os.system(command)\n\nif __name__ == '__main__':\n    cli()\n</code></pre> <p>Save this as <code>devhelper.py</code> and use it:</p> <pre><code># Generate a function\npython devhelper.py generate function binary_search \"search sorted array\"\n\n# Generate test data  \npython devhelper.py generate testdata 10 --type user\n\n# Explain error codes\npython devhelper.py explain ECONNREFUSED\n\n# Get git commands\npython devhelper.py git \"undo last commit but keep changes\"\n</code></pre>"},{"location":"examples/tooling/#best-practices","title":"Best Practices","text":"<p>CLI Tool Guidelines</p> <ol> <li>Keep prompts specific: Clear, detailed prompts give better results</li> <li>Add confirmation prompts: For destructive operations, ask before executing</li> <li>Save outputs to files: Generate content to files for later use</li> <li>Use consistent formatting: Same input should always produce same output</li> <li>Add help text: Use Click's built-in help system</li> </ol> <p>Benefits of Deterministic CLI Tools</p> <ul> <li>Reliable automation: Scripts work consistently</li> <li>Easier testing: Predictable outputs make testing simple</li> <li>User trust: Users know what to expect</li> <li>Debugging: Reproducible behavior makes issues easier to track</li> <li>Documentation: Examples in docs always work</li> </ul> <p>Considerations</p> <ul> <li>Creative vs. Deterministic: SteadyText prioritizes consistency over creativity</li> <li>Context limits: Model has limited context window</li> <li>Update impacts: SteadyText updates may change outputs (major versions only)</li> </ul> <p>This approach creates reliable, testable CLI tools that users can depend on for consistent behavior.</p>"},{"location":"mojo-max/","title":"Mojo and MAX Documentation","text":"<p>This documentation covers the Modular platform's Community Edition, including the Mojo programming language and MAX inference server. All features documented here are available for free under the Community License.</p>"},{"location":"mojo-max/#overview","title":"Overview","text":"<p>Mojo is a Python-compatible systems programming language that enables high-performance CPU and GPU programming without CUDA. It combines Python's ease of use with the performance of C++ and Rust.</p> <p>MAX is a high-performance inference framework for serving AI models with an OpenAI-compatible API. It supports 500+ models and provides state-of-the-art performance on both CPUs and GPUs.</p>"},{"location":"mojo-max/#community-edition","title":"Community Edition","text":"<p>The Community Edition is free forever and includes: - Unlimited use on CPUs and NVIDIA GPUs - Up to 8 discrete GPUs from other vendors (AMD, etc.) - Free for both personal and commercial use - Perpetual license guarantee (cannot be revoked)</p> <p>For commercial use, Modular asks that you notify them at usage@modular.com so they can showcase your usage.</p>"},{"location":"mojo-max/#documentation-structure","title":"Documentation Structure","text":""},{"location":"mojo-max/#getting-started","title":"Getting Started","text":"<ul> <li>Installation - Install Mojo and MAX</li> <li>Quickstart - Run your first model</li> <li>Hello World - Basic Mojo programs</li> </ul>"},{"location":"mojo-max/#mojo-language","title":"Mojo Language","text":"<ul> <li>Basics - Core language features</li> <li>Python Interoperability - Using Python from Mojo</li> <li>Calling Mojo from Python - Creating Python extensions</li> <li>Functions - Function definitions and calls</li> <li>Structs and Traits - Type system</li> <li>Metaprogramming - Compile-time programming</li> </ul>"},{"location":"mojo-max/#max-inference","title":"MAX Inference","text":"<ul> <li>Overview - MAX architecture and features</li> <li>Serving Models - Running inference servers</li> <li>Offline Inference - Direct Python API</li> <li>API Reference - OpenAI-compatible endpoints</li> <li>Deployment - Docker and production setup</li> </ul>"},{"location":"mojo-max/#community-edition_1","title":"Community Edition","text":"<ul> <li>Licensing - License terms and usage</li> <li>Limitations - Current limitations and roadmap</li> </ul>"},{"location":"mojo-max/#examples","title":"Examples","text":"<ul> <li>Basic Examples - Simple Mojo programs</li> <li>Python Integration - Interop examples</li> </ul>"},{"location":"mojo-max/#key-features-for-python-developers","title":"Key Features for Python Developers","text":"<ol> <li>Drop-in Performance: Write performance-critical code in Mojo and call it from Python</li> <li>Python Compatibility: Import and use any Python library directly in Mojo</li> <li>Zero-Config Inference: Deploy models with a single command</li> <li>OpenAI API Compatible: Use existing client code with MAX endpoints</li> </ol>"},{"location":"mojo-max/#quick-links","title":"Quick Links","text":"<ul> <li>Official Docs</li> <li>Model Repository</li> <li>Community Forum</li> <li>GitHub Examples</li> </ul>"},{"location":"mojo-max/community-edition/licensing/","title":"Community Edition Licensing","text":"<p>The Modular Community Edition provides free access to MAX and Mojo for both personal and commercial use. This document explains the licensing terms in plain language.</p>"},{"location":"mojo-max/community-edition/licensing/#key-points","title":"Key Points","text":""},{"location":"mojo-max/community-edition/licensing/#free-forever","title":"Free Forever","text":"<ul> <li>The Community Edition is free and will remain free</li> <li>No hidden costs or surprise charges</li> <li>Full access to core features</li> </ul>"},{"location":"mojo-max/community-edition/licensing/#commercial-use-allowed","title":"Commercial Use Allowed","text":"<ul> <li>You can use MAX and Mojo for commercial products</li> <li>No revenue sharing or royalties required</li> <li>Simple notification requested for commercial use</li> </ul>"},{"location":"mojo-max/community-edition/licensing/#hardware-limits","title":"Hardware Limits","text":"<ul> <li>Unlimited: CPUs and NVIDIA GPUs</li> <li>Up to 8 devices: Other accelerators (AMD GPUs, etc.)</li> <li>Beyond 8 devices: Contact Modular for Enterprise license</li> </ul>"},{"location":"mojo-max/community-edition/licensing/#license-summary","title":"License Summary","text":""},{"location":"mojo-max/community-edition/licensing/#what-you-can-do","title":"What You Can Do \u2705","text":"<ol> <li>Personal Projects</li> <li>Any personal or hobby projects</li> <li>Learning and experimentation</li> <li> <p>Open source contributions</p> </li> <li> <p>Commercial Use</p> </li> <li>Production deployments</li> <li>Commercial products and services</li> <li>Internal business applications</li> <li> <p>Client projects</p> </li> <li> <p>Hardware Usage</p> </li> <li>Unlimited CPU cores</li> <li>Unlimited NVIDIA GPUs</li> <li>Up to 8 AMD or other GPUs</li> <li> <p>Development on any machine</p> </li> <li> <p>Distribution</p> </li> <li>Share your Mojo code</li> <li>Distribute compiled binaries</li> <li>Create libraries and frameworks</li> <li>Build tools on top of MAX/Mojo</li> </ol>"},{"location":"mojo-max/community-edition/licensing/#what-you-need-to-do","title":"What You Need to Do \ud83d\udccb","text":"<p>For commercial use, Modular asks (but doesn't require) that you:</p> <ol> <li>Notify them: Email usage@modular.com</li> <li>Allow logo use: Let them showcase your company as a user</li> <li>That's it! No fees, no contracts</li> </ol>"},{"location":"mojo-max/community-edition/licensing/#what-you-cannot-do","title":"What You Cannot Do \u274c","text":"<ol> <li>Remove attributions: Keep Modular's copyright notices</li> <li>Misrepresent ownership: Don't claim you created MAX/Mojo</li> <li>Exceed device limits: Stay within 8 non-NVIDIA accelerators for commercial use</li> </ol>"},{"location":"mojo-max/community-edition/licensing/#perpetual-license-guarantee","title":"Perpetual License Guarantee","text":"<p>The Community License includes an important protection:</p> <p>If Modular stops updating the software, you automatically get a perpetual license to continue using the last version.</p> <p>This means: - You won't lose access if Modular pivots or shuts down - Your investment in MAX/Mojo code is protected - More liberal than many proprietary licenses (including CUDA)</p>"},{"location":"mojo-max/community-edition/licensing/#commercial-use-examples","title":"Commercial Use Examples","text":""},{"location":"mojo-max/community-edition/licensing/#allowed-under-community-edition","title":"Allowed Under Community Edition","text":"<ol> <li> <p>SaaS Application <pre><code># Running on 4 NVIDIA A100 GPUs - \u2705 Allowed\nmax serve --model-path=llama-70b --tensor-parallel-size=4\n</code></pre></p> </li> <li> <p>On-Premise Deployment <pre><code># Running on 100 CPU cores - \u2705 Allowed\nmax serve --model-path=model --device=cpu\n</code></pre></p> </li> <li> <p>Edge Deployment <pre><code># Running on 8 AMD GPUs - \u2705 Allowed (at the limit)\nmax serve --model-path=model --device=rocm\n</code></pre></p> </li> </ol>"},{"location":"mojo-max/community-edition/licensing/#requires-enterprise-license","title":"Requires Enterprise License","text":"<ol> <li>Large AMD GPU Cluster <pre><code># Running on 16 AMD GPUs - \u274c Needs Enterprise\n# Contact sales@modular.com\n</code></pre></li> </ol>"},{"location":"mojo-max/community-edition/licensing/#notification-process","title":"Notification Process","text":"<p>For commercial use, send a simple email to usage@modular.com:</p> <pre><code>Subject: Commercial Use Notification - [Your Company]\n\nHi Modular Team,\n\nWe're using MAX/Mojo in production at [Company Name].\n\nUse case: [Brief description]\nWebsite: [Your website]\n\nWe're happy to be featured as a user.\n\nBest,\n[Your name]\n</code></pre>"},{"location":"mojo-max/community-edition/licensing/#faq","title":"FAQ","text":""},{"location":"mojo-max/community-edition/licensing/#q-do-i-need-to-pay-for-commercial-use","title":"Q: Do I need to pay for commercial use?","text":"<p>A: No, commercial use is free under the Community Edition.</p>"},{"location":"mojo-max/community-edition/licensing/#q-what-if-im-a-consultant-using-max-for-client-work","title":"Q: What if I'm a consultant using MAX for client work?","text":"<p>A: That's allowed! It counts as commercial use, so please notify Modular.</p>"},{"location":"mojo-max/community-edition/licensing/#q-can-i-use-max-in-my-startups-product","title":"Q: Can I use MAX in my startup's product?","text":"<p>A: Yes! The Community Edition is perfect for startups.</p>"},{"location":"mojo-max/community-edition/licensing/#q-what-about-internal-company-tools","title":"Q: What about internal company tools?","text":"<p>A: Also allowed under commercial use provisions.</p>"},{"location":"mojo-max/community-edition/licensing/#q-do-i-need-a-license-for-development","title":"Q: Do I need a license for development?","text":"<p>A: No, development and testing are always free.</p>"},{"location":"mojo-max/community-edition/licensing/#q-can-i-benchmark-max-against-other-frameworks","title":"Q: Can I benchmark MAX against other frameworks?","text":"<p>A: Yes, benchmarking and comparisons are allowed.</p>"},{"location":"mojo-max/community-edition/licensing/#q-what-if-i-exceed-8-non-nvidia-gpus-accidentally","title":"Q: What if I exceed 8 non-NVIDIA GPUs accidentally?","text":"<p>A: Contact Modular to discuss Enterprise options. They're reasonable!</p>"},{"location":"mojo-max/community-edition/licensing/#open-source-projects","title":"Open Source Projects","text":"<p>Using MAX/Mojo in open source projects is encouraged:</p> <ul> <li>No notification required for open source</li> <li>Can use unlimited hardware</li> <li>Consider adding a \"Powered by MAX\" badge</li> <li>Share your projects with the community</li> </ul>"},{"location":"mojo-max/community-edition/licensing/#license-text","title":"License Text","text":"<p>The full legal license is available at: https://www.modular.com/legal/community</p> <p>Key sections: - Grant of rights (Section 2) - Commercial use (Section 3) - Hardware limitations (Section 4) - Perpetual license clause (Section 7)</p>"},{"location":"mojo-max/community-edition/licensing/#comparison-with-other-licenses","title":"Comparison with Other Licenses","text":"Aspect Modular Community CUDA PyTorch TensorFlow Commercial Use \u2705 Free \u2705 Free* \u2705 Free \u2705 Free Source Available \u274c \u274c \u2705 \u2705 Perpetual Clause \u2705 \u274c N/A N/A Hardware Limits Some None None None Attribution Required Required Required Required <p>*CUDA has complex licensing for certain use cases</p>"},{"location":"mojo-max/community-edition/licensing/#enterprise-benefits","title":"Enterprise Benefits","text":"<p>If you need more than the Community Edition offers:</p> <ol> <li>Unlimited Hardware: No device restrictions</li> <li>Priority Support: Direct access to Modular team</li> <li>Custom Features: Influence roadmap</li> <li>SLAs: Guaranteed response times</li> <li>Training: Onboarding and optimization help</li> </ol> <p>Contact: sales@modular.com</p>"},{"location":"mojo-max/community-edition/licensing/#summary","title":"Summary","text":"<p>The Modular Community License is designed to be: - Developer-friendly: Use it for anything - Startup-friendly: No costs as you grow - Future-proof: Perpetual license protection - Simple: No complex terms or restrictions</p> <p>For most users and companies, the Community Edition provides everything needed to build and deploy AI applications with MAX and Mojo.</p>"},{"location":"mojo-max/community-edition/limitations/","title":"Current Limitations and Roadmap","text":"<p>This document outlines the current limitations of Mojo and MAX in the Community Edition, along with known roadmap items.</p>"},{"location":"mojo-max/community-edition/limitations/#mojo-language-limitations","title":"Mojo Language Limitations","text":""},{"location":"mojo-max/community-edition/limitations/#1-missing-language-features","title":"1. Missing Language Features","text":"<p>Classes - No Python-style dynamic classes yet - Only static structs are available - Workaround: Use structs with traits</p> <p>Async/Await - No native async support - Workaround: Use Python's asyncio through interop</p> <p>Decorators - Limited decorator support - Available: <code>@export</code>, <code>@fieldwise_init</code>, <code>@parameter</code> - Missing: Custom decorators</p> <p>Exception Handling - Basic try/except support - No custom exception types yet - Limited exception information</p>"},{"location":"mojo-max/community-edition/limitations/#2-standard-library-gaps","title":"2. Standard Library Gaps","text":"<p>Missing Modules - No native HTTP client - Limited file I/O (use Python interop) - No JSON parser (use Python's json) - No regex support (use Python's re) - No database drivers</p> <p>Collections - Basic List and Dict support - No OrderedDict, defaultdict, etc. - Limited set operations</p>"},{"location":"mojo-max/community-edition/limitations/#3-python-interop-limitations","title":"3. Python Interop Limitations","text":"<p>Calling Mojo from Python - Max 3 arguments for regular functions - No keyword arguments (dict workaround needed) - No direct <code>__init__</code> binding - No static methods or properties - Limited type conversions</p> <p>Performance Overhead - Boundary crossing has cost - Type conversions can be expensive - Not all NumPy operations are optimized</p>"},{"location":"mojo-max/community-edition/limitations/#max-inference-limitations","title":"MAX Inference Limitations","text":""},{"location":"mojo-max/community-edition/limitations/#1-model-support","title":"1. Model Support","text":"<p>Model Formats - Best support for GGUF format - PyTorch models need conversion - Some models may not work out-of-box</p> <p>Model Types - Focus on transformer architectures - Limited support for custom architectures - Some specialized layers unsupported</p>"},{"location":"mojo-max/community-edition/limitations/#2-hardware-support","title":"2. Hardware Support","text":"<p>Community Edition Limits - Up to 8 non-NVIDIA GPUs - No TPU support - Limited ARM optimization</p> <p>GPU Features - NVIDIA has best support - AMD support is newer/limited - No Intel GPU support yet</p>"},{"location":"mojo-max/community-edition/limitations/#3-serving-features","title":"3. Serving Features","text":"<p>Missing Features - No built-in model versioning - Limited A/B testing support - No automatic model reloading - Basic metrics only</p> <p>Scaling Limitations - Manual scaling configuration - No auto-scaling out of box - Limited load balancing options</p>"},{"location":"mojo-max/community-edition/limitations/#development-environment","title":"Development Environment","text":""},{"location":"mojo-max/community-edition/limitations/#1-tooling","title":"1. Tooling","text":"<p>IDE Support - Basic VSCode extension - No IntelliJ/PyCharm plugin - Limited debugging support - No profiler integration</p> <p>Package Management - No native package manager - Manual dependency handling - No private package registry</p>"},{"location":"mojo-max/community-edition/limitations/#2-documentation","title":"2. Documentation","text":"<p>API Docs - Some APIs undocumented - Examples still being added - Community docs emerging</p> <p>Tutorials - Limited advanced tutorials - Few real-world examples - More samples needed</p>"},{"location":"mojo-max/community-edition/limitations/#platform-limitations","title":"Platform Limitations","text":""},{"location":"mojo-max/community-edition/limitations/#1-operating-systems","title":"1. Operating Systems","text":"<p>Current Support - Linux (best support) - macOS (good support) - Windows (WSL only)</p> <p>Missing - Native Windows support - Mobile platforms - Embedded systems</p>"},{"location":"mojo-max/community-edition/limitations/#2-deployment","title":"2. Deployment","text":"<p>Container Limitations - Large container size - No official Helm charts - Limited orchestration examples</p> <p>Edge Deployment - Not optimized for edge - Large binary size - High memory requirements</p>"},{"location":"mojo-max/community-edition/limitations/#known-issues","title":"Known Issues","text":""},{"location":"mojo-max/community-edition/limitations/#1-performance","title":"1. Performance","text":"<p>CPU Performance - Some operations not vectorized - Limited SIMD usage in places - Thread scaling issues</p> <p>Memory Usage - Higher than expected for some models - Memory leaks in edge cases - Inefficient caching</p>"},{"location":"mojo-max/community-edition/limitations/#2-stability","title":"2. Stability","text":"<p>Beta Features - Python interop is beta - May have breaking changes - Some crashes possible</p> <p>Error Messages - Sometimes cryptic - Stack traces can be unclear - Limited error recovery</p>"},{"location":"mojo-max/community-edition/limitations/#roadmap-items","title":"Roadmap Items","text":""},{"location":"mojo-max/community-edition/limitations/#near-term-likely-2025","title":"Near Term (Likely 2025)","text":"<ol> <li>Mojo Improvements</li> <li>Python-style classes</li> <li>Better error handling</li> <li> <p>More standard library</p> </li> <li> <p>MAX Enhancements</p> </li> <li>More model formats</li> <li>Better quantization</li> <li> <p>Improved serving features</p> </li> <li> <p>Platform Support</p> </li> <li>Native Windows</li> <li>Better ARM support</li> <li>Smaller binaries</li> </ol>"},{"location":"mojo-max/community-edition/limitations/#medium-term","title":"Medium Term","text":"<ol> <li>Language Features</li> <li>Async/await support</li> <li>Custom decorators</li> <li> <p>Better metaprogramming</p> </li> <li> <p>Ecosystem</p> </li> <li>Package manager</li> <li>More integrations</li> <li>Better tooling</li> </ol>"},{"location":"mojo-max/community-edition/limitations/#long-term-vision","title":"Long Term Vision","text":"<ol> <li>Full Python Compatibility</li> <li>Run any Python code</li> <li>Better performance</li> <li> <p>Seamless integration</p> </li> <li> <p>Universal Deployment</p> </li> <li>Mobile support</li> <li>Edge optimization</li> <li>WebAssembly target</li> </ol>"},{"location":"mojo-max/community-edition/limitations/#workarounds-and-tips","title":"Workarounds and Tips","text":""},{"location":"mojo-max/community-edition/limitations/#1-missing-features","title":"1. Missing Features","text":"<p>Use Python Interop <pre><code># For missing stdlib features\nvar json = Python.import_module(\"json\")\nvar requests = Python.import_module(\"requests\")\n</code></pre></p> <p>Community Packages - Check GitHub - Join Discord for help - Share your solutions</p>"},{"location":"mojo-max/community-edition/limitations/#2-performance-issues","title":"2. Performance Issues","text":"<p>Profile First - Identify actual bottlenecks - Don't premature optimize - Use Python for non-critical paths</p> <p>Batch Operations - Minimize boundary crossing - Process in chunks - Reuse allocations</p>"},{"location":"mojo-max/community-edition/limitations/#3-stability-problems","title":"3. Stability Problems","text":"<p>Defensive Coding <pre><code>try:\n    # Potentially unstable operation\n    result = risky_operation()\nexcept:\n    # Fallback to Python\n    result = python_fallback()\n</code></pre></p>"},{"location":"mojo-max/community-edition/limitations/#community-resources","title":"Community Resources","text":""},{"location":"mojo-max/community-edition/limitations/#getting-help","title":"Getting Help","text":"<ol> <li>Official Channels</li> <li>Forum</li> <li>GitHub Issues</li> <li> <p>Discord</p> </li> <li> <p>Community Projects</p> </li> <li>Awesome Mojo</li> <li>Community Packages</li> <li>Examples Repo</li> </ol>"},{"location":"mojo-max/community-edition/limitations/#contributing","title":"Contributing","text":"<ol> <li>Report Issues</li> <li>Use GitHub issues</li> <li>Provide minimal examples</li> <li> <p>Include system info</p> </li> <li> <p>Share Solutions</p> </li> <li>Post workarounds</li> <li>Create examples</li> <li>Help others</li> </ol>"},{"location":"mojo-max/community-edition/limitations/#summary","title":"Summary","text":"<p>While Mojo and MAX have limitations, they offer:</p> <ol> <li>Significant Performance: Often 10-100x faster than Python</li> <li>Growing Ecosystem: Rapid development and improvements</li> <li>Strong Community: Active and helpful</li> <li>Clear Vision: Becoming the best AI infrastructure</li> </ol> <p>Most limitations have workarounds, and the roadmap addresses key gaps. The Community Edition provides everything needed for serious development and deployment.</p> <p>For production use cases hitting these limits, consider: - Using Python interop for missing features - Joining the community for solutions - Considering Enterprise Edition for advanced needs - Contributing feedback to shape the roadmap</p> <p>Remember: Mojo is young but rapidly evolving. What's a limitation today might be solved tomorrow!</p>"},{"location":"mojo-max/examples/python-integration/","title":"Python Integration Examples","text":"<p>This guide provides practical examples of integrating Mojo with Python code, focusing on real-world use cases and performance optimization patterns.</p>"},{"location":"mojo-max/examples/python-integration/#example-1-accelerating-numpy-operations","title":"Example 1: Accelerating NumPy Operations","text":""},{"location":"mojo-max/examples/python-integration/#problem","title":"Problem","text":"<p>Python/NumPy code that's too slow for large arrays.</p>"},{"location":"mojo-max/examples/python-integration/#python-original","title":"Python Original","text":"<pre><code>import numpy as np\n\ndef compute_distances_python(points1, points2):\n    \"\"\"Compute pairwise Euclidean distances between two sets of points.\"\"\"\n    n1, d = points1.shape\n    n2, _ = points2.shape\n    distances = np.zeros((n1, n2))\n\n    for i in range(n1):\n        for j in range(n2):\n            diff = points1[i] - points2[j]\n            distances[i, j] = np.sqrt(np.sum(diff ** 2))\n\n    return distances\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#mojo-acceleration","title":"Mojo Acceleration","text":"<p>Create <code>fast_distances.mojo</code>:</p> <pre><code>from python import Python, PythonObject\nfrom python.bindings import PythonModuleBuilder\nfrom math import sqrt\nfrom memory import memset_zero\nfrom os import abort\n\n@export\nfn PyInit_fast_distances() -&gt; PythonObject:\n    try:\n        var m = PythonModuleBuilder(\"fast_distances\")\n        m.def_function[compute_distances](\"compute_distances\")\n        return m.finalize()\n    except e:\n        return abort[PythonObject](str(e))\n\nfn compute_distances(points1_obj: PythonObject, points2_obj: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Compute pairwise distances using Mojo for speed.\"\"\"\n    var np = Python.import_module(\"numpy\")\n\n    # Get dimensions\n    var n1 = Int(points1_obj.shape[0])\n    var n2 = Int(points2_obj.shape[0])\n    var d = Int(points1_obj.shape[1])\n\n    # Convert to Mojo arrays for fast processing\n    var points1 = DynamicVector[Float64](n1 * d)\n    var points2 = DynamicVector[Float64](n2 * d)\n\n    # Copy data\n    for i in range(n1 * d):\n        points1.push_back(Float64(points1_obj.flat[i]))\n    for i in range(n2 * d):\n        points2.push_back(Float64(points2_obj.flat[i]))\n\n    # Compute distances\n    var distances = DynamicVector[Float64](n1 * n2)\n\n    @parameter\n    fn compute_row(i: Int):\n        for j in range(n2):\n            var sum_sq = 0.0\n            for k in range(d):\n                var diff = points1[i * d + k] - points2[j * d + k]\n                sum_sq += diff * diff\n            distances[i * n2 + j] = sqrt(sum_sq)\n\n    # Parallel computation\n    for i in range(n1):\n        compute_row(i)\n\n    # Convert back to NumPy\n    var result = np.zeros((n1, n2))\n    for i in range(n1):\n        for j in range(n2):\n            result[i, j] = distances[i * n2 + j]\n\n    return result\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#usage-example","title":"Usage Example","text":"<pre><code>import numpy as np\nimport max.mojo.importer\nimport sys\nsys.path.insert(0, \"\")\nimport fast_distances\n\n# Generate test data\npoints1 = np.random.randn(1000, 3)\npoints2 = np.random.randn(800, 3)\n\n# Compare performance\nimport time\n\nstart = time.time()\ndist_python = compute_distances_python(points1, points2)\npython_time = time.time() - start\n\nstart = time.time()\ndist_mojo = fast_distances.compute_distances(points1, points2)\nmojo_time = time.time() - start\n\nprint(f\"Python time: {python_time:.3f}s\")\nprint(f\"Mojo time: {mojo_time:.3f}s\")\nprint(f\"Speedup: {python_time/mojo_time:.1f}x\")\nprint(f\"Results match: {np.allclose(dist_python, dist_mojo)}\")\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#example-2-string-processing-acceleration","title":"Example 2: String Processing Acceleration","text":""},{"location":"mojo-max/examples/python-integration/#problem_1","title":"Problem","text":"<p>Text preprocessing that's bottlenecking your NLP pipeline.</p>"},{"location":"mojo-max/examples/python-integration/#mojo-string-processor","title":"Mojo String Processor","text":"<pre><code>from python import Python, PythonObject\nfrom python.bindings import PythonModuleBuilder\n\n@export\nfn PyInit_text_utils() -&gt; PythonObject:\n    try:\n        var m = PythonModuleBuilder(\"text_utils\")\n        m.def_function[clean_text](\"clean_text\")\n        m.def_function[tokenize_fast](\"tokenize_fast\")\n        m.def_function[batch_normalize](\"batch_normalize\")\n        return m.finalize()\n    except e:\n        return abort[PythonObject](str(e))\n\nfn clean_text(text_obj: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Remove special characters and normalize whitespace.\"\"\"\n    var text = String(text_obj)\n    var result = String()\n\n    for i in range(len(text)):\n        var c = text[i]\n        if c.isalnum() or c.isspace():\n            if c.isspace():\n                # Normalize whitespace\n                if result and result[-1] != ' ':\n                    result += ' '\n            else:\n                result += c.lower() if c.isupper() else c\n\n    return PythonObject(result.strip())\n\nfn tokenize_fast(text_obj: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Fast word tokenization.\"\"\"\n    var text = String(text_obj)\n    var tokens = List[String]()\n    var current_token = String()\n\n    for i in range(len(text)):\n        var c = text[i]\n        if c.isalnum():\n            current_token += c\n        else:\n            if current_token:\n                tokens.append(current_token)\n                current_token = String()\n\n    if current_token:\n        tokens.append(current_token)\n\n    # Convert to Python list\n    var py_list = PythonObject([])\n    for token in tokens:\n        py_list.append(PythonObject(token[]))\n\n    return py_list\n\nfn batch_normalize(texts_obj: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Normalize a batch of texts in parallel.\"\"\"\n    var texts_len = len(texts_obj)\n    var results = PythonObject([])\n\n    for i in range(texts_len):\n        var cleaned = clean_text(texts_obj[i])\n        results.append(cleaned)\n\n    return results\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#integration-in-python-pipeline","title":"Integration in Python Pipeline","text":"<pre><code>import text_utils\n\nclass FastTextPreprocessor:\n    \"\"\"Drop-in replacement for slow text preprocessing.\"\"\"\n\n    def __init__(self):\n        self.tokenizer = text_utils.tokenize_fast\n        self.cleaner = text_utils.clean_text\n\n    def preprocess_batch(self, texts):\n        \"\"\"Process multiple texts efficiently.\"\"\"\n        # Clean all texts\n        cleaned = text_utils.batch_normalize(texts)\n\n        # Tokenize\n        tokenized = []\n        for text in cleaned:\n            tokens = self.tokenizer(text)\n            tokenized.append(tokens)\n\n        return tokenized\n\n# Usage\npreprocessor = FastTextPreprocessor()\ntexts = [\"Hello, World!\", \"This is a TEST.\", \"Mojo is FAST!!!\"]\nprocessed = preprocessor.preprocess_batch(texts)\nprint(processed)\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#example-3-custom-data-structure","title":"Example 3: Custom Data Structure","text":""},{"location":"mojo-max/examples/python-integration/#problem_2","title":"Problem","text":"<p>Need a high-performance data structure not available in Python.</p>"},{"location":"mojo-max/examples/python-integration/#mojo-implementation","title":"Mojo Implementation","text":"<pre><code>struct RingBuffer[T: AnyType]:\n    \"\"\"High-performance ring buffer implementation.\"\"\"\n    var data: List[T]\n    var capacity: Int\n    var head: Int\n    var tail: Int\n    var size: Int\n\n    fn __init__(out self, capacity: Int):\n        self.capacity = capacity\n        self.data = List[T](capacity=capacity)\n        for _ in range(capacity):\n            self.data.append(T())  # Pre-allocate\n        self.head = 0\n        self.tail = 0\n        self.size = 0\n\n    fn push(mut self, value: T) raises:\n        if self.size == self.capacity:\n            raise Error(\"Buffer is full\")\n\n        self.data[self.tail] = value\n        self.tail = (self.tail + 1) % self.capacity\n        self.size += 1\n\n    fn pop(mut self) raises -&gt; T:\n        if self.size == 0:\n            raise Error(\"Buffer is empty\")\n\n        var value = self.data[self.head]\n        self.head = (self.head + 1) % self.capacity\n        self.size -= 1\n        return value\n\n    fn is_empty(self) -&gt; Bool:\n        return self.size == 0\n\n    fn is_full(self) -&gt; Bool:\n        return self.size == self.capacity\n\n# Python bindings\n@export\nfn PyInit_ring_buffer() -&gt; PythonObject:\n    try:\n        var m = PythonModuleBuilder(\"ring_buffer\")\n        m.def_function[create_buffer](\"create_buffer\")\n        m.def_function[push_item](\"push\")\n        m.def_function[pop_item](\"pop\")\n        m.def_function[get_size](\"size\")\n        return m.finalize()\n    except e:\n        return abort[PythonObject](str(e))\n\nvar global_buffer: RingBuffer[Float64]\n\nfn create_buffer(capacity_obj: PythonObject) raises -&gt; PythonObject:\n    var capacity = Int(capacity_obj)\n    global_buffer = RingBuffer[Float64](capacity)\n    return PythonObject(True)\n\nfn push_item(value_obj: PythonObject) raises -&gt; PythonObject:\n    var value = Float64(value_obj)\n    global_buffer.push(value)\n    return PythonObject(None)\n\nfn pop_item(dummy: PythonObject) raises -&gt; PythonObject:\n    return PythonObject(global_buffer.pop())\n\nfn get_size(dummy: PythonObject) raises -&gt; PythonObject:\n    return PythonObject(global_buffer.size)\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#example-4-integrating-with-steadytext","title":"Example 4: Integrating with SteadyText","text":"<p>Here's how you might accelerate SteadyText's core operations:</p>"},{"location":"mojo-max/examples/python-integration/#mojo-accelerated-text-generation","title":"Mojo-Accelerated Text Generation","text":"<pre><code>from python import Python, PythonObject\nfrom random import random_si64\n\nfn deterministic_token_selection(\n    logits_obj: PythonObject, \n    seed_obj: PythonObject,\n    temperature_obj: PythonObject\n) raises -&gt; PythonObject:\n    \"\"\"Fast deterministic token selection for text generation.\"\"\"\n    var np = Python.import_module(\"numpy\")\n\n    # Convert inputs\n    var seed = Int(seed_obj)\n    var temperature = Float64(temperature_obj)\n    var logits_array = logits_obj\n    var vocab_size = Int(len(logits_obj))\n\n    # Apply temperature\n    if temperature &gt; 0:\n        logits_array = logits_array / temperature\n\n    # Compute softmax efficiently\n    var max_logit = Float64(np.max(logits_array))\n    var exp_sum = 0.0\n\n    for i in range(vocab_size):\n        exp_sum += exp(Float64(logits_array[i]) - max_logit)\n\n    # Create probability distribution\n    var probs = np.zeros(vocab_size)\n    for i in range(vocab_size):\n        probs[i] = exp(Float64(logits_array[i]) - max_logit) / exp_sum\n\n    # Deterministic sampling using seed\n    random.seed(seed)\n    var cumsum = 0.0\n    var rand_val = Float64(random_si64(0, 1000000)) / 1000000.0\n\n    for i in range(vocab_size):\n        cumsum += Float64(probs[i])\n        if cumsum &gt; rand_val:\n            return PythonObject(i)\n\n    return PythonObject(vocab_size - 1)\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#integration-with-steadytext","title":"Integration with SteadyText","text":"<pre><code># In steadytext/core/generator.py\nimport max.mojo.importer\nimport sys\nsys.path.insert(0, \"\")\n\ntry:\n    import mojo_accelerators\n    MOJO_AVAILABLE = True\nexcept ImportError:\n    MOJO_AVAILABLE = False\n\nclass DeterministicGenerator:\n    def __init__(self, model_path, seed=42):\n        self.seed = seed\n        self.use_mojo = MOJO_AVAILABLE\n        # ... existing init code ...\n\n    def _sample_token(self, logits, temperature=1.0):\n        \"\"\"Sample token with optional Mojo acceleration.\"\"\"\n        if self.use_mojo:\n            try:\n                token_id = mojo_accelerators.deterministic_token_selection(\n                    logits, self.seed, temperature\n                )\n                return int(token_id)\n            except:\n                # Fallback to Python implementation\n                pass\n\n        # Existing Python implementation\n        return self._sample_token_python(logits, temperature)\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#performance-tips","title":"Performance Tips","text":""},{"location":"mojo-max/examples/python-integration/#1-batch-operations","title":"1. Batch Operations","text":"<pre><code>fn process_batch(items_obj: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Process multiple items in one call to minimize overhead.\"\"\"\n    var n = len(items_obj)\n    var results = PythonObject([])\n\n    # Convert all at once\n    var mojo_items = List[Float64](capacity=n)\n    for i in range(n):\n        mojo_items.append(Float64(items_obj[i]))\n\n    # Process in Mojo\n    for i in range(n):\n        mojo_items[i] *= 2.0  # Example operation\n\n    # Convert back\n    for item in mojo_items:\n        results.append(PythonObject(item[]))\n\n    return results\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#2-memory-reuse","title":"2. Memory Reuse","text":"<pre><code>struct ProcessorState:\n    \"\"\"Reusable state to avoid allocations.\"\"\"\n    var buffer: List[Float64]\n    var temp_storage: List[Float64]\n\n    fn __init__(out self, max_size: Int):\n        self.buffer = List[Float64](capacity=max_size)\n        self.temp_storage = List[Float64](capacity=max_size)\n\n    fn process(mut self, data: PythonObject) raises -&gt; PythonObject:\n        # Reuse allocated memory\n        self.buffer.clear()\n        # ... processing ...\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#3-parallel-processing","title":"3. Parallel Processing","text":"<pre><code>from algorithm import parallelize\n\nfn parallel_operation(data_obj: PythonObject) raises -&gt; PythonObject:\n    var n = len(data_obj)\n    var results = List[Float64](n)\n\n    @parameter\n    fn process_chunk(i: Int):\n        # Process independent chunks in parallel\n        results[i] = expensive_operation(Float64(data_obj[i]))\n\n    parallelize[process_chunk](n)\n\n    # Convert results\n    var py_results = PythonObject([])\n    for r in results:\n        py_results.append(PythonObject(r[]))\n    return py_results\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#debugging-integration-issues","title":"Debugging Integration Issues","text":""},{"location":"mojo-max/examples/python-integration/#1-type-checking","title":"1. Type Checking","text":"<pre><code>def safe_mojo_call(mojo_func, *args):\n    \"\"\"Wrapper with type checking and fallback.\"\"\"\n    try:\n        # Validate inputs\n        for arg in args:\n            if not isinstance(arg, (int, float, str, list, np.ndarray)):\n                raise TypeError(f\"Unsupported type: {type(arg)}\")\n\n        # Call Mojo function\n        return mojo_func(*args)\n    except Exception as e:\n        print(f\"Mojo call failed: {e}\")\n        # Fallback to Python implementation\n        return python_fallback(*args)\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#2-performance-profiling","title":"2. Performance Profiling","text":"<pre><code>import time\nimport functools\n\ndef profile_mojo(func):\n    \"\"\"Decorator to profile Mojo vs Python performance.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Time Mojo version\n        start = time.perf_counter()\n        mojo_result = func(*args, **kwargs)\n        mojo_time = time.perf_counter() - start\n\n        # Time Python equivalent\n        python_func = globals().get(f\"{func.__name__}_python\")\n        if python_func:\n            start = time.perf_counter()\n            python_result = python_func(*args, **kwargs)\n            python_time = time.perf_counter() - start\n\n            print(f\"{func.__name__}:\")\n            print(f\"  Mojo: {mojo_time:.6f}s\")\n            print(f\"  Python: {python_time:.6f}s\")\n            print(f\"  Speedup: {python_time/mojo_time:.2f}x\")\n\n        return mojo_result\n    return wrapper\n</code></pre>"},{"location":"mojo-max/examples/python-integration/#next-steps","title":"Next Steps","text":"<ul> <li>Explore MAX Serving for model deployment</li> <li>Learn about Mojo Metaprogramming</li> <li>Check the Community Examples</li> </ul>"},{"location":"mojo-max/getting-started/hello-world/","title":"Hello World in Mojo","text":"<p>This guide walks through creating your first Mojo programs, from simple Hello World to more advanced examples.</p>"},{"location":"mojo-max/getting-started/hello-world/#your-first-mojo-program","title":"Your First Mojo Program","text":""},{"location":"mojo-max/getting-started/hello-world/#basic-hello-world","title":"Basic Hello World","text":"<p>Create a file <code>hello.mojo</code>:</p> <pre><code>def main():\n    print(\"Hello, World!\")\n</code></pre> <p>Run it: <pre><code>mojo run hello.mojo\n</code></pre></p> <p>Output: <pre><code>Hello, World!\n</code></pre></p>"},{"location":"mojo-max/getting-started/hello-world/#using-functions","title":"Using Functions","text":"<p>Create <code>greetings.mojo</code>:</p> <pre><code>fn greet(name: String) -&gt; String:\n    return \"Hello, \" + name + \"!\"\n\ndef main():\n    var message = greet(\"Mojo\")\n    print(message)\n\n    # Multiple greetings\n    var names = [\"Alice\", \"Bob\", \"Charlie\"]\n    for name in names:\n        print(greet(name))\n</code></pre>"},{"location":"mojo-max/getting-started/hello-world/#interactive-mojo-repl","title":"Interactive Mojo REPL","text":"<p>Start the Mojo REPL: <pre><code>mojo repl\n</code></pre></p> <p>Try some commands: <pre><code>&gt; print(\"Hello from REPL!\")\nHello from REPL!\n\n&gt; var x = 42\n&gt; var y = x * 2\n&gt; print(y)\n84\n\n&gt; fn square(n: Int) -&gt; Int:\n.     return n * n\n.\n&gt; print(square(5))\n25\n</code></pre></p> <p>Exit with <code>Ctrl+D</code> or <code>exit()</code>.</p>"},{"location":"mojo-max/getting-started/hello-world/#command-line-arguments","title":"Command Line Arguments","text":"<p>Create <code>args.mojo</code>:</p> <pre><code>from sys import argv\n\ndef main():\n    print(\"Program name:\", argv()[0])\n\n    var args = argv()\n    if len(args) &gt; 1:\n        print(\"Hello,\", args[1] + \"!\")\n    else:\n        print(\"Hello, World!\")\n        print(\"Usage:\", args[0], \"&lt;name&gt;\")\n</code></pre> <p>Run it: <pre><code>mojo run args.mojo Alice\n</code></pre></p> <p>Output: <pre><code>Program name: args.mojo\nHello, Alice!\n</code></pre></p>"},{"location":"mojo-max/getting-started/hello-world/#working-with-user-input","title":"Working with User Input","text":"<p>Create <code>interactive.mojo</code>:</p> <pre><code>from python import Python\n\ndef main():\n    var builtins = Python.import_module(\"builtins\")\n\n    # Get user input\n    var name = builtins.input(\"What's your name? \")\n    print(\"Nice to meet you,\", name + \"!\")\n\n    # Get a number\n    var age_str = builtins.input(\"How old are you? \")\n    try:\n        var age = Int(String(age_str))\n        print(\"In 10 years, you'll be\", age + 10)\n    except:\n        print(\"That's not a valid number!\")\n</code></pre>"},{"location":"mojo-max/getting-started/hello-world/#file-io-example","title":"File I/O Example","text":"<p>Create <code>file_io.mojo</code>:</p> <pre><code>from python import Python\n\ndef main():\n    var builtins = Python.import_module(\"builtins\")\n\n    # Write to file\n    print(\"Writing to file...\")\n    var file = builtins.open(\"hello.txt\", \"w\")\n    file.write(\"Hello from Mojo!\\n\")\n    file.write(\"This is line 2.\\n\")\n    file.close()\n\n    # Read from file\n    print(\"\\nReading from file:\")\n    file = builtins.open(\"hello.txt\", \"r\")\n    var content = file.read()\n    print(content)\n    file.close()\n</code></pre>"},{"location":"mojo-max/getting-started/hello-world/#simple-calculator","title":"Simple Calculator","text":"<p>Create <code>calculator.mojo</code>:</p> <pre><code>from python import Python\n\nfn add(a: Float64, b: Float64) -&gt; Float64:\n    return a + b\n\nfn subtract(a: Float64, b: Float64) -&gt; Float64:\n    return a - b\n\nfn multiply(a: Float64, b: Float64) -&gt; Float64:\n    return a * b\n\nfn divide(a: Float64, b: Float64) raises -&gt; Float64:\n    if b == 0:\n        raise Error(\"Division by zero!\")\n    return a / b\n\ndef main():\n    var builtins = Python.import_module(\"builtins\")\n\n    print(\"Simple Calculator\")\n    print(\"-\" * 20)\n\n    try:\n        var num1_str = builtins.input(\"Enter first number: \")\n        var num2_str = builtins.input(\"Enter second number: \")\n\n        var num1 = Float64(String(num1_str))\n        var num2 = Float64(String(num2_str))\n\n        print(\"\\nResults:\")\n        print(num1, \"+\", num2, \"=\", add(num1, num2))\n        print(num1, \"-\", num2, \"=\", subtract(num1, num2))\n        print(num1, \"*\", num2, \"=\", multiply(num1, num2))\n\n        try:\n            print(num1, \"/\", num2, \"=\", divide(num1, num2))\n        except e:\n            print(\"Division error:\", e)\n\n    except:\n        print(\"Invalid input! Please enter numbers.\")\n</code></pre>"},{"location":"mojo-max/getting-started/hello-world/#working-with-lists","title":"Working with Lists","text":"<p>Create <code>lists.mojo</code>:</p> <pre><code>def main():\n    # Create a list\n    var numbers = List[Int]()\n\n    # Add elements\n    for i in range(5):\n        numbers.append(i * i)\n\n    # Print list\n    print(\"Squares:\", end=\" \")\n    for num in numbers:\n        print(num[], end=\" \")\n    print()\n\n    # Sum elements\n    var total = 0\n    for num in numbers:\n        total += num[]\n    print(\"Sum:\", total)\n\n    # List operations\n    print(\"Length:\", len(numbers))\n    print(\"First:\", numbers[0])\n    print(\"Last:\", numbers[len(numbers) - 1])\n</code></pre>"},{"location":"mojo-max/getting-started/hello-world/#temperature-converter","title":"Temperature Converter","text":"<p>Create <code>temperature.mojo</code>:</p> <pre><code>fn celsius_to_fahrenheit(celsius: Float64) -&gt; Float64:\n    return (celsius * 9.0 / 5.0) + 32.0\n\nfn fahrenheit_to_celsius(fahrenheit: Float64) -&gt; Float64:\n    return (fahrenheit - 32.0) * 5.0 / 9.0\n\nfn celsius_to_kelvin(celsius: Float64) -&gt; Float64:\n    return celsius + 273.15\n\ndef main():\n    print(\"Temperature Converter\")\n    print(\"=\" * 25)\n\n    # Example conversions\n    var temp_c = 25.0\n    print(\"\\n\", temp_c, \"\u00b0C is:\")\n    print(\"  \", celsius_to_fahrenheit(temp_c), \"\u00b0F\")\n    print(\"  \", celsius_to_kelvin(temp_c), \"K\")\n\n    var temp_f = 77.0\n    print(\"\\n\", temp_f, \"\u00b0F is:\")\n    print(\"  \", fahrenheit_to_celsius(temp_f), \"\u00b0C\")\n\n    # Temperature table\n    print(\"\\nConversion Table (C -&gt; F):\")\n    print(\"C    F\")\n    print(\"-\" * 10)\n    for c in range(-10, 41, 10):\n        var f = celsius_to_fahrenheit(Float64(c))\n        print(c, \"  \", f)\n</code></pre>"},{"location":"mojo-max/getting-started/hello-world/#fizzbuzz","title":"FizzBuzz","text":"<p>Create <code>fizzbuzz.mojo</code>:</p> <pre><code>def main():\n    print(\"FizzBuzz from 1 to 30:\")\n    print(\"-\" * 20)\n\n    for i in range(1, 31):\n        if i % 15 == 0:\n            print(\"FizzBuzz\")\n        elif i % 3 == 0:\n            print(\"Fizz\")\n        elif i % 5 == 0:\n            print(\"Buzz\")\n        else:\n            print(i)\n</code></pre>"},{"location":"mojo-max/getting-started/hello-world/#prime-number-checker","title":"Prime Number Checker","text":"<p>Create <code>primes.mojo</code>:</p> <pre><code>fn is_prime(n: Int) -&gt; Bool:\n    if n &lt;= 1:\n        return False\n    if n &lt;= 3:\n        return True\n    if n % 2 == 0 or n % 3 == 0:\n        return False\n\n    var i = 5\n    while i * i &lt;= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return False\n        i += 6\n\n    return True\n\ndef main():\n    print(\"Prime numbers between 1 and 100:\")\n\n    var count = 0\n    for n in range(2, 101):\n        if is_prime(n):\n            print(n, end=\" \")\n            count += 1\n            if count % 10 == 0:\n                print()  # New line every 10 primes\n\n    print(\"\\n\\nTotal primes found:\", count)\n</code></pre>"},{"location":"mojo-max/getting-started/hello-world/#building-standalone-executables","title":"Building Standalone Executables","text":"<p>You can compile Mojo programs to standalone executables:</p> <pre><code># Build executable\nmojo build hello.mojo -o hello\n\n# Run executable\n./hello\n</code></pre> <p>For optimized builds: <pre><code>mojo build -O3 calculator.mojo -o calculator\n</code></pre></p>"},{"location":"mojo-max/getting-started/hello-world/#next-steps","title":"Next Steps","text":"<p>Now that you've written your first Mojo programs:</p> <ol> <li>Learn the Language: Continue with Mojo Basics</li> <li>Try Python Integration: See Python Interop</li> <li>Build Something: Create a small project using Mojo</li> <li>Explore Performance: Try optimizing numeric code</li> <li>Join the Community: Share your projects on the forum</li> </ol>"},{"location":"mojo-max/getting-started/hello-world/#tips-for-beginners","title":"Tips for Beginners","text":"<ol> <li>Start Simple: Don't try to optimize everything at first</li> <li>Use Python: Import Python libraries when needed</li> <li>Read Errors: Mojo's error messages are helpful</li> <li>Experiment: Try different approaches in the REPL</li> <li>Ask Questions: The community is friendly and helpful</li> </ol> <p>Happy coding with Mojo! \ud83d\udd25</p>"},{"location":"mojo-max/getting-started/installation/","title":"Installation Guide","text":"<p>This guide covers installing Mojo and MAX on your system. The Community Edition supports Mac, Linux, and WSL environments.</p>"},{"location":"mojo-max/getting-started/installation/#system-requirements","title":"System Requirements","text":"<ul> <li>Operating System: macOS, Linux, or Windows (via WSL)</li> <li>Python: 3.8 or later</li> <li>Hardware: Any x86_64 CPU; NVIDIA GPUs supported for acceleration</li> </ul>"},{"location":"mojo-max/getting-started/installation/#installation-methods","title":"Installation Methods","text":""},{"location":"mojo-max/getting-started/installation/#1-using-pip-recommended","title":"1. Using pip (Recommended)","text":"<pre><code># Create a virtual environment\npython3 -m venv mojo-env\nsource mojo-env/bin/activate  # On Windows: mojo-env\\Scripts\\activate\n\n# Install the modular package\npip install modular \\\n  --extra-index-url https://download.pytorch.org/whl/cpu \\\n  --extra-index-url https://modular.gateway.scarf.sh/simple/\n</code></pre>"},{"location":"mojo-max/getting-started/installation/#2-using-uv-fast-python-package-manager","title":"2. Using uv (Fast Python Package Manager)","text":"<pre><code># Install uv\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create project and install\nuv init my-project &amp;&amp; cd my-project\nuv venv &amp;&amp; source .venv/bin/activate\nuv pip install modular \\\n  --extra-index-url https://download.pytorch.org/whl/cpu \\\n  --extra-index-url https://modular.gateway.scarf.sh/simple/ \\\n  --index-strategy unsafe-best-match\n</code></pre>"},{"location":"mojo-max/getting-started/installation/#3-using-conda","title":"3. Using conda","text":"<pre><code># Create conda environment\nconda create -n mojo-env\nconda activate mojo-env\n\n# Install from Modular's conda channel\nconda install -c conda-forge -c https://conda.modular.com/max/ modular\n</code></pre>"},{"location":"mojo-max/getting-started/installation/#4-using-pixi","title":"4. Using pixi","text":"<pre><code># Install pixi\ncurl -fsSL https://pixi.sh/install.sh | sh\n\n# Create project\npixi init my-project -c https://conda.modular.com/max/ -c conda-forge\ncd my-project\n\n# Add modular package\npixi add \"modular=25.4\"\npixi shell\n</code></pre>"},{"location":"mojo-max/getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>After installation, verify everything is working:</p> <pre><code># Check Mojo version\nmojo --version\n\n# Check MAX CLI\nmax --version\n\n# Run a simple Mojo program\necho 'def main(): print(\"Hello from Mojo!\")' &gt; hello.mojo\nmojo run hello.mojo\n</code></pre>"},{"location":"mojo-max/getting-started/installation/#environment-variables","title":"Environment Variables","text":"<p>Optional environment variables for configuration:</p> <pre><code># Set custom cache directory (default: ~/.cache/modular)\nexport MODULAR_HOME=/path/to/cache\n\n# Enable debug logging\nexport MODULAR_LOG_LEVEL=debug\n</code></pre>"},{"location":"mojo-max/getting-started/installation/#docker-installation","title":"Docker Installation","text":"<p>For containerized deployments:</p> <pre><code># Pull the MAX container\ndocker pull modular/max:latest\n\n# Run with GPU support\ndocker run --gpus all -it modular/max:latest\n</code></pre>"},{"location":"mojo-max/getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mojo-max/getting-started/installation/#common-issues","title":"Common Issues","text":"<ol> <li>Import errors: Ensure you've activated your virtual environment</li> <li>Permission denied: Use <code>sudo</code> for system-wide installation (not recommended)</li> <li>GPU not detected: Install NVIDIA drivers and CUDA toolkit</li> <li>SSL errors: Update certificates or use <code>--trusted-host</code> flag</li> </ol>"},{"location":"mojo-max/getting-started/installation/#getting-help","title":"Getting Help","text":"<ul> <li>Community Discord: Join via modular.com/community</li> <li>GitHub Issues: github.com/modular/mojo/issues</li> <li>Forum: forum.modular.com</li> </ul>"},{"location":"mojo-max/getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Follow the Quickstart Guide to run your first model</li> <li>Learn Mojo Basics for writing high-performance code</li> <li>Explore the Model Repository for pre-optimized models</li> </ul>"},{"location":"mojo-max/getting-started/quickstart/","title":"Quickstart Guide","text":"<p>Get up and running with MAX inference in minutes. This guide shows you how to serve a model locally and make inference requests.</p>"},{"location":"mojo-max/getting-started/quickstart/#step-1-set-up-your-project","title":"Step 1: Set Up Your Project","text":"<pre><code># Create a project directory\nmkdir max-quickstart &amp;&amp; cd max-quickstart\n\n# Create and activate a virtual environment\npython3 -m venv .venv\nsource .venv/bin/activate  # On Windows: .venv\\Scripts\\activate\n\n# Install required packages\npip install modular openai \\\n  --extra-index-url https://download.pytorch.org/whl/cpu \\\n  --extra-index-url https://modular.gateway.scarf.sh/simple/\n</code></pre>"},{"location":"mojo-max/getting-started/quickstart/#step-2-run-offline-inference","title":"Step 2: Run Offline Inference","text":"<p>Create a file <code>offline_inference.py</code>:</p> <pre><code>from max.entrypoints.llm import LLM\nfrom max.pipelines import PipelineConfig\n\ndef main():\n    # Use a non-gated model (no HuggingFace token required)\n    model_path = \"modularai/Llama-3.1-8B-Instruct-GGUF\"\n    pipeline_config = PipelineConfig(model_path=model_path)\n    llm = LLM(pipeline_config)\n\n    prompts = [\n        \"In the beginning, there was\",\n        \"I believe the meaning of life is\",\n        \"The fastest way to learn python is\",\n    ]\n\n    print(\"Generating responses...\")\n    responses = llm.generate(prompts, max_new_tokens=50)\n    for i, (prompt, response) in enumerate(zip(prompts, responses)):\n        print(f\"========== Response {i} ==========\")\n        print(prompt + response)\n        print()\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run it:</p> <pre><code>python offline_inference.py\n</code></pre> <p>Expected output: <pre><code>========== Response 0 ==========\nIn the beginning, there was darkness. Then came the light...\n\n========== Response 1 ==========\nI believe the meaning of life is to find purpose and happiness...\n\n========== Response 2 ==========\nThe fastest way to learn python is through hands-on practice...\n</code></pre></p>"},{"location":"mojo-max/getting-started/quickstart/#step-3-start-an-inference-server","title":"Step 3: Start an Inference Server","text":"<p>Start a local endpoint with OpenAI-compatible API:</p> <pre><code>max serve --model-path=modularai/Llama-3.1-8B-Instruct-GGUF\n</code></pre> <p>Wait for the message: <pre><code>Server ready on http://0.0.0.0:8000 (Press CTRL+C to quit)\n</code></pre></p>"},{"location":"mojo-max/getting-started/quickstart/#step-4-send-requests-to-the-server","title":"Step 4: Send Requests to the Server","text":"<p>Create <code>chat_client.py</code>:</p> <pre><code>from openai import OpenAI\n\n# Configure client to use local endpoint\nclient = OpenAI(\n    base_url=\"http://0.0.0.0:8000/v1\",\n    api_key=\"EMPTY\",  # MAX doesn't require an API key\n)\n\n# Send a chat completion request\ncompletion = client.chat.completions.create(\n    model=\"modularai/Llama-3.1-8B-Instruct-GGUF\",\n    messages=[\n        {\n            \"role\": \"user\",\n            \"content\": \"Who won the world series in 2020?\"\n        },\n    ],\n)\n\nprint(completion.choices[0].message.content)\n</code></pre> <p>Run it (in a new terminal):</p> <pre><code>python chat_client.py\n</code></pre> <p>Expected output: <pre><code>The Los Angeles Dodgers won the 2020 World Series. They defeated the Tampa Bay Rays 4 games to 2.\n</code></pre></p>"},{"location":"mojo-max/getting-started/quickstart/#common-use-cases","title":"Common Use Cases","text":""},{"location":"mojo-max/getting-started/quickstart/#1-streaming-responses","title":"1. Streaming Responses","text":"<pre><code># Stream tokens as they're generated\nstream = client.chat.completions.create(\n    model=\"modularai/Llama-3.1-8B-Instruct-GGUF\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True,\n)\n\nfor chunk in stream:\n    if chunk.choices[0].delta.content is not None:\n        print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"mojo-max/getting-started/quickstart/#2-custom-generation-parameters","title":"2. Custom Generation Parameters","text":"<pre><code>completion = client.chat.completions.create(\n    model=\"modularai/Llama-3.1-8B-Instruct-GGUF\",\n    messages=[{\"role\": \"user\", \"content\": \"Write a haiku\"}],\n    temperature=0.7,\n    max_tokens=100,\n    top_p=0.9,\n)\n</code></pre>"},{"location":"mojo-max/getting-started/quickstart/#3-using-different-models","title":"3. Using Different Models","text":"<pre><code># Serve a different model\nmax serve --model-path=microsoft/Phi-3.5-mini-instruct\n\n# Or use a larger model\nmax serve --model-path=meta-llama/Llama-3.1-70B-Instruct\n</code></pre>"},{"location":"mojo-max/getting-started/quickstart/#server-options","title":"Server Options","text":"<pre><code># Change port\nmax serve --model-path=MODEL --port=8080\n\n# Limit concurrent requests\nmax serve --model-path=MODEL --max-concurrent-requests=10\n\n# Enable GPU acceleration (if available)\nmax serve --model-path=MODEL --device=cuda\n\n# Use multiple GPUs\nmax serve --model-path=MODEL --tensor-parallel-size=2\n</code></pre>"},{"location":"mojo-max/getting-started/quickstart/#performance-tips","title":"Performance Tips","text":"<ol> <li>Model Format: Use GGUF models for best performance/memory trade-off</li> <li>Batch Processing: Send multiple prompts at once for better throughput</li> <li>Caching: Models are cached locally after first download</li> <li>GPU Usage: Automatically uses GPU if available</li> </ol>"},{"location":"mojo-max/getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Explore 500+ supported models</li> <li>Learn about advanced serving features</li> <li>Try Mojo for custom operators</li> <li>Deploy with Docker</li> </ul>"},{"location":"mojo-max/getting-started/quickstart/#troubleshooting","title":"Troubleshooting","text":""},{"location":"mojo-max/getting-started/quickstart/#model-download-issues","title":"Model Download Issues","text":"<ul> <li>Check internet connection</li> <li>Verify model name is correct</li> <li>Some models require HuggingFace token</li> </ul>"},{"location":"mojo-max/getting-started/quickstart/#out-of-memory","title":"Out of Memory","text":"<ul> <li>Use quantized models (Q4, Q8)</li> <li>Reduce batch size</li> <li>Use CPU offloading</li> </ul>"},{"location":"mojo-max/getting-started/quickstart/#slow-performance","title":"Slow Performance","text":"<ul> <li>Enable GPU with <code>--device=cuda</code></li> <li>Use smaller models for testing</li> <li>Check system resources with <code>htop</code></li> </ul>"},{"location":"mojo-max/max-inference/overview/","title":"MAX Inference Overview","text":"<p>MAX is a high-performance inference framework that enables you to deploy AI models with state-of-the-art performance. It provides an OpenAI-compatible API and supports 500+ models out of the box.</p>"},{"location":"mojo-max/max-inference/overview/#key-features","title":"Key Features","text":""},{"location":"mojo-max/max-inference/overview/#1-universal-model-support","title":"1. Universal Model Support","text":"<ul> <li>500+ Pre-optimized Models: LLMs, vision models, embeddings, and more</li> <li>Multiple Formats: GGUF, PyTorch, Hugging Face models</li> <li>Automatic Optimization: Hardware-specific optimizations applied automatically</li> </ul>"},{"location":"mojo-max/max-inference/overview/#2-openai-compatible-api","title":"2. OpenAI-Compatible API","text":"<ul> <li>Drop-in replacement for OpenAI endpoints</li> <li>Support for chat completions, embeddings, and more</li> <li>Compatible with existing OpenAI client libraries</li> </ul>"},{"location":"mojo-max/max-inference/overview/#3-production-ready","title":"3. Production-Ready","text":"<ul> <li>High-performance serving with batching</li> <li>Multi-GPU support with tensor parallelism</li> <li>Kubernetes-ready with scaling capabilities</li> <li>Docker container for easy deployment</li> </ul>"},{"location":"mojo-max/max-inference/overview/#4-hardware-flexibility","title":"4. Hardware Flexibility","text":"<ul> <li>CPUs: Optimized for x86_64 processors</li> <li>GPUs: NVIDIA GPUs (unlimited in Community Edition)</li> <li>Other Accelerators: AMD GPUs (up to 8 in Community Edition)</li> </ul>"},{"location":"mojo-max/max-inference/overview/#architecture","title":"Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Python API    \u2502     \u2502   REST API      \u2502\n\u2502  (Offline Mode) \u2502     \u2502 (Server Mode)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                       \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n            \u2502   MAX Engine    \u2502\n            \u2502                 \u2502\n            \u2502 \u2022 Model Loading \u2502\n            \u2502 \u2022 Optimization  \u2502\n            \u2502 \u2022 Execution     \u2502\n            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                       \u2502\n    \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510            \u250c\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2510\n    \u2502   CPU   \u2502            \u2502   GPU   \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#supported-model-types","title":"Supported Model Types","text":""},{"location":"mojo-max/max-inference/overview/#large-language-models-llms","title":"Large Language Models (LLMs)","text":"<ul> <li>Llama Family: Llama 3.1, Llama 3.2, CodeLlama</li> <li>Mistral Models: Mistral, Mixtral, Codestral</li> <li>Other Popular: Qwen, Phi, Gemma, DeepSeek</li> </ul>"},{"location":"mojo-max/max-inference/overview/#vision-models","title":"Vision Models","text":"<ul> <li>CLIP: Image embeddings and classification</li> <li>Stable Diffusion: Image generation</li> <li>Vision Transformers: Image classification</li> </ul>"},{"location":"mojo-max/max-inference/overview/#specialized-models","title":"Specialized Models","text":"<ul> <li>Embedding Models: Text embeddings for RAG</li> <li>Code Models: Code generation and completion</li> <li>Audio Models: Whisper for transcription</li> </ul>"},{"location":"mojo-max/max-inference/overview/#performance-features","title":"Performance Features","text":""},{"location":"mojo-max/max-inference/overview/#1-quantization-support","title":"1. Quantization Support","text":"<pre><code># Use quantized models for better performance/memory trade-off\npipeline = PipelineConfig(\n    model_path=\"model.Q4_K_M.gguf\",  # 4-bit quantized\n)\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#2-continuous-batching","title":"2. Continuous Batching","text":"<ul> <li>Dynamic batching of requests</li> <li>Optimal GPU utilization</li> <li>Reduced latency for concurrent requests</li> </ul>"},{"location":"mojo-max/max-inference/overview/#3-kv-cache-optimization","title":"3. KV Cache Optimization","text":"<ul> <li>Efficient memory management</li> <li>Paged attention for long contexts</li> <li>Cache sharing across requests</li> </ul>"},{"location":"mojo-max/max-inference/overview/#4-speculative-decoding","title":"4. Speculative Decoding","text":"<pre><code># Use a draft model for faster generation\npipeline = PipelineConfig(\n    model_path=\"main_model.gguf\",\n    draft_model_path=\"draft_model.gguf\",\n)\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#deployment-options","title":"Deployment Options","text":""},{"location":"mojo-max/max-inference/overview/#1-local-development","title":"1. Local Development","text":"<pre><code># Quick start with CLI\nmax serve --model-path=modularai/Llama-3.1-8B-Instruct-GGUF\n\n# Python API for integration\nfrom max.entrypoints.llm import LLM\nllm = LLM(PipelineConfig(model_path=\"...\"))\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#2-docker-deployment","title":"2. Docker Deployment","text":"<pre><code># Run with Docker\ndocker run -p 8000:8000 modular/max \\\n  max serve --model-path=MODEL_ID\n\n# With GPU support\ndocker run --gpus all -p 8000:8000 modular/max \\\n  max serve --model-path=MODEL_ID\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#3-kubernetescloud","title":"3. Kubernetes/Cloud","text":"<ul> <li>Pre-built Kubernetes manifests</li> <li>Auto-scaling support</li> <li>Multi-node inference for large models</li> </ul>"},{"location":"mojo-max/max-inference/overview/#model-management","title":"Model Management","text":""},{"location":"mojo-max/max-inference/overview/#model-sources","title":"Model Sources","text":"<ol> <li>Hugging Face Hub: Direct model IDs</li> <li>Local Files: Pre-downloaded models</li> <li>Custom Models: Your fine-tuned models</li> </ol>"},{"location":"mojo-max/max-inference/overview/#model-caching","title":"Model Caching","text":"<pre><code># Models are cached locally after first download\n# Default: ~/.cache/modular/models/\n\n# Custom cache directory\nimport os\nos.environ[\"MODULAR_CACHE_DIR\"] = \"/path/to/cache\"\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#api-compatibility","title":"API Compatibility","text":""},{"location":"mojo-max/max-inference/overview/#openai-endpoints","title":"OpenAI Endpoints","text":"<ul> <li><code>/v1/chat/completions</code> - Chat generation</li> <li><code>/v1/completions</code> - Text completion</li> <li><code>/v1/embeddings</code> - Text embeddings</li> <li><code>/v1/models</code> - List available models</li> </ul>"},{"location":"mojo-max/max-inference/overview/#extended-features","title":"Extended Features","text":"<ul> <li>Structured output with JSON schema</li> <li>Function calling for agents</li> <li>Custom sampling parameters</li> <li>Log probabilities access</li> </ul>"},{"location":"mojo-max/max-inference/overview/#integration-examples","title":"Integration Examples","text":""},{"location":"mojo-max/max-inference/overview/#basic-chat-completion","title":"Basic Chat Completion","text":"<pre><code>from openai import OpenAI\n\nclient = OpenAI(\n    base_url=\"http://localhost:8000/v1\",\n    api_key=\"EMPTY\"\n)\n\nresponse = client.chat.completions.create(\n    model=\"model-name\",\n    messages=[{\"role\": \"user\", \"content\": \"Hello!\"}]\n)\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#streaming-response","title":"Streaming Response","text":"<pre><code>stream = client.chat.completions.create(\n    model=\"model-name\",\n    messages=[{\"role\": \"user\", \"content\": \"Tell me a story\"}],\n    stream=True\n)\n\nfor chunk in stream:\n    print(chunk.choices[0].delta.content, end=\"\")\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#embeddings","title":"Embeddings","text":"<pre><code>embeddings = client.embeddings.create(\n    model=\"model-name\",\n    input=[\"Text to embed\", \"Another text\"]\n)\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"mojo-max/max-inference/overview/#logging","title":"Logging","text":"<pre><code># Enable debug logging\nMODULAR_LOG_LEVEL=debug max serve --model-path=MODEL\n\n# Log to file\nmax serve --model-path=MODEL --log-file=server.log\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#metrics","title":"Metrics","text":"<ul> <li>Request latency</li> <li>Throughput (tokens/second)</li> <li>GPU utilization</li> <li>Memory usage</li> </ul>"},{"location":"mojo-max/max-inference/overview/#health-checks","title":"Health Checks","text":"<pre><code># Check server status\ncurl http://localhost:8000/health\n\n# List loaded models\ncurl http://localhost:8000/v1/models\n</code></pre>"},{"location":"mojo-max/max-inference/overview/#best-practices","title":"Best Practices","text":"<ol> <li>Model Selection</li> <li>Use quantized models for better memory efficiency</li> <li>Choose model size based on available hardware</li> <li> <p>Test different quantization levels (Q4, Q5, Q8)</p> </li> <li> <p>Performance Tuning</p> </li> <li>Adjust batch size for throughput vs latency</li> <li>Use tensor parallelism for large models</li> <li> <p>Enable GPU acceleration when available</p> </li> <li> <p>Production Deployment</p> </li> <li>Use Docker for reproducible deployments</li> <li>Implement proper health checks</li> <li>Monitor resource usage</li> <li>Set up auto-scaling based on load</li> </ol>"},{"location":"mojo-max/max-inference/overview/#comparison-with-alternatives","title":"Comparison with Alternatives","text":"Feature MAX vLLM TGI Ollama OpenAI API \u2705 \u2705 \u2705 \u2705 Model Support 500+ Many Many Limited Quantization \u2705 Limited \u2705 \u2705 Multi-GPU \u2705 \u2705 \u2705 \u274c CPU Optimized \u2705 \u274c Limited \u2705 Custom Ops \u2705 \u274c \u274c \u274c"},{"location":"mojo-max/max-inference/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Try the Quickstart Guide</li> <li>Learn about Serving Models</li> <li>Explore Offline Inference</li> <li>Check Deployment Options</li> </ul>"},{"location":"mojo-max/mojo-language/basics/","title":"Mojo Language Basics","text":"<p>Mojo is a Python-compatible systems programming language that combines Python's ease of use with the performance of C++ and Rust. This guide covers the essential Mojo syntax and concepts.</p>"},{"location":"mojo-max/mojo-language/basics/#hello-world","title":"Hello World","text":"<p>Every Mojo program needs a <code>main()</code> function as the entry point:</p> <pre><code>def main():\n    print(\"Hello, world!\")\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#variables","title":"Variables","text":"<p>Mojo supports both implicit and explicit variable declarations:</p> <pre><code>def main():\n    # Implicit declaration\n    x = 10\n    y = x * x\n\n    # Explicit declaration with var\n    var z: Int = 100\n    var name: String = \"Mojo\"\n\n    # Type is inferred if not specified\n    var result = x + z  # Int type inferred\n</code></pre> <p>Variables in Mojo are statically typed - once a type is assigned, it cannot change:</p> <pre><code>x = 10\nx = \"Hello\"  # Error: Cannot convert \"StringLiteral\" to \"Int\"\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#functions","title":"Functions","text":"<p>Mojo supports two function declaration styles:</p>"},{"location":"mojo-max/mojo-language/basics/#def-functions-python-style","title":"def Functions (Python-style)","text":"<pre><code>def greet(name: String) -&gt; String:\n    return \"Hello, \" + name + \"!\"\n\ndef add(x: Int, y: Int) -&gt; Int:\n    return x + y\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#fn-functions-strict-mode","title":"fn Functions (Strict mode)","text":"<pre><code>fn multiply(x: Int, y: Int) -&gt; Int:\n    return x * y\n\nfn process(data: String) -&gt; None:\n    print(\"Processing:\", data)\n</code></pre> <p>Key differences: - <code>fn</code> functions have stricter error handling - <code>fn</code> functions require explicit type annotations - <code>fn</code> functions have immutable arguments by default</p>"},{"location":"mojo-max/mojo-language/basics/#control-flow","title":"Control Flow","text":""},{"location":"mojo-max/mojo-language/basics/#conditionals","title":"Conditionals","text":"<pre><code>def check_value(x: Int):\n    if x &gt; 0:\n        print(\"Positive\")\n    elif x &lt; 0:\n        print(\"Negative\")\n    else:\n        print(\"Zero\")\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#loops","title":"Loops","text":"<pre><code>def loop_examples():\n    # For loop\n    for i in range(5):\n        print(i)\n\n    # While loop\n    var count = 0\n    while count &lt; 3:\n        print(\"Count:\", count)\n        count += 1\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#structs","title":"Structs","text":"<p>Structs are Mojo's primary way to define custom types:</p> <pre><code>struct Point:\n    var x: Float64\n    var y: Float64\n\n    fn __init__(out self, x: Float64, y: Float64):\n        self.x = x\n        self.y = y\n\n    fn distance_to(self, other: Point) -&gt; Float64:\n        var dx = self.x - other.x\n        var dy = self.y - other.y\n        return (dx * dx + dy * dy).sqrt()\n\ndef use_point():\n    var p1 = Point(0.0, 0.0)\n    var p2 = Point(3.0, 4.0)\n    print(p1.distance_to(p2))  # Prints 5.0\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#simplified-structs-with-decorators","title":"Simplified Structs with Decorators","text":"<pre><code>@fieldwise_init\nstruct Person(Copyable, Movable):\n    var name: String\n    var age: Int\n\n    def describe(self):\n        print(self.name, \"is\", self.age, \"years old\")\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#traits","title":"Traits","text":"<p>Traits define interfaces that structs can implement:</p> <pre><code>trait Drawable:\n    fn draw(self): ...\n\ntrait Movable:\n    fn move_to(mut self, x: Int, y: Int): ...\n\nstruct Shape(Drawable, Movable):\n    var x: Int\n    var y: Int\n\n    fn draw(self):\n        print(\"Drawing at\", self.x, self.y)\n\n    fn move_to(mut self, x: Int, y: Int):\n        self.x = x\n        self.y = y\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#memory-management","title":"Memory Management","text":"<p>Mojo provides fine-grained control over memory:</p> <pre><code>fn memory_example():\n    # Stack allocation\n    var x = 42\n\n    # Heap allocation with Pointer\n    var ptr = Pointer[Int].alloc(1)\n    ptr.store(100)\n    print(ptr.load())\n    ptr.free()\n\n    # Reference semantics\n    var original = String(\"Hello\")\n    var borrowed = original  # Copies reference, not data\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#error-handling","title":"Error Handling","text":"<p>Mojo uses exceptions for error handling:</p> <pre><code>fn divide(a: Int, b: Int) raises -&gt; Int:\n    if b == 0:\n        raise Error(\"Division by zero\")\n    return a // b\n\ndef safe_divide():\n    try:\n        var result = divide(10, 0)\n        print(result)\n    except e:\n        print(\"Error:\", e)\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#collections","title":"Collections","text":""},{"location":"mojo-max/mojo-language/basics/#lists","title":"Lists","text":"<pre><code>def list_example():\n    var numbers = List[Int]()\n    numbers.append(1)\n    numbers.append(2)\n    numbers.append(3)\n\n    for num in numbers:\n        print(num)\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#dictionaries","title":"Dictionaries","text":"<pre><code>def dict_example():\n    var ages = Dict[String, Int]()\n    ages[\"Alice\"] = 30\n    ages[\"Bob\"] = 25\n\n    print(ages[\"Alice\"])\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#type-parameters","title":"Type Parameters","text":"<p>Mojo supports generic programming with type parameters:</p> <pre><code>struct Container[T: AnyType]:\n    var value: T\n\n    fn __init__(out self, value: T):\n        self.value = value\n\n    fn get(self) -&gt; T:\n        return self.value\n\ndef use_container():\n    var int_container = Container[Int](42)\n    var str_container = Container[String](\"Hello\")\n\n    print(int_container.get())\n    print(str_container.get())\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#simd-and-vectorization","title":"SIMD and Vectorization","text":"<p>Mojo has built-in support for SIMD operations:</p> <pre><code>from math import sqrt\n\nfn simd_example():\n    # Process 4 float32 values at once\n    var vec = SIMD[DType.float32, 4](1.0, 4.0, 9.0, 16.0)\n    var result = sqrt(vec)\n    print(result)  # [1.0, 2.0, 3.0, 4.0]\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#compile-time-programming","title":"Compile-Time Programming","text":"<p>Use parameters for compile-time computation:</p> <pre><code>fn repeat[count: Int](msg: String):\n    @parameter\n    for i in range(count):\n        print(msg)\n\ndef use_repeat():\n    repeat[3](\"Hello\")  # Unrolled at compile time\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#best-practices","title":"Best Practices","text":"<ol> <li>Use <code>fn</code> for performance-critical code: Better optimization</li> <li>Prefer <code>var</code> for clarity: Makes code more readable</li> <li>Use type annotations: Helps catch errors early</li> <li>Leverage SIMD: For numerical computations</li> <li>Profile before optimizing: Mojo is already fast</li> </ol>"},{"location":"mojo-max/mojo-language/basics/#common-patterns","title":"Common Patterns","text":""},{"location":"mojo-max/mojo-language/basics/#builder-pattern","title":"Builder Pattern","text":"<pre><code>struct Config:\n    var host: String\n    var port: Int\n    var debug: Bool\n\n    fn __init__(out self):\n        self.host = \"localhost\"\n        self.port = 8080\n        self.debug = False\n\n    fn with_host(mut self, host: String) -&gt; Self:\n        self.host = host\n        return self\n\n    fn with_port(mut self, port: Int) -&gt; Self:\n        self.port = port\n        return self\n\ndef use_builder():\n    var config = Config().with_host(\"0.0.0.0\").with_port(3000)\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#resource-management","title":"Resource Management","text":"<pre><code>struct File:\n    var handle: Int\n\n    fn __init__(out self, path: String):\n        # Open file\n        self.handle = 1  # Simplified\n\n    fn __del__(owned self):\n        # Automatically close file\n        print(\"Closing file\")\n</code></pre>"},{"location":"mojo-max/mojo-language/basics/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about Python Interoperability</li> <li>Explore Advanced Functions</li> <li>Understand Structs and Traits</li> <li>Try Metaprogramming</li> </ul>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/","title":"Calling Mojo from Python","text":"<p>This guide explains how to create Mojo extensions that can be called from Python, allowing you to write performance-critical code in Mojo while keeping your Python application structure.</p> <p>Beta Feature: This functionality is in early development and the API may change significantly.</p>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#overview","title":"Overview","text":"<p>Mojo can compile to Python extension modules (<code>.so</code> files) that Python can import like any other module. This enables you to: - Write performance-critical functions in Mojo - Keep your existing Python codebase - Gradually migrate bottlenecks to Mojo</p>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#basic-example","title":"Basic Example","text":""},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#project-structure","title":"Project Structure","text":"<pre><code>my_project/\n\u251c\u2500\u2500 main.py           # Python entry point\n\u2514\u2500\u2500 mojo_math.mojo    # Mojo module\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#step-1-write-mojo-module","title":"Step 1: Write Mojo Module","text":"<p>Create <code>mojo_math.mojo</code>:</p> <pre><code>from python import PythonObject\nfrom python.bindings import PythonModuleBuilder\nimport math\nfrom os import abort\n\n@export\nfn PyInit_mojo_math() -&gt; PythonObject:\n    \"\"\"Initialize the Python module.\"\"\"\n    try:\n        var m = PythonModuleBuilder(\"mojo_math\")\n\n        # Register functions\n        m.def_function[factorial](\"factorial\", docstring=\"Compute n!\")\n        m.def_function[fibonacci](\"fibonacci\", docstring=\"Compute nth Fibonacci number\")\n        m.def_function[is_prime](\"is_prime\", docstring=\"Check if n is prime\")\n\n        return m.finalize()\n    except e:\n        return abort[PythonObject](String(\"Error creating module: \" + str(e)))\n\nfn factorial(py_n: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Compute factorial of n.\"\"\"\n    var n = Int(py_n)\n    if n &lt; 0:\n        raise Error(\"Factorial not defined for negative numbers\")\n    return PythonObject(math.factorial(n))\n\nfn fibonacci(py_n: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Compute nth Fibonacci number.\"\"\"\n    var n = Int(py_n)\n    if n &lt;= 0:\n        return PythonObject(0)\n    elif n == 1:\n        return PythonObject(1)\n\n    var a = 0\n    var b = 1\n    for _ in range(2, n + 1):\n        var temp = a + b\n        a = b\n        b = temp\n    return PythonObject(b)\n\nfn is_prime(py_n: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Check if n is prime.\"\"\"\n    var n = Int(py_n)\n    if n &lt;= 1:\n        return PythonObject(False)\n    if n &lt;= 3:\n        return PythonObject(True)\n    if n % 2 == 0 or n % 3 == 0:\n        return PythonObject(False)\n\n    var i = 5\n    while i * i &lt;= n:\n        if n % i == 0 or n % (i + 2) == 0:\n            return PythonObject(False)\n        i += 6\n    return PythonObject(True)\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#step-2-use-from-python","title":"Step 2: Use from Python","text":"<p>Create <code>main.py</code>:</p> <pre><code>import max.mojo.importer\nimport sys\n\n# Add current directory to Python path\nsys.path.insert(0, \"\")\n\n# Import the Mojo module\nimport mojo_math\n\n# Use Mojo functions\nprint(f\"10! = {mojo_math.factorial(10)}\")\nprint(f\"20th Fibonacci = {mojo_math.fibonacci(20)}\")\nprint(f\"Is 17 prime? {mojo_math.is_prime(17)}\")\n\n# Performance comparison\nimport time\nimport math as py_math\n\ndef python_factorial(n):\n    return py_math.factorial(n)\n\n# Time comparison\nn = 20\nstart = time.time()\nfor _ in range(10000):\n    python_factorial(n)\npy_time = time.time() - start\n\nstart = time.time()\nfor _ in range(10000):\n    mojo_math.factorial(n)\nmojo_time = time.time() - start\n\nprint(f\"\\nPython time: {py_time:.4f}s\")\nprint(f\"Mojo time: {mojo_time:.4f}s\")\nprint(f\"Speedup: {py_time/mojo_time:.2f}x\")\n</code></pre> <p>Run it: <pre><code>python main.py\n</code></pre></p>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#working-with-custom-types","title":"Working with Custom Types","text":""},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#defining-and-exposing-mojo-types","title":"Defining and Exposing Mojo Types","text":"<pre><code>from python import PythonObject\nfrom python.bindings import PythonModuleBuilder\n\nstruct Vector2D(Movable, Defaultable, Representable):\n    var x: Float64\n    var y: Float64\n\n    fn __init__(out self, x: Float64 = 0.0, y: Float64 = 0.0):\n        self.x = x\n        self.y = y\n\n    fn __repr__(self) -&gt; String:\n        return \"Vector2D(\" + str(self.x) + \", \" + str(self.y) + \")\"\n\n    fn magnitude(self) -&gt; Float64:\n        return (self.x * self.x + self.y * self.y).sqrt()\n\n    fn dot(self, other: Vector2D) -&gt; Float64:\n        return self.x * other.x + self.y * other.y\n\n@export\nfn PyInit_vectors() -&gt; PythonObject:\n    try:\n        var m = PythonModuleBuilder(\"vectors\")\n\n        # Register the type\n        m.add_type[Vector2D](\"Vector2D\")\n\n        # Register constructor function (workaround for __init__ limitation)\n        m.def_function[create_vector](\"create_vector\")\n        m.def_function[vector_magnitude](\"magnitude\")\n        m.def_function[vector_dot](\"dot\")\n\n        return m.finalize()\n    except e:\n        return abort[PythonObject](str(e))\n\nfn create_vector(x_obj: PythonObject, y_obj: PythonObject) raises -&gt; PythonObject:\n    var x = Float64(x_obj)\n    var y = Float64(y_obj)\n    return PythonObject(alloc=Vector2D(x, y))\n\nfn vector_magnitude(vec_obj: PythonObject) raises -&gt; PythonObject:\n    var vec = vec_obj.downcast_value_ptr[Vector2D]()\n    return PythonObject(vec[].magnitude())\n\nfn vector_dot(vec1_obj: PythonObject, vec2_obj: PythonObject) raises -&gt; PythonObject:\n    var vec1 = vec1_obj.downcast_value_ptr[Vector2D]()\n    var vec2 = vec2_obj.downcast_value_ptr[Vector2D]()\n    return PythonObject(vec1[].dot(vec2[]))\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#handling-multiple-arguments","title":"Handling Multiple Arguments","text":"<p>For functions with more than 3 arguments, use variadic functions:</p> <pre><code>@export\nfn PyInit_advanced() -&gt; PythonObject:\n    try:\n        var b = PythonModuleBuilder(\"advanced\")\n        b.def_py_function[sum_many](\"sum_many\")\n        b.def_py_function[concat_strings](\"concat_strings\")\n        return b.finalize()\n    except e:\n        return abort[PythonObject](str(e))\n\nfn sum_many(py_self: PythonObject, args: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Sum any number of numeric arguments.\"\"\"\n    var total = 0.0\n    for i in range(len(args)):\n        total += Float64(args[i])\n    return PythonObject(total)\n\nfn concat_strings(py_self: PythonObject, args: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Concatenate any number of strings.\"\"\"\n    if len(args) == 0:\n        return PythonObject(\"\")\n\n    var result = String(args[0])\n    for i in range(1, len(args)):\n        result += String(args[i])\n    return PythonObject(result)\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#keyword-arguments-pattern","title":"Keyword Arguments Pattern","text":"<p>Since native keyword arguments aren't supported, use a dict pattern:</p> <p>Python wrapper (<code>wrapper.py</code>): <pre><code>import mojo_module\n\ndef process_data(data, *, normalize=True, scale=1.0, offset=0.0):\n    \"\"\"Python wrapper that handles keyword arguments.\"\"\"\n    return mojo_module._process_data(data, {\n        \"normalize\": normalize,\n        \"scale\": scale,\n        \"offset\": offset\n    })\n</code></pre></p> <p>Mojo implementation: <pre><code>fn _process_data(data_obj: PythonObject, kwargs: PythonObject) raises -&gt; PythonObject:\n    var data = data_obj  # Process as needed\n\n    # Extract keyword arguments with defaults\n    var normalize = Bool(kwargs.get(\"normalize\", PythonObject(True)))\n    var scale = Float64(kwargs.get(\"scale\", PythonObject(1.0)))\n    var offset = Float64(kwargs.get(\"offset\", PythonObject(0.0)))\n\n    # Process data...\n    return data\n</code></pre></p>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#building-extension-modules","title":"Building Extension Modules","text":""},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#manual-build","title":"Manual Build","text":"<p>Instead of using the auto-import, you can build modules manually:</p> <pre><code># Build as shared library\nmojo build mojo_math.mojo --emit shared-lib -o mojo_math.so\n\n# Use directly in Python\npython -c \"import mojo_math; print(mojo_math.factorial(5))\"\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#build-script","title":"Build Script","text":"<p>Create <code>build.py</code>:</p> <pre><code>import subprocess\nimport sys\nimport os\n\ndef build_mojo_modules():\n    modules = [\"mojo_math.mojo\", \"vectors.mojo\"]\n\n    for module in modules:\n        if os.path.exists(module):\n            output = module.replace(\".mojo\", \".so\")\n            cmd = [\"mojo\", \"build\", module, \"--emit\", \"shared-lib\", \"-o\", output]\n\n            print(f\"Building {module}...\")\n            result = subprocess.run(cmd, capture_output=True, text=True)\n\n            if result.returncode != 0:\n                print(f\"Error building {module}:\")\n                print(result.stderr)\n                sys.exit(1)\n            else:\n                print(f\"Successfully built {output}\")\n\nif __name__ == \"__main__\":\n    build_mojo_modules()\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#performance-tips","title":"Performance Tips","text":""},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#1-minimize-conversions","title":"1. Minimize Conversions","text":"<pre><code># Inefficient: Multiple conversions\nfn process_list_slow(py_list: PythonObject) raises -&gt; PythonObject:\n    var result = PythonObject([])\n    for i in range(len(py_list)):\n        var item = Int(py_list[i])  # Conversion on each iteration\n        result.append(PythonObject(item * 2))\n    return result\n\n# Efficient: Batch conversion\nfn process_list_fast(py_list: PythonObject) raises -&gt; PythonObject:\n    var size = len(py_list)\n    var mojo_list = List[Int](capacity=size)\n\n    # Convert once\n    for i in range(size):\n        mojo_list.append(Int(py_list[i]))\n\n    # Process in Mojo\n    for i in range(size):\n        mojo_list[i] *= 2\n\n    # Convert back once\n    var result = PythonObject([])\n    for item in mojo_list:\n        result.append(PythonObject(item[]))\n    return result\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#2-use-simd-for-numerical-operations","title":"2. Use SIMD for Numerical Operations","text":"<pre><code>fn compute_distances(points1: PythonObject, points2: PythonObject) raises -&gt; PythonObject:\n    \"\"\"Compute pairwise distances between two sets of 2D points.\"\"\"\n    var n1 = len(points1)\n    var n2 = len(points2)\n\n    # Pre-convert to Mojo arrays for SIMD\n    # ... conversion code ...\n\n    # Use SIMD for distance computation\n    # ... SIMD implementation ...\n\n    return result\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#debugging","title":"Debugging","text":""},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#enable-debug-output","title":"Enable Debug Output","text":"<pre><code>fn debug_function(obj: PythonObject) raises -&gt; PythonObject:\n    print(\"Type:\", Python.type(obj))\n    print(\"Value:\", obj)\n    print(\"Attributes:\", Python.dir(obj))\n\n    # Safe attribute access\n    if Python.hasattr(obj, \"shape\"):\n        print(\"Shape:\", obj.shape)\n\n    return obj\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#error-handling","title":"Error Handling","text":"<pre><code>fn safe_operation(obj: PythonObject) raises -&gt; PythonObject:\n    try:\n        # Attempt conversion\n        var value = Int(obj)\n        return PythonObject(value * 2)\n    except:\n        # Provide helpful error message\n        raise Error(\"Expected integer, got \" + str(Python.type(obj)))\n</code></pre>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#current-limitations","title":"Current Limitations","text":"<ol> <li>Maximum 3 arguments for regular functions (use variadic for more)</li> <li>No native keyword arguments (use dict pattern)</li> <li>No direct <code>__init__</code> binding (use factory functions)</li> <li>No static methods (use module-level functions)</li> <li>No properties (use getter/setter functions)</li> <li>Limited automatic conversions (implement manually)</li> </ol>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#best-practices","title":"Best Practices","text":"<ol> <li>Start Small: Convert one function at a time</li> <li>Profile First: Identify actual bottlenecks</li> <li>Minimize Boundary Crossing: Batch operations</li> <li>Document Types: Clear docstrings for Python users</li> <li>Provide Python Wrappers: For better ergonomics</li> <li>Test Thoroughly: Both Mojo and Python sides</li> </ol>"},{"location":"mojo-max/mojo-language/calling-mojo-from-python/#next-steps","title":"Next Steps","text":"<ul> <li>Explore MAX Inference for model serving</li> <li>Learn about Performance Optimization</li> <li>Check Community Examples</li> </ul>"},{"location":"mojo-max/mojo-language/python-interop/","title":"Python Interoperability","text":"<p>Mojo provides seamless integration with Python, allowing you to use existing Python libraries and gradually migrate performance-critical code to Mojo.</p>"},{"location":"mojo-max/mojo-language/python-interop/#importing-python-modules","title":"Importing Python Modules","text":"<p>Use the <code>Python</code> module to import any Python library:</p> <pre><code>from python import Python\n\ndef main():\n    # Import Python modules\n    var np = Python.import_module(\"numpy\")\n    var pd = Python.import_module(\"pandas\")\n    var torch = Python.import_module(\"torch\")\n\n    # Use them like in Python\n    var arr = np.arange(15).reshape(3, 5)\n    print(arr)\n    print(arr.shape)\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#working-with-pythonobject","title":"Working with PythonObject","text":"<p>All Python values in Mojo are represented as <code>PythonObject</code>:</p> <pre><code>from python import Python, PythonObject\n\ndef python_operations():\n    var py = Python.import_module(\"builtins\")\n\n    # Create Python objects\n    var py_list = PythonObject([1, 2, 3, 4, 5])\n    var py_dict = PythonObject({\"name\": \"Alice\", \"age\": 30})\n\n    # Access attributes\n    print(py_dict[\"name\"])\n\n    # Call methods\n    var doubled = py_list.__mul__(2)\n    print(doubled)  # [1, 2, 3, 4, 5, 1, 2, 3, 4, 5]\n\n    # Use Python built-ins\n    var sum_result = py.sum(py_list)\n    print(sum_result)  # 15\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#type-conversions","title":"Type Conversions","text":""},{"location":"mojo-max/mojo-language/python-interop/#python-to-mojo","title":"Python to Mojo","text":"<p>Convert Python objects to Mojo types:</p> <pre><code>def convert_from_python():\n    var py_int = PythonObject(42)\n    var py_float = PythonObject(3.14)\n    var py_str = PythonObject(\"Hello\")\n    var py_bool = PythonObject(True)\n    var py_list = PythonObject([1, 2, 3])\n\n    # Convert to Mojo types\n    var mojo_int = Int(py_int)\n    var mojo_float = Float64(py_float)\n    var mojo_str = String(py_str)\n    var mojo_bool = Bool(py_bool)\n\n    # Convert list elements\n    var mojo_list = List[Int]()\n    for i in range(len(py_list)):\n        mojo_list.append(Int(py_list[i]))\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#mojo-to-python","title":"Mojo to Python","text":"<p>Convert Mojo values to Python objects:</p> <pre><code>def convert_to_python():\n    # Automatic conversion for basic types\n    var py_obj1 = PythonObject(42)\n    var py_obj2 = PythonObject(3.14)\n    var py_obj3 = PythonObject(\"Hello Mojo\")\n    var py_obj4 = PythonObject(True)\n\n    # Convert collections\n    var mojo_list = List[Int](1, 2, 3, 4, 5)\n    var py_list = PythonObject([])\n    for item in mojo_list:\n        py_list.append(PythonObject(item[]))\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#common-python-libraries","title":"Common Python Libraries","text":""},{"location":"mojo-max/mojo-language/python-interop/#numpy-integration","title":"NumPy Integration","text":"<pre><code>def numpy_example():\n    var np = Python.import_module(\"numpy\")\n\n    # Create arrays\n    var arr1 = np.array([1, 2, 3, 4, 5])\n    var arr2 = np.linspace(0, 1, 10)\n    var arr3 = np.random.randn(3, 3)\n\n    # Operations\n    var result = np.dot(arr3, arr3.T)\n    var mean = np.mean(arr1)\n    var std = np.std(arr1)\n\n    print(\"Mean:\", mean, \"Std:\", std)\n\n    # Convert to Mojo for performance\n    var size = Int(arr1.size)\n    var sum = 0.0\n    for i in range(size):\n        sum += Float64(arr1[i])\n    print(\"Mojo sum:\", sum)\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#pandas-integration","title":"Pandas Integration","text":"<pre><code>def pandas_example():\n    var pd = Python.import_module(\"pandas\")\n\n    # Create DataFrame\n    var data = PythonObject({\n        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n        \"age\": [25, 30, 35],\n        \"city\": [\"NYC\", \"LA\", \"Chicago\"]\n    })\n    var df = pd.DataFrame(data)\n\n    # Operations\n    print(df.head())\n    print(df.describe())\n\n    # Filter\n    var adults = df[df[\"age\"] &gt; 25]\n    print(adults)\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#matplotlib-integration","title":"Matplotlib Integration","text":"<pre><code>def plotting_example():\n    var plt = Python.import_module(\"matplotlib.pyplot\")\n    var np = Python.import_module(\"numpy\")\n\n    # Generate data\n    var x = np.linspace(0, 10, 100)\n    var y = np.sin(x)\n\n    # Create plot\n    plt.figure(figsize=(8, 6))\n    plt.plot(x, y, label=\"sin(x)\")\n    plt.xlabel(\"x\")\n    plt.ylabel(\"y\")\n    plt.title(\"Sine Wave\")\n    plt.legend()\n    plt.grid(True)\n    plt.savefig(\"sine_wave.png\")\n    plt.close()\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#working-with-python-classes","title":"Working with Python Classes","text":"<pre><code>def python_class_example():\n    var py = Python.import_module(\"builtins\")\n\n    # Define a Python class using exec\n    var code = \"\"\"\nclass Person:\n    def __init__(self, name, age):\n        self.name = name\n        self.age = age\n\n    def greet(self):\n        return f\"Hello, I'm {self.name}\"\n\n    def birthday(self):\n        self.age += 1\n\"\"\"\n\n    var globals = PythonObject({})\n    py.exec(code, globals)\n\n    # Use the class\n    var Person = globals[\"Person\"]\n    var alice = Person(\"Alice\", 30)\n\n    print(alice.greet())\n    alice.birthday()\n    print(\"Age after birthday:\", alice.age)\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#exception-handling","title":"Exception Handling","text":"<p>Handle Python exceptions in Mojo:</p> <pre><code>def handle_python_errors():\n    var py = Python.import_module(\"builtins\")\n\n    try:\n        # This will raise a Python exception\n        var result = py.int(\"not a number\")\n    except e:\n        print(\"Python error:\", e)\n\n    # Check for None\n    var maybe_none = PythonObject(None)\n    if maybe_none.is_none():\n        print(\"Got None value\")\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#performance-considerations","title":"Performance Considerations","text":""},{"location":"mojo-max/mojo-language/python-interop/#when-to-use-python-vs-mojo","title":"When to Use Python vs Mojo","text":"<pre><code>def performance_comparison():\n    var np = Python.import_module(\"numpy\")\n    var time = Python.import_module(\"time\")\n\n    # Python/NumPy version\n    var start = time.time()\n    var py_arr = np.arange(1000000)\n    var py_sum = np.sum(py_arr * py_arr)\n    var py_time = time.time() - start\n\n    # Mojo version\n    start = time.time()\n    var mojo_sum = 0\n    for i in range(1000000):\n        mojo_sum += i * i\n    var mojo_time = time.time() - start\n\n    print(\"Python/NumPy time:\", py_time)\n    print(\"Mojo time:\", mojo_time)\n    print(\"Speedup:\", Float64(py_time) / Float64(mojo_time), \"x\")\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#best-practices","title":"Best Practices","text":"<ol> <li>Use Python for: </li> <li>Existing libraries (pandas, scikit-learn, etc.)</li> <li>Prototyping and experimentation</li> <li> <p>I/O operations and file handling</p> </li> <li> <p>Use Mojo for:</p> </li> <li>Performance-critical loops</li> <li>Numerical computations</li> <li> <p>Low-level operations</p> </li> <li> <p>Optimization Strategy:</p> </li> <li>Start with Python</li> <li>Profile to find bottlenecks</li> <li>Rewrite hot paths in Mojo</li> <li>Keep Python for everything else</li> </ol>"},{"location":"mojo-max/mojo-language/python-interop/#advanced-patterns","title":"Advanced Patterns","text":""},{"location":"mojo-max/mojo-language/python-interop/#python-context-managers","title":"Python Context Managers","text":"<pre><code>def use_context_manager():\n    var py = Python.import_module(\"builtins\")\n\n    # Using with statement equivalent\n    var file = py.open(\"example.txt\", \"w\")\n    try:\n        file.write(\"Hello from Mojo!\")\n    finally:\n        file.close()\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#async-python-code","title":"Async Python Code","text":"<pre><code>def async_example():\n    var asyncio = Python.import_module(\"asyncio\")\n    var aiohttp = Python.import_module(\"aiohttp\")\n\n    var code = \"\"\"\nasync def fetch_data(url):\n    async with aiohttp.ClientSession() as session:\n        async with session.get(url) as response:\n            return await response.text()\n\nresult = asyncio.run(fetch_data('https://api.example.com/data'))\n\"\"\"\n\n    var globals = PythonObject({})\n    globals[\"aiohttp\"] = aiohttp\n    globals[\"asyncio\"] = asyncio\n\n    Python.import_module(\"builtins\").exec(code, globals)\n    print(globals[\"result\"])\n</code></pre>"},{"location":"mojo-max/mojo-language/python-interop/#debugging-tips","title":"Debugging Tips","text":"<ol> <li> <p>Print PythonObject type:     <pre><code>print(type(py_obj))\n</code></pre></p> </li> <li> <p>Check attributes:    <pre><code>print(dir(py_obj))\n</code></pre></p> </li> <li> <p>Handle missing attributes:    <pre><code>if hasattr(py_obj, \"some_attr\"):\n    var value = py_obj.some_attr\n</code></pre></p> </li> </ol>"},{"location":"mojo-max/mojo-language/python-interop/#next-steps","title":"Next Steps","text":"<ul> <li>Learn Calling Mojo from Python</li> <li>Explore Performance Optimization</li> <li>See Real-world Examples</li> </ul>"}]}